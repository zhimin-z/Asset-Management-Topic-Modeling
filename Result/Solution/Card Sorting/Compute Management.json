[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":132.9492333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi all,    <\/p>\n<p>I am following the steps on this tutorial:    <br \/>\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\">tutorial-score-model-predict-spark-pool<\/a>    <br \/>\nI tried to used a model created with AutoML and another from designer and I am getting this error: <em>RuntimeError: Load model failed<\/em>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/155981-capture.png?platform=QnA\" alt=\"155981-capture.png\" \/>    <\/p>\n<p>I am using the model according to this: <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html?childToView=637754#comment-637754\">https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html?childToView=637754#comment-637754<\/a>     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/156021-2.png?platform=QnA\" alt=\"156021-2.png\" \/>    <\/p>\n<p>Thank you for your help.    <\/p>",
        "Challenge_closed_time":1639460160303,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638981543063,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a \"RuntimeError: Load model failed\" error while following the steps of a tutorial to score machine learning models with PREDICT in serverless Apache Spark pools using models created with AutoML and Designer. The user has provided screenshots and links to the tutorial and documentation they are following.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/656548\/runtimeerror-load-model-failed-score-machine-learn",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":20.1,
        "Challenge_reading_time":17.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":132.9492333334,
        "Challenge_title":"RuntimeError: Load model failed - Score machine learning models with PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=4bb27b25-616e-491c-b986-136b5bf96f77\">@Anaid  <\/a>,    <\/p>\n<p>Before running this script, update it with the URI for ADLS Gen2 data file along with model output return data type and ADLS\/AML URI for the model file.    <\/p>\n<pre><code>#Set model URI  \n       #Set AML URI, if trained model is registered in AML  \n          AML_MODEL_URI = &quot;&lt;aml model uri&gt;&quot; #In URI &quot;:x&quot; signifies model version in AML. You can   choose which model version you want to run. If &quot;:x&quot; is not provided then by default   latest version will be picked.  \n  \n       #Set ADLS URI, if trained model is uploaded in ADLS  \n          ADLS_MODEL_URI = &quot;abfss:\/\/&lt;filesystemname&gt;@&lt;account name&gt;.dfs.core.windows.net\/&lt;model   mlflow folder path&gt;&quot;  \n<\/code><\/pre>\n<p><strong>Model URI from AML Workspace:<\/strong>    <\/p>\n<pre><code>DATA_FILE = &quot;abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv&quot;  \nAML_MODEL_URI_SKLEARN = &quot;aml:\/\/mlflow_sklearn:1&quot; #Here &quot;:1&quot; signifies model version in AML. We can choose which version we want to run. If &quot;:1&quot; is not provided then by default latest version will be picked  \nRETURN_TYPES = &quot;INT&quot;  \nRUNTIME = &quot;mlflow&quot;  \n<\/code><\/pre>\n<p><strong>Model URI uploaded to ADLS Gen2:<\/strong>    <\/p>\n<pre><code>DATA_FILE = &quot;abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv&quot;  \nAML_MODEL_URI_SKLEARN = &quot;abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/linear_regression\/linear_regression&quot; #Here &quot;:1&quot; signifies model version in AML. We can choose which version we want to run. If &quot;:1&quot; is not provided then by default latest version will be picked  \nRETURN_TYPES = &quot;INT&quot;  \nRUNTIME = &quot;mlflow&quot;  \n<\/code><\/pre>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.8,
        "Solution_reading_time":38.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":295.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"load model failed"
    },
    {
        "Answerer_created_time":1562578633160,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":92.103195,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to python programming. Following the AWS learning path:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult<\/a><\/p>\n<p>I am getting an error when excuting the following block (in conda_python3):<\/p>\n<pre><code>test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\nxgb_predictor.content_type = 'text\/csv' # set the data type for an inference\nxgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an \narray\nprint(predictions_array.shape)\n<\/code><\/pre>\n<blockquote>\n<p>AttributeError                            Traceback (most recent call last)\n in \n1 test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n----&gt; 2 xgb_predictor.content_type = 'text\/csv' # set the data type for an inference\n3 xgb_predictor.serializer = csv_serializer # set the serializer type\n4 predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n5 predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array<\/p>\n<\/blockquote>\n<blockquote>\n<p>AttributeError: can't set attribute<\/p>\n<\/blockquote>\n<p>I have looked at several prior questions but couldn't find much information related to this error when it comes to creating data types.<\/p>\n<p>Thanks in advance for any help.<\/p>",
        "Challenge_closed_time":1609399068572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609067497070,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an AttributeError while executing a block of code in AWS Sagemaker. The error occurs when trying to set the data type for an inference and the serializer type. The user has looked at prior questions but couldn't find much information related to this error when it comes to creating data types.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65465114",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":20.9,
        "Challenge_reading_time":27.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":92.103195,
        "Challenge_title":"AWS Sagemaker AttributeError: can't set attribute error",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2479.0,
        "Challenge_word_count":161,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1374284754808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":279.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.\nxgb_predictor.content_type = 'text\/csv'<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":1.87,
        "Solution_score_count":8.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"AttributeError in Sagemaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.9001127778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello.  <br \/>\nYou can only select up to one maximum node when Create an Azure Machine Learning compute cluster. How do I select multiple nodes?<\/p>",
        "Challenge_closed_time":1635456257703,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635431417297,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in creating a computing cluster with Azure Machine Learning as they are only able to select one maximum node and are unsure of how to select multiple nodes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/607903\/about-creating-a-computing-cluster-with-azure-mach",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.8,
        "Challenge_reading_time":2.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.9001127778,
        "Challenge_title":"About creating a computing cluster with Azure Machine Learning",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, you can only select min and max number of nodes that you want to provision. The compute will autoscale to a maximum of this node count when a job is submitted. For more details, review <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python\">Create an AML Compute Cluster<\/a>.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.7,
        "Solution_reading_time":5.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to select multiple nodes"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":321.5504361111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>A VAE built using PyTorch runs smoothly when I train it directly. However, with wandb sweeps, I encounter the following BrokerPipeError. According to some forum threads, the main cause seems to be the more than one <code>num_workers<\/code> in the DataLoader module when running on Windows OS. However, I have a DGX-station running Ubuntu, and I still get the error.<\/p>\n<pre><code class=\"lang-auto\">wandb: Waiting for W&amp;B process to finish... (failed 1). Press Control-C to abort syncing.\nException in thread ChkStopThr:\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"\/opt\/conda\/lib\/python3.8\/threading.py\", line 870, in run\n    Exception in thread self._target(*self._args, **self._kwargs)NetStatThr\n:\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 276, in check_stop_status\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/threading.py\", line 932, in _bootstrap_inner\n    self._loop_check_status(\n      File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 214, in _loop_check_status\nself.run()\n  File \"\/opt\/conda\/lib\/python3.8\/threading.py\", line 870, in run\n    local_handle = request()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 787, in deliver_stop_status\n    self._target(*self._args, **self._kwargs)\nreturn self._deliver_stop_status(status)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 258, in check_network_status\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 585, in _deliver_stop_status\n    self._loop_check_status(\nreturn self._deliver_record(record)  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 214, in _loop_check_status\n\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 560, in _deliver_record\n    handle = mailbox._deliver_record(record, interface=self)\n    local_handle = request()  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/mailbox.py\", line 455, in _deliver_record\n\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 795, in deliver_network_status\n    interface._publish(record)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 221, in send_record_publish\n    return self._deliver_network_status(status)\n      File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 601, in _deliver_network_status\nself.send_server_request(server_req)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 152, in _send_message\n    return self._deliver_record(record)\n      File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 560, in _deliver_record\nself._sendall_with_error_handle(header + data)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 130, in _sendall_with_error_handle\n    handle = mailbox._deliver_record(record, interface=self)\nsent = self._sock.send(data)  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/mailbox.py\", line 455, in _deliver_record\n\nBrokenPipeError: [Errno 32] Broken pipe\n    interface._publish(record)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nBrokenPipeError: [Errno 32] Broken pipe\n<\/code><\/pre>\n<p>Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Challenge_closed_time":1680807676343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679650094773,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a BrokenPipeError while using wandb sweeps on a VAE built using PyTorch. The error is usually caused by having more than one num_workers in the DataLoader module when running on Windows OS, but the user is running Ubuntu on a DGX-station and still getting the error. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/brokenpipeerror-on-ubuntu-machine\/4117",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":18.3,
        "Challenge_reading_time":59.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":59,
        "Challenge_solved_time":321.5504361111,
        "Challenge_title":"BrokenPipeError on Ubuntu machine",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":295.0,
        "Challenge_word_count":313,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello! Looks like it there is a Connection issue between your machine and the <code>wandb<\/code> server. Is there a load balancer, a VPN, or a proxy that your machine is behind that may be blocking the connection? The reason I ask is because  <code>sent = self._sock.send(data)<\/code>  is the main error in the stack trace which means that the client is struggling to send data to the server.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":4.86,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"BrokenPipeError in wandb sweeps"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":102.2437183333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there any plan? Any date we can expect?<\/p>",
        "Challenge_closed_time":1662345271896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661977194510,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about whether Azure supports distributed GPU and is seeking information on any plans or expected dates for its implementation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/989398\/is-azure-supporting-distributed-gpu",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":2.6,
        "Challenge_reading_time":1.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":102.2437183333,
        "Challenge_title":"Is Azure supporting distributed GPU?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":13,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=7f2ff54e-2fc4-4d74-b946-fc6ec46d4863\">@nam  <\/a>     <\/p>\n<p>I hope yo are doing well. We have multiple options for Distributed GPU for Azure Machine Learnig for SDK v1 as below -     <br \/>\n<strong>Message Passing Interface (MPI)<\/strong>    <br \/>\nHorovod    <br \/>\nDeepSpeed    <br \/>\nEnvironment variables from Open MPI    <br \/>\n<strong>PyTorch<\/strong>    <br \/>\nProcess group initialization    <br \/>\nLaunch options    <br \/>\nDistributedDataParallel (per-process-launch)    <br \/>\nUsing torch.distributed.launch (per-node-launch)    <br \/>\nPyTorch Lightning    <br \/>\nHugging Face Transformers    <br \/>\n<strong>TensorFlow<\/strong>    <br \/>\nEnvironment variables for TensorFlow (TF_CONFIG)    <br \/>\n<strong>Accelerate GPU training with InfiniBand<\/strong>    <\/p>\n<p>For V2 there should be big change. Please feel free to let us know any problems. Thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":11.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"distributed GPU support"
    },
    {
        "Answerer_created_time":1556553177963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":347.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":465.8558136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a mismatch in shapes between inputs and the model of my reinforcement learning project.<\/p>\n\n<p>I have been closely following the AWS examples, specifically the cartpole example. However I have built my own custom environment. What I am struggling to understand is how to change my environment so that it is able to work with the prebuilt Ray RLEstimator.<\/p>\n\n<p>Here is the code for the environment:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\n\nimport numpy as np\n\n# from float_space import FloatSpace\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        self.observation_space = Box(np.array(0.0),np.array(1000))\n#         self.observation_space = FloatSpace(price)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n        next_state = self.price\n#         next_state = self.observation_space.sample()\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n\n#         self.observation_space.value = np.random.rand()\n#         return self.observation_space.sample()\n        self.price = np.random.rand()\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        self.price += PRICE_ADJUSTMENT[action]\n\n    def _get_reward(self,price):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    env = gym.make('ArrivalSim-v0')\n\n    env.reset()\n    for _ in range(20):\n        test = env.action_space.sample()\n        print(test)\n        print(env.observation_space)\n        env.step(test)  # take a random action\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n\n<\/code><\/pre>\n\n<p>Here is the training script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nfrom ray.tune.registry import register_env\nfrom gym.envs.registration import register\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\n\n\nclass MyLauncher(SageMakerRayLauncher):\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n\n            }\n          }\n        }\n\nif __name__ == \"__main__\":\n    MyLauncher().train_main()\n<\/code><\/pre>\n\n<p>Here is the code I run in Jupyter:<\/p>\n\n<pre><code>metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\nenvironment = env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\n\nestimator = RLEstimator(entry_point=\"train.py\",\n                        source_dir='.',\n                        toolkit=RLToolkit.RAY,\n                        toolkit_version='0.6.5',\n                        framework=RLFramework.TENSORFLOW,\n                        dependencies=[\"sagemaker_rl\"],\n#                         image_name='price-response-ray-cpu',\n                        role=role,\n#                         train_instance_type=\"ml.c5.2xlarge\",\n                        train_instance_type='local',\n                        train_instance_count=1,\n#                         output_path=s3_output_path,\n#                         base_job_name=job_name_prefix,\n                        metric_definitions=metric_definitions\n#                         hyperparameters={\n                          # Attention scientists!  You can override any Ray algorithm parameter here:\n                          #\"rl.training.config.horizon\": 5000,\n                          #\"rl.training.config.num_sgd_iter\": 10,\n                        #}\n                    )\n\nestimator.fit(wait=True)\njob_name = estimator.latest_training_job.job_name\nprint(\"Training job: %s\" % job_name)\n<\/code><\/pre>\n\n<p>The error message I have been receiving has been the following:<\/p>\n\n<pre><code>algo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.1\/4.1 GB\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 2\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.4\/4.1 GB\nalgo-1-dxwxx_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-dxwxx_1  | Number of trials: 1 ({'RUNNING': 1})\nalgo-1-dxwxx_1  | RUNNING trials:\nalgo-1-dxwxx_1  |  - PPO_ArrivalSim-v0_0:   RUNNING\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,030  WARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,063   ERROR trial_runner.py:460 -- Error processing event.\nalgo-1-dxwxx_1  | Traceback (most recent call last):\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trial_runner.py\", line 409, in _process_trial\nalgo-1-dxwxx_1  |     result = self.trial_executor.fetch_result(trial)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/ray_trial_executor.py\", line 314, in fetch_result\nalgo-1-dxwxx_1  |     result = ray.get(trial_future[0])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/worker.py\", line 2316, in get\nalgo-1-dxwxx_1  |     raise value\nalgo-1-dxwxx_1  | ray.exceptions.RayTaskError: ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 83, in __init__\nalgo-1-dxwxx_1  |     restored, num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 135, in _build_layers_v2\nalgo-1-dxwxx_1  |     raise NotImplementedError\nalgo-1-dxwxx_1  | NotImplementedError\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | During handling of the above exception, another exception occurred:\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 276, in __init__\nalgo-1-dxwxx_1  |     Trainable.__init__(self, config, logger_creator)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trainable.py\", line 88, in __init__\nalgo-1-dxwxx_1  |     self._setup(copy.deepcopy(self.config))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 373, in _setup\nalgo-1-dxwxx_1  |     self._init()\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo.py\", line 77, in _init\nalgo-1-dxwxx_1  |     self.env_creator, self._policy_graph)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 506, in make_local_evaluator\nalgo-1-dxwxx_1  |     extra_config or {}))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 714, in _make_evaluator\nalgo-1-dxwxx_1  |     async_remote_worker_envs=config[\"async_remote_worker_envs\"])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 288, in __init__\nalgo-1-dxwxx_1  |     self._build_policy_map(policy_dict, policy_config)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 661, in _build_policy_map\nalgo-1-dxwxx_1  |     policy_map[name] = cls(obs_space, act_space, merged_conf)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo_policy_graph.py\", line 176, in __init__\nalgo-1-dxwxx_1  |     seq_lens=existing_seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 215, in get_model\nalgo-1-dxwxx_1  |     seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 255, in _get_model\nalgo-1-dxwxx_1  |     num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 86, in __init__\nalgo-1-dxwxx_1  |     input_dict[\"obs\"], num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/fcnet.py\", line 37, in _build_layers\nalgo-1-dxwxx_1  |     scope=label)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/framework\/python\/ops\/arg_scope.py\", line 182, in func_with_args\nalgo-1-dxwxx_1  |     return func(*args, **current_args)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/layers\/python\/layers\/layers.py\", line 1854, in fully_connected\nalgo-1-dxwxx_1  |     outputs = layer.apply(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 817, in apply\nalgo-1-dxwxx_1  |     return self.__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/layers\/base.py\", line 374, in __call__\nalgo-1-dxwxx_1  |     outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 730, in __call__\nalgo-1-dxwxx_1  |     self._assert_input_compatibility(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 1493, in _assert_input_compatibility\nalgo-1-dxwxx_1  |     str(x.shape.as_list()))\nalgo-1-dxwxx_1  | ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,064   INFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_ArrivalSim-v0_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,076   INFO trial_runner.py:497 -- Attempting to recover trial state from last checkpoint.\nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,041  INFO policy_evaluator.py:278 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n<\/code><\/pre>\n\n<p>I am not sure how to change the input the environment gives to the model or the models setup itself. It seems the documentations are quite obscure. I have a hunch that problem lies with the observation and action spaces<\/p>\n\n<p>Here is the reference to the original aws project example:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray<\/a><\/p>",
        "Challenge_closed_time":1567460466780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567158570927,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"the user is encountering a challenge with mismatched shapes between the inputs and model of their reinforcement learning project, and is struggling to understand how to change their environment to work with the prebuilt ray rlestimator.",
        "Challenge_last_edit_time":1567160893503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57724414",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":159.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":119,
        "Challenge_solved_time":83.8599591667,
        "Challenge_title":"How to make the inputs and model have the same shape (RLlib Ray Sagemaker reinforcement learning)",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1492.0,
        "Challenge_word_count":999,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565097950652,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p><strong>Possible reason:<\/strong><\/p>\n\n<p>The error message:<\/p>\n\n<p><code>ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]<\/code><\/p>\n\n<p>Your original environment obs space is <code>self.observation_space = Box(np.array(0.0),np.array(1000))<\/code>.<\/p>\n\n<p>Displaying the shape of your environment obs space gives:<\/p>\n\n<p><code>print(Box(np.array(0.0), np.array(1000), dtype=np.float32).shape)<\/code> = <code>()<\/code><\/p>\n\n<p>This could be indicated by <code>Full shape received: [None]<\/code> in the error message.<\/p>\n\n<p>If you pass the shape <code>(1,1)<\/code> into <code>np.zeros<\/code>, you get the expected  <code>min_ndim=2<\/code>:<\/p>\n\n<p><code>x = np.zeros((1, 1))\nprint(x)\n[[0.]]\nprint(x.ndim)\n2<\/code><\/p>\n\n<p><strong>Suggested solution:<\/strong><\/p>\n\n<p>I assume that you want your environment obs space to range from 0.0 to 1000.0 as indicated by the <code>self.price = np.random.rand()<\/code> in your <code>reset<\/code> function.<\/p>\n\n<p>Try using the following for your environment obs space:<\/p>\n\n<p><code>self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)<\/code><\/p>\n\n<p>I hope that by setting the <code>Box<\/code> with an explicit <code>shape<\/code> helps.<\/p>\n\n<p><strike>\n<strong>EDIT (20190903):<\/strong><\/p>\n\n<p>I have modified your training script. This modification includes new imports, custom model class, model registration &amp; addition of registered custom model to config. For readability, only sections added are shown below. The entire modified training script is available in this <a href=\"https:\/\/gist.github.com\/ChuaCheowHuan\/ddf70654bd928d70e5415c947d4d43f3\" rel=\"nofollow noreferrer\">gist<\/a>. Please run with the proposed obs space as describe above.<\/p>\n\n<p>New additional imports:<\/p>\n\n<pre><code># new imports\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\nfrom ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n\nfrom ray.rllib.utils import try_import_tf\nfrom ray.tune import grid_search\n\ntf = try_import_tf()\n# end new imports\n<\/code><\/pre>\n\n<p>Custom model class:<\/p>\n\n<pre><code># Custom model class (fcnet)\nclass CustomModel(TFModelV2):\n    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n\n    def __init__(self, obs_space, action_space, num_outputs, model_config,\n                 name):\n        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n                                          model_config, name)\n        self.model = FullyConnectedNetwork(obs_space, action_space,\n                                           num_outputs, model_config, name)\n        self.register_variables(self.model.variables())\n\n    def forward(self, input_dict, state, seq_lens):\n        return self.model.forward(input_dict, state, seq_lens)\n\n    def value_function(self):\n        return self.model.value_function()\n<\/code><\/pre>\n\n<p>Registered &amp; add custom model:<\/p>\n\n<pre><code>    def get_experiment_config(self):\n\n\n        # Register custom model\n        ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n\n\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n\n\n              \"model\": {\"custom_model\": \"my_model\"}, # Add registered custom model\n\n\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n            }\n          }\n        }\n<\/code><\/pre>\n\n<p><\/strike><\/p>\n\n<p><strong>EDIT 2 (20190910):<\/strong><\/p>\n\n<p>To show that it works, truncated output from Sagemaker (Jupyter notebook instance):<\/p>\n\n<pre><code>.\n.\n.\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 10.103020961393266 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 9.663759180463124 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.189261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.18926178] 5.67785342790426 False {}\nalgo-1-y2ayw_1  | price b = 0.189261780930142\nalgo-1-y2ayw_1  | price a = -0.06073821906985799\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [-0.06073822] 0 True {}\nalgo-1-y2ayw_1  | Result for PPO_ArrivalSim-v0_0:\nalgo-1-y2ayw_1  |   date: 2019-09-10_11-51-13\nalgo-1-y2ayw_1  |   done: true\nalgo-1-y2ayw_1  |   episode_len_mean: 126.72727272727273\nalgo-1-y2ayw_1  |   episode_reward_max: 15772.677709596366\nalgo-1-y2ayw_1  |   episode_reward_mean: 2964.4609668691965\nalgo-1-y2ayw_1  |   episode_reward_min: 0.0\nalgo-1-y2ayw_1  |   episodes: 5\nalgo-1-y2ayw_1  |   experiment_id: 5d3b9f2988854a0db164a2e5e9a7550f\nalgo-1-y2ayw_1  |   hostname: 2dae585dcc65\nalgo-1-y2ayw_1  |   info:\nalgo-1-y2ayw_1  |     cur_lr: 4.999999873689376e-05\nalgo-1-y2ayw_1  |     entropy: 1.0670874118804932\nalgo-1-y2ayw_1  |     grad_time_ms: 1195.066\nalgo-1-y2ayw_1  |     kl: 3.391784191131592\nalgo-1-y2ayw_1  |     load_time_ms: 44.725\nalgo-1-y2ayw_1  |     num_steps_sampled: 463\nalgo-1-y2ayw_1  |     num_steps_trained: 463\nalgo-1-y2ayw_1  |     policy_loss: -0.05383850634098053\nalgo-1-y2ayw_1  |     sample_time_ms: 621.282\nalgo-1-y2ayw_1  |     total_loss: 2194493.5\nalgo-1-y2ayw_1  |     update_time_ms: 145.352\nalgo-1-y2ayw_1  |     vf_explained_var: -5.519390106201172e-05\nalgo-1-y2ayw_1  |     vf_loss: 2194492.5\nalgo-1-y2ayw_1  |   iterations_since_restore: 2\nalgo-1-y2ayw_1  |   node_ip: 172.18.0.2\nalgo-1-y2ayw_1  |   pid: 77\nalgo-1-y2ayw_1  |   policy_reward_mean: {}\nalgo-1-y2ayw_1  |   time_since_restore: 4.55129861831665\nalgo-1-y2ayw_1  |   time_this_iter_s: 1.3484764099121094\nalgo-1-y2ayw_1  |   time_total_s: 4.55129861831665\nalgo-1-y2ayw_1  |   timestamp: 1568116273\nalgo-1-y2ayw_1  |   timesteps_since_restore: 463\nalgo-1-y2ayw_1  |   timesteps_this_iter: 234\nalgo-1-y2ayw_1  |   timesteps_total: 463\nalgo-1-y2ayw_1  |   training_iteration: 2\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000781a7b5b94a203683f8f789e593abbb1.\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000d3507bc6b41ee1c9fc36292eeae69557.\nalgo-1-y2ayw_1  | == Status ==\nalgo-1-y2ayw_1  | Using FIFO scheduling algorithm.\nalgo-1-y2ayw_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-y2ayw_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-y2ayw_1  | TERMINATED trials:\nalgo-1-y2ayw_1  |  - PPO_ArrivalSim-v0_0:   TERMINATED [pid=77], 4 s, 2 iter, 463 ts, 2.96e+03 rew\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | Saved model configuration.\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.extra_data as \/opt\/ml\/model\/checkpoint.extra_data\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.tune_metadata as \/opt\/ml\/model\/checkpoint.tune_metadata\nalgo-1-y2ayw_1  | Created LogSyncer for \/root\/ray_results\/PPO_ArrivalSim-v0_2019-09-10_11-51-13xdn_5i34 -&gt; None\nalgo-1-y2ayw_1  | 2019-09-10 11:51:13.941718: I tensorflow\/core\/common_runtime\/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | LocalMultiGPUOptimizer devices ['\/cpu:0']\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to save.\nalgo-1-y2ayw_1  | No assets to save.\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to write.\nalgo-1-y2ayw_1  | No assets to write.\nalgo-1-y2ayw_1  | INFO:tensorflow:SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | Saved TensorFlow serving model!\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000f352d985b807ca399460941fe2264899.\n\nalgo-1-y2ayw_1  | 2019-09-10 11:51:20,075 sagemaker-containers INFO\n\n Reporting training SUCCESS\n\ntmpwwb4b358_algo-1-y2ayw_1 exited with code 0\n\nAborting on container exit...\nFailed to delete: \/tmp\/tmpwwb4b358\/algo-1-y2ayw Please remove it manually.\n\n===== Job Complete =====\n<\/code><\/pre>\n\n<p>This time I make edits in all 3 files. Your environment, training script &amp; the Jupyter notebook but it turns out that there isn't a need to define custom models for your custom environment. However, that remains viable. And you're right, the main cause of the issue is still in the obs space.<\/p>\n\n<p>I set <code>self.price<\/code> to be a 1D numpy array to make it talk better with Ray RLlib. The creation of the custom environment in the training script was done in a simpler way as shown below. As for the notebook, I used version 0.5.3 instead of 0.6.5 for toolkit_version &amp; the training is done in local mode (in the docker container on the Sagemaker Jupyter notebook instance, still on AWS) with CPU only. However, it will also work with any ML instance (e.g ml.m4.xlarge) with GPU.<\/p>\n\n<p>The entire package along with all dependencies is in <a href=\"https:\/\/github.com\/ChuaCheowHuan\/sagemaker_Ray_RLlib_custom_env\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The edited env:<\/p>\n\n<pre><code># new\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n# end new\n\n\nfrom enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\nimport numpy as np\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        # original obs space:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space initially suggested:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space suggested in this edit:\n        self.observation_space = spaces.Box(np.array([0.0]), np.array([1000.0]), dtype=np.float32)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n\n        next_state = self.price\n        print('(self.price).shape =', (self.price).shape)\n        #next_state = self.observation_space.sample()\n\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n        #self.observation_space.value = np.random.rand()\n        #return self.observation_space.sample()\n\n        self.price = np.random.rand(1)\n\n        print('reset -&gt; (self.price).shape = ', (self.price).shape)\n\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        #print('price b =', self.price)\n        print('price b =', self.price[0])\n        #print('price b =', self.price[[0]])\n        #self.price += PRICE_ADJUSTMENT[action]\n        self.price[0] += PRICE_ADJUSTMENT[action]\n        #self.price[[0]] += PRICE_ADJUSTMENT[action]\n        #print('price a =', self.price)\n        print('price a =', self.price[0])\n        #print('price a =', self.price[[0]])\n\n    #def _get_reward(self, price):\n    def _get_reward(self):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        #self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        #return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    \"\"\"\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40.0}\n    )\n    env = gym.make('ArrivalSim-v0')\n    \"\"\"\n    env = ArrivalSim(30.0)\n\n    val = env.reset()\n    print('val.shape = ', val.shape)\n\n    for _ in range(5):\n        print('env.observation_space =', env.observation_space)\n        act = env.action_space.sample()\n        print('\\nact =', act)\n        next_state, reward, done, _ = env.step(act)  # take a random action\n        print('next_state = ', next_state)\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n<\/code><\/pre>\n\n<p>The edited training script:<\/p>\n\n<pre><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nimport ray.rllib.agents.a3c as a3c\nimport ray.rllib.agents.ppo as ppo\nfrom ray.tune.registry import register_env\nfrom mod_op_env import ArrivalSim\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\"\"\"\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\"\"\"\ndef create_environment(env_config):\n    price = 30.0\n    # This import must happen inside the method so that worker processes import this code\n    from mod_op_env import ArrivalSim\n    return ArrivalSim(price)\n\n\nclass MyLauncher(SageMakerRayLauncher):\n    def __init__(self):        \n        super(MyLauncher, self).__init__()\n        self.num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))\n        self.hosts_info = json.loads(os.environ.get(\"SM_RESOURCE_CONFIG\"))[\"hosts\"]\n        self.num_total_gpus = self.num_gpus * len(self.hosts_info)\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"training_iteration\": 3,\n            },\n\n            \"local_dir\": \"\/opt\/ml\/model\/\",\n            \"checkpoint_freq\" : 3,\n\n            \"config\": {                                \n              #\"num_workers\": max(self.num_total_gpus-1, 1),\n              \"num_workers\": max(self.num_cpus-1, 1),\n              #\"use_gpu_for_workers\": False,\n              \"train_batch_size\": 128, #5,\n              \"sample_batch_size\": 32, #1,\n              \"gpu_fraction\": 0.3,\n              \"optimizer\": {\n                \"grads_per_step\": 10\n              },\n            },\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 1), \"extra_cpu\": 0},\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 0),\n            #                    \"extra_cpu\": max(self.num_cpus-1, 1)},\n            \"trial_resources\": {\"cpu\": 1,\n                                \"extra_cpu\": max(self.num_cpus-1, 1)},              \n          }\n        }\n\nif __name__ == \"__main__\":\n    os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n    os.environ[\"LANG\"] = \"C.UTF-8\"\n    os.environ[\"RAY_USE_XRAY\"] = \"1\"\n    print(ppo.DEFAULT_CONFIG)\n    MyLauncher().train_main()\n\n<\/code><\/pre>\n\n<p>The notebook code:<\/p>\n\n<pre><code>!\/bin\/bash .\/setup.sh\n\nfrom time import gmtime, strftime\nimport sagemaker \nrole = sagemaker.get_execution_role()\n\nsage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3:\/\/{}\/'.format(s3_bucket)\nprint(\"S3 bucket path: {}\".format(s3_output_path))\n\njob_name_prefix = 'ArrivalSim'\n\nfrom sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n\nestimator = RLEstimator(entry_point=\"mod_op_train.py\", # Our launcher code\n                        source_dir='src', # Directory where the supporting files are at. All of this will be\n                                          # copied into the container.\n                        dependencies=[\"common\/sagemaker_rl\"], # some other utils files.\n                        toolkit=RLToolkit.RAY, # We want to run using the Ray toolkit against the ray container image.\n                        framework=RLFramework.TENSORFLOW, # The code is in tensorflow backend.\n                        toolkit_version='0.5.3', # Toolkit version. This will also choose an apporpriate tf version.                                               \n                        #toolkit_version='0.6.5', # Toolkit version. This will also choose an apporpriate tf version.                        \n                        role=role, # The IAM role that we created at the begining.\n                        #train_instance_type=\"ml.m4.xlarge\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_type=\"local\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_count=1, # Single instance will also work, but running distributed makes things \n                                                # fast, particularly in the case of multiple rollout training.\n                        output_path=s3_output_path, # The path where we can expect our trained model.\n                        base_job_name=job_name_prefix, # This is the name we setup above to be to track our job.\n                        hyperparameters = {      # Some hyperparameters for Ray toolkit to operate.\n                          \"s3_bucket\": s3_bucket,\n                          \"rl.training.stop.training_iteration\": 2, # Number of iterations.\n                          \"rl.training.checkpoint_freq\": 2,\n                        },\n                        #metric_definitions=metric_definitions, # This will bring all the logs out into the notebook.\n                    )\n\nestimator.fit()\n<\/code><\/pre>",
        "Solution_comment_count":19.0,
        "Solution_last_edit_time":1568837974432,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":222.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":215.0,
        "Solution_word_count":1601.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"mismatched input shapes"
    },
    {
        "Answerer_created_time":1396289884603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"The Netherlands",
        "Answerer_reputation_count":480.0,
        "Answerer_view_count":96.0,
        "Challenge_adjusted_solved_time":1.2545936111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Running AWS SageMaker with a custom model, the TrainingJob fails with an <strong>Algorithm Error<\/strong> when using Keras plus a Tensorflow backend in multi-gpu configuration:<\/p>\n<pre><code>from keras.utils import multi_gpu_model\n\nparallel_model = multi_gpu_model(model, gpus=K)\nparallel_model.compile(loss='categorical_crossentropy',\noptimizer='rmsprop')\nparallel_model.fit(x, y, epochs=20, batch_size=256)\n<\/code><\/pre>\n<p>This simple parallel model loading will fail. There is no further error or exception from CloudWatch logging. This configuration works properly on local machine with 2x NVIDIA GTX 1080, same Keras Tensorflow backend.<\/p>\n<p>According to SageMaker documentation and <a href=\"https:\/\/github.com\/awslabs\/keras-apache-mxnet\/wiki\/Multi-GPU-Model-Training-with-Keras-MXNet\" rel=\"nofollow noreferrer\">tutorials<\/a> the <code>multi_gpu_model<\/code> utility will work ok when Keras backend is MXNet, but I did not find any mention when the backend is Tensorflow with the same multi gpu configuration.<\/p>\n<p><strong>[UPDATE]<\/strong><\/p>\n<p>I have updated the code with the suggested answer below, and I'm adding some logging before the TrainingJob hangs<\/p>\n<p>This logging repeats twice<\/p>\n<pre><code>2018-11-27 10:02:49.878414: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\n2018-11-27 10:02:49.878462: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-27 10:02:49.878471: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:988] 0 1 2 3\n2018-11-27 10:02:49.878477: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1001] 0: N Y Y Y\n2018-11-27 10:02:49.878481: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1001] 1: Y N Y Y\n2018-11-27 10:02:49.878486: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1001] 2: Y Y N Y\n2018-11-27 10:02:49.878492: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1001] 3: Y Y Y N\n2018-11-27 10:02:49.879340: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1115] Created TensorFlow device (\/device:GPU:0 with 14874 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n2018-11-27 10:02:49.879486: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1115] Created TensorFlow device (\/device:GPU:1 with 14874 MB memory) -&gt; physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n2018-11-27 10:02:49.879694: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1115] Created TensorFlow device (\/device:GPU:2 with 14874 MB memory) -&gt; physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\n2018-11-27 10:02:49.879872: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1115] Created TensorFlow device (\/device:GPU:3 with 14874 MB memory) -&gt; physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\n<\/code><\/pre>\n<p>Before there is some logging info about each GPU, that repeats 4 times<\/p>\n<pre><code>2018-11-27 10:02:46.447639: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1432] Found device 3 with properties:\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\npciBusID: 0000:00:1e.0\ntotalMemory: 15.78GiB freeMemory: 15.37GiB\n<\/code><\/pre>\n<p>According to the logging all the 4 GPUs are visible and loaded in the Tensorflow Keras backend. After that no application logging follows, the TrainingJob status is <strong>inProgress<\/strong> for a while, after that it becomes <strong>Failed<\/strong> with the same <strong>Algorithm Error<\/strong>.<\/p>\n<p>Looking at CloudWatch logging I can see some metrics at work. Specifically <code>GPU Memory Utilization<\/code>, <code>CPU Utilization<\/code> are ok, while <code>GPU utilization<\/code> is 0%.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/260hL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/260hL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>[UPDATE]<\/strong><\/p>\n<p>Due to a <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/8123#issuecomment-354857044\" rel=\"nofollow noreferrer\">known<\/a> bug on Keras that is about saving a multi gpu model, I'm using this override of the <strong>multi_gpu_model<\/strong> utility in <strong>keras.utils<\/strong><\/p>\n<pre><code>from keras.layers import Lambda, concatenate\nfrom keras import Model\nimport tensorflow as tf\n    \ndef multi_gpu_model(model, gpus):\n    #source: https:\/\/github.com\/keras-team\/keras\/issues\/8123#issuecomment-354857044\n  if isinstance(gpus, (list, tuple)):\n    num_gpus = len(gpus)\n    target_gpu_ids = gpus\n  else:\n    num_gpus = gpus\n    target_gpu_ids = range(num_gpus)\n\n  def get_slice(data, i, parts):\n    shape = tf.shape(data)\n    batch_size = shape[:1]\n    input_shape = shape[1:]\n    step = batch_size \/\/ parts\n    if i == num_gpus - 1:\n      size = batch_size - step * i\n    else:\n      size = step\n    size = tf.concat([size, input_shape], axis=0)\n    stride = tf.concat([step, input_shape * 0], axis=0)\n    start = stride * i\n    return tf.slice(data, start, size)\n\n  all_outputs = []\n  for i in range(len(model.outputs)):\n    all_outputs.append([])\n\n  # Place a copy of the model on each GPU,\n  # each getting a slice of the inputs.\n  for i, gpu_id in enumerate(target_gpu_ids):\n    with tf.device('\/gpu:%d' % gpu_id):\n      with tf.name_scope('replica_%d' % gpu_id):\n        inputs = []\n        # Retrieve a slice of the input.\n        for x in model.inputs:\n          input_shape = tuple(x.get_shape().as_list())[1:]\n          slice_i = Lambda(get_slice,\n                           output_shape=input_shape,\n                           arguments={'i': i,\n                                      'parts': num_gpus})(x)\n          inputs.append(slice_i)\n\n        # Apply model on slice\n        # (creating a model replica on the target device).\n        outputs = model(inputs)\n        if not isinstance(outputs, list):\n          outputs = [outputs]\n\n        # Save the outputs for merging back together later.\n        for o in range(len(outputs)):\n          all_outputs[o].append(outputs[o])\n\n  # Merge outputs on CPU.\n  with tf.device('\/cpu:0'):\n    merged = []\n    for name, outputs in zip(model.output_names, all_outputs):\n      merged.append(concatenate(outputs,\n                                axis=0, name=name))\n    return Model(model.inputs, merged)\n<\/code><\/pre>\n<p>This works ok on local <code>2x NVIDIA GTX 1080 \/ Intel Xeon \/ Ubuntu 16.04<\/code>. It will fails on SageMaker Training Job.<\/p>\n<p>I have posted this issue on AWS Sagemaker forum in<\/p>\n<ul>\n<li><p><a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=879769\" rel=\"nofollow noreferrer\">TrainingJob custom algorithm with Keras backend and multi GPU<\/a><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=294095&amp;tstart=0\" rel=\"nofollow noreferrer\">SageMaker Fails when using Multi-GPU with\nkeras.utils.multi_gpu_model<\/a><\/p>\n<\/li>\n<\/ul>\n<p><strong>[UPDATE]<\/strong><\/p>\n<p>I have slightly modified the <code>tf.session<\/code> code adding some initializers<\/p>\n<pre><code>with tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n<\/code><\/pre>\n<p>and now at least I can see that one GPU (I assume device <code>gpu:0<\/code>) is used from the instance metrics. The multi-gpu does not work anyways.<\/p>",
        "Challenge_closed_time":1543270023360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543265506823,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an Algorithm Error when using Keras with a Tensorflow backend in a multi-GPU configuration on AWS SageMaker. The multi_gpu_model utility fails to work properly, even though it works on the user's local machine with the same configuration. The user has tried modifying the code and adding initializers to the tf.session code, but the multi-GPU still does not work. The user has posted the issue on AWS SageMaker forum for assistance.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53488870",
        "Challenge_link_count":7,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":93.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":84,
        "Challenge_solved_time":1.2545936111,
        "Challenge_title":"SageMaker fails when using Multi-GPU with keras.utils.multi_gpu_model",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1295.0,
        "Challenge_word_count":737,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305708350447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bologna, Italy",
        "Poster_reputation_count":14823.0,
        "Poster_view_count":1847.0,
        "Solution_body":"<p>This might not be the best answer for your problem, but this is what I am using for a multi-gpu model with Tensorflow backend. First i initialize using: <\/p>\n\n<pre><code>def setup_multi_gpus():\n    \"\"\"\n    Setup multi GPU usage\n\n    Example usage:\n    model = Sequential()\n    ...\n    multi_model = multi_gpu_model(model, gpus=num_gpu)\n    multi_model.fit()\n\n    About memory usage:\n    https:\/\/stackoverflow.com\/questions\/34199233\/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n    \"\"\"\n    import tensorflow as tf\n    from keras.utils.training_utils import multi_gpu_model\n    from tensorflow.python.client import device_lib\n\n    # IMPORTANT: Tells tf to not occupy a specific amount of memory\n    from keras.backend.tensorflow_backend import set_session  \n    config = tf.ConfigProto()  \n    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU  \n    sess = tf.Session(config=config)  \n    set_session(sess)  # set this TensorFlow session as the default session for Keras.\n\n\n    # getting the number of GPUs \n    def get_available_gpus():\n       local_device_protos = device_lib.list_local_devices()\n       return [x.name for x in local_device_protos if x.device_type    == 'GPU']\n\n    num_gpu = len(get_available_gpus())\n    print('Amount of GPUs available: %s' % num_gpu)\n\n    return num_gpu\n<\/code><\/pre>\n\n<p>Then i call<\/p>\n\n<pre><code># Setup multi GPU usage\nnum_gpu = setup_multi_gpus()\n<\/code><\/pre>\n\n<p>and create a model.<\/p>\n\n<pre><code>...\n<\/code><\/pre>\n\n<p>After which you're able to make it a multi GPU model.<\/p>\n\n<pre><code>multi_model = multi_gpu_model(model, gpus=num_gpu)\nmulti_model.compile...\nmulti_model.fit...\n<\/code><\/pre>\n\n<p>The only thing here that is different from what you are doing is the way Tensorflow is initializing the GPU's. I can't imagine it being the problem, but it might be worth trying out. <\/p>\n\n<p>Good luck! <\/p>\n\n<p>Edit: I noticed sequence to sequence not being able to work with multi GPU. Is that the type of model you are trying to train?<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":24.55,
        "Solution_score_count":3.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":225.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"multi-GPU not working"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":2025.5465147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Status:<\/p>\n<ul>\n<li>Custom container is built using the doc - <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/li>\n<li>predict.py is coded to accommodate the custom inference script and its working well<\/li>\n<li>Using the classsagemaker.model.Model() class to pass the trained model.tar.gz and custom container image inorder to deploy the model<\/li>\n<\/ul>\n<p>Challenge:<\/p>\n<ul>\n<li>In the same Model class there is a ENV  parameter through which we can apparently send the environment variables to the custom image<\/li>\n<li>Tried passing a python dict to this , but facing difficulty to read this json dict inide the predict.py script<\/li>\n<\/ul>\n<p>Somebody faced the same difficulty ?<\/p>",
        "Challenge_closed_time":1645489319000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1638197351547,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in passing a python dictionary as an environment variable to a custom inference container in Sagemaker. The predict.py script is unable to read the JSON dictionary passed through the ENV parameter of the sagemaker.model.Model() class. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70156631",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":15.2,
        "Challenge_reading_time":12.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2025.5465147222,
        "Challenge_title":"How to pass additional parameters (as a dict) to sagemeker custom inference container?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":108,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can pass your environment dict in your Model as:<\/p>\n<pre><code>Model(\n.\n.\nenv= {&quot;my_env&quot;: &quot;my_env_value&quot;}\n.\n.\n)\n<\/code><\/pre>\n<p>SageMaker will pass the enviroments dict to your container and you can access it in your predict.py script for example with:<\/p>\n<pre><code>my_env = os.environ.get('my_env',&quot;env key not set in Model&quot;)\nprint(my_env)\n<\/code><\/pre>\n<p>If your env dict was passed to your Model containing they <code>my_env<\/code> then you will receive the output : <code>my_env_value<\/code>. Else, then you will receive <code>env key not set in Model<\/code><\/p>\n<p>I work for AWS and my opinions are my own.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":8.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to pass dictionary"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0788888889,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI'm currently facing a problem regarding the polyaxon tracking module. When running a job, I can see the logged metrics, parameters, artifacts in the lineage tab, although on the Dashboard tab, I can see only the chart's name and no values being tracked.\n\nHave anyone encountered this issue before? Any help will be much appreciated.\n\nI have tried a basic example of MNIST, when training it on CPU it tracks the metrics, but on GPU it doesn't.\n\nAlso, when I'm running a training job (MNIST) on the CPU it tracks the metrics and I can see the charts in the Dashboard, but when I configure the polyaxonfile to use GPU, the logs are indeed saying that the GPU is being used, but the metrics charts are empty.",
        "Challenge_closed_time":1650441842000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650441558000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the Polyaxon tracking module where the artifacts lineage is being tracked but the Dashboard and Artifacts tabs are empty. The user has tried a basic example of MNIST and found that when training on CPU, metrics are tracked but not on GPU. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1498",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0788888889,
        "Challenge_title":"Artifacts lineage is tracked but the Dashboard and Artifacts tabs are empty",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":140,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You are probably deploying Polyaxon with the default artifacts store and you are using multiple nodes.\n\nIf that's the case you will need to upgrade your deployment with one if the artifacts stores that supports multi-node deployment: https:\/\/polyaxon.com\/integrations\/#Artifacts\n\nMore info about the artifacts store default behavior: https:\/\/polyaxon.com\/docs\/setup\/connections\/#artifactsstore\n\nAlso, when I'm running a training job (MNIST) on the CPU it tracks the metrics and I can see the charts in the Dashboard, but when I configure the polyaxonfile to use GPU, the logs are indeed saying that the GPU is being used, but the metrics charts are empty.\n\nYour GPU is on different node that's why it shows the metrics and artifacts when running with CPU, note that even with CPU if it's on a different node nothing will show up, so you will need a PVC \/ blob storage accessible to all nodes.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":10.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":142.0,
        "Tool":"Polyaxon",
        "Challenge_type":"anomaly",
        "Challenge_summary":"empty Dashboard and Artifacts tabs"
    },
    {
        "Answerer_created_time":1424063473423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":334.0,
        "Answerer_view_count":347.0,
        "Challenge_adjusted_solved_time":810.2807111111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I followed the instructions <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">here<\/a> to set up an EMR cluster and a SageMaker notebook. I did not have any errors until the last step.<\/p>\n\n<p>When I open a new Notebook in Sagemaker, I get the message:<\/p>\n\n<pre><code>The kernel appears to have died. It will restart automatically.\n<\/code><\/pre>\n\n<p>And then:<\/p>\n\n<pre><code>        The kernel has died, and the automatic restart has failed.\n        It is possible the kernel cannot be restarted. \n        If you are not able to restart the kernel, you will still be able to save the \nnotebook, but running code will no longer work until the notebook is reopened.\n<\/code><\/pre>\n\n<p>This only happens when I use the pyspark\/Sparkmagic kernel. Notebooks opened with the Conda kernel or any other kernel work fine. <\/p>\n\n<p>My EMR cluster is set up exactly as in the instructions, with an added rule:<\/p>\n\n<pre><code>[\n  {\n    \"Classification\": \"spark\",\n    \"Properties\": {\n      \"maximizeResourceAllocation\": \"true\"\n    }\n  }\n]\n<\/code><\/pre>\n\n<p>I'd appreciate any pointers on why this is happening and how I can debug\/fix.<\/p>\n\n<p>P.S.: I've done this successfully in the past without any issues. When I tried re-doing this today, I ran into this issue. I tried re-creating the EMR clusters and Sagemaker notebooks, but that didn't help. <\/p>",
        "Challenge_closed_time":1531254842950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1528337832390,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user followed instructions to set up an EMR cluster and a SageMaker notebook, but when they open a new notebook in Sagemaker using the pyspark\/Sparkmagic kernel, they receive an error message stating that the kernel has died and cannot be restarted. This issue does not occur with other kernels. The user is seeking help to debug and fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50732094",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":17.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":810.2807111111,
        "Challenge_title":"Sagemaker PySpark: Kernel Dead",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1768.0,
        "Challenge_word_count":200,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1430255275880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pittsburgh, PA",
        "Poster_reputation_count":125.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>Thank you for using Amazon SageMaker.<\/p>\n\n<p>The issue here is Pandas 0.23.0 changed the location of a core class named DataError and SparkMagic has not been updated to require DataError from correct namespace.<\/p>\n\n<p>The workaround for this issue is to downgrade Pandas version in SageMaker Notebook Instance with <code>pip install pandas==0.22.0<\/code>.<\/p>\n\n<p>You can get more information in this open github issue <a href=\"https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458\" rel=\"noreferrer\">https:\/\/github.com\/jupyter-incubator\/sparkmagic\/issues\/458<\/a>.<\/p>\n\n<p>Let us know if there is any other way we can be of assistance.<\/p>\n\n<p>Thanks,<br>\nNeelam<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.68,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"kernel dies in Sagemaker"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":303.4149847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use  TensorFlow Hub in Azure ML Studio<\/p>\n<p>I am using the kernel Python 3.8 PT and TF<\/p>\n<p>And I installed  a few modules:<\/p>\n<pre><code>!pip install bert-for-tf2\n!pip install sentencepiece\n!pip install &quot;tensorflow&gt;=2.0.0&quot;\n!pip install --upgrade tensorflow-hub\n<\/code><\/pre>\n<p>With pip list, I can see they are installed:<\/p>\n<pre><code>tensorflow                              2.8.0\ntensorflow-estimator                    2.3.0\ntensorflow-gpu                          2.3.0\ntensorflow-hub                          0.12.0\ntensorflow-io-gcs-filesystem            0.24.0\n<\/code><\/pre>\n<p>However when I try to use it as per the documentation (<a href=\"https:\/\/www.tensorflow.org\/hub\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/hub<\/a>)<\/p>\n<p>Then I get the classic:<\/p>\n<pre><code>ModuleNotFoundError: No module named 'tensorflow_hub'\n<\/code><\/pre>",
        "Challenge_closed_time":1650950165912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649857871967,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to use TensorFlow Hub in Azure ML Studio. They have installed the required modules and verified their installation, but when they try to use it as per the documentation, they encounter a \"ModuleNotFoundError: No module named 'tensorflow_hub'\" error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71858668",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":303.4149847222,
        "Challenge_title":"How to use tensorflow hub in Azure ML",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>To resolve this <code>ModuleNotFoundError: No module named 'tensorflow_hub'<\/code>  error, try following ways:<\/p>\n<ul>\n<li>Try installing\/upgrading the latest version of <code>tensorflow<\/code> and <code>tensorflow-hub<\/code> and then import:<\/li>\n<\/ul>\n<pre><code>!pip install --upgrade tensorflow\n\n!pip install --upgrade tensorflow_hub\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n<\/code><\/pre>\n<ul>\n<li>Install the current environment as a new kernel:<\/li>\n<\/ul>\n<pre><code>python3 -m ipykernel install --user --name=testenvironment\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/stackoverflow.com\/questions\/63884339\/modulenotfounderror-no-module-named-tensorflow-hub\">ModuleNotFoundError: No module named 'tensorflow_hub', No module named 'tensorflow_hub'<\/a> and <a href=\"https:\/\/github.com\/tensorflow\/hub\/issues\/767\" rel=\"nofollow noreferrer\">How to use Tensorflow Hub Model?<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":12.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"module not found error"
    },
    {
        "Answerer_created_time":1384795490587,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1012.0,
        "Answerer_view_count":102.0,
        "Challenge_adjusted_solved_time":2.9863186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Having some difficulty executing the following code in AWS SageMaker. It is supposed to just list all of the tables in DynamoDB.<\/p>\n\n<pre><code>import boto3\nresource = boto3.resource('dynamodb', region_name='xxxx')\nresponse  = resource.tables.all()\nfor r in response:\n    print(r.name)\n<\/code><\/pre>\n\n<p>If the SageMaker notebook kernel is set to \"conda_python3\" the code executes fine and the tables are listed out in the notebook as expected (this happens pretty much instantly).<\/p>\n\n<p>However, if I set the kernel to \"Sparkmagic (PySpark)\" the same code infinitely runs and doesn't output the table list at all.<\/p>\n\n<p>Does anyone know why this would happen for the PySpark kernel but not for the conda3 kernel? Ideally I need to run this code as part of a bigger script that relies on PySpark, so would like to get it working with PySpark.<\/p>",
        "Challenge_closed_time":1574430450807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574419108777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in executing a code in AWS SageMaker notebook that is supposed to list all the tables in DynamoDB using boto3 and PySpark. The code executes fine when the kernel is set to \"conda_python3\" but infinitely runs and doesn't output the table list when the kernel is set to \"Sparkmagic (PySpark)\". The user is seeking help to understand why this is happening and how to get it working with PySpark.",
        "Challenge_last_edit_time":1574419700060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58992447",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":11.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3.1505638889,
        "Challenge_title":"AWS SageMaker notebook list tables using boto3 and PySpark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":440.0,
        "Challenge_word_count":137,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384795490587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1012.0,
        "Poster_view_count":102.0,
        "Solution_body":"<p>Figured out what the issue was, you need to end an endpoint to tour VPC for DyanmoDB.<\/p>\n\n<p>To do this navigate to:<\/p>\n\n<ol>\n<li>AWS VPC<\/li>\n<li>Endpoints<\/li>\n<li>Create Endpoint<\/li>\n<li>Select the dynamodb service (will be type Gateway)<\/li>\n<li>Select the VPC your Notebook is using<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":3.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"infinite code execution"
    },
    {
        "Answerer_created_time":1604138895270,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Straubing, Deutschland",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":93.5123102778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running a python script in the command line with <code>python3 CTGAN_noscale.py --database_name CTGAN_noshift<\/code> and receive the following error (with faulthandler):<\/p>\n<pre><code>Fatal Python error: Segmentation fault\n\nCurrent thread 0x00007f57e97fe700 (most recent call first):\n&lt;no Python frame&gt;\n\nThread 0x00007f593db07740 (most recent call first):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/torch\/autograd\/__init__.py&quot;, line 145 in backward\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/torch\/tensor.py&quot;, line 245 in backward\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/ctgan\/synthesizers\/ctgan.py&quot;, line 374 in fit\n  File &quot;CTGAN_noscale.py&quot;, line 140 in objective\n  File &quot;CTGAN_noscale.py&quot;, line 162 in &lt;lambda&gt;\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/optuna\/_optimize.py&quot;, line 216 in _run_trial\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/optuna\/_optimize.py&quot;, line 162 in _optimize_sequential\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/optuna\/_optimize.py&quot;, line 65 in _optimize\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/optuna\/study.py&quot;, line 401 in optimize\n  File &quot;CTGAN_noscale.py&quot;, line 162 in run_CTGAN\n  File &quot;CTGAN_noscale.py&quot;, line 210 in &lt;module&gt;\nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>It seems that the problem is somehow with optuna.\nThe weird part is that everything worked fine on another server, after changing the server it crashed like this.<\/p>\n<p><strong>Update<\/strong><\/p>\n<p>I found out that the the problem doesn't occur when I don't use a docker container OR use a docker container without GPU.<\/p>",
        "Challenge_closed_time":1630389747607,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630053103290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"Segmentation fault (core dumped)\" error while running a Python script with optuna. The error seems to be related to the use of a docker container with GPU, as the problem doesn't occur when not using a container or using a container without GPU. The issue worked fine on another server, but after changing the server, it crashed.",
        "Challenge_last_edit_time":1659952934500,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68950349",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":22.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":93.5123102778,
        "Challenge_title":"Segmentation fault (core dumped) with optuna",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":170,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604138895270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Straubing, Deutschland",
        "Poster_reputation_count":11.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I solved the problem by rebuilding a new image and derived a container from this image. In this container somehow the error didn't appear anymore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":1.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"segmentation fault error"
    },
    {
        "Answerer_created_time":1635310523496,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":1.9224463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have type error when I run for training on sagemaker by using xgboost conatiner.\nPlease advise me to fix the issue.<\/p>\n<pre><code>container = 'southeast-2','783357654285.dkr.ecr.ap-southeast-2.amazonaws.com\/sagemaker- xgboost:latest'`\n\ntrain_input = TrainingInput(s3_data='s3:\/\/{}\/train'.format(bucket, prefix), content_type='csv')\nvalidation_input = TrainingInput(s3_data='s3:\/\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\nsess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(\ncontainer,\nrole, \ninstance_count=1,\ninstance_type='ml.t2.medium',\noutput_path='s3:\/\/{}\/output'.format(bucket, prefix),\nsagemaker_session=sess\n)\n\nxgb.set_hyperparameters(\nmax_depth=5,\neta=0.1,\ngamma=4,\nmin_child_weight=6,\nsubsample=0.8,\nsilent=0,\nobjective=&quot;binary:logistic&quot;,\nnum_round=25,\n)\n\nxgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})\n<\/code><\/pre>\n<hr \/>\n<p>TypeError                                 Traceback (most recent call last)\n in \n21 )\n22\n---&gt; 23 xgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n685                 * <code>TrialComponentDisplayName<\/code> is used for display in Studio.\n686         &quot;&quot;&quot;\n--&gt; 687         self._prepare_for_training(job_name=job_name)\n688\n689         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _prepare_for_training(self, job_name)\n446                 constructor if applicable.\n447         &quot;&quot;&quot;\n--&gt; 448         self._current_job_name = self._get_or_create_name(job_name)\n449\n450         # if output_path was specified we use it otherwise initialize here.<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _get_or_create_name(self, name)\n435             return name\n436\n--&gt; 437         self._ensure_base_job_name()\n438         return name_from_base(self.base_job_name)\n439<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _ensure_base_job_name(self)\n420         # honor supplied base_job_name or generate it\n421         if self.base_job_name is None:\n--&gt; 422             self.base_job_name = base_name_from_image(self.training_image_uri())\n423\n424     def _get_or_create_name(self, name=None):<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/utils.py in base_name_from_image(image)\n95         str: Algorithm name, as extracted from the image name.\n96     &quot;&quot;&quot;\n---&gt; 97     m = re.match(&quot;^(.+\/)?([^:\/]+)(:[^:]+)?$&quot;, image)\n98     algo_name = m.group(2) if m else image\n99     return algo_name<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py in match(pattern, string, flags)\n170     &quot;&quot;&quot;Try to apply the pattern at the start of the string, returning\n171     a match object, or None if no match was found.&quot;&quot;&quot;\n--&gt; 172     return _compile(pattern, flags).match(string)\n173\n174 def fullmatch(pattern, string, flags=0):<\/p>\n<p>TypeError: expected string or bytes-like object<\/p>",
        "Challenge_closed_time":1638769174620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638762253813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a type error while training on sagemaker using xgboost container. The error occurs while running the fit function and is related to the expected string or bytes-like object. The user is seeking advice to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70240640",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":41.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":1.9224463889,
        "Challenge_title":"xgboost sagemaker train failure",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":245,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635310523496,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<pre><code>import sagemaker\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker.session import TrainingInput\nfrom sagemaker import image_uris\nfrom sagemaker.session import Session\n\n# initialize hyperparameters\nhyperparameters = {\n    &quot;max_depth&quot;:&quot;5&quot;,\n    &quot;eta&quot;:&quot;0.1&quot;,\n    &quot;gamma&quot;:&quot;4&quot;,\n    &quot;min_child_weight&quot;:&quot;6&quot;,\n    &quot;subsample&quot;:&quot;0.7&quot;,\n    &quot;objective&quot;:&quot;binary:logistic&quot;,\n    &quot;num_round&quot;:&quot;25&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\noutput_path = 's3:\/\/{}\/{}\/output'.format(bucket, 'rain-xgb-built-in-algo')\n\n\n# this line automatically looks for the XGBoost image URI and builds an \nXGBoost container.\n# specify the repo_version depending on your preference.\nxgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'ap-southeast- \n2', &quot;1.3-1&quot;)\n\n\n# construct a SageMaker estimator that calls the xgboost-container\nestimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n                                      hyperparameters=hyperparameters,\n                                      role=sagemaker.get_execution_role(),\n                                      instance_count=1, \n                                      instance_type='ml.m5.large', \n                                      volume_size=5, # 5 GB \n                                      output_path=output_path)\n\n\n\n # define the data type and paths to the training and validation datasets\n\n train_input = TrainingInput(&quot;s3:\/\/{}\/{}\/&quot;.format(bucket,'train'), \n content_type='csv')\n validation_input = TrainingInput(&quot;s3:\/\/{}\/{}&quot;.format(bucket,'validation'), \n content_type='csv')\n\n\n # execute the XGBoost training job\n estimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I have rewritten as above and could run training.\nthank you !<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":21.5,
        "Solution_reading_time":23.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"type error in sagemaker"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":0.8939655556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to <a href=\"https:\/\/spacy.io\/usage\/training\" rel=\"nofollow noreferrer\">train a spaCy model<\/a> , but turning the code into a Vertex AI Pipeline <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#:%7E:text=Step%201%3A%20Create%20a%20Python%20function%20based%20component\" rel=\"nofollow noreferrer\">Component<\/a>. My current code is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_name: str, dev_name: str) -&gt; NamedTuple(&quot;output&quot;, [(&quot;model_path&quot;, str)]):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_name : Name of the spaCy &quot;train&quot; set, used for model training.\n    dev_name: Name of the spaCy &quot;dev&quot; set, , used for model training.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE FAILS TO BE COMPILED HERE\n    \n    # NOTE: The remaining code has already been tested and proven to be functional.\n    #       It has been edited since the project is private.\n    \n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    location = &quot;gcs\/secret_model_destination_path\/TestModel&quot;\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, location,\n                    &quot;--paths.train&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(train_name),\n                    &quot;--paths.dev&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(dev_name),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n    \n    return (location,)\n<\/code><\/pre>\n<p>The Vertex AI Logs display the following as main cause of the failure:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The libraries are successfully installed, and yet I feel like there is some missing library \/ setting (as I know by <a href=\"https:\/\/dev.to\/davidgerva\/spacy-3-on-a-google-cloud-compute-instance-to-train-a-ner-transformer-model-23hf\" rel=\"nofollow noreferrer\">experience<\/a>); however I don't know  how to make it &quot;Python-based Vertex AI Components Compatible&quot;. BTW, the use of GPU is <strong>mandatory<\/strong> in my code.<\/p>\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":1651093924243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650978304193,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to train a spaCy model as a Vertex AI Pipeline component, but the image fails to compile due to a missing library or setting. The user has installed the required libraries and believes that there may be a compatibility issue with Python-based Vertex AI components. The code requires the use of a GPU. The user is seeking suggestions to resolve the issue.",
        "Challenge_last_edit_time":1651090705967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72014493",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":16.7,
        "Challenge_reading_time":37.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":32.1166805556,
        "Challenge_title":"Training spaCy model as a Vertex AI Pipeline \"Component\"",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":234.0,
        "Challenge_word_count":227,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>After some rehearsals, I think I have figured out what my code was missing. Actually, the <code>train<\/code> component definition was correct (with some minor tweaks relative to what was originally posted); however <strong>the pipeline was missing the GPU definition<\/strong>. I will first include a dummy example code, which trains a NER model using spaCy, and orchestrates everything via Vertex AI Pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output, OutputPath, InputPath\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Component definition\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;generate.yaml&quot;\n)\ndef generate_spacy_file(train_path: OutputPath(), dev_path: OutputPath()):\n    &quot;&quot;&quot;\n    Generates a small, dummy 'train.spacy' &amp; 'dev.spacy' file\n    \n    Returns:\n    -------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    &quot;&quot;&quot;\n    import spacy\n    from spacy.training import Example\n    from spacy.tokens import DocBin\n\n    td = [    # Train (dummy) dataset, in 'spacy V2 presentation'\n              (&quot;Walmart is a leading e-commerce company&quot;, {&quot;entities&quot;: [(0, 7, &quot;ORG&quot;)]}),\n              (&quot;I reached Chennai yesterday.&quot;, {&quot;entities&quot;: [(19, 28, &quot;GPE&quot;)]}),\n              (&quot;I recently ordered a book from Amazon&quot;, {&quot;entities&quot;: [(24,32, &quot;ORG&quot;)]}),\n              (&quot;I was driving a BMW&quot;, {&quot;entities&quot;: [(16,19, &quot;PRODUCT&quot;)]}),\n              (&quot;I ordered this from ShopClues&quot;, {&quot;entities&quot;: [(20,29, &quot;ORG&quot;)]}),\n              (&quot;Fridge can be ordered in Amazon &quot;, {&quot;entities&quot;: [(0,6, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a new Washer&quot;, {&quot;entities&quot;: [(16,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a old table&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a fancy dress&quot;, {&quot;entities&quot;: [(18,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a camera&quot;, {&quot;entities&quot;: [(12,18, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a tent for our trip&quot;, {&quot;entities&quot;: [(12,16, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a screwdriver from our neighbour&quot;, {&quot;entities&quot;: [(12,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I repaired my computer&quot;, {&quot;entities&quot;: [(15,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my clock fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my truck fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n    ]\n    \n    dd = [    # Development (dummy) dataset (CV), in 'spacy V2 presentation'\n              (&quot;Flipkart started it's journey from zero&quot;, {&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Max&quot;, {&quot;entities&quot;: [(24,27, &quot;ORG&quot;)]}),\n              (&quot;Flipkart is recognized as leader in market&quot;,{&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Swiggy&quot;, {&quot;entities&quot;: [(24,29, &quot;ORG&quot;)]})\n    ]\n\n    \n    # Converting Train &amp; Development datasets, from 'spaCy V2' to 'spaCy V3'\n    nlp = spacy.blank(&quot;en&quot;)\n    db_train = DocBin()\n    db_dev = DocBin()\n\n    for text, annotations in td:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_train.add(example.reference)\n        \n    for text, annotations in dd:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_dev.add(example.reference)\n    \n    db_train.to_disk(train_path + &quot;.spacy&quot;)  # &lt;== Obtaining and storing &quot;train.spacy&quot;\n    db_dev.to_disk(dev_path + &quot;.spacy&quot;)      # &lt;== Obtaining and storing &quot;dev.spacy&quot;\n    \n\n# ----------------------- ORIGINALLY POSTED CODE -----------------------\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_path: InputPath(), dev_path: InputPath(), output_path: OutputPath()):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE NOW MANAGES TO GET BUILT!\n\n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, output_path,\n                    &quot;--paths.train&quot;, &quot;{}.spacy&quot;.format(train_path),\n                    &quot;--paths.dev&quot;, &quot;{}.spacy&quot;.format(dev_path),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n\n# ----------------------------------------------------------------------\n    \n\n# Pipeline definition\n\n@pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;spacy-dummy-pipeline&quot;,\n)\ndef spacy_pipeline():\n    &quot;&quot;&quot;\n    Builds a custom pipeline\n    &quot;&quot;&quot;\n    # Generating dummy &quot;train.spacy&quot; + &quot;dev.spacy&quot;\n    train_dev_sets = generate_spacy_file()\n    # With the output of the previous component, train a spaCy modeL    \n    model = train(\n        train_dev_sets.outputs[&quot;train_path&quot;],\n        train_dev_sets.outputs[&quot;dev_path&quot;]\n    \n    # ------ !!! THIS SECTION DOES THE TRICK !!! ------\n    ).add_node_selector_constraint(\n        label_name=&quot;cloud.google.com\/gke-accelerator&quot;,\n        value=&quot;NVIDIA_TESLA_T4&quot;\n    ).set_gpu_limit(1).set_memory_limit('32G')\n    # -------------------------------------------------\n\n# Pipeline compilation   \n\ncompiler.Compiler().compile(\n    pipeline_func=spacy_pipeline, package_path=&quot;pipeline_spacy_job.json&quot;\n)\n\n\n# Pipeline run\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun = aiplatform.PipelineJob(  # Include your own naming here\n    display_name=&quot;spacy-dummy-pipeline&quot;,\n    template_path=&quot;pipeline_spacy_job.json&quot;,\n    job_id=&quot;ml-pipeline-spacydummy-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={},\n    enable_caching=True,\n)\n\n\n# Pipeline gets submitted\n\nrun.submit()\n<\/code><\/pre>\n<p>Now, the explanation; according to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/machine-types\" rel=\"nofollow noreferrer\">Google<\/a>:<\/p>\n<blockquote>\n<p>By default, the component will run on as a Vertex AI CustomJob using an e2-standard-4 machine, with 4 core CPUs and 16GB memory.<\/p>\n<\/blockquote>\n<p>Therefore, when the <code>train<\/code> component gets compiled, it fails as &quot;<em>it was not seeing any GPU available as resource<\/em>&quot;; in the same link however, all the available settings for both CPU and GPU are mentioned. In my case as you can see, I set <code>train<\/code> component to run under ONE (1) <code>NVIDIA_TESLA_T4<\/code> GPU card, and I also increased my CPU memory, to 32GB. With these modifications, the resulting pipeline looks as follows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0I31.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0I31.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And as you can see, it gets compiled successfully, as well as trains (and eventually obtains) a functional spaCy model. From here, you can tweak this code, to fit your own needs.<\/p>\n<p>I hope this helps to anyone who might be interested.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.7,
        "Solution_reading_time":104.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":686.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed image compilation"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":431.6069444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThere are some missing details for how to connect to Neptune from a MacOS device, we should add them to our doc on connecting to neptune via ssh-tunnel found [here](https:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune)\r\n\r\nOne main piece that we are missing is that a host alias needs to be made in order to get things working properly.\r\n\r\n**Additional context**\r\nThis is coming from a bug report from connectivity not working as found in #40 ",
        "Challenge_closed_time":1608658157000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607104372000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an AttributeError when using PyTorchLightningPruningCallback to search for the best hyperparameters. The error message states that the 'AcceleratorConnector' object has no attribute 'distributed_backend'. The user has provided a code snippet and expects the code to run without any errors. The user has provided the environment details, including the PyTorch Lightning version, PyTorch version, Python version, OS, and CUDA\/cuDNN version.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.2,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":25.0,
        "Challenge_repo_fork_count":129.0,
        "Challenge_repo_issue_count":493.0,
        "Challenge_repo_star_count":546.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":431.6069444444,
        "Challenge_title":"[BUG] Missing documentation on connecting to Neptune from MacOS",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune",
        "Challenge_type":"anomaly",
        "Challenge_summary":"AttributeError in PyTorchLightningPruningCallback"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.3347222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I need to run XGBoost inferences on 15MM samples (3.9Gb when stored as csv). Since Batch transform does not seem to work on such large batches (max payload 100MB) I split my input file into 646 files, each around 6Mb, stored in S3. I am running the code below:\n\n    transformer = XGB.transformer(\n        instance_count=2, instance_type='ml.c5.9xlarge',\n        output_path='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/xgbtransform\/',\n        max_payload=100)\n\n    transformer.transform(\n        data='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/testchunks\/',\n        split_type='Line')\n\nBut the job fails - Sagemaker tells \"ClientError: Too many objects failed. See logs for more information\" and cloudwatch logs show:\n\n    Bad HTTP status returned from invoke: 415\n    'NoneType' object has no attribute 'lower'\n\nDid I forget something in my batch transform settings?",
        "Challenge_closed_time":1532634125000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532625720000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a 415 error while running XGBoost inferences on 15MM samples using Sagemaker batch transform. The error occurs despite splitting the input file into 646 files, each around 6Mb, stored in S3. The error message suggests that there may be an issue with the batch transform settings.",
        "Challenge_last_edit_time":1668556023886,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUr4Vq95ScROqSguzxNQYDOg\/sagemaker-batch-transform-415-error",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":2.3347222222,
        "Challenge_title":"Sagemaker batch transform 415 error",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":568.0,
        "Challenge_word_count":105,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This indicates that the algorithm thinks it has been passed bad data. Perhaps a problem with your splitting?\n\nI would suggest two things: \n\n1. Try running the algorithm on the original data using the `\"SplitType\": \"Line\"` and `\"BatchStrategy\": \"MultiRecord\"` arguments and see if you have better luck.\n2. Look in the cloudwatch logs for your run and see if  there's any helpful information about what the algorithm didn't like. You can find these in the log group \"\/aws\/sagemaker\/TransformJobs\" in the log stream that begins with your job name.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925589052,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"415 error in batch transform"
    },
    {
        "Answerer_created_time":1320746685067,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg",
        "Answerer_reputation_count":7552.0,
        "Answerer_view_count":456.0,
        "Challenge_adjusted_solved_time":87.8971722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Challenge_closed_time":1579206604950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579190415880,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job. While calling the function from a Spark context (in a UDF), the user is encountering a serialization error. The error message suggests that the issue is with failing to deserialize the Predictor.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":67.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":51,
        "Challenge_solved_time":4.4969638889,
        "Challenge_title":"Using Sagemaker predictor in a Spark UDF function",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":322.0,
        "Challenge_word_count":474,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1579506845700,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":35.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":288.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"serialization error in Spark job"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":57.0580416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am execute a python script in Azure machine learning studio. I am including other python scripts and python library, Theano. I can see the Theano get loaded and I got the proper result after script executed. But I saw the error message:<\/p>\n\n<blockquote>\n  <p>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.<\/p>\n<\/blockquote>\n\n<p>Did anyone know how to solve this problem? Thanks!<\/p>",
        "Challenge_closed_time":1539764377230,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539726201673,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while executing a python script in Azure Machine Learning Studio. They are using the Theano library and are receiving a warning message stating that Theano is unable to execute optimized C-implementations for both CPU and GPU, which will result in degraded performance. The user is seeking a solution to this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52844431",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":9.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":10.6043213889,
        "Challenge_title":"Azure Machine Learning Studio execute python script, Theano unable to execute optimized C-implementations (for both CPU and GPU)",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":99.0,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337362023536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":581.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>I don't think you can fix that - the Python script environment in Azure ML Studio is rather locked down, you can't really configure it (except for choosing from a small selection of Anaconda\/Python versions). <\/p>\n\n<p>You might be better off using the new Azure ML service, which allows you considerably more configuration options (including using GPUs and the like). <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1539931610623,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":4.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"degraded performance warning"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":7.7506297223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server\" rel=\"nofollow noreferrer\">NVIDIA Triton<\/a>\u00a0vs\u00a0<a href=\"https:\/\/pytorch.org\/serve\/\" rel=\"nofollow noreferrer\">TorchServe<\/a>\u00a0for SageMaker inference? When to recommend each?<\/p>\n<p>Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker.<\/p>\n<p>Anyone has a good comparison matrix for both?<\/p>",
        "Challenge_closed_time":1663971240670,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663943338403,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking a comparison between NVIDIA Triton and TorchServe for SageMaker inference, as both are modern, production grade inference servers. They are specifically looking for a comparison matrix to determine when to recommend each.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73829280",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.7506297223,
        "Challenge_title":"NVIDIA Triton vs TorchServe for SageMaker Inference",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":12.0,
        "Challenge_word_count":57,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>Important notes to add here where both serving stacks differ:<\/p>\n<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.\nif you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).\nThere is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code\/handlers instead of just being able to serve a model's forward function.<\/p>\n<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.<\/p>\n<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized\/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":27.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":363.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"compare Triton and TorchServe"
    },
    {
        "Answerer_created_time":1327481639092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Poznan, Poland",
        "Answerer_reputation_count":2923.0,
        "Answerer_view_count":838.0,
        "Challenge_adjusted_solved_time":2025.0750175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Within AzureML, I have a CSV file which contains <code>2 columns<\/code> of data with <code>thousands of rows<\/code>. I'm looking to run this entire file as training, and find a pattern between these 2 sets of numbers, for example:<\/p>\n\n<pre><code>x -&gt; y\n\n... 10k x\n<\/code><\/pre>\n\n<p>And after all that training, I'd want to give this one line as the score model, so It'd look like:\nx -> ? (Predict answer from training)\n-- Note, the question mark here wouldn't need to be an exact match, as long as it is somewhat around what that actual number would turn out to be like.<\/p>\n\n<p>Is their a ML method (Inside <code>Azure ML<\/code>) that does such thing? Any points would be great.<\/p>\n\n<p>tl;dr: <code>Finding any type of pattern between 2 numbers (w\/ intense training).<\/code><\/p>",
        "Challenge_closed_time":1448906765136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1441616495073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on the best ML method to use within AzureML to find a pattern between two sets of numbers in a CSV file with thousands of rows. The user wants to use the entire file for training and then use one line as a score model to predict an answer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32434805",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":10.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2025.0750175,
        "Challenge_title":"What would be the best ML method for this use case?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":137,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416688064183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":347.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>Read about <code>linear regression<\/code>. This is answer for your question. And here is the link to Azure ML tutorial <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-create-experiment\/\" rel=\"nofollow\">link<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.8,
        "Solution_reading_time":3.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"best ML method for pattern recognition"
    },
    {
        "Answerer_created_time":1327234712912,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":53015.0,
        "Answerer_view_count":3262.0,
        "Challenge_adjusted_solved_time":0.0405816667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Basically I'm receiving an output like this from my azure ws output:<\/p>\n\n<pre><code>{\n    'Results': {\n        'WSOutput': {\n            'type': 'table',\n            'value': {\n                'ColumnNames': ['ID', 'Start', 'Ask', 'Not', 'Passed', 'Suggest'],\n                'ColumnTypes': ['Int32', 'Int32', 'Int32', 'Double', 'Int64', 'Int32'],\n                'Values': [['13256025', '25000', '19000', '0.35', '1', '25000']]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The string, as you can see, has the info to create a datatable object. Now, I can't seem to find an easy way to cast it to an actual datatable POCO. I'm able to manually code a parser with Newtonsoft.Json.Linq but there has to be an easier way. <\/p>\n\n<p>Does anybody know how? I can't seem to find anything on the net.<\/p>",
        "Challenge_closed_time":1519929928907,
        "Challenge_comment_count":7,
        "Challenge_created_time":1519929782813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble converting a datatable string received from Azure ML WS into an actual Datatable C# object. The string contains the necessary information to create a datatable object, but the user is unable to find an easy way to cast it. The user has tried manually coding a parser with Newtonsoft.Json.Linq but is looking for an easier solution.",
        "Challenge_last_edit_time":1519930038023,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49056593",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":6.5,
        "Challenge_reading_time":9.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0405816667,
        "Challenge_title":"Convert a datatable string from Azure ML WS to an actual Datatable C# Object?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":115,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324654920387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Waterloo, ON, Canada",
        "Poster_reputation_count":5211.0,
        "Poster_view_count":449.0,
        "Solution_body":"<p>Yes, there is a open source online gernator on the net (<a href=\"http:\/\/jsonutils.com\/\" rel=\"nofollow noreferrer\">http:\/\/jsonutils.com\/<\/a>). Copy paste your result will give you that:<\/p>\n\n<pre><code> public class Value\n    {\n        public IList&lt;string&gt; ColumnNames { get; set; }\n        public IList&lt;string&gt; ColumnTypes { get; set; }\n        public IList&lt;IList&lt;string&gt;&gt; Values { get; set; }\n    }\n\n    public class WSOutput\n    {\n        public string type { get; set; }\n        public Value value { get; set; }\n    }\n\n    public class Results\n    {\n        public WSOutput WSOutput { get; set; }\n    }\n\n    public class Example\n    {\n        public Results Results { get; set; }\n    }\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":7.82,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"trouble converting datatable string"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":70.62626,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello to all, <\/p>\n<p>I am working on a machine learning project, I have trained my model on azure auto ml studio, I would like to import it in onnx format, but it is not in the download options, so I want to modify the resulting code directly in azure auto ml, in the script.py I have modified the recording format of the model but I can't find how to execute this script because it tells me that the script.py is not found, <\/p>\n<p>Could you help me please ? <\/p>\n<p>thank you <\/p>\n<p>Lysa<\/p>",
        "Challenge_closed_time":1680506244446,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680251989910,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in modifying the template script on Azure Auto ML to import their model in ONNX format. They have modified the recording format of the model in script.py but are unable to execute the script as it is not found. They are seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1195000\/how-to-modify-the-template-script-on-azure-auto-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":70.62626,
        "Challenge_title":"how to modify the template script on azure auto ml?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=2d96c782-8477-4987-b5a4-5ea497b49eb9\">AMROUN Lysa<\/a> I believe you are looking to create a model wihich is ONNX compatible with AutoML as per your previous thread. In this case in the automl config if you setup <code>enable_onnx_compatible_models<\/code> to true in the automl config the model should be available for download. Something similar to what is provided as guidance in this <a href=\"https:\/\/github.com\/Azure\/azureml-examples\/blob\/main\/v1\/python-sdk\/tutorials\/automl-with-azureml\/classification-bank-marketing-all-features\/auto-ml-classification-bank-marketing-all-features.ipynb\">notebook<\/a>.<\/p>\n<p>I would recommend running through this notebook and then change your experiment settings in a similar way to retrieve the best onnx format model. <\/p>\n<pre><code>automl_settings = {\n    &quot;experiment_timeout_hours&quot;: 0.3,\n    &quot;enable_early_stopping&quot;: True,\n    &quot;iteration_timeout_minutes&quot;: 5,\n    &quot;max_concurrent_iterations&quot;: 4,\n    &quot;max_cores_per_iteration&quot;: -1,\n    # &quot;n_cross_validations&quot;: 2,\n    &quot;primary_metric&quot;: &quot;AUC_weighted&quot;,\n    &quot;featurization&quot;: &quot;auto&quot;,\n    &quot;verbosity&quot;: logging.INFO,\n    &quot;enable_code_generation&quot;: True,\n}\n\nautoml_config = AutoMLConfig(\n    task=&quot;classification&quot;,\n    debug_log=&quot;automl_errors.log&quot;,\n    compute_target=compute_target,\n    experiment_exit_score=0.9984,\n    blocked_models=[&quot;KNN&quot;, &quot;LinearSVM&quot;],\n    enable_onnx_compatible_models=True,\n    training_data=train_data,\n    label_column_name=label,\n    validation_data=validation_dataset,\n    **automl_settings,\n)\n\n<\/code><\/pre>\n<p>If this answers your query, do click <code>Accept Answer<\/code> and <code>Yes<\/code> for was this answer helpful. And, if you have any further query do let us know.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.8,
        "Solution_reading_time":24.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":146.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"script not found"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.0094444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, does SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?\nI'm trying [this sample with 1.6][1] and the deployment fails with error\n\n    FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/model\/model.pth'\n\nthe documentation doesn't mention such a constraint\n\n  [1]: https:\/\/github.com\/aws-samples\/amazon-sagemaker-bert-classify-pytorch",
        "Challenge_closed_time":1610121735000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610118101000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a sample with SageMaker PyTorch Hosting 1.6. The deployment fails with an error stating that the model.pth file is not found. The user is questioning if SageMaker PyTorch Hosting 1.6 only works with artifacts named model.pth, as this constraint is not mentioned in the documentation.",
        "Challenge_last_edit_time":1668602387619,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUn-IyC9nySDKKibLL3Lvw3A\/sagemaker-pytorch-hosting-1-6-works-only-with-artifacts-named-model-pth",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.0094444444,
        "Challenge_title":"SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":52,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes - from the thread on this open issue: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/86\n\nIn the issue thread they note that the new Pytorch 1.6 image requires that the model filename is `model.pth`, linking to the relevant code where this default is set: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/9a6869e\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L121\n\nAlso noted in the thread is that users have successfully adapted their code to use torchserve in Pytorch 1.6 by changing it to save their model in a file named `model.pth`. Once renamed, they were still able to use custom inference scripts to load their model by defining a custom `model_fn`: https:\/\/github.com\/data-science-on-aws\/workshop\/blob\/374329adf15bf1810bfc4a9e73501ee5d3b4e0f5\/09_deploy\/wip\/pytorch\/code\/inference.py",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1667925547320,
        "Solution_link_count":3.0,
        "Solution_readability":11.9,
        "Solution_reading_time":10.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"model.pth not found"
    },
    {
        "Answerer_created_time":1604146329127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":281.9153205556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Idea:<\/p>\n<ul>\n<li>To use experiments and trials to log the training parameters and artifacts in sagemaker while using MWAA as the pipeline orchestrator<\/li>\n<\/ul>\n<p>I am using the training_config to create the dict to pass the training configuration to the Tensorflow estimator, but there is no parameter to pass the experiment configuration<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point='train_model.py',\n                                      source_dir= source\n                                      role=sagemaker.get_execution_role(),\n                                      instance_count=1,\n                                      framework_version='2.3.0',\n                                      instance_type=instance_type,\n                                      py_version='py37',\n                                      script_mode=True,\n                                      enable_sagemaker_metrics = True,\n                                      metric_definitions=metric_definitions,\n                                      output_path=output\n\nmodel_training_config = training_config(\n                    estimator=tf_estimator,\n                    inputs=input\n                    job_name=training_jobname,\n                )\n    \n\n\n\ntraining_task = SageMakerTrainingOperator(\n                    task_id=test_id,\n                    config=model_training_config,\n                    aws_conn_id=&quot;airflow-sagemaker&quot;,  \n                    print_log=True,\n                    wait_for_completion=True,\n                    check_interval=60  \n                )\n<\/code><\/pre>",
        "Challenge_closed_time":1646393704707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645378809553,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use experiments and trials to log the training parameters and artifacts in Sagemaker while using MWAA as the pipeline orchestrator. However, they are facing a challenge in passing the experiment configuration to a SagemakerTrainingOperator while training as there is no parameter to pass the experiment configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71197045",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":25.7,
        "Challenge_reading_time":14.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":281.9153205556,
        "Challenge_title":"How to pass the experiment configuration to a SagemakerTrainingOperator while training?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":90,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The only way that i found right now is to use the CreateTrainigJob API (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html#sagemaker-CreateTrainingJob-request-RoleArn<\/a>). The following steps are needed:<\/p>\n<ul>\n<li>I am not sure if this will work with Bring your own script method for E.g with a Tensorflow estimator<\/li>\n<li>it works with a build your own container approach<\/li>\n<li>Using the CreateTrainigJob API i created the configs which in turn includes all the needed configs like - training, experiment, algporthm etc and passed that to SagemakerTrainingOperator<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.2,
        "Solution_reading_time":10.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to pass experiment config"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617627646000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The mlflow run status shows \"FINISHED\" instead of \"FAILED\" when the kedro run fails, making it difficult to distinguish between successful and failed runs in the mlflow ui. The potential solution suggested is to replace certain lines of code or retrieve the current run status from mlflow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":121,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect run status"
    },
    {
        "Answerer_created_time":1490674180056,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"%Temp%",
        "Answerer_reputation_count":302.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":243.7168994444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have the dataset like this (just a sample of it):<\/p>\n\n<pre><code>DATE_REF,MONTH,YEAR,DAY_OF_YEAR,DAY_OF_MONTH,WEEK_DAY,WEEK_DAY_1,WEEK_DAY_2,WEEK_DAY_3,WEEK_DAY_4,WEEK_DAY_5,WEEK_DAY_6,WEEK_DAY_7,WEEK_NUMBER_IN_MONTH,WEEKEND,WORK_DAY,AMOUNT_SOLD\n20100101,1,2010,1,1,6,0,0,0,0,0,1,0,1,0,0,0\n20100102,1,2010,2,2,7,0,0,0,0,0,0,1,1,1,0,2\n20100103,1,2010,3,3,1,1,0,0,0,0,0,0,2,1,0,0\n20100104,1,2010,4,4,2,0,1,0,0,0,0,0,2,0,1,12830\n20100105,1,2010,5,5,3,0,0,1,0,0,0,0,2,0,1,19200\n20100106,1,2010,6,6,4,0,0,0,1,0,0,0,2,0,1,22930\n20100107,1,2010,7,7,5,0,0,0,0,1,0,0,2,0,1,23495\n20100108,1,2010,8,8,6,0,0,0,0,0,1,0,2,0,1,23215\n20100109,1,2010,9,9,7,0,0,0,0,0,0,1,2,1,0,172\n20100110,1,2010,10,10,1,1,0,0,0,0,0,0,3,1,0,0\n20100111,1,2010,11,11,2,0,1,0,0,0,0,0,3,0,1,18815\n20100112,1,2010,12,12,3,0,0,1,0,0,0,0,3,0,1,25415\n20100113,1,2010,13,13,4,0,0,0,1,0,0,0,3,0,1,25262\n20100114,1,2010,14,14,5,0,0,0,0,1,0,0,3,0,1,27967\n20100115,1,2010,15,15,6,0,0,0,0,0,1,0,3,0,1,26352\n20100116,1,2010,16,16,7,0,0,0,0,0,0,1,3,1,0,202\n20100117,1,2010,17,17,1,1,0,0,0,0,0,0,4,1,0,10\n20100118,1,2010,18,18,2,0,1,0,0,0,0,0,4,0,1,20295\n20100119,1,2010,19,19,3,0,0,1,0,0,0,0,4,0,1,25982\n20100120,1,2010,20,20,4,0,0,0,1,0,0,0,4,0,1,24745\n20100121,1,2010,21,21,5,0,0,0,0,1,0,0,4,0,1,28087\n20100122,1,2010,22,22,6,0,0,0,0,0,1,0,4,0,1,28417\n20100123,1,2010,23,23,7,0,0,0,0,0,0,1,4,1,0,115\n20100124,1,2010,24,24,1,1,0,0,0,0,0,0,5,1,0,5\n20100125,1,2010,25,25,2,0,1,0,0,0,0,0,5,0,1,20185\n20100126,1,2010,26,26,3,0,0,1,0,0,0,0,5,0,1,25932\n20100127,1,2010,27,27,4,0,0,0,1,0,0,0,5,0,1,31710\n20100128,1,2010,28,28,5,0,0,0,0,1,0,0,5,0,1,21020\n20100129,1,2010,29,29,6,0,0,0,0,0,1,0,5,0,1,51460\n20100130,1,2010,30,30,7,0,0,0,0,0,0,1,5,1,0,670\n20100131,1,2010,31,31,1,1,0,0,0,0,0,0,6,1,0,17\n<\/code><\/pre>\n\n<p>I'm trying to predict the <code>AMOUNT_SOLD<\/code> for new dates (<code>DATE_REF<\/code>) using the following experiment on Azure ML:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/7Mfhs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7Mfhs.png\" alt=\"Azure ML Experiment\"><\/a><\/p>\n\n<p>Then I deployed the Web Service and tested the prediction, but all I got was zero for the <code>AMOUNT_SOLD<\/code> column.<\/p>\n\n<p>What may I be missing? <\/p>",
        "Challenge_closed_time":1503036396008,
        "Challenge_comment_count":1,
        "Challenge_created_time":1502159015170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a dataset with various features including the amount sold, and is trying to predict the amount sold for new dates using an experiment on Azure ML. However, when the user deploys the web service and tests the prediction, the amount sold column returns zero. The user is seeking help to identify what they may be missing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45558337",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":13.8,
        "Challenge_reading_time":32.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":243.7168994444,
        "Challenge_title":"What is wrong with my experiment (Trying to predict car sales)?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":178.0,
        "Challenge_word_count":105,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1328174321836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte - MG, Brasil",
        "Poster_reputation_count":7753.0,
        "Poster_view_count":743.0,
        "Solution_body":"<p>As much as I want to replicate your Azure ML experiment, I do not have enough data. But what I've done are as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XNaeg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XNaeg.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I copied your sample data, and then multiplied it by 4 times (<strong>Add Rows x 2<\/strong>).\nThen <strong>Split Data<\/strong> (70%\/30%), random seed 7 (for reproducible results).\nThe <strong>Boosted Decision Tree Regression<\/strong> has default parameters.\nOn <strong>Tune Model Hyperparameters<\/strong>, I selected <strong><em>AMOUNT_SOLD<\/em><\/strong> as the label column.\nThen <strong>Score Model<\/strong> and <strong>Evaluate Model<\/strong>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/aIJlk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aIJlk.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Accuracy \/ Coefficient of Determination was pretty good.<\/p>\n\n<p>After that, to deploy this as a web service, you must setup first a Predictive Experiment from your Training Experiment. <code>Setup Web Service &gt; Predictive Experiment<\/code> You experiment will move like magic.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/gTEOl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gTEOl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The <strong>Web Service Input<\/strong> module will be placed by default at the top of the experiment. I <strong>moved it and connected at the right side of Score Model<\/strong>, so that when you are inputting the parameters of your web service, it <em>will be predicted using your Trained Model<\/em>.<\/p>\n\n<p>After the Score Model module, I placed a <strong>Select Columns in Dataset<\/strong> module and selected only the column named <strong>Scored Labels<\/strong>. This column contains the model's predictions. Then I used <strong>Edit Metadata<\/strong> module to rename the Scored Labels column, before passing it to the <strong>Web Service Output<\/strong> module.<\/p>\n\n<p>Your experiment is now ready to deploy as a web service.<\/p>\n\n<p>To predict new values, I tested the web service using the current date details as input. (<strong>Although the DATE_REF input must be 20170818<\/strong> :D )<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fPm65.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fPm65.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And then the output looks like this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/R6N4B.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R6N4B.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Your web service can now predict new values.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":10.0,
        "Solution_readability":11.2,
        "Solution_reading_time":35.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":27.0,
        "Solution_word_count":306.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"zero amount sold prediction"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.7375,
        "Challenge_answer_count":0,
        "Challenge_body":"Invoke Endpoint response time out. \r\n\r\n### Reproduction Steps\r\n\r\n{\r\n  \"trainingJob\": {\r\n    \"hyperparameters\": {\r\n    \"n-hidden\": \"2\",\r\n    \"n-epochs\": \"100\",\r\n    \"lr\":\"1e-2\"\r\n    },\r\n    \"instanceType\": \"ml.c5.9xlarge\",\r\n    \"timeoutInSeconds\": 10800    \r\n  }\r\n}\r\n\r\n\r\n\r\n### Error Log\r\nIn Inference Lambda CloudWatch:\r\n\r\nTask timed out after 120.10 seconds\r\n\r\n\r\nIn Sagemaker Training CloudWatch:\r\n\r\n2021-04-09   04:53:46,902 [INFO ] main org.pytorch.serve.ModelServer - Loading initial   models: model.mar\r\n--\r\n2021-04-09 04:53:49,837 [INFO ] main   org.pytorch.serve.archive.ModelArchive - eTag   8ff2b3de4bed4fb1bc7fe969652117ff\r\n2021-04-09 04:53:49,847 [INFO ] main   org.pytorch.serve.wlm.ModelManager - Model model loaded.\r\n2021-04-09 04:53:49,865 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Inference server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Inference API bind to: http:\/\/0.0.0.0:8080\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Metrics server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,931 [INFO ] main   org.pytorch.serve.ModelServer - Metrics API bind to: http:\/\/127.0.0.1:8082\r\nModel server started.\r\n2021-04-09 04:53:49,957 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on   port: \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]55\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker   started.\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime:   3.6.13\r\n2021-04-09 04:53:49,963 [INFO ]   W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to:   \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,972 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection   accepted: \/home\/model-server\/tmp\/.ts.sock.9000.\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   CPUUtilization.Percent:33.3\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskAvailable.Gigabytes:19.622234344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUsage.Gigabytes:4.731609344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUtilization.Percent:19.4\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryAvailable.Megabytes:30089.12109375\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUsed.Megabytes:902.6953125\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUtilization.Percent:4.1\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Setting the   default backend to \"pytorch\". You can change it in the   ~\/.dgl\/config.json file or export the DGLBACKEND environment variable.\u00a0 Valid options are: pytorch, mxnet,   tensorflow (all lowercase)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   ------------------ Loading model -------------------\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker   process died.\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most   recent call last):\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 176, in <module>\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 worker.run_server()\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 148, in run_server\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self.handle_connection(cl_socket)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 112, in handle_connection\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service, result, code =   self.load_model(msg)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 85, in load_model\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service = model_loader.load(model_name,   model_dir, handler, gpu, batch_size)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_loader.py\", line   117, in load\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   model_service.initialize(service.context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/home\/model-server\/tmp\/models\/8ff2b3de4bed4fb1bc7fe969652117ff\/handler_service.py\",   line 51, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 super().initialize(context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/default_handler_service.py\",   line 66, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self._service.validate_and_initialize(model_dir=model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py\",   line 158, in validate_and_initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self._model = self._model_fn(model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/ml\/model\/code\/fd_sl_deployment_entry_point.py\", line 149, in   model_fn\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 rgcn_model.load_state_dict(stat_dict)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\",   line 1045, in load_state_dict\r\n2021-04-09   04:53:51,251 [INFO ] W-9000-model_1-stdout   org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self.__class__.__name__, \"     \\t\".join(error_msgs)))\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError:   Error(s) in loading state_dict for HeteroRGCN:\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.weight: copying a   param with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.bias: copying a   param with shape torch.Size([2]) from checkpoint, the shape in current model   is torch.Size([16]).\r\n\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** <!-- Output of `cdk version` -->\r\n  - **Framework Version:**\r\n  - **Node.js Version:** <!-- Version of Node.js (run the command `node -v`) -->\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\nCause of this bug:\r\n\r\nBackend worker process died.\r\nSagemaker Endpoint deployment code and model training code parameter conflict on n-hidden and hidden_size.\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":1618279737000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618212282000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sagemaker Multi-GPU distributed data training where \"model.generate\" is returning empty tensors. They are unsure if this is due to the feature not yet being supported on Sagemaker Multi-GPU or if there is an issue with their own modified scripts.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/57",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":17.5,
        "Challenge_reading_time":124.73,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":32.0,
        "Challenge_repo_issue_count":1277.0,
        "Challenge_repo_star_count":169.0,
        "Challenge_repo_watch_count":22.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":107,
        "Challenge_solved_time":18.7375,
        "Challenge_title":"sagemaker endpoint fail to deploy or time out server error(0) bug",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":650,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"empty tensors from model.generate"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to understand what is the optimal way in Kedro to convert Spark dataframe coming out of one node into Pandas required as input for another node without creating a redundant conversion step.<\/p>",
        "Challenge_closed_time":1573500781436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573500781437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on the best way to convert a Spark data frame to Pandas and vice versa in Kedro without having to go through a redundant conversion step.",
        "Challenge_last_edit_time":1573553002123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58807540",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to convert Spark data frame to Pandas and back in Kedro?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":800.0,
        "Challenge_word_count":45,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1393579668636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":1450.0,
        "Poster_view_count":162.0,
        "Solution_body":"<p>Kedro currently supports 2 strategies for that:<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/04_data_catalog.html#transcoding-datasets\" rel=\"nofollow noreferrer\">Transcoding<\/a> feature<\/h3>\n\n<p>This requires one to define two <code>DataCatalog<\/code> entries for the same dataset, working with the same file in a common format (Parquet, JSON, CSV, etc.), in your <code>catalog.yml<\/code>:<\/p>\n\n<pre><code>my_dataframe@spark:\n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: data\/02_intermediate\/data.parquet\n\nmy_dataframe@pandas:\n  type: ParquetLocalDataSet\n  filepath: data\/02_intermediate\/data.parquet\n<\/code><\/pre>\n\n<p>And then use them in the pipeline like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func1, \"spark_input\", \"my_dataframe@spark\"),\n    node(my_func2, \"my_dataframe@pandas\", \"output\"),\n])\n<\/code><\/pre>\n\n<p>In this case, <code>kedro<\/code> understands that <code>my_dataframe<\/code> is the same dataset in both cases and resolves the node execution order properly. At the same time, <code>kedro<\/code> would use the <code>SparkDataSet<\/code> implementation for saving and <code>ParquetLocalDataSet<\/code> for loading, so the first node should output <code>pyspark.sql.DataFrame<\/code>, while the second node would receive a <code>pandas.Dataframe<\/code>.<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.pandas_to_spark.html\" rel=\"nofollow noreferrer\">Pandas to Spark<\/a> and <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.spark_to_pandas.html\" rel=\"nofollow noreferrer\">Spark to Pandas<\/a> node decorators<\/h3>\n\n<p><strong>Note:<\/strong> <code>Spark &lt;-&gt; Pandas<\/code> in-memory conversion is <a href=\"https:\/\/stackoverflow.com\/a\/47536675\/3364156\">notorious<\/a> for its memory demands, so this is a viable option only if the dataframe is known to be small.<\/p>\n\n<p>One can decorate the node as per the docs:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from spark import get_spark\nfrom kedro.contrib.decorators import pandas_to_spark\n\n@pandas_to_spark(spark_session)\ndef my_func3(data):\n    data.show() # data is pyspark.sql.DataFrame\n<\/code><\/pre>\n\n<p>Or even the whole pipeline:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func4, \"pandas_input\", \"some_output\"),\n    ...\n]).decorate(pandas_to_spark)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1573501243163,
        "Solution_link_count":4.0,
        "Solution_readability":16.3,
        "Solution_reading_time":31.94,
        "Solution_score_count":3.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":207.0,
        "Tool":"Kedro",
        "Challenge_type":"inquiry",
        "Challenge_summary":"convert Spark to Pandas"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":159.9731416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello :)     <\/p>\n<p>Do you have any kind of idea when Azure Machine Learning Python SDK V2 could support parallel computing? We are testing things out with the machine learning studio and we are in a bit confusing stage that should we go with the SDK V1 or V2, but seemingly the V2 is not yet supporting multiple nodes in compute clusters.    <\/p>\n<p>Best regards,    <br \/>\nTuomas<\/p>",
        "Challenge_closed_time":1657774403743,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657198500433,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about when Azure Machine Learning Python SDK V2 will support parallel computing as they are currently testing with the machine learning studio and are unsure whether to use SDK V1 or V2, as V2 does not yet support multiple nodes in compute clusters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/918129\/parallel-computing-with-python-sdk-v2",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":5.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":159.9731416667,
        "Challenge_title":"Parallel computing with Python SDK V2",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=618a88ea-c762-4907-9a08-ae41864a250e\">@Tuomas Partanen  <\/a>     <\/p>\n<p>I have a good news for you, we are testing Parallel Run Step NOW in private preview of V2.     <\/p>\n<p>For your scenario, v1 is stable and serving all production customers. v2 (through DPv2) is still in private preview, and there are some dependency on new dataset\/mltable implementation. So if you want to seriously put some production traffic, I suggest guide to v1; but if you just want to have some prototypes, v2 may be better, as v2 is growing but v1 will not. Also, V2 will have the feature you want - Parallel.     <\/p>\n<p>The estimate time is not confirmed but should be around October.    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/em>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":10.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":138.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"parallel computing support in SDK V2"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":142.9037780555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying out the sample notebooks in AWS Sagemaker, currently in the mxnet mnist example which demonstrates bringing your own code. The entry point parameter passed in when instantiating an estimator instance, only mentions the source file (mnist.py) and not a method name or any other point inside the source file.<\/p>\n\n<p>So how does aws sagemaker figure out which method to send the training data to? <\/p>",
        "Challenge_closed_time":1520316895088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1519726485703,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a challenge in AWS Sagemaker while trying to specify the entry point to the code for the mxnet mnist example. The entry point parameter only mentions the source file and not a method name or any other point inside the source file, leaving the user unsure of how AWS Sagemaker will determine which method to send the training data to.",
        "Challenge_last_edit_time":1519802441487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49006174",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":6.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":164.0026069444,
        "Challenge_title":"How is the entry point to the code specified in AWS sagemaker bring your own code?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":3327.0,
        "Challenge_word_count":82,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450260166772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1587.0,
        "Poster_view_count":540.0,
        "Solution_body":"<p>Your python script should implement a few methods like train, model_fn, transform_fn, input_fn etc. SagaMaker would call appropriate method when needed. <\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.9,
        "Solution_reading_time":5.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unclear entry point method"
    },
    {
        "Answerer_created_time":1596548853443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":40.8375211111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Information:<\/strong>\nI am loading an existing trained model.tar.gz from an S3 bucket, and want to perform a batch transform with a .csv containing the input data. The data.csv is structured in such a way that reading it into a pandas DataFrame gives me rows of complete prediction inputs.<\/p>\nNotes:\n<ul>\n<li>This is done on Amazon Sagemaker using the Python SDK<\/li>\n<li>BATCH_TRANSFORM_INPUT is the path to data.csv.<\/li>\n<li>I'm able to load the contents inside model.tar.gz and use them for inference on my local machine using tensorflow, and the logs show <code>2020-08-04 13:35:01.123557: I tensorflow_serving\/core\/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}<\/code>so the model seems to have been trained and saved properly.<\/li>\n<li>The data.csv is in the exact same format as the training data, which means one row per &quot;prediction&quot; where all columns in that row represents the different features.<\/li>\n<li>Changing the argument strategy to 'MultiRecord' gives the same error<\/li>\n<li>[path in s3] is a substitute for the real path as i don't want to reveal any bucket information.<\/li>\n<li>TensorFlow ModelServer: 2.0.0+dev.sha.ab786af<\/li>\n<li>TensorFlow Library: 2.0.2<\/li>\n<\/ul>\n<p>Where 1-5 are features, the file data.csv looks like:<\/p>\n<pre><code>+------+-------------------------+---------+----------+---------+----------+----------+\n| UNIT | TS                      | 1       | 2        | 3       | 4        | 5        |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:01:00.000 | 1.81766 | 0.178043 | 1.33607 | 25.42162 | 12.85445 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:02:00.000 | 1.81673 | 0.178168 | 1.30159 | 25.48204 | 12.87305 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:03:00.000 | 1.8155  | 0.176242 | 1.38399 | 25.35309 | 12.47222 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:04:00.000 | 1.81530 | 0.176398 | 1.39781 | 25.18216 | 12.16837 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:05:00.000 | 1.81505 | 0.151682 | 1.38451 | 25.22351 | 12.41623 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n<\/code><\/pre>\n<p>inference.py currently looks like:<\/p>\n<pre><code>def input_handler(data, context):\n    import pandas as pd\n    if context.request_content_type == 'text\/csv':\n        payload = pd.read_csv(data)\n        instance = [{&quot;dataset&quot;: payload}]\n        return json.dumps({&quot;instances&quot;: instance})\n    else:\n        _return_error(416, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or 'Unknown'))\n<\/code><\/pre>\n<h3>The problem:<\/h3>\n<p>When the following code runs in my jupyter Notebook:<\/p>\n<pre><code>sagemaker_model = Model(model_data = '[path in s3]\/savedmodel\/model.tar.gz'),  \n                        sagemaker_session=sagemaker_session,\n                        role = role,\n                        framework_version='2.0',\n                        entry_point = os.path.join('training', 'inference.py')\n                        )\n\ntf_serving_transformer = sagemaker_model.transformer(instance_count=1,\n                                                     instance_type='ml.p2.xlarge',\n                                                     max_payload=1,\n                                                     output_path=BATCH_TRANSFORM_OUTPUT_DIR,\n                                                     strategy='SingleRecord')\n\n\ntf_serving_transformer.transform(data=BATCH_TRANSFORM_INPUT, data_type='S3Prefix', content_type='text\/csv')\ntf_serving_transformer.wait()\n<\/code><\/pre>\n<p>The model seems to get loaded, but I end up with the following error:\n<code>2020-08-04T09:54:27.415:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=1, BatchStrategy=SINGLE_RECORD 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: ClientError: 400 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv:  2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: Message: 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: { &quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: \\&quot;\\&quot; Type: String is not of expected type: float&quot; } <\/code><\/p>\n<p>Error more clearly:<\/p>\n<p><strong>ClientError: 400\nMessage: {&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float&quot;}<\/strong><\/p>\n<p>If i understand this error correctly, something is wrong with the way my data is structured, so that sagemaker fails to deliver the input data to the TFS model. I suppose there is some &quot;input handling&quot; missing in my inference.py. Maybe the csv data has to somehow be translated into a compatible JSON, for TFS to use it? What exactly has to be done in input_handler() ?<\/p>\n<p>I appreciate all help, and am sorry for this confusing case. If there is any additional information needed, please ask and I'll gladly provide what I can.<\/p>",
        "Challenge_closed_time":1596696668183,
        "Challenge_comment_count":3,
        "Challenge_created_time":1596549653107,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to perform a batch transform with a .csv file containing input data on Amazon Sagemaker using the Python SDK. The data.csv is structured in such a way that reading it into a pandas DataFrame gives rows of complete prediction inputs. The user is encountering an error where the input data is not being delivered to the TensorFlow Serving (TFS) model. The error message suggests that there is something wrong with the way the data is structured, and the user suspects that there is some \"input handling\" missing in the inference.py file. The user is seeking help to understand what needs to be done in the input_handler() function to resolve the issue.",
        "Challenge_last_edit_time":1596696699176,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63248562",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.6,
        "Challenge_reading_time":65.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":40.8375211111,
        "Challenge_title":"How to handle a .csv input for use in Tensorflow Serving batch transform?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":774.0,
        "Challenge_word_count":499,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596548853443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p><strong>Solution:<\/strong> The problem was solved by saving the dataframe as .csv using the arguments header=False, index=False. This makes the saved csv not include the dataframe indexing labels. TFS accepted a clean .csv with only float values (without labels). I assume the error message <em>Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float<\/em> refers to the first cell in the csv, which if the csv was exported with labels is just an empty cell. When it got an empty string instead of a float value it got confused.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":6.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"input data not delivered"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2330.3280555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am training a resnet model on multi core tpus on kaggle. I get this error:\r\n```\r\nDumping Computation:\r\n2021-10-08 23:57:50.220206: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92108 = s32[] constant(0)\r\n2021-10-08 23:57:50.220217: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92110 = pred[] compare(s32[] %constant.92102, s32[] %constant.92108), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220227: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92109 = f32[] constant(1)\r\n2021-10-08 23:57:50.220238: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92111 = f32[] convert(s32[] %constant.92102)\r\n2021-10-08 23:57:50.220248: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92112 = f32[] divide(f32[] %constant.92109, f32[] %convert.92111)\r\n2021-10-08 23:57:50.220260: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92113 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220271: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92114 = f32[] select(pred[] %compare.92110, f32[] %divide.92112, f32[] %constant.92113)\r\n2021-10-08 23:57:50.220281: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92115 = f32[] multiply(f32[] %reduce.92107, f32[] %select.92114)\r\n2021-10-08 23:57:50.220292: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92116 = f32[] convert(f32[] %multiply.92115)\r\n2021-10-08 23:57:50.220302: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134449 = f32[1]{0} reshape(f32[] %convert.92116)\r\n2021-10-08 23:57:50.220312: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92081 = f32[1]{0} reshape(f32[] %p3148.47101)\r\n2021-10-08 23:57:50.220323: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92082 = f32[1]{0} concatenate(f32[1]{0} %reshape.92081), dimensions={0}\r\n2021-10-08 23:57:50.220333: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92083 = f32[] constant(0)\r\n2021-10-08 23:57:50.220343: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92089 = f32[] reduce(f32[1]{0} %concatenate.92082, f32[] %constant.92083), dimensions={0}, to_apply=%AddComputation.92085\r\n2021-10-08 23:57:50.220353: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92084 = s32[] constant(1)\r\n2021-10-08 23:57:50.220364: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92090 = s32[] constant(0)\r\n2021-10-08 23:57:50.220375: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92092 = pred[] compare(s32[] %constant.92084, s32[] %constant.92090), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220387: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92091 = f32[] constant(1)\r\n2021-10-08 23:57:50.220397: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92093 = f32[] convert(s32[] %constant.92084)\r\n2021-10-08 23:57:50.220408: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92094 = f32[] divide(f32[] %constant.92091, f32[] %convert.92093)\r\n2021-10-08 23:57:50.220418: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92095 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220465: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92096 = f32[] select(pred[] %compare.92092, f32[] %divide.92094, f32[] %constant.92095)\r\n2021-10-08 23:57:50.220482: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92097 = f32[] multiply(f32[] %reduce.92089, f32[] %select.92096)\r\n2021-10-08 23:57:50.220494: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92098 = f32[] convert(f32[] %multiply.92097)\r\n2021-10-08 23:57:50.220504: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134450 = f32[1]{0} reshape(f32[] %convert.92098)\r\n2021-10-08 23:57:50.220515: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92063 = f32[1]{0} reshape(f32[] %p3147.47082)\r\n2021-10-08 23:57:50.220525: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92064 = f32[1]{0} concatenate(f32[1]{0} %reshape.92063), dimensions={0}\r\n2021-10-08 23:57:50.220535: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92065 = f32[] constant(0)\r\n2021-10-08 23:57:50.220545: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92071 = f32[] reduce(f32[1]{0} %concatenate.92064, f32[] %constant.92065), dimensions={0}, to_apply=%AddComputation.92067\r\n2021-10-08 23:57:50.220556: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92066 = s32[] constant(1)\r\n2021-10-08 23:57:50.220566: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92072 = s32[] constant(0)\r\n2021-10-08 23:57:50.220576: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92074 = pred[] compare(s32[] %constant.92066, s32[] %constant.92072), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220587: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92073 = f32[] constant(1)\r\n2021-10-08 23:57:50.220598: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92075 = f32[] convert(s32[] %constant.92066)\r\n2021-10-08 23:57:50.220608: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92076 = f32[] divide(f32[] %constant.92073, f32[] %convert.92075)\r\n2021-10-08 23:57:50.220618: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92077 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220629: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92078 = f32[] select(pred[] %compare.92074, f32[] %divide.92076, f32[] %constant.92077)\r\n2021-10-08 23:57:50.220640: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92079 = f32[] multiply(f32[] %reduce.92071, f32[] %select.92078)\r\n2021-10-08 23:57:50.220650: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92080 = f32[] convert(f32[] %multiply.92079)\r\n2021-10-08 23:57:50.220660: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134451 = f32[1]{0} reshape(f32[] %convert.92080)\r\n2021-10-08 23:57:50.220670: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92045 = f32[1]{0} reshape(f32[] %p3146.47063)\r\n2021-10-08 23:57:50.220680: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92046 = f32[1]{0} concatenate(f32[1]{0} %reshape.92045), dimensions={0}\r\n2021-10-08 23:57:50.220691: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92047 = f32[] constant(0)\r\n2021-10-08 23:57:50.220701: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92053 = f32[] reduce(f32[1]{0} %concatenate.92046, f32[] %constant.92047), dimensions={0}, to_apply=%AddComputation.92049\r\n2021-10-08 23:57:50.220711: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92048 = s32[] constant(1)\r\n2021-10-08 23:57:50.220722: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92054 = s32[] constant(0)\r\n2021-10-08 23:57:50.220733: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92056 = pred[] compare(s32[] %constant.92048, s32[] %constant.92054), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220759: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92055 = f32[] constant(1)\r\n2021-10-08 23:57:50.220770: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92057 = f32[] convert(s32[] %constant.92048)\r\n2021-10-08 23:57:50.220781: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92058 = f32[] divide(f32[] %constant.92055, f32[] %convert.92057)\r\n2021-10-08 23:57:50.220792: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92059 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220803: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92060 = f32[] select(pred[] %compare.92056, f32[] %divide.92058, f32[] %constant.92059)\r\n2021-10-08 23:57:50.220813: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92061 = f32[] multiply(f32[] %reduce.92053, f32[] %select.92060)\r\n2021-10-08 23:57:50.220823: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92062 = f32[] convert(f32[] %multiply.92061)\r\n2021-10-08 23:57:50.220833: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134452 = f32[1]{0} reshape(f32[] %convert.92062)\r\n2021-10-08 23:57:50.220843: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92027 = f32[1]{0} reshape(f32[] %p3145.47044)\r\n2021-10-08 23:57:50.220854: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92028 = f32[1]{0} concatenate(f32[1]{0} %reshape.92027), dimensions={0}\r\n2021-10-08 23:57:50.220865: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92029 = f32[] constant(0)\r\n2021-10-08 23:57:50.220876: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92035 = f32[] reduce(f32[1]{0} %concatenate.92028, f32[] %constant.92029), dimensions={0}, to_apply=%AddComputation.92031\r\n2021-10-08 23:57:50.220888: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92030 = s32[] constant(1)\r\n2021-10-08 23:57:50.220899: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92036 = s32[] constant(0)\r\n2021-10-08 23:57:50.220910: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92038 = pred[] compare(s32[] %constant.92030, s32[] %constant.92036), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220921: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92037 = f32[] constant(1)\r\n2021-10-08 23:57:50.220932: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92039 = f32[] convert(s32[] %constant.92030)\r\n2021-10-08 23:57:50.220942: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92040 = f32[] divide(f32[] %constant.92037, f32[] %convert.92039)\r\n2021-10-08 23:57:50.220953: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92041 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220964: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92042 = f32[] select(pred[] %compare.92038, f32[] %divide.92040, f32[] %constant.92041)\r\n2021-10-08 23:57:50.220975: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92043 = f32[] multiply(f32[] %reduce.92035, f32[] %select.92042)\r\n2021-10-08 23:57:50.220986: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92044 = f32[] convert(f32[] %multiply.92043)\r\n```\r\nThis text goes on and on for several pages.\r\n\r\nThe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nNote that this only happens when using a logger (wandb or comet.ml) and everything works fine when I do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> I have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### To Reproduce\r\n\r\nSee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-comet-ml) with comet.ml.\r\n\r\n### Expected behavior\r\n\r\nTraining should run normally with no issues and logging should work.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1+cpu\r\n\t- pytorch-lightning: 1.4.4\r\n\t- tqdm:              4.62.1\r\n\t- pytorch-xla  1.7\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\r\n### Additional context\r\nNone\r\n\n\ncc @kaushikb11 @rohitgr7 @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire",
        "Challenge_closed_time":1642181493000,
        "Challenge_comment_count":11,
        "Challenge_created_time":1633792312000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"Upgrading from pytorch-lightning 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments, one for each GPU, when running a ddp multi-gpu experiment on a SLURM cluster. Only one of them logs any metrics, the others just sit. The expected behavior is a single comet experiment for a single call to trainer.fit(). The user is unable to provide a reproducible example due to the inability to do multi-gpu ddp in colab and the need for a comet authentication.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9879",
        "Challenge_link_count":3,
        "Challenge_participation_count":11,
        "Challenge_readability":16.4,
        "Challenge_reading_time":156.3,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":236,
        "Challenge_solved_time":2330.3280555556,
        "Challenge_title":"\"dumps computation\" at the start of validation loop when using wandb\/comet.ml logger during multi-core tpu training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":786,
        "Discussion_body":"Thanks @rusty-electron for opening the issue.\r\n\r\nIs there any more information before the line \"Dumping Computation:\"?  No error output, just the logs from wandb logger and the progressbars created by `tqdm`. Dear @rusty-electron,\r\n\r\nWe are working with the Wandb Team on a large fix. Hopefully it will work for this use-case too.\r\n\r\nWe will keep you updated.\r\n\r\nBest,\r\nT.C @tchaton Thanks for the info. I shall be looking out for the fix. @tchaton Is there an issue to track the Wandb updates? @borisdayma Any idea ?\r\n It's actually a few different PR's ongoing.\r\nI think we should have something next week that will handle these scenarios. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n @borisdayma Did it end up being an issue on the wandb side? I didn't follow the development lately. If it's still work in progress, could you point us to a PR or issue? Thx in advance <3  We're actually still in the process of updating the way multiprocess is supported.\r\nThere's been good progress, just a few edge cases to handle. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_type":"anomaly",
        "Challenge_summary":"multiple comet experiments on GPUs"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":36.6310980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a pandas's DataFrame created by:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TB_HISTORICO_MODELO = pd.read_sql(&quot;&quot;&quot;select DAT_INICIO_SEMANA_PLAN\n,COD_NEGOCIO\n,VENDA\n,LUCRO\n,MODULADO\n,RUPTURA\n,QTD_ESTOQUE_MEDIO\n,PECAS from TB&quot;&quot;&quot;, cursor)\n\nTB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;] = pd.to_datetime(TB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;])\n\ndataset = TB_HISTORICO_MODELO[TB_HISTORICO_MODELO['COD_NEGOCIO']=='A101'].drop(columns=['COD_NEGOCIO']) .reset_index(drop=True)\n<\/code><\/pre>\n<p>Everything look like right.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; dataset.dtypes\nDAT_INICIO_SEMANA_PLAN    datetime64[ns]\nVENDA                            float64\nLUCRO                            float64\nMODULADO                           int64\nRUPTURA                            int64\nQTD_ESTOQUE_MEDIO                  int64\nPECAS                            float64\ndtype: object\n<\/code><\/pre>\n<p>But when I rum this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>#%% Create the AutoML Config file and run the experiment on Azure\n\nfrom azureml.train.automl import AutoMLConfig\n\ntime_series_settings = {\n   'time_column_name': 'DAT_INICIO_SEMANA_PLAN',\n   'max_horizon': 14,\n   'country_or_region': 'BR',\n   'target_lags': 'auto'\n}\n\nautoml_config = AutoMLConfig(task='forecasting',\n                            primary_metric='normalized_root_mean_squared_error',\n                            blocked_models=['ExtremeRandomTrees'],\n                            experiment_timeout_minutes=30,\n                            training_data=dataset,\n                            label_column_name='VENDA',\n                            compute_target = compute_cluster,\n                            enable_early_stopping=True,\n                            n_cross_validations=3,\n                            # max_concurrent_iterations=4,\n                            # max_cores_per_iteration=-1,\n                            verbosity=logging.INFO,\n                            **time_series_settings)\n\nremote_run = Experimento.submit(automl_config, show_output=True)\n<\/code><\/pre>\n<p>I get the message<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; remote_run = Experimento.submit(automl_config, show_output=True)\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/core\/experiment.py&quot;, line 219, in submit\n    run = submit_func(config, self.workspace, self.name, **kwargs)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 92, in _automl_static_submit\n    automl_config_object._validate_config_settings(workspace)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 1775, in _validate_config_settings\n    supported_types=&quot;, &quot;.join(SupportedInputDatatypes.REMOTE_RUN_SCENARIO)\nazureml.train.automl.exceptions.ConfigException: ConfigException:\n        Message: Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]\n        InnerException: None\n        ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/AutoMLConfig&quot;,\n        &quot;target&quot;: &quot;training_data&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;BadArgument&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;ArgumentInvalid&quot;,\n                &quot;inner_error&quot;: {\n                    &quot;code&quot;: &quot;InvalidInputDatatype&quot;\n                }\n            }\n        }\n    }\n}\n\n<\/code><\/pre>\n<p>Where is wrong?<\/p>\n<p>documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig<\/a><\/p>",
        "Challenge_closed_time":1603004866532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602873387197,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run an experiment on Azure using AutoMLConfig. The error message states that the input of type 'Unknown' is not supported and provides a list of supported types. The user is unsure of what is causing the error and seeks assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64394661",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":30.2,
        "Challenge_reading_time":55.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":36.5220375,
        "Challenge_title":"Erro InvalidInputDatatype: Input of type 'Unknown' is not supported in azure (azureml.train.automl)",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":382.0,
        "Challenge_word_count":220,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522634496792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, RJ, Brasil",
        "Poster_reputation_count":264.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">Configure AutoML Doc<\/a> says:<\/p>\n<blockquote>\n<p>For remote experiments, training data must be accessible from the remote compute. AutoML only accepts Azure Machine Learning TabularDatasets when working on a remote compute.<\/p>\n<\/blockquote>\n<p>It looks as if your <code>dataset<\/code> object is a Pandas DataFrame, when it should really be an Azure ML <code>Dataset<\/code>. Check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">this doc<\/a> on creating Datasets.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1603005259150,
        "Solution_link_count":2.0,
        "Solution_readability":16.5,
        "Solution_reading_time":9.99,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported input type"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":9.1737127778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to train a neural network (Tensorflow) on AWS. I have some AWS credits. From my understanding AWS SageMaker is the one best for the job. I managed to load the Jupyter Lab console on SageMaker and tried to find a GPU kernel since, I know it is the best for training neural networks. However, I could not find such kernel. <\/p>\n\n<p>Would anyone be able to help in this regard.<\/p>\n\n<p>Thanks &amp; Best Regards<\/p>\n\n<p>Michael<\/p>",
        "Challenge_closed_time":1585261947503,
        "Challenge_comment_count":1,
        "Challenge_created_time":1585228922137,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to train a neural network on AWS using SageMaker, but is unable to find a GPU kernel which is necessary for training neural networks. They are seeking help in finding a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60868257",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":3.7,
        "Challenge_reading_time":5.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":9.1737127778,
        "Challenge_title":"AWS SageMaker on GPU",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":13664.0,
        "Challenge_word_count":81,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484485648903,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Melbourne Australia",
        "Poster_reputation_count":605.0,
        "Poster_view_count":133.0,
        "Solution_body":"<p>You train models on GPU in the SageMaker ecosystem via 2 different components:<\/p>\n\n<ol>\n<li><p>You can instantiate a GPU-powered <strong><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"noreferrer\">SageMaker Notebook Instance<\/a><\/strong>, for example <code>p2.xlarge<\/code> (NVIDIA K80) or <code>p3.2xlarge<\/code> (NVIDIA V100). This is convenient for interactive development - you have the GPU right under your notebook and can run code on the GPU interactively and monitor the GPU via <code>nvidia-smi<\/code> in a terminal tab - a great development experience. However when you develop directly from a GPU-powered machine, there are times when you may not use the GPU. For example when you write code or browse some documentation. All that time you pay for a GPU that sits idle. In that regard, it may not be the most cost-effective option for your use-case. <\/p><\/li>\n<li><p>Another option is to use a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\"><strong>SageMaker Training Job<\/strong><\/a> running on a GPU instance. This is a preferred option for training, because training metadata (data and model path, hyperparameters, cluster specification, etc) is persisted in the SageMaker metadata store, logs and metrics stored in Cloudwatch and the instance automatically shuts down itself at the end of training. Developing on a small CPU instance and launching training tasks using SageMaker Training API will help you make the most of your budget, while helping you retain metadata and artifacts of all your experiments. You can see <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/using-tensorflow-eager-execution-with-amazon-sagemaker-script-mode\/\" rel=\"noreferrer\">here a well documented TensorFlow example<\/a><\/p><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.8,
        "Solution_reading_time":23.39,
        "Solution_score_count":20.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":229.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing GPU kernel"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":8.7381941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use the pipeline functionality of dvc in a git repository. The data is managed otherwise and should not be versioned by dvc. The only functionality which is needed is that dvc reproduces the needed steps of the pipeline when <code>dvc repro<\/code> is called. Checking out the repository on a new system should lead to an 'empty' repository, where none of the pipeline steps are stored.<\/p>\n<p>Thus, - if I understand correctly - there is no need to track the dvc.lock file in the repository. However, adding dvc.lock to the .gitginore file leads to an error message:<\/p>\n<pre><code>ERROR: 'dvc.lock' is git-ignored.\n<\/code><\/pre>\n<p>Is there any way to disable the dvc.lock in .gitignore check for this usecase?<\/p>",
        "Challenge_closed_time":1624393487732,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624362030233,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to use the pipeline functionality of dvc in a git repository without versioning the data by dvc. The user needs dvc to reproduce the pipeline steps when called, and checking out the repository on a new system should lead to an 'empty' repository. However, adding dvc.lock to the .gitignore file leads to an error message, and the user is looking for a way to disable the dvc.lock in .gitignore check for this use case.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68082912",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":9.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":8.7381941667,
        "Challenge_title":"git-ignore dvc.lock in repositories where only the DVC pipelines are used",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":493.0,
        "Challenge_word_count":126,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542537900087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":147.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). See also helper <code>dvc stage add -O<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#options\" rel=\"nofollow noreferrer\">big O<\/a>, alias of <code>--outs-no-cache<\/code>).<\/p>\n<p>And the same for initial data dependencies, you can <code>dvc add --no-commit<\/code> them (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#options\" rel=\"nofollow noreferrer\">ref<\/a>).<\/p>\n<p>You do want to track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc.lock<\/a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.<\/p>\n<p>You'll be responsible for placing the right data files\/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro<\/code> or <code>dvc exp run<\/code> to behave as expected. <code>dvc checkout<\/code> won't be able to help you.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.9,
        "Solution_reading_time":17.17,
        "Solution_score_count":3.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":141.0,
        "Tool":"DVC",
        "Challenge_type":"inquiry",
        "Challenge_summary":"disable dvc.lock check"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":123.815,
        "Challenge_answer_count":0,
        "Challenge_body":"Forgot to create an issue in recent days.\r\nWhen tested with ```resume``` argument in ```WandBCallbacks```, i encountered this error. Here's the log:\r\n```python\r\n\r\n[Errno 2] No such file or directory: 'main'\r\n\/content\/main\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:override:78 - Overriding configuration...\r\n2022-04-04 12:21:56 | INFO     | classification\/pipeline.py:__init__:51 - {\r\n    \"global\": {\r\n        \"exp_name\": null,\r\n        \"exist_ok\": false,\r\n        \"debug\": true,\r\n        \"cfg_transform\": \"configs\/classification\/transform.yaml\",\r\n        \"save_dir\": \"\/content\/main\/runs\",\r\n        \"device\": \"cuda:0\",\r\n        \"use_fp16\": true,\r\n        \"pretrained\": null,\r\n        \"resume\": null\r\n    },\r\n    \"trainer\": {\r\n        \"name\": \"SupervisedTrainer\",\r\n        \"args\": {\r\n            \"num_iterations\": 2000,\r\n            \"clip_grad\": 10.0,\r\n            \"evaluate_interval\": 1,\r\n            \"print_interval\": 20,\r\n            \"save_interval\": 500\r\n        }\r\n    },\r\n    \"model\": {\r\n        \"name\": \"BaseTimmModel\",\r\n        \"args\": {\r\n            \"name\": \"convnext_tiny\",\r\n            \"from_pretrained\": true,\r\n            \"num_classes\": 180\r\n        }\r\n    },\r\n    \"loss\": {\r\n        \"name\": \"FocalLoss\"\r\n    },\r\n    \"callbacks\": [\r\n        {\r\n            \"name\": \"LoggerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"CheckpointCallbacks\",\r\n            \"args\": {\r\n                \"best_key\": \"bl_acc\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"VisualizerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"TensorboardCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"WandbCallbacks\",\r\n            \"args\": {\r\n                \"username\": \"lannguyen\",\r\n                \"project_name\": \"theseus_classification\",\r\n                \"resume\": true\r\n            }\r\n        }\r\n    ],\r\n    \"metrics\": [\r\n        {\r\n            \"name\": \"Accuracy\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"BalancedAccuracyMetric\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"F1ScoreMetric\",\r\n            \"args\": {\r\n                \"average\": \"weighted\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"ConfusionMatrix\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"ErrorCases\",\r\n            \"args\": null\r\n        }\r\n    ],\r\n    \"optimizer\": {\r\n        \"name\": \"AdamW\",\r\n        \"args\": {\r\n            \"lr\": 0.001,\r\n            \"weight_decay\": 0.0005,\r\n            \"betas\": [\r\n                0.937,\r\n                0.999\r\n            ]\r\n        }\r\n    },\r\n    \"scheduler\": {\r\n        \"name\": \"SchedulerWrapper\",\r\n        \"args\": {\r\n            \"scheduler_name\": \"cosine2\",\r\n            \"t_initial\": 7,\r\n            \"t_mul\": 0.9,\r\n            \"eta_mul\": 0.9,\r\n            \"eta_min\": 1e-06\r\n        }\r\n    },\r\n    \"data\": {\r\n        \"dataset\": {\r\n            \"train\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/train\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/val\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            }\r\n        },\r\n        \"dataloader\": {\r\n            \"train\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": true,\r\n                    \"shuffle\": false,\r\n                    \"collate_fn\": {\r\n                        \"name\": \"MixupCutmixCollator\",\r\n                        \"args\": {\r\n                            \"mixup_alpha\": 0.4,\r\n                            \"cutmix_alpha\": 1.0,\r\n                            \"weight\": [\r\n                                0.2,\r\n                                0.2\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"sampler\": {\r\n                        \"name\": \"BalanceSampler\",\r\n                        \"args\": null\r\n                    }\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": false,\r\n                    \"shuffle\": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:load_yaml:36 - Loading config from configs\/classification\/transform.yaml...\r\n2022-04-04 12:21:57 | DEBUG    | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - Calculating class distribution...\r\nDownloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth\r\nTraceback (most recent call last):\r\n  File \"\/content\/main\/configs\/classification\/train.py\", line 9, in <module>\r\n    train_pipeline = Pipeline(opts)\r\n  File \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__\r\n    registry=CALLBACKS_REGISTRY\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in <listcomp>\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively\r\n    return registry.get(config['name'])(**args, **kwargs)\r\nTypeError: type object got multiple values for keyword argument 'resume'\r\n```\r\n\r\nI guess because of the ```resume``` arg is both repeated in ```global``` and ```WandBCallbacks```. Maybe it also happens with ```Tensorboard```.",
        "Challenge_closed_time":1649731891000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1649286157000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create an index name using wandb and is looking for a solution to modify the name to \"modelname + save_folder_name\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kaylode\/theseus\/issues\/33",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.5,
        "Challenge_reading_time":51.74,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":51.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":123.815,
        "Challenge_title":"Resume error in WandB.",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":326,
        "Discussion_body":"I will look into this soon. Crazily busy at the moment. This is not a bug, this happended because WandbCallbacks were used in the wrong way\r\n\r\nIn `pipeline.yaml`\r\n```python\r\n\"name\": \"WandbCallbacks\",\r\n\"args\": {\r\n    \"username\": \"lannguyen\",\r\n    \"project_name\": \"theseus_classification\",\r\n    \"resume\": true # <----- you didnt have to specify this\r\n}\r\n```\r\n\r\nThe repo havent been fully-well documented therefore it will be confusing sometimes.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error creating index name"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":210.9636111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen we use the basic mlflow logging via `with mlflow.start_run(): ...` context manager, we get a better supplementary info about the run (git commit sha, user, filename) rendered in the Tracking UI ([system tags](https:\/\/mlflow.org\/docs\/latest\/tracking.html#system-tags))\r\n\r\nBut when we use `MLFlowLogger` as a logger in pytorch_lightning, this info is not logged. As a user, I'd like to have a mirrored functionality out-of-the-box.\r\n\r\nI inspected the `start_run()` method of mlflow and deduced that the only thing is left while creating the run via MLflowClient is to add `resolve_tags` from the `context` package:\r\n```python\r\n# pytorch_lightning\/loggers\/mlflow.py\r\nfrom mlflow.tracking.context.registry import resolve_tags\r\n...\r\n    def experiment(self) -> MLflowClient:\r\n        if self._run_id is None:\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n```\r\n\r\nI think it's a better idea to add those tags internally (meaning not to expect users doing that manually) as first - it's as seamless as in the default API, secondly - it's the pytorch_lightning that manages the mlflow's run anyways.\r\n\r\n**PR is following ...**",
        "Challenge_closed_time":1617875123000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617115654000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug where using log_gpu_memory with MLFLow logger causes an error due to an unsupported metric name. The expected behavior is for log_gpu_memory to log gpu memory correctly when using an MLFlow logger. The issue was reproduced in a Colab environment with specific versions of CUDA, packages, and system.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6745",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.5,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":210.9636111111,
        "Challenge_title":"mlflow run context is not logged when using MLFlowLogger",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported metric name"
    },
    {
        "Answerer_created_time":1444758849803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11962.0,
        "Answerer_view_count":960.0,
        "Challenge_adjusted_solved_time":3.0605805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook? <\/p>\n\n<p>Like you would do a \"select * from tablename limit 10\" in an RDS using SQL, similarly is there a way to get a sense of the graph data through Jupyter Notebook?<\/p>",
        "Challenge_closed_time":1584902412783,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584891394693,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook, similar to how one can view data in an RDS using SQL.",
        "Challenge_last_edit_time":1637708419888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60801292",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.0605805556,
        "Challenge_title":"View Neptune Graph Schema using Jupyter notebook",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":831.0,
        "Challenge_word_count":56,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584891066787,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:<\/p>\n\n<pre><code>g.V().groupCount().by(label)\ng.E().groupCount().by(label)\n<\/code><\/pre>\n\n<p>If you have a very large graph try putting something like <code>limit(100000)<\/code> before the <code>groupCount<\/code> step.<\/p>\n\n<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()<\/code> terminal step to the queries as in:<\/p>\n\n<pre><code>g.V().groupCount().by(label).next()\ng.E().groupCount().by(label).next()\n<\/code><\/pre>\n\n<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called \"person\".<\/p>\n\n<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()\n<\/code><\/pre>\n\n<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.81,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":162.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"view Neptune schema in Jupyter"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.1764397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How is the output of Fisher Linear Discriminant Analysis experiment interpreted now that the column labels in the output are replaced with Col1, Col2, Col3.......etc? How can the model be used to predict clusters of other input data as deployed web service requires even the dependent valuable(the same same ones we wish to predict)?<\/p>",
        "Challenge_closed_time":1621895240423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621855005240,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in interpreting the output of Fisher Linear Discriminant Analysis experiment as the column labels are replaced with Col1, Col2, Col3, etc. They are also unsure how to use the model to predict clusters of other input data as the deployed web service requires the same dependent variables that they wish to predict.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/407053\/fisher-linear-discriminant-analysis-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":4.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":11.1764397222,
        "Challenge_title":"Fisher Linear Discriminant Analysis Azure",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Are you referring to the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/latent-dirichlet-allocation#lda-transformed-dataset\">categories<\/a> generated from LDA module? If so, then that's expected. LDA is an unsupervised technique, it groups words into categories\/topics and it's up to the analyst to interpret it by observing the results and transforming the output dataset accordingly. Here's are some <a href=\"https:\/\/gallery.azure.ai\/browse?s=lda\">examples<\/a> of LDA approach in Azure AI Gallery. Hope this helps.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":7.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"uninterpretable output, unable to predict"
    },
    {
        "Answerer_created_time":1452814040316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":21.0570775,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The Import Data module for Azure Table documention can be found here: <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/mt674699\" rel=\"nofollow\">https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/mt674699<\/a><\/p>\n\n<p>In there it mentions that:<\/p>\n\n<blockquote>\n  <p>The Import Data module does not support filtering as data is being read. The exception is reading from data feeds, which sometimes allow you to specify a filter condition as part of the feed URL.<\/p>\n<\/blockquote>\n\n<p>There is a large amount of data in our table storage and it is not feasible to re-download the entire data set each time we run the experiment. I'm aware that there is the option to cache the data, however there is new data constantly being inserted and we would like to be able to use the new data whenever the experiment is run.<\/p>\n\n<p>Is there an alternative to the Import Data module that we could use to get table storage data with an ODATA query instead?<\/p>",
        "Challenge_closed_time":1474678989116,
        "Challenge_comment_count":0,
        "Challenge_created_time":1474603183637,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in importing data from Azure Table Storage into Azure Machine Learning Studio with an ODATA query. The Import Data module does not support filtering as data is being read, and caching the data is not feasible due to constantly inserting new data. The user is seeking an alternative to the Import Data module that supports ODATA query.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39652384",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":21.0570775,
        "Challenge_title":"How can I import into Azure machine learning studio from azure table storage with ODATA query?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":324.0,
        "Challenge_word_count":159,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450652266692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brisbane, Australia",
        "Poster_reputation_count":802.0,
        "Poster_view_count":152.0,
        "Solution_body":"<p>There is no generic way to incrementally update a dataset. <\/p>\n\n<p>However, depending on what you want to do with the data, there are different options for adding new data:<\/p>\n\n<p>The Add Rows module effectively concatenates two datasets. So you could use the old, cached dataset on the left-hand input and add the new data on the right-hand input. That way you only have to read in the new data.\nHowever, you would have to create some complex logic for figuring out which rows were new and old, and then maintain that outside Azure ML.<\/p>\n\n<p>You could create an OData feed based on table storage, to enable filtering and get the new data that way. Just be aware that right now only public feeds are supported. And you would have to use Join or Add Rows to recombine the old and new data as described above. <\/p>\n\n<p>You might also look into ways of using the <a href=\"https:\/\/blog.maartenballiauw.be\/post\/2012\/10\/08\/what-partitionkey-and-rowkey-are-for-in-windows-azure-table-storage.html\" rel=\"nofollow\">table names<\/a>, partitions, and rowkeys to chunk your data. <\/p>\n\n<p>If you are retraining a model and you want to update your feature statistics, the <a href=\"https:\/\/msdn.microsoft.com\/library\/dn913056.aspx\" rel=\"nofollow\">Learning with Counts<\/a> modules support incremental updates of count-based features. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":16.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":196.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"alternative to Import Data module"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":60.6923205556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm preparing for the Azure Machine Learning exam, and here is a question confuses me.<\/p>\n<blockquote>\n<p>You are designing an Azure Machine Learning workflow. You have a\ndataset that contains two million large digital photographs. You plan\nto detect the presence of trees in the photographs. You need to ensure\nthat your model supports the following:<\/p>\n<p>Solution: You create a Machine\nLearning experiment that implements the Multiclass Decision Jungle\nmodule. Does this meet the goal?<\/p>\n<p>Solution: You create a Machine Learning experiment that implements the\nMulticlass Neural Network module. Does this meet the goal?<\/p>\n<\/blockquote>\n<p>The answer for the first question is No while for second is Yes, but I cannot understand why Multiclass Decision Jungle doesn't meet the goal since it is a classifier. Can someone explain to me the reason?<\/p>",
        "Challenge_closed_time":1559895314607,
        "Challenge_comment_count":3,
        "Challenge_created_time":1559676822253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is preparing for the Azure Machine Learning exam and is confused about a question related to image classification. The user needs to detect the presence of trees in a dataset of two million large digital photographs and is unsure why the Multiclass Decision Jungle module does not meet the goal while the Multiclass Neural Network module does. The user is seeking an explanation for this.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56450223",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.2,
        "Challenge_reading_time":11.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":60.6923205556,
        "Challenge_title":"Image Classification in Azure Machine Learning",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":538.0,
        "Challenge_word_count":137,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449453613688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":388.0,
        "Poster_view_count":322.0,
        "Solution_body":"<p>I suppose that this is part of a series of questions that present the same scenario. And there should be definitely some constraints in the scenario. \nMoreover if you have a look on the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/multiclass-neural-network\" rel=\"nofollow noreferrer\">Azure documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>However, recent research has shown that deep neural networks (DNN)\n  with many layers can be very effective in complex tasks such as image\n  or speech recognition. The successive layers are used to model\n  increasing levels of semantic depth.<\/p>\n<\/blockquote>\n\n<p>Thus, Azure recommends using Neural Networks for image classification. Remember, that the goal of the exam is to test your capacity to design data science solution <strong>using Azure<\/strong> so better to use their official documentation as a reference.<\/p>\n\n<p>And comparing to the other solutions:<\/p>\n\n<ol>\n<li>You create an Azure notebook that supports the Microsoft Cognitive\nToolkit.<\/li>\n<li>You create a Machine Learning experiment that implements\nthe Multiclass Decision Jungle module.<\/li>\n<li>You create an endpoint to the\nComputer vision API. <\/li>\n<li>You create a Machine Learning experiment that\nimplements the Multiclass Neural Network module.<\/li>\n<li>You create an Azure\nnotebook that supports the Microsoft Cognitive Toolkit.<\/li>\n<\/ol>\n\n<p>There are only 2 Azure ML Studio modules, and as the question is about constructing a <strong>workflow<\/strong> I guess we can only choose between them. (CNTK is actually the best solution as it allows constructing a deep neural network with ReLU whereas AML Studio doesn't, and API call is not about data science at all). <\/p>\n\n<p>Finally, I do agree with the other contributors that the question is absurd. Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":22.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":255.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"difference in module performance"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":3.0591802778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to use my own sickit-learn ML model with SageMaker using the github example.<\/p>\n\n<p>The python code is below : <\/p>\n\n<pre><code># Define IAM role import boto3 \nimport re \nimport os \nimport numpy as np \nimport pandas as pd \nfrom sagemaker import get_execution_role \nimport sagemaker as sage from time \nimport gmtime, strftime \nrole = get_execution_role()\n\ness =  sage.Session()\naccount = sess.boto_session.client('sts').get_caller_identity()['Account']\nregion = sess.boto_session.region_name\nimage = '{}.dkr.ecr.{}.amazonaws.com\/decision-trees-sample:latest'.format(account, region)\n\n\noutput_path=\"s3:\/\/output\"\n\nsess\n\ntree = sage.estimator.Estimator(image,\n                      role, 1, 'ml.c4.2xlarge',\n                     output_path='s3-eu-west-1.amazonaws.com\/output',\n                    sagemaker_session=sess)\n\ntree.fit(\"s3:\/\/output\/iris.csv\")\n<\/code><\/pre>\n\n<p>But I get this error : <\/p>\n\n<blockquote>\n  <p>INFO:sagemaker:Creating training-job with name:\n  decision-trees-sample-2018-04-24-13-13-38-281<\/p>\n  \n  <p>--------------------------------------------------------------------------- ClientError                               Traceback (most recent call\n  last)  in ()\n       14                     sagemaker_session=sess)\n       15 \n  ---> 16 tree.fit(\"s3:\/\/inteldatastore-cyrine\/iris.csv\")<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py\n  in fit(self, inputs, wait, logs, job_name)\n      161             self.output_path = 's3:\/\/{}\/'.format(self.sagemaker_session.default_bucket())\n      162 \n  --> 163         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n      164         if wait:\n      165             self.latest_training_job.wait(logs=logs)<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py\n  in start_new(cls, estimator, inputs)\n      336                                           input_config=input_config, role=role,\n  job_name=estimator._current_job_name,\n      337                                           output_config=output_config, resource_config=resource_config,\n  --> 338                                           hyperparameters=hyperparameters, stop_condition=stop_condition)\n      339 \n      340         return cls(estimator.sagemaker_session, estimator._current_job_name)<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py\n  in train(self, image, input_mode, input_config, role, job_name,\n  output_config, resource_config, hyperparameters, stop_condition)\n      242         LOGGER.info('Creating training-job with name: {}'.format(job_name))\n      243         LOGGER.debug('train request: {}'.format(json.dumps(train_request, indent=4)))\n  --> 244         self.sagemaker_client.create_training_job(**train_request)\n      245 \n      246     def create_model(self, name, role, primary_container):<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py\n  in _api_call(self, *args, **kwargs)\n      312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n      313             # The \"self\" in this scope is referring to the BaseClient.\n  --> 314             return self._make_api_call(operation_name, kwargs)\n      315 \n      316         _api_call.<strong>name<\/strong> = str(py_operation_name)<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/botocore\/client.py\n  in _make_api_call(self, operation_name, api_params)\n      610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n      611             error_class = self.exceptions.from_code(error_code)\n  --> 612             raise error_class(parsed_response, operation_name)\n      613         else:\n      614             return parsed_response<\/p>\n  \n  <p>ClientError: An error occurred (AccessDeniedException) when calling\n  the CreateTrainingJob operation: User:\n  arn:aws:sts::307504647302:assumed-role\/default\/SageMaker is\n  not authorized to perform: sagemaker:CreateTrainingJob on resource:\n  arn:aws:sagemaker:eu-west-1:307504647302:training-job\/decision-trees-sample-2018-04-24-13-13-38-281<\/p>\n<\/blockquote>\n\n<p>Can you help me to resolve the problem?<\/p>\n\n<p>Thank you<\/p>",
        "Challenge_closed_time":1524587840612,
        "Challenge_comment_count":1,
        "Challenge_created_time":1524576827563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run a training job with AWS SageMaker. The error message indicates that the user is not authorized to perform the sagemaker:CreateTrainingJob on the resource. The user has shared the python code used for the training job and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50003050",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":19.9,
        "Challenge_reading_time":48.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":3.0591802778,
        "Challenge_title":"error to run training job with aws Sagemaker",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":4369.0,
        "Challenge_word_count":263,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518617852856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":495.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>Looks like you don't have access to the resource <\/p>\n\n<pre><code>arn:aws:sagemaker:eu-west-1:307504647302:training-job\/decision-trees-sample-2018-04-24-13-13-38-281\n<\/code><\/pre>\n\n<p>Can you check if the resource url is correct and the proper permissions are set in the security group. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.0,
        "Solution_reading_time":3.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unauthorized to create training job"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":184.4022222222,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n*Description:*\r\n\r\nAn apt-get error is seen in `sagemaker-local-test` builds as below. This is because `apt-get` process is already running and in active state.\r\n\r\n```\r\nE: Could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: Resource temporarily unavailable)\r\n--\r\n294 | E: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it?\r\n```\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1598551848000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597888000000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to train on multiple GPUs in Sagemaker Notebook Terminal using Autogluon 0.4.0 TextPredictor. The user gets an error in spawning multiprocessing. However, when the user trains on a single GPU within the same instance and setup, it trains without any problem. The user expects to train across all 4 GPUs in the p3.8xl instance with no errors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/517",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":112.0,
        "Challenge_repo_fork_count":384.0,
        "Challenge_repo_issue_count":3057.0,
        "Challenge_repo_star_count":725.0,
        "Challenge_repo_watch_count":45.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":184.4022222222,
        "Challenge_title":"[bug] apt-get failure in sagemaker-local-test builds",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"multiprocessing error on multiple GPUs"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.2800777778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I\u2019ve been wanting to export the data on GPU usage for my algorithm, but when I export the CSV file there is a single line which does not contain all the data.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b17a445f7fc35f370a78c8a4d9ea242ef5797b42.png\" data-download-href=\"\/uploads\/short-url\/pk2t25q25HlYQzj2LOJ1W7xb1e2.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b17a445f7fc35f370a78c8a4d9ea242ef5797b42.png\" alt=\"image\" data-base62-sha1=\"pk2t25q25HlYQzj2LOJ1W7xb1e2\" width=\"690\" height=\"35\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b17a445f7fc35f370a78c8a4d9ea242ef5797b42_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1893\u00d797 7.21 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>For some reason all other data exports work, but any data which has to do with the GPU does not. I have 4 GPUs. The plot shows the right data:<\/p>\n<p>I could also not find the complete data using the API. Is this a bug?<\/p>\n<p>Best,<\/p>\n<p>Mario<\/p>",
        "Challenge_closed_time":1649362551468,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649321943188,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while exporting GPU utilization and power usage data for their algorithm. The CSV file they export contains only a single line that does not have all the data. The user has 4 GPUs, and the plot shows the correct data. The user also could not find the complete data using the API and is wondering if this is a bug.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/exporting-gpu-utilization-power-usage-data\/2194",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":14.5,
        "Challenge_reading_time":19.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":11.2800777778,
        "Challenge_title":"Exporting GPU utilization, power usage data",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":641.0,
        "Challenge_word_count":118,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/meerio\">@meerio<\/a>,<\/p>\n<p>You should be able to retrieve your System Metrics history from the run using the following line of code:<\/p>\n<pre><code class=\"lang-python\">metrics = run.history(stream='events')\n<\/code><\/pre>\n<p>where <code>run<\/code> is a <code>Run<\/code> object accessed through the API. This should allow you to access all your system metrics data. Please let me know if this does not work for you or if you need any further assistance.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":6.57,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":68.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incomplete GPU data"
    },
    {
        "Answerer_created_time":1310699693432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2889.0,
        "Answerer_view_count":62.0,
        "Challenge_adjusted_solved_time":139.2068527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to replicate <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb<\/a><\/p>\n\n<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:<\/p>\n\n<pre><code>%%time\nimport boto3\n\nregion = boto3.Session().region_name\nsaved_model = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/model\/resnet\/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)\n\nimport sagemaker\nfrom sagemaker.tensorflow.serving import Model\n\nrole = sagemaker.get_execution_role()\n\ntensorflow_model = Model(model_data=saved_model,\nrole=role,\nframework_version='1.14')\ntf_predictor = tensorflow_model.deploy(initial_instance_count=1,\ninstance_type='local',\naccelerator_type='local_sagemaker_notebook')\n<\/code><\/pre>\n\n<p>I am getting following log in the notebook:<\/p>\n\n<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1\nalgo-1-7ynb1_1 | INFO:main:starting services\nalgo-1-7ynb1_1 | INFO:main:using default model name: Servo\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving model config:\nalgo-1-7ynb1_1 | model_config_list: {\nalgo-1-7ynb1_1 | config: {\nalgo-1-7ynb1_1 | name: \"Servo\",\nalgo-1-7ynb1_1 | base_path: \"\/opt\/ml\/model\/export\/Servo\",\nalgo-1-7ynb1_1 | model_platform: \"tensorflow\"\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:nginx config:\nalgo-1-7ynb1_1 | load_module modules\/ngx_http_js_module.so;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_processes auto;\nalgo-1-7ynb1_1 | daemon off;\nalgo-1-7ynb1_1 | pid \/tmp\/nginx.pid;\nalgo-1-7ynb1_1 | error_log \/dev\/stderr error;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_rlimit_nofile 4096;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | events {\nalgo-1-7ynb1_1 | worker_connections 2048;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | http {\nalgo-1-7ynb1_1 | include \/etc\/nginx\/mime.types;\nalgo-1-7ynb1_1 | default_type application\/json;\nalgo-1-7ynb1_1 | access_log \/dev\/stdout combined;\nalgo-1-7ynb1_1 | js_include tensorflow-serving.js;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream tfs_upstream {\nalgo-1-7ynb1_1 | server localhost:8501;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream gunicorn_upstream {\nalgo-1-7ynb1_1 | server unix:\/tmp\/gunicorn.sock fail_timeout=1;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | server {\nalgo-1-7ynb1_1 | listen 8080 deferred;\nalgo-1-7ynb1_1 | client_max_body_size 0;\nalgo-1-7ynb1_1 | client_body_buffer_size 100m;\nalgo-1-7ynb1_1 | subrequest_output_buffer_size 100m;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | set $tfs_version 1.14;\nalgo-1-7ynb1_1 | set $default_tfs_model Servo;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/tfs {\nalgo-1-7ynb1_1 | rewrite ^\/tfs\/(.) \/$1 break;\nalgo-1-7ynb1_1 | proxy_redirect off;\nalgo-1-7ynb1_1 | proxy_pass_request_headers off;\nalgo-1-7ynb1_1 | proxy_set_header Content-Type 'application\/json';\nalgo-1-7ynb1_1 | proxy_set_header Accept 'application\/json';\nalgo-1-7ynb1_1 | proxy_pass http:\/\/tfs_upstream;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ping {\nalgo-1-7ynb1_1 | js_content ping;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/invocations {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location ~ ^\/models\/(.)\/invoke {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/models {\nalgo-1-7ynb1_1 | proxy_pass http:\/\/gunicorn_upstream\/models;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ {\nalgo-1-7ynb1_1 | return 404 '{\"error\": \"Not Found\"}';\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | keepalive_timeout 3;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)\nalgo-1-7ynb1_1 | INFO:main:nginx version info:\nalgo-1-7ynb1_1 | nginx version: nginx\/1.16.1\nalgo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)\nalgo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016\nalgo-1-7ynb1_1 | TLS SNI support enabled\nalgo-1-7ynb1_1 | configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\nalgo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\nalgo-1-7ynb1_1 |\n!algo-1-7ynb1_1 | 172.18.0.1 - - [17\/Jun\/2020:05:02:10 +0000] \"GET \/ping HTTP\/1.1\" 200 3 \"-\" \"-\"\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-7ynb1_1 | EI Error Code: [3, 16, 8]\nalgo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | EI Client Version: 1.5.3\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\n`\n<\/code><\/pre>\n\n<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.<\/p>\n\n<p>I can see the docker of the model running\n<a href=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I try to infer\/predict from that model, I get error:<\/p>\n\n<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]\n\nalgo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-iikpj_1 | EI Error Code: [3, 16, 8]\nalgo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-iikpj_1 | EI Client Version: 1.5.3\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-iikpj_1 | INFO:main:tensorflow version info:\nalgo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-iikpj_1 | TensorFlow Library: 1.14.0\nalgo-1-iikpj_1 | EI Version: EI-1.4\nalgo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 172.18.0.1 - - [17\/Jun\/2020:05:29:47 +0000] \"POST \/invocations HTTP\/1.1\" 502 157 \"-\" \"-\"\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nJSONDecodeError Traceback (most recent call last)\nin ()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/serving.py in predict(self, data, initial_args)\n116 args[\"CustomAttributes\"] = self._model_attributes\n117\n--&gt; 118 return super(Predictor, self).predict(data, args)\n119\n120\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model)\n109 request_args = self._create_request_args(data, initial_args, target_model)\n110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n--&gt; 111 return self._handle_response(response)\n112\n113 def _handle_response(self, response):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in _handle_response(self, response)\n119 if self.deserializer is not None:\n120 # It's the deserializer's responsibility to close the stream\n--&gt; 121 return self.deserializer(response_body, response[\"ContentType\"])\n122 data = response_body.read()\n123 response_body.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in call(self, stream, content_type)\n578 \"\"\"\n579 try:\n--&gt; 580 return json.load(codecs.getreader(\"utf-8\")(stream))\n581 finally:\n582 stream.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n297 cls=cls, object_hook=object_hook,\n298 parse_float=parse_float, parse_int=parse_int,\n--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n300\n301\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 \"\"\"\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\n<\/code><\/pre>\n\n<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.\nI also tried Rest API post to docker deployed model:<\/p>\n\n<pre><code>curl -v -X POST \\ -H 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/Servo:predict\nbut still getting error:\n[![enter image description here][1]][1]\n<\/code><\/pre>\n\n<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.<\/p>\n\n<p>Please help me out. Thanks<\/p>",
        "Challenge_closed_time":1592876274340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592375129670,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering challenges with deploying a tensorflow model with elastic inference on a notebook instance, resulting in errors such as \"jsondecodeerror: expecting value: line 1 column 1 (char 0)\" and \"curl -v -x post \\ -h 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/servo",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62422682",
        "Challenge_link_count":9,
        "Challenge_participation_count":1,
        "Challenge_readability":19.9,
        "Challenge_reading_time":261.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":164,
        "Challenge_solved_time":139.2068527778,
        "Challenge_title":"sagemaker notebook instance Elastic Inference tensorflow model local deployment",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":1504,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310699693432,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2889.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>Solved it. The error I was getting is due to roles\/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.2,
        "Solution_reading_time":4.99,
        "Solution_score_count":-1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"errors deploying TensorFlow model"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":8.6530769444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm training a large-ish model, trying to use for the purpose <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Azure Machine Learning service<\/a> in Azure notebooks.<\/p>\n\n<p>I thus create an <code>Estimator<\/code> to train locally:<\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\n\nestimator = Estimator(source_directory='.\/source_dir',\n                      compute_target='local',\n                      entry_script='train.py')\n<\/code><\/pre>\n\n<p>(my <code>train.py<\/code> should load and train starting from a large word vector file).<\/p>\n\n<p>When running with <\/p>\n\n<pre><code>run = experiment.submit(config=estimator)\n<\/code><\/pre>\n\n<p>I get <\/p>\n\n<blockquote>\n  <p>TrainingException: <\/p>\n  \n  <p>====================================================================<\/p>\n  \n  <p>While attempting to take snapshot of\n  \/data\/home\/username\/notebooks\/source_dir Your total\n  snapshot size exceeds the limit of 300.0 MB. Please see\n  <a href=\"http:\/\/aka.ms\/aml-largefiles\" rel=\"nofollow noreferrer\">http:\/\/aka.ms\/aml-largefiles<\/a> on how to work with large files.<\/p>\n  \n  <p>====================================================================<\/p>\n<\/blockquote>\n\n<p>The link provided in the error is likely <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/26076\" rel=\"nofollow noreferrer\">broken<\/a>. \nContents in my <code>.\/source_dir<\/code> indeed exceed 300 MB.<br>\nHow can I solve this?<\/p>",
        "Challenge_closed_time":1554445793380,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554414642303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a TrainingException while training a large model with Azure Machine Learning service in Azure notebooks. The error message indicates that the total snapshot size exceeds the limit of 300.0 MB, and the link provided in the error is broken. The user is seeking a solution to overcome this issue.",
        "Challenge_last_edit_time":1554642637940,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55525445",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":20.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":8.6530769444,
        "Challenge_title":"How to overcome TrainingException when training a large model with Azure Machine Learning service?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1127.0,
        "Challenge_word_count":135,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>You can place the training files outside <code>source_dir<\/code> so that they don't get uploaded as part of submitting the experiment, and then upload them separately to the data store (which is basically using the Azure storage associated with your workspace). All you need to do then is reference the training files from <code>train.py<\/code>. <\/p>\n\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">Train model tutorial<\/a> for an example of how to upload data to the data store and then access it from the training file.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1554449103928,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":7.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"snapshot size limit exceeded"
    },
    {
        "Answerer_created_time":1447522907212,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":758.0,
        "Answerer_view_count":95.0,
        "Challenge_adjusted_solved_time":39.3668766667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to use optuna for searching hyper parameter spaces.<\/p>\n\n<p>In one particular scenario I train a model on a machine with a few GPUs.\nThe model and batch size allows me to run 1 training per 1 GPU.\nSo, ideally I would like to let optuna spread all trials across the available GPUs\nso that there is always 1 trial running on each GPU.<\/p>\n\n<p>In the <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#how-can-i-use-two-gpus-for-evaluating-two-trials-simultaneously\" rel=\"nofollow noreferrer\">docs<\/a> it says, I should just start one process per GPU in a separate terminal like:<\/p>\n\n<pre><code>CUDA_VISIBLE_DEVICES=0 optuna study optimize foo.py objective --study foo --storage sqlite:\/\/\/example.db\n<\/code><\/pre>\n\n<p>I want to avoid that because the whole hyper parameter search continues in multiple rounds after that. I don't want to always manually start a process per GPU, check when all are finished, then start the next round.<\/p>\n\n<p>I saw <code>study.optimize<\/code> has a <code>n_jobs<\/code> argument.\nAt first glance this seems to be perfect.\n<em>E.g.<\/em> I could do this:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial):\n    # the actual model would be trained here\n    # the trainer here would need to know which GPU\n    # it should be using\n    best_val_loss = trainer(**trial.params)\n    return best_val_loss\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100, n_jobs=8)\n<\/code><\/pre>\n\n<p>This starts multiple threads each starting a training.\nHowever, the trainer within <code>objective<\/code> somehow needs to know which GPU it should be using.\nIs there a trick to accomplish that?<\/p>",
        "Challenge_closed_time":1589464897583,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589323176827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use optuna for hyperparameter search and wants to spread all trials across available GPUs. The documentation suggests starting one process per GPU in a separate terminal, but the user wants to avoid that. The user has tried using the `n_jobs` argument in `study.optimize` to start multiple threads, but the trainer within `objective` needs to know which GPU to use. The user is looking for a way to pass arguments to multiple jobs in optuna.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61763206",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":21.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":39.3668766667,
        "Challenge_title":"Is there a way to pass arguments to multiple jobs in optuna?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2684.0,
        "Challenge_word_count":231,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447522907212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":758.0,
        "Poster_view_count":95.0,
        "Solution_body":"<p>After a few mental breakdowns I figured out that I can do what I want using a <code>multiprocessing.Queue<\/code>. To get it into the objective function I need to define it as a lambda function or as a class (I guess partial also works). <em>E.g.<\/em><\/p>\n\n<pre><code>from contextlib import contextmanager\nimport multiprocessing\nN_GPUS = 2\n\nclass GpuQueue:\n\n    def __init__(self):\n        self.queue = multiprocessing.Manager().Queue()\n        all_idxs = list(range(N_GPUS)) if N_GPUS &gt; 0 else [None]\n        for idx in all_idxs:\n            self.queue.put(idx)\n\n    @contextmanager\n    def one_gpu_per_process(self):\n        current_idx = self.queue.get()\n        yield current_idx\n        self.queue.put(current_idx)\n\n\nclass Objective:\n\n    def __init__(self, gpu_queue: GpuQueue):\n        self.gpu_queue = gpu_queue\n\n    def __call__(self, trial: Trial):\n        with self.gpu_queue.one_gpu_per_process() as gpu_i:\n            best_val_loss = trainer(**trial.params, gpu=gpu_i)\n            return best_val_loss\n\nif __name__ == '__main__':\n    study = optuna.create_study()\n    study.optimize(Objective(GpuQueue()), n_trials=100, n_jobs=8)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":13.34,
        "Solution_score_count":6.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":109.0,
        "Tool":"Optuna",
        "Challenge_type":"inquiry",
        "Challenge_summary":"pass arguments to multiple jobs"
    },
    {
        "Answerer_created_time":1402755934688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Porto, Portugal",
        "Answerer_reputation_count":5998.0,
        "Answerer_view_count":426.0,
        "Challenge_adjusted_solved_time":5.7713575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm deploying a <code>tensorflow.serving<\/code> endpoint with a custom <code>inference.py<\/code> script via the <code>entry point<\/code> parameter<\/p>\n<pre><code>model = Model(role='xxx',\n              framework_version='2.2.0',\n              entry_point='inference.py',\n              model_data='xxx')\n\npredictor = model.deploy(instance_type='xxx',\n                         initial_instance_count=1,\n                         endpoint_name='xxx')\n<\/code><\/pre>\n<p>inference.py constains an <code>input_handler<\/code> and an <code>output_handler<\/code> functions, but when i call predict with:<\/p>\n<pre><code>model = Predictor(endpoint_name='xxx')\nurl = 'xxx'\n\ninput = {\n    'instances': [url]\n}\n\npredictions = model.predict(input)\n<\/code><\/pre>\n<p>I'm getting the following <code>error<\/code>:<\/p>\n<p><em>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;{&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;xxx&quot; Type: String is not of expected type: float&quot; }&quot;<\/em><\/p>\n<p>It seems the function is never calling the <code>input_handler<\/code> function in inference.py script. Do you know why this might be happening?<\/p>",
        "Challenge_closed_time":1595952661830,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595851293460,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is deploying a tensorflow.serving endpoint with a custom inference.py script via the entry point parameter. The inference.py script contains an input_handler and an output_handler functions, but when the user calls predict, they are getting an error that suggests the function is never calling the input_handler function in the inference.py script.",
        "Challenge_last_edit_time":1595931884943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63114905",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.8,
        "Challenge_reading_time":17.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":28.1578805556,
        "Challenge_title":"Sagemaker tensorflow endpoint not calling the input_handler when being invoked for a prediction",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":474.0,
        "Challenge_word_count":129,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1402755934688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Porto, Portugal",
        "Poster_reputation_count":5998.0,
        "Poster_view_count":426.0,
        "Solution_body":"<p>Found the problem thanks to AWS support:<\/p>\n<p>I was creating an endpoint that already had an endpoint configuration with the same name and the new configuration wasn't being utilized.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":2.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"input_handler not called"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":81.0187747222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pipeline defined in Azure Machine Learning. It was launched every day with Azure Data Factory with Machine Learning Execute Pipeline activity. This solution worked without any issues for a few weeks, but since 12\/09\/2021 all pipeline runs have failed with error: User starting the run is not an owner or assigned user to the Compute Instance.  <br \/>\nI did not change anything in ADF or AML.   <\/p>\n<p>Should I assign compute to ADF? How to do this?<\/p>",
        "Challenge_closed_time":1639690537336,
        "Challenge_comment_count":1,
        "Challenge_created_time":1639398869747,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's pipeline in Azure Machine Learning was previously launched daily with Azure Data Factory, but suddenly stopped working with an error message stating that the user is not an owner or assigned user to the Compute Instance. The user did not make any changes to either ADF or AML and is seeking advice on whether they should assign compute to ADF and how to do so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/661588\/executing-pipeline-in-aml-from-adf-suddenly-stoppe",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":81.0187747222,
        "Challenge_title":"Executing pipeline in AML from ADF suddenly stopped working",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I ran into this same issue in a slightly different context. I didn't manage to figure out the root cause but managed to resolve it in practice by standing up a Compute Cluster instead of a Compute Instance (see <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python<\/a>)<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.2,
        "Solution_reading_time":5.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"compute ownership error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":36.5390777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, here is the details of my issue.  <br \/>\nI want to execute a distributed training run with the Tensorflow framework and Horovod.  <br \/>\nTo do this, I've configured a environment called &quot;tf_env&quot; as follow :<\/p>\n<pre><code># Create the environment : the dependencies are in the .yml file\ntf_env = Environment.from_conda_specification(name=&quot;tensorflow_environment&quot;, file_path=&quot;experiments\/package-list.yml&quot;)\n\n# Register the environment\ntf_env.register(workspace=ws)\n\n# Specify a GPU base image\ntf_env.docker.enabled = True\ntf_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n<\/code><\/pre>\n<p>Where my &quot;package-list.yml&quot; contains all the dependencies my &quot;train_script.py&quot; requires.  <br \/>\nI've defined my ScriptConfigRun as follow :<\/p>\n<pre><code>arguments = [\n    (... other arguments ...)\n    &quot;--ds&quot;,  images_ds.as_mount()\n]\n\nsrc = ScriptRunConfig(\n    source_directory=&quot;experiments&quot;,\n    script='train_script.py',\n    arguments=arguments,\n    compute_target=compute_target,\n    environment=tf_env,\n    distributed_job_config=MpiConfiguration(node_count=2)\n)\n<\/code><\/pre>\n<p>Then, when I want to submit the run :<\/p>\n<pre><code>run = best_model_experiment.submit(config=src)\n<\/code><\/pre>\n<p>... it raises this error I don't understand :<\/p>\n<pre><code>ExperimentExecutionException: ExperimentExecutionException:\n    Message: {\n    &quot;error_details&quot;: {\n        &quot;componentName&quot;: &quot;execution&quot;,\n        &quot;correlation&quot;: {\n            &quot;operation&quot;: &quot;***&quot;,\n            &quot;request&quot;: &quot;***&quot;\n        },\n        &quot;environment&quot;: &quot;westeurope&quot;,\n        &quot;error&quot;: {\n            &quot;code&quot;: &quot;UserError&quot;,\n            &quot;message&quot;: &quot;Error when parsing request; unable to deserialize request body&quot;\n        },\n        &quot;location&quot;: &quot;westeurope&quot;,\n        &quot;time&quot;: &quot;***&quot;\n    },\n    &quot;status_code&quot;: 400,\n    &quot;url&quot;: &quot;https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment***&quot;\n}\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;{\\n    \\&quot;error_details\\&quot;: {\\n        \\&quot;componentName\\&quot;: \\&quot;execution\\&quot;,\\n        \\&quot;correlation\\&quot;: {\\n            \\&quot;operation\\&quot;: \\&quot;***\\&quot;,\\n            \\&quot;request\\&quot;: \\&quot;***\\&quot;\\n        },\\n        \\&quot;environment\\&quot;: \\&quot;westeurope\\&quot;,\\n        \\&quot;error\\&quot;: {\\n            \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n            \\&quot;message\\&quot;: \\&quot;Error when parsing request; unable to deserialize request body\\&quot;\\n        },\\n        \\&quot;location\\&quot;: \\&quot;westeurope\\&quot;,\\n        \\&quot;time\\&quot;: \\&quot;***\\&quot;\\n    },\\n    \\&quot;status_code\\&quot;: 400,\\n    \\&quot;url\\&quot;: \\&quot;https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment_***\\&quot;\\n}&quot;\n    }\n}\n<\/code><\/pre>\n<p>Could you please help me decrypt this error ?  <br \/>\nThank you.<\/p>",
        "Challenge_closed_time":1620024521707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619892981027,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue while submitting a distributed training run with Tensorflow and Horovod on Azure Machine Learning. They have configured an environment and defined a ScriptConfigRun, but when they try to submit the run, they receive an ExperimentExecutionException with an error message that they do not understand. They are seeking help to decrypt the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/379458\/azure-machine-learning-experimentexecutionexceptio",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.5,
        "Challenge_reading_time":44.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":36.5390777778,
        "Challenge_title":"Azure Machine Learning ExperimentExecutionException while submitting a distributed training run !",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":216,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Issue solved ! I've given a list in arguments to argparse so it could'nt deserialized the object.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":1.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unrecognized error message"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.9002777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nAfter Sagemaker workspace stopped automatically, workspace env status is not updated.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Set sagemaker workspace config's AutoStopIdleTimeInMinutes as 10 minutes\r\n2. Create sagemaker workspace and wait for more than 10 minutes,\r\n3. Check sagemaker notebook instances to confirm the instance status is Stopped\r\n4. Check Service Workbench workspace status, it is still \"AVAILABLE\"\r\n\r\n**Expected behavior**\r\n1. Above step 4, workspace status should be \"STOPPED\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1657702977000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657451336000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is requesting to add three dependencies to the Sagemaker section, which are urllib3, PyYAML, and ipython, to train a Deepracer model locally with GPU support.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/44",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":42.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":148.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":69.9002777778,
        "Challenge_title":"[Bug] Sagemaker template, after auto stoped, workspace env status is not updated",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Discussion_body":"Fixed, refer [commit](https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/commit\/2339efe78d0f4705ef0cd6d2b1c5f06a810e6730) ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"add dependencies for Deepracer"
    },
    {
        "Answerer_created_time":1373922677212,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1350.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":24155.5917811111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a simple experiment in Azure ML and trigger it with an http client. In Azure ML workspace, everything works ok when executed. However, the experiment times out and fails when I trigger the experiment using an http client. Setting a timeout value for the http client does not seem to work.<\/p>\n\n<p>Is there any way we can set this timeout value so that the experiment does not fail?<\/p>",
        "Challenge_closed_time":1522603988172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1435643857760,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an experiment in Azure ML and is triggering it with an HTTP client. While the experiment works fine in the Azure ML workspace, it times out and fails when triggered using the HTTP client. The user is looking for a way to set the timeout value for the HTTP client to prevent the experiment from failing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31130629",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24155.5917811111,
        "Challenge_title":"Azure ML web service times out",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1681.0,
        "Challenge_word_count":76,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1313488868532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":209.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>Looks like it isn't possible to set this timeout based on <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/6472562-configurable-timeout-for-experiments-and-web-servi\" rel=\"nofollow noreferrer\">a feature request that is still marked as \"planned\" as of 4\/1\/2018<\/a>.<\/p>\n\n<p>The recommendation from <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/sqlserver\/en-US\/cb4ee96d-c2ca-4c65-b02f-0ccb26181f7f\/timeout-in-web-service?forum=MachineLearning\" rel=\"nofollow noreferrer\">MSDN forums from 2017<\/a> is to use the Batch Execution Service, which starts the machine learning experiment and then asynchronously asks whether it's done.<\/p>\n\n<p>Here's a code snippet from the Azure ML Web Services Management Sample Code (all comments are from their sample code):<\/p>\n\n<pre><code>        using (HttpClient client = new HttpClient())\n        {\n            var request = new BatchExecutionRequest()\n            {\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt; () {\n                    {\n                        \"output\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = storageConnectionString,\n                            RelativeLocation = string.Format(\"{0}\/outputresults.file_extension\", StorageContainerName) \/*Replace this with the location you would like to use for your output file, and valid file extension (usually .csv for scoring results, or .ilearner for trained models)*\/\n                        }\n                    },\n                },    \n\n                GlobalParameters = new Dictionary&lt;string, string&gt;() {\n                }\n            };\n\n            client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\n            \/\/ WARNING: The 'await' statement below can result in a deadlock\n            \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n            \/\/ One way to address this would be to call ConfigureAwait(false)\n            \/\/ so that the execution does not attempt to resume on the original context.\n            \/\/ For instance, replace code such as:\n            \/\/      result = await DoSomeTask()\n            \/\/ with the following:\n            \/\/      result = await DoSomeTask().ConfigureAwait(false)\n\n            Console.WriteLine(\"Submitting the job...\");\n\n            \/\/ submit the job\n            var response = await client.PostAsJsonAsync(BaseUrl + \"?api-version=2.0\", request);\n\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobId = await response.Content.ReadAsAsync&lt;string&gt;();\n            Console.WriteLine(string.Format(\"Job ID: {0}\", jobId));\n\n            \/\/ start the job\n            Console.WriteLine(\"Starting the job...\");\n            response = await client.PostAsync(BaseUrl + \"\/\" + jobId + \"\/start?api-version=2.0\", null);\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobLocation = BaseUrl + \"\/\" + jobId + \"?api-version=2.0\";\n            Stopwatch watch = Stopwatch.StartNew();\n            bool done = false;\n            while (!done)\n            {\n                Console.WriteLine(\"Checking the job status...\");\n                response = await client.GetAsync(jobLocation);\n                if (!response.IsSuccessStatusCode)\n                {\n                    await WriteFailedResponse(response);\n                    return;\n                }\n\n                BatchScoreStatus status = await response.Content.ReadAsAsync&lt;BatchScoreStatus&gt;();\n                if (watch.ElapsedMilliseconds &gt; TimeOutInMilliseconds)\n                {\n                    done = true;\n                    Console.WriteLine(string.Format(\"Timed out. Deleting job {0} ...\", jobId));\n                    await client.DeleteAsync(jobLocation);\n                }\n                switch (status.StatusCode) {\n                    case BatchScoreStatusCode.NotStarted:\n                        Console.WriteLine(string.Format(\"Job {0} not yet started...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Running:\n                        Console.WriteLine(string.Format(\"Job {0} running...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Failed:\n                        Console.WriteLine(string.Format(\"Job {0} failed!\", jobId));\n                        Console.WriteLine(string.Format(\"Error details: {0}\", status.Details));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Cancelled:\n                        Console.WriteLine(string.Format(\"Job {0} cancelled!\", jobId));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Finished:\n                        done = true;\n                        Console.WriteLine(string.Format(\"Job {0} finished!\", jobId));\n                        ProcessResults(status);\n                        break;\n                }\n\n                if (!done) {\n                    Thread.Sleep(1000); \/\/ Wait one second\n                }\n            }\n        }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.3,
        "Solution_reading_time":50.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":45.0,
        "Solution_word_count":338.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"experiment times out"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.316325,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, I am training my models via Azure Machine Learning.<\/p>\n<p>On other day, my training is running with GPU support, however today I found my training is running on a CPU.  <br \/>\nI'm not modified training environment, only training script was modified.  <br \/>\nMy computing cluster is NC6v3 - have a GPU.<\/p>\n<p>I investigate a situation, and I found training script is running on PyTorch 1.6.0.  <br \/>\nOn other day, it ran on Pytorch 1.8.1.  <br \/>\nI think my &quot;don't use GPU&quot; problem is caused by the situation that CUDA toolkit version is not suitable for Pytorch version.<\/p>\n<p>Then, I output a installed package to the log.  <br \/>\nThe log says 'Pytorch 1.8.1 was installed, however uses 1.6.0'.  <br \/>\nI confused by this weird circumstances.  <br \/>\nCan someone tell me the solution?<\/p>\n<p>&lt;My code snippet&gt;  <br \/>\n&lt;&lt;conda_dependencies.yaml&gt;&gt;<\/p>\n<p>channels:  <\/p>\n<ul>\n<li> conda-forge  <\/li>\n<li> pytorch  <\/li>\n<li> nvidia  <br \/>\ndependencies:  <\/li>\n<li> python=3.8.10  <\/li>\n<li> mesa-libgl-cos6-x86_64  <\/li>\n<li> cudatoolkit=11.1  <\/li>\n<li> pytorch==1.8.1  <\/li>\n<li> torchvision==0.9.1  <\/li>\n<li> tqdm  <\/li>\n<li> scikit-learn  <\/li>\n<li> matplotlib  <\/li>\n<li> pandas  <\/li>\n<li> pip &lt; 20.3  <\/li>\n<li> pip:  <\/li>\n<li> azureml-defaults  <\/li>\n<li> opencv-python-headless  <\/li>\n<li> pillow==8.2.0<\/li>\n<\/ul>\n<p>&lt;&lt;Environment definition&gt;&gt;  <br \/>\nenvironment_definition_file = experiment_dir \/ 'conda_dependencies.yaml'  <br \/>\nenvironment_name = 'pytorch-1.8.1-gpu'  <br \/>\nbase_image_name = 'mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04'  <br \/>\nenvironment = Environment.from_docker_image(environment_name, base_image_name, conda_specification = environment_definition_file)  <br \/>\ndocker_run_config = DockerConfiguration(use_docker=True)<\/p>\n<p>script_run_config = ScriptRunConfig(  <br \/>\nsource_directory = experiment_dir,  <br \/>\nscript = SCRIPT_FILE_NAME,  <br \/>\narguments = arguments,  <br \/>\ncompute_target = compute_target,  <br \/>\ndocker_runtime_config = docker_run_config,  <br \/>\nenvironment = environment)<\/p>\n<p>&lt;&lt;Output a log in the training script&gt;&gt;  <br \/>\nimport torch  <br \/>\nimport pip<\/p>\n<p>pip.main(['list'])  <br \/>\nprint(f'PyTorch version: {torch.<strong>version<\/strong>}')<\/p>\n<p>&lt;My logs&gt;  <br \/>\nPackage Version<\/p>\n<hr \/>\n<p>adal 1.2.7  <br \/>\napplicationinsights 0.11.10  <br \/>\n(omission)  <br \/>\ntorch 1.8.1  <br \/>\ntorchvision 0.9.0a0  <br \/>\n(omission)<\/p>\n<p>PyTorch version: 1.6.0<\/p>",
        "Challenge_closed_time":1629156299363,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629119160593,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning where their training script is running on a CPU instead of a GPU. Upon investigation, they found that the training script is running on PyTorch 1.6.0 instead of the required version of PyTorch 1.8.1. The user suspects that the issue is caused by the unsuitable CUDA toolkit version for PyTorch version. The user has provided their code snippet and log output for reference and is seeking a solution to the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/515579\/azure-machine-learning-uses-invalid-pytorch-versio",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":32.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":10.316325,
        "Challenge_title":"Azure Machine Learning - Uses invalid Pytorch version when training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":285,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. These are the <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.pytorch?view=azure-ml-py\">supported versions<\/a> for PyTorch. Please refer to this document for creating a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-pytorch#create-a-custom-environment\">custom environment<\/a>. As shown, you'll need to use versions &lt;= 1.6.0. Hope this helps.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.0,
        "Solution_reading_time":6.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":37.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"running on CPU, wrong PyTorch version"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.8071302778,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a state-machine workflow with 3 following states:  \n\n[screenshot-of-my-workflow](https:\/\/i.stack.imgur.com\/4xJTE.png)   \n\n1. A 'Pass' block that adds a list of strings(SageMaker endpoint names) to the original input. (*this 'Pass' will be replaced by a call to DynamoDB to fetch list in future.*) \n2. Use map to call SageMaker endpoints dictated by the array(or list) from above result.\n3. Send the result of above 'Map' to a Lambda function and exit the workflow.\n\n\nHere's the entire workflow in .asl.json, inspired from [this aws blog](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-map-state.html).\n```\n{\n  \"Comment\": \"A description of my state machine\",\n  \"StartAt\": \"Pass\",\n  \"States\": {\n    \"Pass\": {\n      \"Type\": \"Pass\",\n      \"Next\": \"InvokeEndpoints\",\n      \"Result\": {\n        \"Endpoints\": [\n          \"sagemaker-endpoint-1\",\n          \"sagemaker-endpoint-2\",\n          \"sagemaker-endpoint-3\"\n        ]\n      },\n      \"ResultPath\": \"$.EndpointList\"\n    },\n    \"InvokeEndpoints\": {\n      \"Type\": \"Map\",\n      \"Next\": \"Post-Processor Lambda\",\n      \"Iterator\": {\n        \"StartAt\": \"InvokeEndpoint\",\n        \"States\": {\n          \"InvokeEndpoint\": {\n            \"Type\": \"Task\",\n            \"End\": true,\n            \"Parameters\": {\n              \"Body\": \"$.InvocationBody\",\n              \"EndpointName\": \"$.EndpointName\"\n            },\n            \"Resource\": \"arn:aws:states:::aws-sdk:sagemakerruntime:invokeEndpoint\",\n            \"ResultPath\": \"$.InvocationResult\"\n          }\n        }\n      },\n      \"ItemsPath\": \"$.EndpointList.Endpoints\",\n      \"MaxConcurrency\": 300,\n      \"Parameters\": {\n        \"InvocationBody.$\": \"$.body.InputData\",\n        \"EndpointName.$\": \"$$.Map.Item.Value\"\n      },\n      \"ResultPath\": \"$.InvocationResults\"\n    },\n    \"Post-Processor Lambda\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n      \"Parameters\": {\n        \"Payload.$\": \"$\",\n        \"FunctionName\": \"arn:aws:lambda:<my-region>:<my-account-id>:function:<my-lambda-function-name>:$LATEST\"\n      },\n      \"Retry\": [\n        {\n          \"ErrorEquals\": [\n            \"Lambda.ServiceException\",\n            \"Lambda.AWSLambdaException\",\n            \"Lambda.SdkClientException\"\n          ],\n          \"IntervalSeconds\": 2,\n          \"MaxAttempts\": 6,\n          \"BackoffRate\": 2\n        }\n      ],\n      \"End\": true\n    }\n  }\n}\n```\n\nAs can be seen in the workflow, I am iterating over the list from the previous 'Pass' block and mapping those to iterate inside 'Map' block and trying to access the Parameters of 'Map' block inside each iteration. Iteration works fine with number of iterators, but I can't access the Parameters inside the iteration. I get this error:\n```\n{\n  \"resourceType\": \"aws-sdk:sagemakerruntime\",\n  \"resource\": \"invokeEndpoint\",\n  \"error\": \"SageMakerRuntime.ValidationErrorException\",\n  \"cause\": \"1 validation error detected: Value '$.EndpointName' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])* (Service: SageMakerRuntime, Status Code: 400, Request ID: ed5cad0c-28d9-4913-853b-e5f9ac924444)\"\n}\n```\nSo, I presume the error is because \"$.EndpointName\" is not being filled with the relevant value. How do I avoid this.\n\nBut, when I open the failed execution and check the InvokeEndpoint block from graph-inspector, input to that is what I expected and above JSON-Paths to fetch the parameters should work, but they don't.  \n[screenshot-of-graph-inspector](https:\/\/i.stack.imgur.com\/3gXsM.jpg)\n\nWhat's causing the error and How do I fix this?",
        "Challenge_closed_time":1647513967263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647503861594,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a validation error when trying to fetch parameters for the SageMaker's InvokeEndpoint block inside the iterator of the Map block in AWS StepFunctions. The error occurs when trying to access the parameters inside the iteration, and the user presumes that the error is because \"$.EndpointName\" is not being filled with the relevant value. However, when checking the failed execution and inspecting the InvokeEndpoint block, the input appears to be correct. The user is seeking guidance on how to fix this issue.",
        "Challenge_last_edit_time":1668586573828,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDc1foN9TQhe3OYkkGzCKhQ\/aws-stepfunctions-sagemaker-s-invokeendpoint-block-throws-validation-error-when-fetching-parameters-for-itself-inside-iterator-of-map-block",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":41.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.8071302778,
        "Challenge_title":"AWS StepFunctions - SageMaker's InvokeEndpoint block throws \"validation error\" when fetching parameters for itself inside iterator of Map block",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":336,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"In general (as mentioned [here in the parameters doc](https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-parameters.html)), you also need to **end the parameter name** with `.$` when using a JSON Path.\n\nIt looks like you're doing that some places in your sample JSON (e.g. `\"InvocationBody.$\": \"$.body.InputData\"`), but not in others (`\"EndpointName\": \"$.EndpointName\"`), so I think the reason you're seeing the validation error here is that Step Functions is trying to interpret `$.EndpointName` as literally the name of the endpoint (which doesn't satisfy `^[a-zA-Z0-9](-*[a-zA-Z0-9])*`!)\n\nSo suggest you change to `EndpointName.$` and `Body.$` in your InvokeEndpoint parameters",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1647513967264,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"validation error in StepFunctions"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":10204.2583313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've fitted a Tensorflow Estimator in SageMaker using Script Mode with <code>framework_version='1.12.0'<\/code> and <code>python_version='py3'<\/code>, using a GPU instance. <\/p>\n\n<p>Calling deploy directly on this estimator works if I select deployment instance type as GPU as well. However, if I select a CPU instance type and\/or try to add an accelerator, it fails with an error that docker cannot find a corresponding image to pull. <\/p>\n\n<p>Anybody know how to train a py3 model on a GPU with Script Mode and then deploy to a CPU+EIA instance? <\/p>\n\n<hr>\n\n<p>I've found a partial workaround by taking the intermediate step of creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not seem to support python 3 (again, doesn't find a corresponding container). If I switch to python_version='py2', it will find the container, but fail to pass health checks because all my code is for python 3.<\/p>",
        "Challenge_closed_time":1549915825300,
        "Challenge_comment_count":1,
        "Challenge_created_time":1549048113447,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue while deploying a Tensorflow Estimator in SageMaker using Script Mode with framework_version='1.12.0' and python_version='py3'. The deployment works fine if the deployment instance type is GPU, but fails with an error when selecting a CPU instance type and\/or adding an accelerator. The user is looking for a way to train a py3 model on a GPU with Script Mode and then deploy it to a CPU+EIA instance. The user has found a partial workaround by creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not support python 3.",
        "Challenge_last_edit_time":1549048982467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54485769",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":12.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":241.0310702778,
        "Challenge_title":"SageMaker deploying to EIA from TF Script Mode Python3",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":378.0,
        "Challenge_word_count":160,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>Unfortunately there are no TF + Python 3 + EI serving images at this time. If you would like to use TF + EI, you'll need to make sure your code is compatible with Python 2.<\/p>\n\n<p>Edit: after I originally wrote this, support for TF + Python 3 + EI has been released. At the time of this writing, I believe TF 1.12.0, 1.13.1, and 1.14.0 all have Python 3 + EI support. For the full list, see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1585784312460,
        "Solution_link_count":2.0,
        "Solution_readability":9.4,
        "Solution_reading_time":7.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deployment error on CPU+EIA instance"
    },
    {
        "Answerer_created_time":1463515446747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":15335.0,
        "Answerer_view_count":1991.0,
        "Challenge_adjusted_solved_time":4.5067230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>please be easy on me, I am new to ML. I am sure somebody will request to close this as subjective but I cannot find my specific answer and don't know how else to ask. <\/p>\n\n<p>If I have a shop, with three areas of the shop. I have sensors to detect when people come in or out of each area. This happens every 15 seconds. So, in my db, I have a count of the occupancy, per room, every 15 seconds. <\/p>\n\n<p>Using this data, I want to predict the occupancy, per room, in the future but also, if somebody comes in the door, predict most likely room they will go to. <\/p>\n\n<p>Is it possible to predict future occupancy per room and also probability of where people will go when the walk in using a dataset that simply lists the rooms and the occupancy of each room every 15 seconds? Is this a regression model?<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Mike<\/p>",
        "Challenge_closed_time":1544846023056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544840911773,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to machine learning and has a dataset that lists the occupancy of three different rooms in a shop every 15 seconds. They want to predict future occupancy per room and the probability of where people will go when they walk in. They are unsure if this is possible and if it requires a regression model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53789057",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.4198008333,
        "Challenge_title":"Is it possible to get two different types of results from dataset",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":170,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455012473430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Temecula, CA, United States",
        "Poster_reputation_count":1652.0,
        "Poster_view_count":198.0,
        "Solution_body":"<p>Predicting the most likely room, which they would walk in. :<\/p>\n\n<p>This falls under the classification problem. The output falls under a set of categories, in this case it is different rooms.<\/p>\n\n<p>Predicting the Occpancy of each room :\nAs mentioned by @poorna is a regression problem. <\/p>\n\n<p>Two ways you can look at this problem, <\/p>\n\n<ol>\n<li><p>Multi- target regression problem with occupancy of each room as one target and past occupancies of all rooms as input. <\/p><\/li>\n<li><p>Independent forecast problem for each room with past occupancies of corresponding room as input.<\/p><\/li>\n<\/ol>\n\n<p>For learning the basics of machine learning, you can go through this <a href=\"https:\/\/scikit-learn.org\/stable\/tutorial\/basic\/tutorial.html\" rel=\"nofollow noreferrer\">link<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1544857135976,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"predict occupancy and probability"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":61.8899861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to implement a self-service solution in Azure so users can run a Jupyter or PySpark notebook on-Demand\/automatically with the dataset they found a search in the Azure Data Catalog.  I visualize, once the user finds the data in a search, there will be a link that will take him\/her to a Notebook and the dataset can be used for analysis.  Any suggestion would be very much appreciated!<\/p>",
        "Challenge_closed_time":1619432530247,
        "Challenge_comment_count":1,
        "Challenge_created_time":1619209726297,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to create a self-service solution in Azure where users can run a Jupyter or PySpark notebook on-demand with a dataset they found in a search in the Azure Data Catalog. The user is looking for suggestions on how to implement this solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/369952\/azure-on-demand-ml-cluster-from-a-search-in-the-da",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":61.8899861111,
        "Challenge_title":"Azure On-Demand ML cluster from a search in the data catalog",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=072ff3f6-8045-41ef-849b-87cd092ffd6e\">@Jairo Melo  <\/a> Thanks for the question.Azure Purview can find, understand, and consume data sources. Please follow the Azure Purview documentation: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/purview\/\">https:\/\/learn.microsoft.com\/en-us\/azure\/purview\/<\/a>    <\/p>\n<p>and We have  Azure Open Datasets where you can download a Notebook for AML, Databricks or Synapse that explores the data: <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/\">Azure Open Datasets Catalog<\/a> | Microsoft Azure. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/open-datasets\/overview-what-are-open-datasets\">What are open datasets? Curated public datasets - Azure Open Datasets | Microsoft Learn.<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.6,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"implement self-service solution"
    },
    {
        "Answerer_created_time":1227171471292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":17500.0,
        "Answerer_view_count":1561.0,
        "Challenge_adjusted_solved_time":13.3215805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using Linear Learner in Sagemaker with MXNet RecordIO, I get <code>&quot;The header of the MXNet RecordIO record at position 5,089,840 in the dataset does not start with a valid magic number&quot;<\/code> after <code>fit()<\/code> has been running 38 minutes.<\/p>\n<p>The file was generated using this code. Note that I tried two ways to upload to S3. I also tried direct upload of the <code>BytesIO<\/code> as well as upload of a file, shown here.<\/p>\n<pre><code>train_file = 'linear_train.data'\n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n \n# Write the stuff\nwith open(train_file, &quot;wb&quot;) as fl:\n    fl.write(f.getvalue())\n\n# Alternative for upload\n# boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\nboto3.client('s3').upload_file(train_file,\n                                  Bucket=bucket,\n                                  Key=os.path.join(prefix, 'train', train_file),\n                                  ExtraArgs={'ACL': 'bucket-owner-full-control'})\n<\/code><\/pre>\n<p>To check that the file was non-corrupt, I downloaded it from S3 and simply read it as follows.<\/p>\n<pre><code>   record = MXRecordIO(fl, 'r')\n   while True:\n      item = record.read()\n      # Here we print the item, break when we reach the end, etc. This confirms that the RecordIO is valid.\n<\/code><\/pre>\n<p>So, the file seems OK.<\/p>\n<p>How can I run Linear Learner?<\/p>\n<p>Here is the error message:<\/p>\n<pre><code>Failure reason if any: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError)\n\nCaused by: [17:04:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.3446.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 5,089,840 in the dataset does not start with a valid magic number.\n\nStack trace returned 10 entries:\n[bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xbca0) [0x7f337885cca0]\n[bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xbffa) [0x7f337885cffa]\n[bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7f33788675e6]\n[bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7f3367272141]\n[bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7f3378893958]\n[bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7f33\nFailed 39.0 min; Failure reason if any: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError)\n\nCaused by: [17:04:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.3446.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 5,089,840 in the dataset does not start with a valid magic number.\n\nStack trace returned 10 entries:\n[bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xbca0) [0x7f337885cca0]\n[bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xbffa) [0x7f337885cffa]\n[bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7f33788675e6]\n[bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7f3367272141]\n[bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7f3378893958]\n[bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7f33\n<\/code><\/pre>",
        "Challenge_closed_time":1606206060190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606157531417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using Linear Learner in Sagemaker with MXNet RecordIO. The error message states that the header of the MXNet RecordIO record does not start with a valid magic number. The user has checked the file and it seems to be non-corrupt. The user is seeking help to resolve the issue and run Linear Learner successfully.",
        "Challenge_last_edit_time":1606158102500,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64974572",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":48.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":13.4802147223,
        "Challenge_title":"RecordIO: \"The header of the MXNet RecordIO record...does not start with a valid magic number\"",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":178.0,
        "Challenge_word_count":347,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1227171471292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":17500.0,
        "Poster_view_count":1561.0,
        "Solution_body":"<p>Is was because of CSV files in the same S3 folder as the RecordIO.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.0,
        "Solution_reading_time":0.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid magic number"
    },
    {
        "Answerer_created_time":1587281590603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":473.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":511.5126291667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using sagemaker model monitor.<\/p>\n\n<p>When capturing data, it outputs the following json file.<\/p>\n\n<pre><code>{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"MSwwLjUzLDAuNDIsMC4xMzUsMC42NzcsMC4yNTY1LDAuMTQxNSwwLjIx\",\"encoding\":\"BASE64\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"MTEuNjQzNDU1NTA1MzcxMDk0\",\"encoding\":\"BASE64\"}},\"eventMetadata\":{\"eventId\":\"33404924-c0d4-4044-9dc2-1e1f5575cb0a\",\"inferenceTime\":\"2020-06-04T05:45:45Z\"},\"eventVersion\":\"0\"}\n<\/code><\/pre>\n\n<p>I want the encoding to be csv but somehow it outputs base64.<br>\nWhen or where do we change the setting of the encoding?<br>\nIs it during the invoking the endpoint? or set when making endpoint config.<br>\nI looked for some documents but I couldn't find it.<\/p>",
        "Challenge_closed_time":1593091560008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591250114543,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sagemaker model monitor where the captured data is outputting in JSON format with BASE64 encoding instead of CSV encoding. The user is seeking guidance on where and how to change the encoding setting, either during endpoint invocation or endpoint configuration, but is unable to find relevant documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62187748",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":11.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":511.5126291667,
        "Challenge_title":"Change datacapture encoding data to csv",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":620.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532422348876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I just came across this same problem! Seems like you need to specify <code>CaptureContentTypeHeader<\/code> params to tell SageMaker which content type headers to treat as CSV (or JSON), versus the default which is to base64 encode the payload!<\/p>\n<p>So e.g. adding the following to your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">CreateEndpointConfig<\/a> call or boto3\/sagemaker SDK equivalent should fix it:<\/p>\n<pre><code>{\n   &quot;DataCaptureConfig&quot;: { \n      &quot;CaptureContentTypeHeader&quot;: { \n         &quot;CsvContentTypes&quot;: [ &quot;text\/csv&quot; ]\n      },\n   }\n}\n<\/code><\/pre>\n<p>I guess this is to allow for non-standard Content-Type headers? Providing a layer of config to resolve e.g:<\/p>\n<ul>\n<li><code>application\/x-mycoolmodel<\/code> -&gt; <code>JSON<\/code>, versus<\/li>\n<li><code>application\/x-secretsauce<\/code> -&gt; <code>BASE64<\/code><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":12.37,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"JSON instead of CSV encoding"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":23.3159841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run this on Amazon Sagemaker but I am getting this error while when I try to run it on my local machine, it works very fine.<\/p>\n<p>this is my code:<\/p>\n<pre><code>import tensorflow as tf\n\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\n    \ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)\n\ncontent_path = tf.keras.utils.get_file('YellowLabradorLooking_nw4.jpg', 'https:\/\/example.com\/IMG_20200216_163015.jpg')\n\n\nstyle_path = tf.keras.utils.get_file('kandinsky3.jpg','https:\/\/example.com\/download+(2).png')\n\n\ndef load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\ncontent_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\nimport tensorflow_hub as hub\nhub_module = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/1')\nstylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)\n\n\nfile_name = 'stylized-image5.png'\ntensor_to_image(stylized_image).save(file_name)\n<\/code><\/pre>\n<p>This is the exact error I get:<\/p>\n<pre><code>---------------------------------------------------------------------------\n<\/code><\/pre>\n<p>TypeError                                 Traceback (most recent call last)<\/p>\n<pre><code>&lt;ipython-input-24-c47a4db4880c&gt; in &lt;module&gt;()\n     53 \n     54 \n---&gt; 55 content_image = load_img(content_path)\n     56 style_image = load_img(style_path)\n     57 \n<\/code><\/pre>\n<p> in load_img(path_to_img)<\/p>\n<pre><code>     34 \n     35     shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n---&gt; 36     long_dim = max(shape)\n     37     scale = max_dim \/ long_dim\n     38 \n<\/code><\/pre>\n<p>~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/ops.py in <strong>iter<\/strong>(self)<\/p>\n<pre><code>    475     if not context.executing_eagerly():\n    476       raise TypeError(\n--&gt; 477           &quot;Tensor objects are only iterable when eager execution is &quot;\n    478           &quot;enabled. To iterate over this tensor use tf.map_fn.&quot;)\n    479     shape = self._shape_tuple()\n<\/code><\/pre>\n<p>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.<\/p>",
        "Challenge_closed_time":1594129639003,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594076057097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a Tensorflow error while trying to run a code on Amazon Sagemaker. The error message states that Tensor objects are only iterable when eager execution is enabled and suggests using tf.map_fn to iterate over the tensor. The code runs fine on the user's local machine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62765658",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":12.0,
        "Challenge_reading_time":41.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":14.8838627778,
        "Challenge_title":"Tensorflow error. TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1530.0,
        "Challenge_word_count":281,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565307837780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":570.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>Your error is being raised in this function <code>load_img<\/code>:<\/p>\n<pre><code>def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n<\/code><\/pre>\n<p>Specifically, this line:<\/p>\n<pre><code>    long_dim = max(shape)\n<\/code><\/pre>\n<p>You are passing a tensor to the <a href=\"https:\/\/docs.python.org\/3\/library\/functions.html#max\" rel=\"nofollow noreferrer\">built-in Python max function<\/a> in graph execution mode. You can only iterate through tensors in eager-execution mode. You probably want to use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/reduce_max\" rel=\"nofollow noreferrer\">tf.reduce_max<\/a> instead:<\/p>\n<pre><code>    long_dim = tf.reduce_max(shape)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1594159994640,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":13.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Tensorflow iteration error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":59.9219444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
        "Challenge_closed_time":1597488847000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1597273128000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error when deploying an MLflow registry model to Triton using the mlflow-triton-plugin with the --flavor=onnx flag. The plugin is trying to create a config.pbtxt in the destination folder before creating the model folder itself. The error can be fixed by creating the folder beforehand, but it could also be handled from the plugin side. The error message indicates that the config.pbtxt file is not found in the destination folder.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2939",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.5,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":59.9219444444,
        "Challenge_title":"mlflow checkpoints in the wrong location ",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":82,
        "Discussion_body":"@david-waterworth mind try the latest 0.9rc12? It was fixed here: #2502 \r\nThe checkpoints subfolder will go here: `mlflow\\1{guid}\\checkpoints`, is that what you want @david-waterworth ?\r\n Thanks @awaelchli  yes that's what I want - thanks!",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing config.pbtxt file"
    },
    {
        "Answerer_created_time":1285875105172,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Cruz, CA, United States",
        "Answerer_reputation_count":4051.0,
        "Answerer_view_count":461.0,
        "Challenge_adjusted_solved_time":147.4295269444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Sagemaker and have a bunch of model.tar.gz files that I need to unpack and load in sklearn. I've been testing using list_objects with delimiter to get to the tar.gz files:<\/p>\n\n<pre><code>response = s3.list_objects(\nBucket = bucket,\nPrefix = 'aleks-weekly\/models\/',\nDelimiter = '.csv'\n)\n\n\nfor i in response['Contents']:\n    print(i['Key'])\n<\/code><\/pre>\n\n<p>And then I plan to extract with<\/p>\n\n<pre><code>import tarfile\ntf = tarfile.open(model.read())\ntf.extractall()\n<\/code><\/pre>\n\n<p>But how do I get to the actual tar.gz file from s3 instead of a some boto3 object? <\/p>",
        "Challenge_closed_time":1566337639600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565806893303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to unpack and load model.tar.gz files in sklearn using Sagemaker. They have used list_objects with delimiter to get to the tar.gz files and plan to extract them using tarfile. However, they are unsure how to access the actual tar.gz file from s3 instead of a boto3 object.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57500105",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":147.4295269444,
        "Challenge_title":"Python boto3 load model tar file from s3 and unpack it",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":4787.0,
        "Challenge_word_count":89,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>You can download objects to files using <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.download_file\" rel=\"nofollow noreferrer\"><code>s3.download_file()<\/code><\/a>. This will make your code look like:<\/p>\n\n<pre><code>s3 = boto3.client('s3')\nbucket = 'my-bukkit'\nprefix = 'aleks-weekly\/models\/'\n\n# List objects matching your criteria\nresponse = s3.list_objects(\n    Bucket = bucket,\n    Prefix = prefix,\n    Delimiter = '.csv'\n)\n\n# Iterate over each file found and download it\nfor i in response['Contents']:\n    key = i['Key']\n    dest = os.path.join('\/tmp',key)\n    print(\"Downloading file\",key,\"from bucket\",bucket)\n    s3.download_file(\n        Bucket = bucket,\n        Key = key,\n        Filename = dest\n    )\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.7,
        "Solution_reading_time":9.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to access tar.gz file"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1746036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi guys, I'm new to Azure ML. Following the URL below, I tried to run my python script on local machine. By local, I meant exactly Windows on my local physical machine in my house.  But it seems python script 'transform_titanic.py' was executed on Azure.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#local-compute-target\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#local-compute-target<\/a>    <\/p>\n<p>I executed the script below on my local computer, and expected it runs 'transform_titanic.py' on my local computer.    <\/p>\n<pre><code>   from azureml.core import Environment, Experiment, ScriptRunConfig, Workspace  \n   from dotenv import load_dotenv  \n     \n   load_dotenv()  \n     \n   ws = Workspace(  \n       os.environ['SUBSCRIPTION_ID']  \n       os.environ['RESOURCE_GROUP']  \n       os.environ['WORKSPACE_NAME']  \n   )  \n     \n   exp = Experiment(workspace=ws, name='experiment')  \n     \n   env = Environment('user-managed-env')  \n   env.python.user_managed_dependencies = True  \n     \n   script_run_config = ScriptRunConfig(  \n       source_directory='src\/transform',  \n       script='transform_titanic.py',  \n       arguments=['--input_dataset_name1', 'titanic'],  \n   )  \n     \n   script_run_config.run_config.target = 'local'  \n   script_run_config.run_config.environment = env  \n     \n   run = exp.submit(config=script_run_config)  \n   print(run.get_portal_url())  \n   run.wait_for_completion()  \n<\/code><\/pre>",
        "Challenge_closed_time":1600495830720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600495202147,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is new to Azure ML and tried to run a Python script on their local Windows machine using the \"local\" compute target. However, the script was executed on Azure instead of their local machine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/99901\/what-does-local-mean-in-compute-target",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":19.0,
        "Challenge_reading_time":18.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.1746036111,
        "Challenge_title":"What does \"local\" mean in compute target?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Sorry, I found it was run on my local computer. Some artifact created in the script was in C:\\Users{username}\\AppData\\Local\\Temp\\azureml_runs\\local_experiment_XXXXXXXXXX  <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.3,
        "Solution_reading_time":2.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"script ran on Azure"
    },
    {
        "Answerer_created_time":1267440784443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Somewhere",
        "Answerer_reputation_count":15705.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":0.1769625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can train a XGBoost model using Sagemaker images like so:<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.inputs import TrainingInput\nimport os\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\ntrain_file_name = 'train.csv'\nval_file_name = 'val.csv'\nrole_arn = 'arn:aws:iam::482777693429:role\/bla_instance_role'\n\nregion_name = boto3.Session().region_name\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nprint(type(s3_input_train))\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;13&quot;,\n        &quot;eta&quot;:&quot;0.15&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\n# 1.5-1\n# 1.3-1\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role_arn,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge',\n                                          #instance_type='local', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>This work for all versions 1.2-2, 1.3-1 and 1.5-1. Unfortunately the following code only works for version 1.2-2:<\/p>\n<pre><code>import boto3\nimport os\nimport pickle as pkl \nimport tarfile\nimport pandas as pd\nimport xgboost as xgb\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\nmodel_path = 'output\/sagemaker-xgboost-2022-04-30-10-52-29-877\/output\/model.tar.gz'\nsession = boto3.Session(profile_name='default')\nsession.resource('s3').Bucket(s3_bucket_name).download_file('{}\/{}'.format(s3_prefix, model_path), 'model.tar.gz')\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel_file_name = 'xgboost-model'\nwith open(model_file_name, &quot;rb&quot;) as input_file:\ne = pkl.load(input_file) \n<\/code><\/pre>\n<p>Otherwise I get a:<\/p>\n<pre><code>_pickle.UnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>Am I missing something? Is my &quot;pickle loading code wrong&quot;?<\/p>\n<p>The version of xgboost is 1.6.0 where I run the pickle code.<\/p>",
        "Challenge_closed_time":1651318339852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651317702787,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while loading pickle files for xgboost images of version > 1.2-2 in Sagemaker. The user is able to train a model using Sagemaker images for all versions 1.2-2, 1.3-1 and 1.5-1, but the code for loading pickle files only works for version 1.2-2, resulting in an unpickling error for other versions. The user is seeking help to understand if they are missing something or if their pickle loading code is wrong. The version of xgboost used is 1.6.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72068059",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":34.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":0.1769625,
        "Challenge_title":"cannot load pickle files for xgboost images of version > 1.2-2 in sagemaker - UnpicklingError",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":185,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>I found the solution <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/2952\" rel=\"nofollow noreferrer\">here<\/a>. I will leave it in case someone come accross the same issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":2.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unpickling error for xgboost"
    },
    {
        "Answerer_created_time":1395230906503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":129.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><code>CSVS3DataSet<\/code>\/<code>HDFS3DataSet<\/code>\/<code>HDFS3DataSet<\/code> use <code>boto3<\/code>, which is known to be not thread-safe <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/resources.html?highlight=multithreading#multithreading-multiprocessing\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/resources.html?highlight=multithreading#multithreading-multiprocessing<\/a><\/p>\n\n<p>Is it OK to use these datasets with the ParallelRunner?<\/p>",
        "Challenge_closed_time":1574069164940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574069164940,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether S3 Kedro datasets, specifically CSVS3DataSet, HDFS3DataSet, and S3DataSet, are thread-safe as they use boto3, which is known to be not thread-safe. The user is asking if it is safe to use these datasets with the ParallelRunner.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58911398",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":34.3,
        "Challenge_reading_time":7.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Are S3 Kedro datasets thread-safe?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":28,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395230906503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":129.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p><code>Kedro<\/code> uses <code>s3fs<\/code>, which uses <code>boto3<\/code> library to access S3. <code>Boto3<\/code> is not thread-safe indeed, but only if you are trying to reuse the same Session object.<\/p>\n\n<p>All <code>Kedro<\/code> S3 datasets maintain separate instances of <code>S3FileSystem<\/code>, which means separate boto sessions, so it's safe.<\/p>\n\n<p>It's probably not great in terms of performance, and if you work with hundreds of S3 data sets in parallel, or thousands of small S3 datasets sequentially - the pipeline might run quite long and even fail on connection errors, but you are totally safe with a few dozens of them.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":8.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":94.0,
        "Tool":"Kedro",
        "Challenge_type":"inquiry",
        "Challenge_summary":"thread-safety of S3 Kedro datasets"
    },
    {
        "Answerer_created_time":1477057589223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH, United States",
        "Answerer_reputation_count":547.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":20.3776283333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure webjob that is calling a ML training experiment via HttpRequests, leveraging the code generated in the ML webportal:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = \"azureStorageConnectionString\",\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/Model_2018421.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>However, the request fails with the following message:<\/p>\n\n<blockquote>\n  <p>The blob reference:\n  experiments\/experimentId\/TenantId\/Model_2018421.ilearner\n  has an invalid or missing file extension. Supported file extensions\n  for this output type are: \\\\\".csv, .tsv, .arff\\\\\"<\/p>\n<\/blockquote>\n\n<p>I'm pretty confused about this, since it's written right the documentation all over the place that if I'm expecting a trained model to use \".ilearner\" as the file extension for the model.<\/p>\n\n<p>I've seen <a href=\"https:\/\/stackoverflow.com\/questions\/47920098\/use-azure-data-factory-updating-azure-machine-learning-models\">this question<\/a> asking about the same error leveraging the DataFactory, and also <a href=\"https:\/\/datascience.stackexchange.com\/questions\/27397\/azure-machine-learning-model-retraining-problem\">this question on datascience.stackexchange<\/a>. Neither one had any clues, answers, or other follow up.<\/p>\n\n<p>Any insight on what I'm missing would be greatly appreciated!<\/p>",
        "Challenge_closed_time":1525787996892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525714637430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure ML Experiment Batch Webservice Call, where the request fails with an error message stating that the blob reference has an invalid or missing file extension. The user is confused as the file extension used for the model is \".ilearner\", which is mentioned in the documentation. The user is seeking insights to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50219664",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.0,
        "Challenge_reading_time":24.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":20.3776283333,
        "Challenge_title":"Azure ML Experiment Batch Webservice Call Fails with Invalid Output Extension",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":167,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477057589223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH, United States",
        "Poster_reputation_count":547.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>For anyone looking for your \"Don't Overthink It\" moment of the day:<\/p>\n\n<p>I needed to provide TWO output blob file references:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}.csv\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameCsv}.csv\"\n                        }\n                    },\n                    {\n                        \"output2\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameIlearner}.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>There's an old saying in American English about not making assumptions, and I assumed the second output was an optional parameter used in batch operations. Since I'm not actually looking for more than one result from each call, I thought I was safe to remove the second output parameter.<\/p>\n\n<p>TL\/DR: Keep all the parameters the webservice portal's \"Consume\" tab generates, and make sure the first one is a .csv file reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.0,
        "Solution_reading_time":17.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid file extension error"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":44.7150719444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When submitting an aml compute job, I want to access the nodes where the compute happens for debugging purposes. The portal gives me the IP address, the port and the nodeID, but no password seems to exist within the portal. <\/p>\n\n<p>How am I supposed to connect to the machine?<\/p>\n\n<p>I am running on NC6 machines for a single node training. I have already tried to run the command given through the portal, but the node is (hopefully so) protected by a password.<\/p>",
        "Challenge_closed_time":1567599301196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567438326937,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing the nodes where the compute happens for debugging purposes while submitting an aml compute job. The portal provides the IP address, port, and nodeID, but there is no password available. The user is running on NC6 machines for single node training and has already tried running the command given through the portal, but the node is password-protected.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57759556",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":6.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":44.7150719444,
        "Challenge_title":"How to access node from azure machine learning compute?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":754.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488451123727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paris",
        "Poster_reputation_count":23.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>In order to be able to log in to the nodes of an AML Compute cluster, you have to provide a username and password and ssh key (ssh key is optional), <strong>when you create the cluster<\/strong>. You can only do that at creation time.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ckh6j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ckh6j.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.6,
        "Solution_reading_time":5.2,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"password-protected compute node"
    },
    {
        "Answerer_created_time":1621409485092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3609.0,
        "Answerer_view_count":2438.0,
        "Challenge_adjusted_solved_time":55.3460088889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run this 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio<\/p>\n<p><a href=\"https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing<\/a><\/p>\n<p>When I get to this step:<\/p>\n<pre><code>import gradio as gr\nimport tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n<\/code><\/pre>\n<p>I get this error:<\/p>\n<pre><code>ContextualVersionConflict: (Flask 1.0.3 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('Flask&gt;=1.1.1'), {'gradio'})\n<\/code><\/pre>\n<p>I tried to install the Flask 1.1.1 version but I get more errors. Any idea what I should do to get past this step in Azure ML Studio?<\/p>\n<pre><code>!pip install \u2013force-reinstall Flask==1.1.1\n\/\/ More errors\n<\/code><\/pre>",
        "Challenge_closed_time":1630303052768,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630103458310,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run a 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio. However, when they try to import gradio, tensorflow, and transformers, they encounter a ContextualVersionConflict error related to Flask. They tried to install Flask 1.1.1 version but it resulted in more errors. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1630103807136,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68959934",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":55.442905,
        "Challenge_title":"Contextual version conflict error, Microsoft Azure Machine Learning Studio",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":96,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1630103231523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The issue is because <code>gradio<\/code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.<\/p>\n<p>To uninstall the existing package:\n<code>!pip uninstall Flask -y<\/code><\/p>\n<p>To install latest package:\n<code>!pip install Flask&gt;=1.1.1<\/code><\/p>\n<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.<\/strong><\/p>\n<p>Finally, import gradio.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":7.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ContextualVersionConflict error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.8703702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am aware that azureml will drop support for python 2.7, but I have got some old codes and have to finish training the models. Since I will not use the codes afterwards anyway, so I do not want to spend much time to port to python 3.  <\/p>\n<p>As I tried to run the codes in python 2.7 on a compute cluster, I got the error <code>ImportError: cannot import name OutputFileDatasetConfig<\/code> coming from this line:  <br \/>\n<code>from azureml.data import OutputFileDatasetConfig<\/code>   <br \/>\nThe environment, that I have created for python 2.7, has azureml-core v1.1.5. I cannot find any documentation for this version, so I do not know, if it supports <code>OutputFileDatasetConfig<\/code>.  <\/p>\n<p>Can someone tell me, how I can run my codes in python 2.7 on compute clusters? Thanks!   <\/p>",
        "Challenge_closed_time":1637207379763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637171846430,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to run Python 2.7 scripts on a compute cluster but is encountering an error related to the import of OutputFileDatasetConfig from azureml.data. The user has created an environment for Python 2.7 with azureml-core v1.1.5 but is unsure if this version supports OutputFileDatasetConfig. The user is seeking guidance on how to run their codes in Python 2.7 on compute clusters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/631040\/how-to-run-python-2-7-scripts-on-a-computer-cluste",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":10.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":9.8703702778,
        "Challenge_title":"How to run python 2.7 scripts on a computer cluster",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":139,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=0f653a2d-e976-46b6-9242-19e12e493242\">@Lu  <\/a>     <\/p>\n<p>Thanks for reaching out to us. I have not found any official document either.     <\/p>\n<p>In this scenario, I think the quickest way to solve the problem is to raise a support ticket.    <\/p>\n<p>Let me know if you have no support plan, please share the ticket id since I will forward this issue to product team to see what we can do more.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":10.0,
        "Solution_reading_time":19.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"run Python 2.7 on clusters"
    },
    {
        "Answerer_created_time":1326951949648,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":788.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":0.4201786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to have a multi-node Dask cluster be the compute for a <code>PythonScriptStep<\/code> with AML Pipelines?<\/p>\n<p>We have a <code>PythonScriptStep<\/code> that uses <code>featuretools<\/code>'s, deep feature synthesis (<code>dfs<\/code>) (<a href=\"https:\/\/docs.featuretools.com\/en\/stable\/generated\/featuretools.dfs.html\" rel=\"nofollow noreferrer\">docs<\/a>). <code>ft.dfs()<\/code> has a param, <code>n_jobs<\/code> which allows for parallelization. When we run on a single machine, the job takes three hours, and runs much faster on a Dask. How can I operationalize this within an Azure ML pipeline?<\/p>",
        "Challenge_closed_time":1596823714436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596822201793,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to use a multi-node Dask cluster as the compute for a PythonScriptStep in AML Pipelines. They are using featuretools' deep feature synthesis which has a parameter for parallelization. The job takes three hours on a single machine and runs faster on a Dask cluster. The user is seeking guidance on how to operationalize this within an Azure ML pipeline.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63306816",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4201786111,
        "Challenge_title":"Use a Dask Cluster in a PythonScriptStep",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":362.0,
        "Challenge_word_count":77,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405457120427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Solution_body":"<p>We've been working and recently released a <code>dask_cloudprovider.AzureMLCluster<\/code> that might be of interest to you: <a href=\"https:\/\/github.com\/dask\/dask-cloudprovider\" rel=\"noreferrer\">link to repo<\/a>. You can install it via <code>pip install dask-cloudprovider<\/code>.<\/p>\n<p>The <code>AzureMLCluster<\/code> instantiates Dask cluster on AzureML service with elasticity of scaling up to 100s of nodes should you require that. The only required parameter is the <code>Workspace<\/code> object, but you can pass your own <code>ComputeTarget<\/code> should you choose to.<\/p>\n<p>An example of how to use it you can <a href=\"https:\/\/github.com\/drabastomek\/GTC\/blob\/master\/SJ_2020\/workshop\/1_Setup\/Setup.ipynb\" rel=\"noreferrer\">found here<\/a>. In this example I use my custom GPU\/RAPIDS docker image but you can use any images within the <code>Environment<\/code> class.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":11.44,
        "Solution_score_count":6.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":101.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use Dask cluster in pipeline"
    },
    {
        "Answerer_created_time":1653396543887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kolkata, India",
        "Answerer_reputation_count":1231.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":1.2397177778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can you kindly show me how do we start the Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook?\n<br> This is working fine in Google Colaboratory by the way.\n<br> What is missing here?<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\nimport sys\n\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\n\nfrom sparknlp.base import *\nfrom sparknlp.common import *\nfrom sparknlp.annotator import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" alt=\"Error\" \/><\/a><\/p>\n<p><br><br> <strong>UPDATE_2022-07-21:<\/strong>\n<br> Hi @Sayan. I am still not able to start Spark session on Vertex AI workbench Jupyterlab notebook after running the commands =(\n<a href=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\n# Included else &quot;JAVA_HOME is not set&quot;\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\nspark = sparknlp.start()\n\nprint(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\nprint(&quot;Apache Spark version: {}&quot;.format(spark.version))\n<\/code><\/pre>\n<p>The error:<\/p>\n<pre><code>\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 71: C:\/Program Files\/Java\/jdk-18.0.1.1\/bin\/java: No such file or directory\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 96: CMD: bad array subscript\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_5831\/489505405.py in &lt;module&gt;\n      6 \n      7 import sparknlp\n----&gt; 8 spark = sparknlp.start()\n      9 \n     10 print(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start(gpu, m1, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\n    242         return SparkRealTimeOutput()\n    243     else:\n--&gt; 244         spark_session = start_without_realtime_output()\n    245         return spark_session\n    246 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start_without_realtime_output()\n    152             builder.config(&quot;spark.jsl.settings.storage.cluster_tmp_dir&quot;, cluster_tmp_dir)\n    153 \n--&gt; 154         return builder.getOrCreate()\n    155 \n    156     def start_with_realtime_output():\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    267                         sparkConf.set(key, value)\n    268                     # This SparkContext may be an existing one.\n--&gt; 269                     sc = SparkContext.getOrCreate(sparkConf)\n    270                     # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    271                     # by all sessions.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    481         with SparkContext._lock:\n    482             if SparkContext._active_spark_context is None:\n--&gt; 483                 SparkContext(conf=conf or SparkConf())\n    484             assert SparkContext._active_spark_context is not None\n    485             return SparkContext._active_spark_context\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\n    193             )\n    194 \n--&gt; 195         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    196         try:\n    197             self._do_init(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    415         with SparkContext._lock:\n    416             if not SparkContext._gateway:\n--&gt; 417                 SparkContext._gateway = gateway or launch_gateway(conf)\n    418                 SparkContext._jvm = SparkContext._gateway.jvm\n    419 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf, popen_kwargs)\n    104 \n    105             if not os.path.isfile(conn_info_file):\n--&gt; 106                 raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\n    107 \n    108             with open(conn_info_file, &quot;rb&quot;) as info:\n\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>",
        "Challenge_closed_time":1658384407350,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658299714453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues in starting a Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook. The user has tried installing Spark NLP and PySpark, setting the JAVA_HOME environment variable, and running the necessary commands, but is still unable to start the Spark session. The user has also provided an error message that they received while trying to start the session.",
        "Challenge_last_edit_time":1658406889243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73047089",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":64.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":23.5258047223,
        "Challenge_title":"How to start Spark session on Vertex AI workbench Jupyterlab notebook?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":238.0,
        "Challenge_word_count":413,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651609055808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>One possible reason is that <code>Java<\/code> is not installed. When you create a <strong>Python-3 Vertex AI Workbench<\/strong> you can have either <code>Debian<\/code> or <code>Ubuntu<\/code> as an OS and it does not come with Java pre-installed. You need to install it manually.\nTo install you can use<\/p>\n<pre><code>sudo apt-get update\nsudo apt-get install default-jdk\n<\/code><\/pre>\n<p>You can follow this <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-install-java-with-apt-get-on-ubuntu-16-04\" rel=\"nofollow noreferrer\">tutorial<\/a> to install Open JDK.<\/p>\n<p>All your problems lie with installing JDK and setting its path in the environment. Once you do this properly you don't need to set path in python also.\nYour code should look something like this<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\n#no need to set the environment path\n\nimport sparknlp\n#all other imports\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><strong>EDIT:<\/strong>\nI have tried your code and had the same error.All I did was Open the terminal inside JupyterLab of the workbench and installed java there.<\/p>\n<p>Opened the JupyterLab from Workbench\n<a href=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Notebook instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Opening the terminal from <em><strong><code>File-&gt;New-&gt;Terminal<\/code><\/strong><\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>From here I downloaded and installed the Java.<\/p>\n<p>You can check whether it has been installed and added to your path by running <code>java --version<\/code> it will return the current version.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1658411352227,
        "Solution_link_count":7.0,
        "Solution_readability":9.7,
        "Solution_reading_time":28.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":240.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to start Spark session"
    },
    {
        "Answerer_created_time":1434117836363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Toronto, ON, Canada",
        "Answerer_reputation_count":78.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":0.2549266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have installed sagemaker using <code>sc.install_pypi_package(&quot;sagemaker==2.5.1&quot;)<\/code>. However, I get the following error when I try to import sagemaker and it is pointing to python2.7.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" alt=\"cannot import name git_utils\" \/><\/a><\/p>\n<p>I checked my EMR master node running pyspark and the version there is pyspark 2.4.5 running python 3.7.6.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hyctk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hyctk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So then I tried to upgrade the python version of my spark context but it says<\/p>\n<blockquote>\n<p>&quot;ValueError: Package already installed for current Spark context!&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So I thought lemme try uninstalling python2.7 from spark context and that does not let me do it, saying<\/p>\n<blockquote>\n<p>&quot;Not uninstalling python at \/usr\/lib64\/python2.7\/lib-dynload, outside\nenvironment \/tmp\/1598628537004-0&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I doing wrong? I believe the sagemaker import is failing due to spark context referring python2.7. How do I fix this?<\/p>",
        "Challenge_closed_time":1598632128583,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598630122480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has installed sagemaker using sc.install_pypi_package() but gets an error when trying to import sagemaker, which is pointing to python2.7. The EMR master node running pyspark has version pyspark 2.4.5 running python 3.7.6. The user tried to upgrade the python version of the spark context but received a ValueError. The user also attempted to uninstall python2.7 from the spark context but received an error message. The user believes the sagemaker import is failing due to the spark context referring to python2.7 and is seeking a solution to fix this issue.",
        "Challenge_last_edit_time":1598631210847,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63637178",
        "Challenge_link_count":8,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":22.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":0.5572508334,
        "Challenge_title":"Spark is running python 3.7.6 but spark context is showing python 2.7. How to fix using the spark context?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":178,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434117836363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":78.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Referred to this <a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/install-python-libraries-on-a-running-cluster-with-emr-notebooks\/\" rel=\"nofollow noreferrer\">link<\/a> and updated the python version of spark context to python3. This fixes the issue:<\/p>\n<pre><code>%%configure -f\n{ &quot;conf&quot;:{\n          &quot;spark.pyspark.python&quot;: &quot;python3&quot;,\n          &quot;spark.pyspark.virtualenv.enabled&quot;: &quot;true&quot;,\n          &quot;spark.pyspark.virtualenv.type&quot;:&quot;native&quot;,\n          &quot;spark.pyspark.virtualenv.bin.path&quot;:&quot;\/usr\/bin\/virtualenv&quot;\n         }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.2,
        "Solution_reading_time":8.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"sagemaker import error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2127.9833333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\nWe fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nThe new version of azure-cli is not compatible with the old azureml package and throws an error when creating AzureML workspace:\r\n\r\n```\r\nUnable to create the workspace. \r\n Azure Error: InvalidRequestContent\r\nMessage: The request content was invalid and could not be deserialized: 'Could not find member 'template' on object of type 'DeploymentDefinition'. Path 'template', line 1, position 12.'.\r\n```\r\n\r\nThere is an open issue at Azure cli about the similar error: https:\/\/github.com\/Azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### In which platform does it happen?\r\nLinux Ubuntu\r\n(Haven't tested on other platforms)\r\n\r\n### How do we replicate the issue?\r\nInstall reco_pyspark and run operationalization notebook.\r\n\r\n### Expected behavior (i.e. solution)\r\nFix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### Other Comments\r\nI'm working on #1158 and #900.\r\nIf fixing the azure-cli-core version is okay, then I will address this issue together.\r\n",
        "Challenge_closed_time":1603980914000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1596320174000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug related to the contrib package in AzureML. The product team has recommended removing the contrib package from `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and ensuring that all tests pass. The user is questioning whether contrib is being used or planned to be used by the team. The expected solution is for everything to run smoothly. The issue is happening on DSVM and DB platforms.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1171",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2749.0,
        "Challenge_repo_issue_count":1927.0,
        "Challenge_repo_star_count":15795.0,
        "Challenge_repo_watch_count":264.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2127.9833333333,
        "Challenge_title":"[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Discussion_body":"Seems we need to fix `azure-mgmt-cosmosdb` version as well... \r\n```\r\nAttributeError: module 'azure.mgmt.cosmosdb' has no attribute 'CosmosDB'\r\n```\r\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"bug in contrib package"
    },
    {
        "Answerer_created_time":1403553737940,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":17835.0,
        "Answerer_view_count":2203.0,
        "Challenge_adjusted_solved_time":18.1621727778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can you introduce a real sample for azure ML and show how can it be possible to see the result of the training?<\/p>",
        "Challenge_closed_time":1483472453408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1483472453410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is requesting for a real sample on how to create a training model using AzureML and how to view the training results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41451123",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":2.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"An example to create a training model on real data in AzureML",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":34,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403553737940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":17835.0,
        "Poster_view_count":2203.0,
        "Solution_body":"<p><a href=\"http:\/\/blog.learningtree.com\/how-to-build-a-predictive-model-using-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> is a good sample to create your first model.\nI should notice that I can't load data from url, as there is a forbidden error to load from url, and I don't know why! \nAnyhow, you can import data manually by copy the data from <a href=\"http:\/\/blog.learningtree.com\/wp-content\/uploads\/2015\/01\/breast-cancer-wisconsin.data_.arff_.txt\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Also, you can find the created model which is published here: \n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1<\/a><\/p>\n\n<p>About see the result of the training model, you can right click on the tick (highlighted by a red circle in the following picture) of Evaluation Model. Then, in the opened menu, go to \"Evaluation Result -> Visualization\".\n<a href=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>After that you can see a window like the following (which shows ROC curve and some related result such as accuracy of the training model):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/B39lI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B39lI.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Besides, you can see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-customer-churn-scenario\" rel=\"nofollow noreferrer\">this example<\/a> as an another sample.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1483537837232,
        "Solution_link_count":11.0,
        "Solution_readability":14.8,
        "Solution_reading_time":24.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":170.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"sample for AzureML training"
    },
    {
        "Answerer_created_time":1393284798160,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Jose, CA",
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":92.0291155556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are using SageMaker Batch Transform job and to fit as many records in a mini-batch as can fit within the <code>MaxPayloadInMB<\/code> limit, we are setting <code>BatchStrategy<\/code> to <code>MultiRecord<\/code> and <code>SplitType<\/code> to <code>Line<\/code>.<\/p>\n<p>Input to the SageMaker batch transform job is:<\/p>\n<pre><code>{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 90, &quot;Experience&quot;: 26, &quot;Income&quot;: 30, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 55, &quot;Experience&quot;: 26, &quot;Income&quot;: 450, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-69e22778-594916685f4ceca66c08bfbc&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:46:32.386Z&quot;}\n<\/code><\/pre>\n<p>This is the SageMaker batch transform job config:<\/p>\n<pre><code>apiVersion: sagemaker.aws.amazon.com\/v1\nkind: BatchTransformJob\nmetadata:\n        generateName: '...-batchtransform'\nspec:\n        batchStrategy: MultiRecord\n        dataProcessing:\n                JoinSource: Input\n                OutputFilter: $\n                inputFilter: $.requestBody\n        modelClientConfig:\n                invocationsMaxRetries: 0\n                invocationsTimeoutInSeconds: 3\n        mName: '..'\n        region: us-west-2\n        transformInput:\n                contentType: application\/json\n                dataSource:\n                        s3DataSource:\n                                s3DataType: S3Prefix\n                                s3Uri: s3:\/\/......\/part-\n                splitType: Line\n        transformOutput:\n                accept: application\/json\n                assembleWith: Line\n                kmsKeyId: '....'\n                s3OutputPath: s3:\/\/....\/batch_output\n        transformResources:\n                instanceCount: ..\n                instanceType: '..'\n<\/code><\/pre>\n<p>The SageMaker batch transform job fails with:<\/p>\n<p>Error in batch transform data-log -<\/p>\n<blockquote>\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n400 Bad Request 2022-01-27T00:55:39.781:[sagemaker\nlogs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:\n<p>Failed to decode JSON object: Extra data: line 2 column 1 (char\n163)<\/p>\n<\/blockquote>\n<p><strong>Observation:<\/strong>\nThis issue occurs when we provide <code>batchStrategy: MultiRecord<\/code> in the manifest along with these data processing configs:<\/p>\n<pre><code>dataProcessing:\n        JoinSource: Input\n        OutputFilter: $\n        inputFilter: $.requestBody\n<\/code><\/pre>\n<p><strong>NOTE:<\/strong> If we put <code>batchStrategy: SingleRecord<\/code> along with the aforementioned data processing configs, it just works fine (job succeeds)!<\/p>\n<p><strong>Question:<\/strong> How can we achieve successful run with <code>batchStrategy: MultiRecord<\/code> along with the aforementioned data processing config?<\/p>\n<p>A successful output with <code>batchStrategy: SingleRecord<\/code> looks like this:<\/p>\n<blockquote>\n<p>{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:90,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:30}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-69e22778-594916685f4ceca66c08bfbc&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:55,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:450}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:46:32.386Z&quot;}\nRegion name \u2013 optional: Relevant resource ARN \u2013 optional:\narn:aws:sagemaker:us-west-2:435945521637:transform-job\/my-pipeline-9v28r-bat-e548fbfb125946528957e0f123456789<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1643938529208,
        "Challenge_comment_count":3,
        "Challenge_created_time":1643344870580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with a SageMaker batch transform job that fails when using 'batchStrategy: MultiRecord' along with data processing. The error message indicates a failure to decode a JSON object due to extra data. The job succeeds when using 'batchStrategy: SingleRecord' with the same data processing configuration. The user is seeking a solution to successfully run the job with 'batchStrategy: MultiRecord' and the data processing configuration.",
        "Challenge_last_edit_time":1643607224392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70888883",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":22.6,
        "Challenge_reading_time":66.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":164.9051744444,
        "Challenge_title":"Sagemaker batch transform job failure for 'batchStrategy: MultiRecord' along with data processing",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":659.0,
        "Challenge_word_count":278,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316155655123,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cupertino, CA, USA",
        "Poster_reputation_count":8141.0,
        "Poster_view_count":1066.0,
        "Solution_body":"<p>When your input data is in JSON line format and you choose a SingleRecord BatchStrategy, your container will receive a single JSON payload body like below<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>However, if you use MultiRecord, Batch transform will split your JSON line input (which might contain 100 lines for example) into multiple records (say 10 records) all sent at once to your container as shown below:<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n.\n.\n.\n{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>Therefore your container should be able to handle such input for it to work. However, from the error message, I can see it is complaining about invalid JSON format as it reads the second row of the request.<\/p>\n<p>I also noticed that you have supplied <code>ContentType<\/code> and <code>AcceptType<\/code> as <code>application\/json<\/code> but instead should be <code>application\/jsonlines<\/code><\/p>\n<p>Could you please test your container to see if it can handle multiple JSON line records per single invocation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":13.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"JSON decoding failure"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.1325,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nI am following the instructions on https:\/\/github.com\/awslabs\/gluon-ts\/blob\/acfd7e14c4ef6eaa62fea6d6233a9e336f6366e4\/examples\/GluonTS_SageMaker_SDK_Tutorial.ipynb but at first step when I ran `!pip install --upgrade mxnet==1.6  git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]` I got the following error,\r\n\r\n## Error message or code output\r\n```Obtaining gluonts[dev] from git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]\r\n  Updating .\/src\/gluonts clone\r\n  Running command git fetch -q --tags\r\n  Running command git reset --hard -q fc203f51f01036e854ce6a0da1a43b562074e187\r\n  Installing build dependencies ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel\r\n       cwd: None\r\n  Complete output (14 lines):\r\n  Traceback (most recent call last):\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/__main__.py\", line 16, in <module>\r\n      from pip._internal.cli.main import main as _main  # isort:skip # noqa\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/_internal\/cli\/main.py\", line 5, in <module>\r\n      import locale\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/locale.py\", line 16, in <module>\r\n      import re\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py\", line 142, in <module>\r\n      class RegexFlag(enum.IntFlag):\r\n  AttributeError: module 'enum' has no attribute 'IntFlag'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel Check the logs for full command output.\r\n\r\n```\r\n\r\n\r\n## Environment\r\nNote: Previously, I installed Gluon-TS (0.5.2) using `! pip install --upgrade mxnet==1.6 gluonts` and if I do `! pip list` I can see the package is installed but when I ran `!pip uninstall glounts` it says `WARNING: Skipping glounts as it is not installed.`\r\n\r\n- Operating system: Sagemaker notebook instance with conda_mxnet_p36 kernel.\r\n- Python version: 3.6\r\n- GluonTS version: 0.5.2 is already installed.\r\n- MXNet version:1.6",
        "Challenge_closed_time":1600271697000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1600235220000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered challenges when attempting to run the example script 'benchmark_m4.py' on a gpu-instance \"ml.p2.xlarge\" on aws, resulting in a keyerror.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/1039",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":13.9,
        "Challenge_reading_time":38.77,
        "Challenge_repo_contributor_count":98.0,
        "Challenge_repo_fork_count":696.0,
        "Challenge_repo_issue_count":2506.0,
        "Challenge_repo_star_count":3591.0,
        "Challenge_repo_watch_count":70.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":10.1325,
        "Challenge_title":"Issue with installing GlounTS on Sagemaker notebook instance from Github",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":254,
        "Discussion_body":"According to [this](https:\/\/github.com\/iterative\/dvc\/issues\/1995), it could be that `enum34` is installed.\r\n\r\nCan you check whether this is also the case here? Thanks @jaheba ! That was the issue and by running `!pip uninstall -y enum34` it is resolved.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"key error on GPU instance"
    },
    {
        "Answerer_created_time":1251699930780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Clara, CA, United States",
        "Answerer_reputation_count":1538.0,
        "Answerer_view_count":198.0,
        "Challenge_adjusted_solved_time":12.0071708333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I uploaded a pretrained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples. It just returned a list of false predictions with no confidence score. I don't see anywhere in the SDK documentation or Google console for how to get batch predictions to include the confidence scores. Is that something Vertex AI can do?<\/p>\n<p>My intent is to automate a batch prediction pipeline using the following code.<\/p>\n<pre><code># Predict\n# &quot;csv&quot;, &quot;&quot;bigquery&quot;, &quot;tf-record&quot;, &quot;tf-record-gzip&quot;, or &quot;file-list&quot;\nbatch_prediction_job = model.batch_predict(\n    job_display_name = job_display_name,\n    gcs_source = input_path,\n    instances_format = &quot;&quot;, # jsonl, csv, bigquery, \n    gcs_destination_prefix = output_path,\n    starting_replica_count = 1,\n    max_replica_count = 10,\n    sync = True,\n)\n\nbatch_prediction_job.wait()\n\nreturn batch_prediction_job.resource_name\n<\/code><\/pre>\n<p>I tried it out in google console as a test to make sure my input data was properly formatted.<\/p>",
        "Challenge_closed_time":1637647501512,
        "Challenge_comment_count":3,
        "Challenge_created_time":1637604275697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user uploaded a pre-trained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples, but the predictions returned were false with no confidence score. The user is looking for a way to get batch predictions to include confidence scores, but cannot find any information in the SDK documentation or Google console. The user's intent is to automate a batch prediction pipeline.",
        "Challenge_last_edit_time":1637686316647,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70070421",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":10.8,
        "Challenge_reading_time":14.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":12.0071708333,
        "Challenge_title":"Return confidence score with custom model for Vertex AI batch predictions",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":319.0,
        "Challenge_word_count":133,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417013182680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA",
        "Poster_reputation_count":1256.0,
        "Poster_view_count":245.0,
        "Solution_body":"<p>I don't think so; the stock sklearn container provided by vertex doesn't provide such a score I guess. You might need to write a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":3.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"false predictions, no score"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":50.3174552778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can you please share any code examples for training random forests with GPU on Azure using libraries.  <br \/>\nI want to run on the multiple nodes.<\/p>",
        "Challenge_closed_time":1595231268032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595050125193,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking code examples for training random forests with GPU on Azure using libraries and wants to run it on multiple nodes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/49008\/random-forests-on-azure-gpu-vm-using-the-sdk",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":2.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":50.3174552778,
        "Challenge_title":"Random forests on Azure GPU VM using the SDK",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@vautoml-0887<\/a> Thanks for the question. You can run LightGBM with boosting=random_forest, Please follow the below documentation:  <br \/>\n<a href=\"https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting\">https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting<\/a><\/p>\n<p>Here is a general tutorial on how to run LightGBM on GPU, You can run it on any Azure GPU VM:  <br \/>\n<a href=\"https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst\">https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst<\/a><\/p>\n<p>If you need to run it on multiple nodes, there is also a distributed spark implementation available at <a href=\"https:\/\/github.com\/Azure\/mmlspark\">https:\/\/github.com\/Azure\/mmlspark<\/a>.<\/p>\n<p>Random Forests for the GPU using PyCUDA: <a href=\"https:\/\/pypi.org\/project\/cudatree\/\">https:\/\/pypi.org\/project\/cudatree\/<\/a>  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.0,
        "Solution_reading_time":12.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"train random forests with GPU"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.4538166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to find the best way to run my machine learning models on GPUs for inference as an http request. Do Azure functions support GPUs? if not, what are other options I can look into?  <\/p>\n<p>note: I also want to use packaged models, not necessarily ones of my own creation (such as <a href=\"https:\/\/github.com\/JaidedAI\/EasyOCR\">easyOCR<\/a> for python)  <\/p>",
        "Challenge_closed_time":1630921359623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630916125883,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for the best way to deploy their machine learning model using GPUs as a web-based API. They are unsure if Azure functions support GPUs and are seeking alternative options. Additionally, they want to use pre-packaged models like easyOCR for Python.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/541074\/what-is-the-best-way-to-deploy-my-machine-learning",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":5.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.4538166667,
        "Challenge_title":"What is the best way to deploy my machine learning model using GPUs, specifically as a web based API?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,    <\/p>\n<p>If you need GPU support on ML inference the only supported option is the Azure Kubernetes Service as stated in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus\">this documentation<\/a>    <\/p>\n<p>For guidance on deploying an ML model to AKS, please refer to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">this documenation on deploying to AKS<\/a>    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":6.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy ML model with GPUs"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":145.8923630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Newbie data scientist here, I am just starting my way in Azure, is there any I should start NLP? Any trained model or code sample? Thank you for any idea<\/p>",
        "Challenge_closed_time":1667776545220,
        "Challenge_comment_count":1,
        "Challenge_created_time":1667251332713,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is a newbie data scientist who is looking for guidance on how to start with NLP in Azure. They are seeking suggestions for trained models or code samples to help them get started.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1069986\/trained-nlp-model-snippet",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":145.8923630556,
        "Challenge_title":"Trained NLP model snippet",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=12b79ebe-b30f-4203-a714-62377c3d557b\">@jackson schmidt  <\/a>     <\/p>\n<p>Sorry I have not heard from you. I have done some researches around NLP in Azure. This can be done by two ways -    <\/p>\n<ol>\n<li> Azure Machine Learning Python SDK\/ ML CLI extension    <br \/>\nWe don't have any trained model you can use in Azure ML but you do have the SDK supporting you to train your model    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli<\/a>    <\/li>\n<li> NLP Server     <br \/>\nApache Spark is a parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Azure Synapse Analytics, Azure HDInsight, and Azure Databricks offer access to Spark and take advantage of its processing power.    <\/li>\n<\/ol>\n<p>For customized NLP workloads, Spark NLP serves as an efficient framework for processing a large amount of text. This open-source NLP library provides Python, Java, and Scala libraries that offer the full functionality of traditional NLP libraries such as spaCy, NLTK, Stanford CoreNLP, and Open NLP. Spark NLP also offers functionality such as spell checking, sentiment analysis, and document classification. Spark NLP improves on previous efforts by providing state-of-the-art accuracy, speed, and scalability.    <\/p>\n<p><img src=\"https:\/\/learn.microsoft.com\/en-us\/azure\/architecture\/data-guide\/images\/natural-language-processing-functionality.png\" alt=\"natural-language-processing-functionality.png\" \/>    <\/p>\n<p>The NLP Server is available in Azure Marketplace. To explore large-scale custom NLP in Azure, see NLP Server - <a href=\"https:\/\/azuremarketplace.microsoft.com\/en-US\/marketplace\/apps\/johnsnowlabsinc1646051154808.nlp_server?ocid=gtmrewards_whatsnewblog_nlp_server_040622\">https:\/\/azuremarketplace.microsoft.com\/en-US\/marketplace\/apps\/johnsnowlabsinc1646051154808.nlp_server?ocid=gtmrewards_whatsnewblog_nlp_server_040622<\/a>    <\/p>\n<ol start=\"3\">\n<li> Azure Language Service    <br \/>\nThough we don't have trained model in Azure ML, but we do have REST APIs you can use for Text Analytics, Sentiment Analytics and so on functions for NLP, I would suggest you to check on the document, it may help you achieve your bussiness goals.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/<\/a>    <\/li>\n<\/ol>\n<p>I hope those information helps. Please let me know if you have any questions regarding to any of above.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.6,
        "Solution_reading_time":36.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":311.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"start NLP in Azure"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":3.4251711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed a machine learning model using Azure ML's clustering. Few of the requests made from the cluster are triggering 404 HTTP error. I followed the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">document<\/a> to do modifications in my swagger.json file. Finally ended up with &quot;list index out of range&quot; error. It seems to be having issue with the global parameter but I am no sure about it. I am using the API from postman with some default headers like mentioned in the body below<\/p>\n<pre><code>{\n    &quot;Inputs&quot;: {\n         &quot;input_1&quot; : &quot;content&quot;\n         &quot;input_2: : &quot;content&quot;\n         ......\n    },\n    &quot;GlobalParameters&quot;: 0\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1651111084096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651094225790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a \"list index out of range\" error while modifying the swagger.json file as per the Azure ML documentation to resolve the 404 HTTP error triggered by some requests made from the cluster. The error seems to be related to the global parameter, but the user is unsure. The user is using the API from Postman with default headers.",
        "Challenge_last_edit_time":1651098753480,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72035391",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":10.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4.6828627778,
        "Challenge_title":"AzureML schema \"list index out of range\" error",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":78.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Change the &quot;GlobalParameter&quot; value to any floating number other than 1.0 or even you can remove it and execute. Sometimes, Global parameter will cause the issue. Check the below documentation.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.0,
        "Solution_reading_time":6.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"list index out of range error"
    },
    {
        "Answerer_created_time":1424548126510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":665.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":129.3699730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My understanding is Dev Endpoints in AWS Glue can be used to develop code iteratively and then deploy it to a Glue job. I find this specially useful when developing Spark jobs because every time you run a job, it takes several minutes to launch a Hadoop cluster in the background. However, I am seeing a discrepancy when using Python shell in Glue instead of Spark. <code>Import pg<\/code> doesn't work in a Dev Endpoint I created using Sagemaker JupyterLab Python notebook, but works in AWS Glue when I create a job using Python shell. Shouldn't the same libraries exist in the dev endpoint that exist in Glue? What is the point of having a dev endpoint if you cannot reproduce the same code in both places (dev endpoint and the Glue job)?<\/p>",
        "Challenge_closed_time":1552530714763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552064664580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is experiencing a discrepancy between AWS Glue and its Dev Endpoint when using Python shell instead of Spark. The user is unable to import a library in a Dev Endpoint created using Sagemaker JupyterLab Python notebook, but it works in AWS Glue when creating a job using Python shell. The user questions the purpose of having a Dev Endpoint if the same code cannot be reproduced in both places.",
        "Challenge_last_edit_time":1552064982860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55067802",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":129.4583841667,
        "Challenge_title":"Discrepancy between AWS Glue and its Dev Endpoint",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":139,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462207957683,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1305.0,
        "Poster_view_count":174.0,
        "Solution_body":"<p>Firstly, Python shell jobs would not launch a Hadooo Cluster in the backend as it does not give you a Spark environment for your jobs.\nSecondly, since PyGreSQL is not written in Pure Python, it will not work with Glue's native environment (Glue Spark Job, Dev endpoint etc)\nThirdly, Python Shell has additional support for certain package built-in.<\/p>\n\n<p>Thus, I don't see a point of using DevEndpoint for Python Shell jobs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":5.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"library import issue"
    },
    {
        "Answerer_created_time":1507660761310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Madrid, Espa\u00f1a",
        "Answerer_reputation_count":492.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":10.7241647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretty big (~200Gb, ~20M lines) raw jsonl dataset. I need to extract important properties from there and store the intermediate dataset in csv for further conversion into something like HDF5, parquet, etc. Obviously, I can't use <code>JSONDataSet<\/code> for loading raw dataset, because it utilizes <code>pandas.read_json<\/code> under the hood, and using pandas for the dataset of such size sounds like a bad idea. So I'm thinking about reading the raw dataset line by line, process and append processed data line by line to the intermediate dataset.<\/p>\n\n<p>What I can't understand is how to make this compatible with <code>AbstractDataSet<\/code> with its <code>_load<\/code> and <code>_save<\/code> methods.<\/p>\n\n<p>P.S. I understand I can move this out of kedro's context, and introduce preprocessed dataset as a raw one, but that kinda breaks the whole idea of complete pipelines. <\/p>",
        "Challenge_closed_time":1582275656936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582237049943,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a large raw jsonl dataset of around 200GB and 20 million lines that needs to be processed to extract important properties and stored in an intermediate dataset in CSV format for further conversion into HDF5, parquet, etc. However, using pandas for such a large dataset is not feasible. The user is considering reading the raw dataset line by line, processing it, and appending the processed data line by line to the intermediate dataset. The challenge is to make this compatible with AbstractDataSet's _load and _save methods while maintaining the idea of complete pipelines.",
        "Challenge_last_edit_time":1583417458307,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60329363",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":11.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10.7241647222,
        "Challenge_title":"How to process huge datasets in kedro",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":656.0,
        "Challenge_word_count":141,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324477592580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1315.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>Try to use pyspark to leverage lazy evaluation and batch execution. \nSparkDataSet is implemented in kedro.contib.io.spark_data_set<\/p>\n\n<p>Sample catalog config for jsonl:<\/p>\n\n<pre><code>your_dataset_name:   \n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: \"\\file_path\"\n  file_format: json\n  load_args:\n    multiline: True\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1582292099020,
        "Solution_link_count":0.0,
        "Solution_readability":13.5,
        "Solution_reading_time":4.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Kedro",
        "Challenge_type":"anomaly",
        "Challenge_summary":"large dataset processing"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":141.0333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi to all\n\nIm trying to run a procedure looking to reduce the number of features for a model.\n\nThe first try was with google Colab pro+ but it keep crashing and nver run the entire process, then I got a VM\u00a0n1-highmem-8 that has:\u00a0\n\nGPUs1 x NVIDIA Tesla V100\u00a0 +\u00a0\u00a0n1-highmem-8 (vCPUs: 8, RAM: 52GB)\n\nand still not getting the process done.\n\nThe question is how to determin which type of machine should I use? Can I get any metric from the cell that is runing in colab and be able to determin the Type of VM that I need?",
        "Challenge_closed_time":1659349860000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658842140000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to reduce the number of features for a model using Google Colab pro+ but it keeps crashing. They have now tried using a VM n1-highmem-8 with 1 NVIDIA Tesla V100 GPU and 8 vCPUs with 52GB RAM, but the process is still not completing. The user is seeking advice on how to determine which type of machine they should use and if there are any metrics from the cell running in Colab that can help them determine the appropriate VM.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-determin-which-GCP-VM-do-I-need-for-ML\/m-p\/447075#M448",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":6.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":141.0333333333,
        "Challenge_title":"How to determin which GCP VM do I need for ML",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":110,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs.\u00a0\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":8.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":130.0,
        "Tool":"Vertex AI",
        "Challenge_type":"inquiry",
        "Challenge_summary":"determine appropriate VM type"
    },
    {
        "Answerer_created_time":1555488556820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Baillargues, France",
        "Answerer_reputation_count":56447.0,
        "Answerer_view_count":9158.0,
        "Challenge_adjusted_solved_time":5.0890466667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a Vertex AI pipeline similar to <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_automl_images.ipynb\" rel=\"nofollow noreferrer\">this.<\/a><\/p>\n<p>Now the pipeline has reference to a csv file. So if this csv file changes the pipeline needs to be recreated.<\/p>\n<p>Is there any way to pass a new csv as a parameter to the pipeline when it is re-run? That is without recreating the pipeline using the notebook?<\/p>\n<p>If not, is there a best practice way of auto updating the dataset, model and deployment?<\/p>",
        "Challenge_closed_time":1641588471208,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641570150640,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a Vertex AI pipeline that references a CSV file, but needs to recreate the pipeline every time the CSV file changes. The user is looking for a way to pass a new CSV file as a parameter to the pipeline without recreating it using the notebook. The user is also asking for best practices for auto-updating the dataset, model, and deployment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70623713",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":5.0890466667,
        "Challenge_title":"How can I pass parameters to a Vertex AI Platform Pipeline?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":611.0,
        "Challenge_word_count":88,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>Have a look to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/build-pipeline\" rel=\"nofollow noreferrer\">that documentation<\/a>.<\/p>\n<p>You can define your pipeline like that<\/p>\n<pre><code>...\n# Define the workflow of the pipeline.\n@kfp.dsl.pipeline(\n    name=&quot;automl-image-training-v2&quot;,\n    pipeline_root=pipeline_root_path)\ndef pipeline(project_id: str):\n...\n<\/code><\/pre>\n<p>(you have something very similar in your notebook sample)<\/p>\n<p>Then, when you invoke your pipeline, you can pass some parameter<\/p>\n<pre><code>import google.cloud.aiplatform as aip\n\njob = aip.PipelineJob(\n    display_name=&quot;automl-image-training-v2&quot;,\n    template_path=&quot;image_classif_pipeline.json&quot;,\n    pipeline_root=pipeline_root_path,\n    parameter_values={\n        'project_id': project_id\n    }\n)\n\njob.submit()\n<\/code><\/pre>\n<p>You can see the <code>project_id<\/code> a dict parameter in the parameter values, and in parameter of your pipeline function.<\/p>\n<p>Do the same for your CSV file name!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":13.16,
        "Solution_score_count":3.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":91.0,
        "Tool":"Vertex AI",
        "Challenge_type":"inquiry",
        "Challenge_summary":"auto-update dataset, model, deployment"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.2563888889,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to control a Spark cluster (using SparkR) from a Sagemaker notebook. I followed these instructions closely: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/  and got it to work.\n\nToday when I try to run the SageMaker notebook (using the exact same code as before) I inexplicably get this error:  \n\n    An error was encountered:\n    [1] \"Error in callJMethod(sparkSession, \\\"read\\\"): Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed.\"\n\nDoes anyone know why this is? I terminated the SparkR kernel and am still getting this error.\n",
        "Challenge_closed_time":1591041375000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591029652000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to control a Spark cluster using SparkR from a Sagemaker notebook and followed the instructions provided by AWS. However, when trying to run the notebook again, the user encountered an error message stating that the Spark operations need to be re-executed. The user terminated the SparkR kernel but is still getting the same error.",
        "Challenge_last_edit_time":1667926595812,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqyiKb_XvRhGxm1RxwjXhJQ\/sparkr-not-working",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.2563888889,
        "Challenge_title":"SparkR not working",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":75.0,
        "Challenge_word_count":87,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You cannot have multiple SparkContexts in one JVM. The issue is resolved as WON'T FIX. You have to stop the spark session which spawned the sparkcontext (which you have already done). \n\n`sparkR.session.stop()`\n\nhttps:\/\/issues.apache.org\/jira\/browse\/SPARK-2243",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925571654,
        "Solution_link_count":1.0,
        "Solution_readability":7.6,
        "Solution_reading_time":3.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"re-execution error in SparkR"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":36.5267972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have lots of run files created by running PyTorch estimator\/ ScriptRunStep experiments that are saved in azureml blob storage container. Previously, I'd been viewing these runs in the Experiments tab of the ml.azure.com portal and associating tags to these runs to categorise and load the desired models.<\/p>\n<p>However, a coworker recently deleted my workspace. I created a new one which is connected to the previously-existing blob container, the run files therefore still exist and can be accessed on this new workspace, but they no longer show up in the Experiment viewer on ml.azure.com. Neither can I see the tags I'd associated to the runs.<\/p>\n<p><strong>Is there any way to load these old run files into the Experiment viewer or is it only possible to view runs created inside the current workspace?<\/strong><\/p>\n<p>Sample scriptrunconfig code:<\/p>\n<pre><code>data_ref = DataReference(datastore=ds,\n                         data_reference_name=&quot;&lt;name&gt;&quot;,        \n                         path_on_datastore = &quot;&lt;path&gt;&quot;)\nargs = ['--data_dir',   str(data_ref),     \n        '--num_epochs', 30,     \n        '--lr',         0.01,          \n        '--classifier', 'int_ext' ]  \n\nsrc = ScriptRunConfig(source_directory='.',                       \n                      arguments=args,                      \n                      compute_target = compute_target,                       \n                      environment = env,                       \n                      script='train.py') \nsrc.run_config.data_references = {data_ref.data_reference_name: \n                                  data_ref.to_config()} \n<\/code><\/pre>",
        "Challenge_closed_time":1616813814780,
        "Challenge_comment_count":7,
        "Challenge_created_time":1616682318310,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has run PyTorch estimator\/ScriptRunStep experiments and saved the run files in an AzureML blob storage container. However, after a coworker deleted their workspace, they created a new one that is connected to the same blob container, but the old run files no longer show up in the Experiment viewer on ml.azure.com. The user is seeking a way to load these old run files into the Experiment viewer or if it is only possible to view runs created inside the current workspace.",
        "Challenge_last_edit_time":1617188538763,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66801546",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":12.8,
        "Challenge_reading_time":17.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":36.5267972222,
        "Challenge_title":"Load Azure ML experiment run information from datastore",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":168,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606130833532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Sorry for your loss! First, I'd make absolutely sure that you can't recover the deleted workspace. Definitely worthwhile to open an priority support ticket with Azure.<\/p>\n<p>Another thing you might try is:<\/p>\n<ol>\n<li>create a new workspace (which will create a new storage account for you for the new workspace's logs)<\/li>\n<li>copy your old workspace's data into the new workspace's storage account.<\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":5.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"view old run files"
    },
    {
        "Answerer_created_time":1266513014020,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2280.0,
        "Answerer_view_count":136.0,
        "Challenge_adjusted_solved_time":487.0128825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to cache part of the stream executed successfully (marked as tick) in Azure ML so that next time run will start from the same point onwards.\nAny help is appreciable.<\/p>",
        "Challenge_closed_time":1429645020147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1427891773770,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking assistance in caching a portion of a successful stream in Azure ML to allow for the next run to start from the same point onwards.",
        "Challenge_last_edit_time":1488123478287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29391016",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":2.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":487.0128825,
        "Challenge_title":"Cache part of experiment in AzureML same as SPSS modeler?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":41,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427358932900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I believe Azure ML already does this.  When you run it the second time, if nothing upstream from that tick has changed it should just load the results from the previous run.  It may take a few seconds for Azure ML to recognize that it is cached and reload it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":3.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"cache stream in Azure ML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.3539691667,
        "Challenge_answer_count":1,
        "Challenge_body":"I am currently utilizing an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. The data consists of 7157 time series with 152 timesteps in the training set and 52 timesteps in the test set respectively. I estimate the run time for the tuning job on this specific instance type to take about 4-5 days. Looking to find out if DeepAR is engineered to take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. Also would be great for recommendations as to which Accelerated Computing instance would be optimal for this scenario.",
        "Challenge_closed_time":1644892187263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644862112974,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is currently using an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. They are looking for advice on whether DeepAR can take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. They are also seeking recommendations for the optimal Accelerated Computing instance for this scenario.",
        "Challenge_last_edit_time":1668563778934,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUnYV-WoO2R3KY4sNEq-Dshw\/optimal-notebook-instance-type-for-deepar-in-aws-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":8.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.3539691667,
        "Challenge_title":"Optimal notebook instance type for DeepAR in AWS Sagemaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":232.0,
        "Challenge_word_count":121,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"(As detailed further on the [algorithm details page](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html#deepar-instances)), **yes**, the SageMaker DeepAR algorithm implementation is able to train on GPU-accelerated instances to speed up more challenging jobs. There's also a handy [reference table here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/common-info-all-im-models.html) listing all the SageMaker built-in algorithms and whether they're likely to be accelerated with GPU.\n\n**However**, to be clear, it shouldn't be the *notebook* instance type that affects this... Typically when training models on SageMaker, the notebook would provide your interactive compute environment but you'd run training in *training jobs* - for example using the [SageMaker Python SDK](https:\/\/sagemaker.readthedocs.io\/en\/stable\/) `Estimator` class as shown in the sample notebooks for DeepAR [electricity](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb) and [synthetic](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/deepar_synthetic\/deepar_synthetic.ipynb). The instance type you select for training is independent of the instance type you use for your notebook - for example in the electricity notebook it's set as follows:\n\n```python\nestimator = sagemaker.estimator.Estimator(\n    image_uri=image_name,\n    sagemaker_session=sagemaker_session,\n    role=role,\n    train_instance_count=1,  # <-- Setting training instance count\n    train_instance_type=\"ml.c4.2xlarge\",  # <-- Setting training instance type\n    base_job_name=\"deepar-electricity-demo\",\n    output_path=s3_output_path,\n)\n```\n\nSo normally I wouldn't expect you to need to change your *notebook* instance type to speed up training - just edit the configuration of your training job from within the notebook.\n\nSuggesting a particular type is tricky because [DeepAR hyperparameters](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar_hyperparameters.html) like `context_length`, `embedding_dimension`, and `mini_batch_size` will affect how much GPU capacity is needed for a particular run. Since you're coming from CPU-only baseline, I'd maybe suggest to start small with trying out single-GPU `g4dn.xlarge`, `g5.xlarge` or `p3.2xlarge` instances, perhaps starting with the lowest cost-per-hour? You can keep  an eye on your jobs' GPUUtilization and GPUMemoryUtilization metrics to check whether utilization is low on instances like p3 with \"bigger\" GPUs. Increasing `mini_batch_size` should help fill extra capacity on these and complete your job faster, but it will probably affect model convergence - so may need to tune other parameters like `learning_rate` to try and compensate. So considering all of this, you may find trade-offs between speed and total cost, or speed and accuracy, for good hyperparameter combinations on your dataset. Of course you could also scale up to multi-GPU instance types if you'd like to accelerate further.\n\nIf I understood right you're also using SageMaker Automatic Hyperparameter Tuning to search these parameters, something like [this XGBoost notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/xgboost_random_log\/hpo_xgboost_random_log.ipynb) with the `HyperparameterTuner` class?\n\nIn that case would also mention:\n- Increasing the `max_parallel_jobs` parameter may accelerate the overall run time (by running more of the individual training jobs in parallel) - with a trade-off on how much information is available when each training job in the budget is kicked off.\n- If you're planning to run this training regularly on a dataset which evolves over time, you probably don't need to run HPO each time: Will likely see good results using your previously-optimized hyperparameters, unless something materially changes in the nature of the data and patterns.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1644892187264,
        "Solution_link_count":7.0,
        "Solution_readability":16.5,
        "Solution_reading_time":50.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":448.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"GPU for DeepAR training"
    },
    {
        "Answerer_created_time":1448457543732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":211.1152302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there possibility to get stream from Spark Streaming or Apache Storm into Azure Machine Learning? In <strong><em>reader<\/em><\/strong> option there is an input to read data from Hive database\n<a href=\"https:\/\/i.stack.imgur.com\/8Em26.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8Em26.png\" alt=\"hive\"><\/a><\/p>\n\n<p>but how to achive real time stream of data from Spark or Storm, for example <strong><em>Real-time fraud detection<\/em><\/strong><\/p>",
        "Challenge_closed_time":1448457555592,
        "Challenge_comment_count":0,
        "Challenge_created_time":1447697540763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking a way to connect Azure Machine Learning with Spark Streaming or Apache Storm to achieve real-time stream of data for tasks such as real-time fraud detection. The reader option in Azure Machine Learning allows input from Hive database, but the user is looking for a way to get a stream of data from Spark or Storm.",
        "Challenge_last_edit_time":1456849966663,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33741912",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":211.1152302778,
        "Challenge_title":"How connect Azure Machine Learning and Spark Streaming or Apache Storm",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":549.0,
        "Challenge_word_count":63,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327481639092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poznan, Poland",
        "Poster_reputation_count":2923.0,
        "Poster_view_count":838.0,
        "Solution_body":"<p>To do real time Fraud detection typically you will create a Model on Azure ML, then publish that model to oWeb service, then on you Spark or Storm system you will call that Web service, in  sequence ( like payment happened on commercial sites for example), then you will get an immediate answer about the actual parameters you had sent in you web service call.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":27.1,
        "Solution_reading_time":4.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"connect AML with Spark\/Storm"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5533333333,
        "Challenge_answer_count":0,
        "Challenge_body":"The following sample notebook fails \r\n### img-classification-part1-training.ipynb\r\n\r\nwhen running:\r\n\r\n### from azureml.core import Dataset\r\n\r\nfrom azureml.core import Dataset\r\nfrom azureml.opendatasets import MNIST\r\n\r\ndata_folder = os.path.join(os.getcwd(), 'data')\r\nos.makedirs(data_folder, exist_ok=True)\r\n\r\nmnist_file_dataset = MNIST.get_file_dataset()\r\nmnist_file_dataset.download(data_folder, overwrite=True)\r\n\r\nmnist_file_dataset = mnist_file_dataset.register(workspace=ws,\r\n                                                 name='mnist_opendataset',\r\n                                                 description='training and test dataset',\r\n                                                 create_new_version=True)\r\n\r\n\r\n**Here is the error**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-ac2e91b46eec> in <module>\r\n----> 1 from azureml.core import Dataset\r\n      2 from azureml.opendatasets import MNIST\r\n      3 \r\n      4 data_folder = os.path.join(os.getcwd(), 'data')\r\n      5 os.makedirs(data_folder, exist_ok=True)\r\n\r\nImportError: cannot import name 'Dataset'\r\n\r\nreference: yml file:\r\nname: img-classification-part1-training\r\ndependencies:\r\n- pip:\r\n  - azureml-sdk\r\n  - azureml-widgets\r\n  - matplotlib\r\n  - sklearn\r\n  - pandas\r\n  - azureml-opendatasets\r\n\r\nAzure ML SDK Version:  1.0.17\r\nPython 3.6 - AzureML\r\n\r\n@microsoft\r\nPlease kindly investigate.\r\nMany thanks :)",
        "Challenge_closed_time":1581440044000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1581438052000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while using Azure Machine Learning with the XGBoostClassifier model. The error message indicates that the blacklisted and whitelisted models are the same, and suggests removing models from the blacklist or adding models to the whitelist. The user provided the automl_settings and automl_config used, and noted that XGBoostClassifier was installed in the notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/787",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":17.47,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.5533333333,
        "Challenge_title":"Import Error - from azureml.core import Dataset  - ImportError: cannot import name 'Dataset'",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Discussion_body":"@andrewkinsella, version `1.0.17` is from [almost a year ago](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2019-02-25). During that time, the `Datasets` class has evolved significantly (for the better). Can you try upgrading the SDK to the newest version and trying again? @MayMSFT  Thank you very much @swanderz \r\nI will try your recommendation.",
        "Discussion_score_count":3.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"same blacklisted and whitelisted models"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":109.7675,
        "Challenge_answer_count":0,
        "Challenge_body":"It seems they both assume that the current working dir is where they can find the `.git` and `.dvc` dirs.\r\nWe should correctly detect those paths, as it affects all our logic to e.g. automatically dvc init on behalf of the user.\r\n\r\nRelevant resources:\r\n1. https:\/\/stackoverflow.com\/a\/957978\r\n2. https:\/\/dvc.org\/doc\/command-reference\/root",
        "Challenge_closed_time":1630227884000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629832721000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the DVC add prompt is always displayed, even when there is no selection to make since the list of files is empty. The prompt should only be displayed if there is something to add.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/92",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.02,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":145.0,
        "Challenge_repo_star_count":369.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":109.7675,
        "Challenge_title":"DVC and Git services don't correctly detect the repo root directory",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unnecessary add prompt"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.6524075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>While distributed dask can be setup manually on AML compute, the process requires lot of configs to be maintained. Is there any native support.<\/p>",
        "Challenge_closed_time":1666266282800,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666227934133,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in setting up distributed dask on Azure Machine Learning compute due to the need for manual configuration. They are seeking information on whether there is native support available for this process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1055350\/ray-dask-native-support-be-added-to-azure-machine",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":2.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":10.6524075,
        "Challenge_title":"ray+dask native support  be added to Azure Machine Learning",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=dfa9d536-725c-462d-87c8-47fbafb1a2bc\">@D-0887  <\/a> Thanks for the question. you can do is to setup the compute cluster &amp; compute instance in the same vnet and pip install ray-on-aml. This allows both interactive and job use of Ray and Dask right within Azure ML.    <\/p>\n<p>Here is the document Library to turn Azure ML Compute into Ray and Dask cluster.    <br \/>\n<a href=\"https:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784\">https:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":9.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"native support for distributed dask"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":0.2048830556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I execute code without parallel computation, <code>n_trials<\/code> in the <code>optimize<\/code> function means how many trials the program runs. When executed via parallel computation (following the tutorial <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/10_key_features\/004_distributed.html\" rel=\"nofollow noreferrer\">here<\/a> via launching it again in another console), it does <code>n_trials<\/code> for each process, not for all the sum of processes like I would like.<\/p>\n<p>Is there a way to make sure that the sum of all parallel processes' trials are equal to a fixed number, regardless of how many process I launch?<\/p>",
        "Challenge_closed_time":1629887227052,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629886489473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with setting the n_trials parameter for multiple processes when using parallelization. When executing code via parallel computation, n_trials is executed for each process instead of the sum of all processes. The user is seeking a way to ensure that the total number of trials is fixed regardless of the number of processes launched.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68920952",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2048830556,
        "Challenge_title":"How to set n_trials for multiple processes when using parallelization?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":92,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529092998780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":788.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Yes, <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.MaxTrialsCallback.html#optuna.study.MaxTrialsCallback\" rel=\"nofollow noreferrer\"><code>MaxTrialsCallback<\/code><\/a> is the exact feature for such a situation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":38.9,
        "Solution_reading_time":3.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":"n_trials not fixed"
    },
    {
        "Answerer_created_time":1359957233372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":2.74399,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When running an ML training job in Amazon SageMaker, the training script is \"deployed\" and given an ML training instance, which takes about 10 minutes to spin up and get the data it needs. <\/p>\n\n<p>I can only get one error message from the training job, then it dies and the instance is killed along with it. <\/p>\n\n<p>After I make a change to the training script to fix it, I need to deploy and run it which takes another 10 minutes or so.<\/p>\n\n<p>How can I accomplish this faster, or keep the training instance running?<\/p>",
        "Challenge_closed_time":1548282026927,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548272148563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in quickly debugging a SageMaker training script as the training instance takes about 10 minutes to spin up and get the data it needs. Additionally, the user can only get one error message from the training job, after which the instance is killed. The user is seeking ways to accomplish this faster or keep the training instance running.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54334462",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.74399,
        "Challenge_title":"How can I quickly debug a SageMaker training script?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1902.0,
        "Challenge_word_count":104,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416193017423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gensokyo",
        "Poster_reputation_count":880.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the \"local mode\" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to \"local\". More details about local mode can be found at <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview<\/a> and the blog post: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.7,
        "Solution_reading_time":13.03,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":102.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"faster SageMaker debugging"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9285602778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this? \n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker. \n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models? \n\nMany thanks \n\nTim",
        "Challenge_closed_time":1638917636668,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638914293851,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to access a large image dataset stored in a Sagemaker notebook instance from outside of that instance, specifically via Python Sagemaker Estimator training call. The user wants to access the data in a fast manner to train an AI model and is looking for the recommended way to do this. The user also wants to know the best way to store data when using Sagemaker estimators for training AI models.",
        "Challenge_last_edit_time":1668630486323,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":11.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.9285602778,
        "Challenge_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1538.0,
        "Challenge_word_count":170,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the \/home\/ec2-user\/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test\/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1638917636668,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":28.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":387.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"access and store data in Sagemaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.5982666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to load files as a dataset in the GUI of Azure ML Studio. These parquet files have been created through Spark.  <\/p>\n<p>In my folder, Spark creates files such as &quot;_SUCCESS&quot; or &quot;_committed_8998000&quot;.   <\/p>\n<p>Azure ML Studio is not able to read them or ignore them and tells me:  <\/p>\n<pre><code>The provided file(s) have invalid byte(s) for the specified file encoding.\n{\n  &quot;message&quot;: &quot; &quot;\n}\n<\/code><\/pre>\n<p>I selected &quot;Ignore unmatched files path&quot; and yet, it still does not work.  <\/p>\n<p>If I remove the &quot;_SUCCESS&quot; and other Spark files, it works.   <\/p>\n<p>Does anyone have an idea about a workaround?  <\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1601535069840,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601468116080,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in creating a dataset in Azure ML Studio from a parquet file created with Azure Spark. The GUI is unable to read the files due to invalid byte(s) for the specified file encoding. Even after selecting \"Ignore unmatched files path,\" the issue persists. The problem is resolved when the user removes the Spark files such as \"_SUCCESS.\" The user is seeking a workaround for this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/112778\/how-can-i-create-a-dataset-in-azure-ml-studio-(thr",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":9.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":18.5982666667,
        "Challenge_title":"How can I create a dataset in Azure ML studio (through the GUI) from a parquet file created with Azure Spark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":124,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I used &quot;path\/<em>\/<\/em>.parquet&quot; in the &quot;Path&quot; field and now it works.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":1.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid byte encoding"
    },
    {
        "Answerer_created_time":1568318861627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":486.0,
        "Answerer_view_count":75.0,
        "Challenge_adjusted_solved_time":170.6160616667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a credit fraud dataset on sagemaker. I'm using a linear learner binary classification algorithm. I divided the data into training and test sets and got the results for test set. When I tried to evaluate model performance characteristics on training set, I'm getting the following error<\/p>\n\n<pre><code> An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error \n(400) from model with message \"unable to evaluate payload provided\".\n<\/code><\/pre>\n\n<p>I mentioned the code below<\/p>\n\n<pre><code>train_data = 's3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'train\/examples') #making train_data\n\n#making test_data\ntest_key = \"{}\/test\/examples\".format(prefix)\ns3.Bucket(bucket).download_file(test_key, 'test_data')\n\n#preparing train channels for training the data\ntrain_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\n\n#training the data\nlinear.fit(inputs=train_channel,  logs=True)\n\n#creating the endpoint\nlinear_predictor = linear.deploy(initial_instance_count=1,\n                             instance_type='ml.m4.xlarge')\n\n#getting the results on test_data\nl = []\nwith open('test_data', 'r') as f:\nfor j in range(0,56962):\n    single_test = f.readline()\n    result = linear_predictor.predict(single_test)\n    l.append(result)\n    if j%10000 ==0 :\n        print(j)\nprint(l[0:10])\n\n#getting the results on train_data\n#THE CODE BELOW IS THROWING THE ABOVE MENTIONED ERROR\nq =[]\nwith open('train_data', 'r') as f:\nfor j in range(0,56962):\n    single_test = f.readline()\n    result = linear_predictor.predict(single_test)\n    q.append(result)\n    if j%10000 ==0 :\n        print(j)\nprint(q[0:10])\n<\/code><\/pre>\n\n<p>I'm getting the results on test data. I stored it in list l. For getting the results on the training set, I followed the similar procedure, but I'm getting the above mentioned error. Can someone please offer a resolution for this?<\/p>",
        "Challenge_closed_time":1574162320510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573477497977,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to evaluate the performance characteristics of a linear learner binary classification algorithm on the training set of a credit fraud dataset on Sagemaker. The error message received is \"unable to evaluate payload provided\" when calling the InvokeEndpoint operation. The user is able to get results on the test data but is unable to do so on the training set. The user has provided the code used for training and testing the data.",
        "Challenge_last_edit_time":1573548102688,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58801976",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":24.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":190.2284813889,
        "Challenge_title":"Evaluating payload provided when calling invoke endpoint operation",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1769.0,
        "Challenge_word_count":219,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568318861627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>\"unable to evaluate payload provided\" occurs only when the input data format is not compatible with the ML model you created. In this case, to get results on training set, we need to remove the last column(label column) before passing it to endpoint<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":3.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to evaluate payload"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":766.9947927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Here is a high-level picture of what I am trying to achieve: I want to train a LightGBM model with spark as a compute <a href=\"https:\/\/microsoft.github.io\/SynapseML\/docs\/features\/lightgbm\/about\/\" rel=\"nofollow noreferrer\">backend<\/a>, all in SageMaker using their Training Job api.\nTo clarify:<\/p>\n<ol>\n<li>I have to use LightGBM in general, there is no option here.<\/li>\n<li>The reason I need to use spark compute backend is because the training with the current dataset does not fit in memory anymore.<\/li>\n<li>I want to use SageMaker Training job setting so I could use SM Hyperparameter optimisation job to find the best hyperparameters for LightGBM. While LightGBM spark interface itself does offer some hyperparameter tuning capabilities, it does not offer Bayesian HP tuning.<\/li>\n<\/ol>\n<p>Now, I know the general approach to running custom training in SM: build a container in a certain way, and then just pull it from ECR and kick-off a training job\/hyperparameter tuning job through <code>sagemaker.Estimator<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">API<\/a>. Now, in this case SM would handle resource provisioning for you, would create an instance and so on. What I am confused about is that essentially, to use spark compute backend, I would need to have an EMR cluster running, so the SDK would have to handle that as well. However, I do not see how this is possible with the API above.<\/p>\n<p>Now, there is also that thing called <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">Sagemaker Pyspark SDK<\/a>. However, the provided <code>SageMakerEstimator<\/code> API from that package does not support on-the-fly cluster configuration either.<\/p>\n<p>Does anyone know a way how to run a Sagemaker training job that would use an EMR cluster so that later the same job could be used for hyperparameter tuning activities?<\/p>\n<p>One way I see is to run an EMR cluster in the background, and then just create a regular SM estimator job that would connect to the EMR cluster and do the training, essentially running a spark driver program in SM Estimator job.<\/p>\n<p>Has anyone done anything similar in the past?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1645793823447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643032642193,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to train a LightGBM model with spark as a compute backend in AWS Sagemaker using their Training Job API. They want to use SM Hyperparameter optimization job to find the best hyperparameters for LightGBM. However, to use spark compute backend, they need to have an EMR cluster running, and they are confused about how to handle this with the API. The provided SageMakerEstimator API from the Sagemaker Pyspark SDK does not support on-the-fly cluster configuration either. The user is looking for a way to run a Sagemaker training job that would use an EMR cluster so that later the same job could be used for hyperparameter tuning activities.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70835006",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":29.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":766.9947927778,
        "Challenge_title":"How to integrate spark.ml pipeline fitting and hyperparameter optimisation in AWS Sagemaker?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":340,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1357233199987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2171.0,
        "Poster_view_count":126.0,
        "Solution_body":"<p>Thanks for your questions. Here are answers:<\/p>\n<ul>\n<li><p><strong>SageMaker PySpark SDK<\/strong> <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/<\/a> does the opposite of what you want: being able to call a non-spark (or spark) SageMaker job from a Spark environment. Not sure that's what you need here.<\/p>\n<\/li>\n<li><p><strong>Running Spark in SageMaker jobs<\/strong>. While you can use SageMaker Notebooks to connect to a remote EMR cluster for interactive coding, you do not need EMR to run Spark in SageMaker jobs (Training and Processing). You have 2 options:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#pysparkprocessor\" rel=\"nofollow noreferrer\">SageMaker Processing has a built-in Spark Container<\/a>, which is easy to use but unfortunately not connected to SageMaker Model Tuning (that works with Training only). If you use this, you will have to find and use a third-party, external parameter search library ; for example <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-distributed-hyperparameter-and-neural-architecture-tuning-jobs-with-syne-tune\/\" rel=\"nofollow noreferrer\">Syne Tune<\/a> from AWS itself (that supports bayesian optimization)<\/p>\n<\/li>\n<li><p>SageMaker Training can run custom docker-based jobs, on one or multiple machines. If you can fit your Spark code within SageMaker Training spec, then you will be able to use SageMaker Model Tuning to tune your Spark code. However there is no framework container for Spark on SageMaker Training, so you would have to build your own, and I am not aware of any examples. Maybe you could get inspiration from the <a href=\"https:\/\/github.com\/aws\/sagemaker-spark-container\" rel=\"nofollow noreferrer\">Processing container code here<\/a> to build a custom Training container<\/p>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>Your idea of using the Training job as a client to launch an EMR cluster is good and should work (if SM has the right permissions), and will indeed allow you to use SM Model Tuning. I'd recommend:<\/p>\n<ul>\n<li>each SM job to create a new transient cluster (auto-terminate after step) to keep costs low and avoid tuning results to be polluted by inter-job contention that could arise if running everything on the same cluster.<\/li>\n<li>use the cheapest possible instance type for the SM estimator, because it will need to stay up during all duration of your EMR experiment to collect and print your final metric (accuracy, duration, cost...)<\/li>\n<\/ul>\n<p>In the same spirit, I once used SageMaker Training myself to launch Batch Transform jobs for the sole purpose of leveraging the bayesian search API to find an inference configuration that minimizes cost.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.6,
        "Solution_reading_time":35.55,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":376.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use EMR cluster for training job"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.66,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nI have two version for using component. One is directly uploading local code to each pod, this work fine that we could see the models artifact and metric curve. The other one use the same code as the first one, except for using private github to load the code. In this case, the \"dashboards\", \"artifacts\" and \"resources\" pages show only \"NO DATA\", and \"logs\" page shows training output message normally.\n\nTo reproduce\nAdd private code connection\nconnections:\n  - name: my-repo\n    kind: git\n    schema:\n      url: https:\/\/github.com\/xxx\/my-repo\n    secret:\n      name: \"github-secret-my-repo\"\n\nruning job config:\nrun:\n  kind: job\n  init:\n    - connection: my-repo\n\nHow we use log_metric and log_model\n# log metric\ntracking.log_metric(\"val_loss\", val_loss, step=epoch)\ntracking.log_metric(\"val_precision\", precision, step=epoch)\ntracking.log_metric(\"val_recall\", recall, step=epoch)\n\n# log model\nmodel_output_dir = tracking.get_outputs_path(\"models\", is_dir=True)\nckpt_file = os.path.join(model_output_dir, 'checkpoint.pth.tar')\ntorch.save({xxx}, ckpt_file)\ntracking.log_model(name=\"checkpoint\", path=ckpt_file, framework=\"pytorch\")\n\nExpected behavior\n\nShowing metric curve and saving models normally.\n\nEnvironment\n\nminikube: v1.15.1\npolyaxon ce: 1.7.5",
        "Challenge_closed_time":1619508955000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619492179000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with a private Github repo while using log_model and log_metric. The \"dashboards\", \"artifacts\" and \"resources\" pages show only \"NO DATA\", and \"logs\" page shows training output message normally. The user has provided the code and configuration details for reproducing the issue. The expected behavior is to show metric curve and save models normally. The environment used is minikube v1.15.1 and polyaxon ce 1.7.5.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1302",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.6,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.66,
        "Challenge_title":"Private github repo fail for using log_model and log_metric",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is very hard to debug, you need to check if the git repo has some more information like a hard-coded NO_OP.\n\nAlso I would check if you added the local cache folder .polyaxon to your .gitignore and .dockerignore. If this folder was added to you git repo, then indeed the metrics and artifacts will be saved to a different run.\n\nYou can also validate that the run is running with the correct run-uuid:\n\n...\nprint(tracking.TRACKING_RUN.run_uuid)\n...\n\nIf the run_uuid does not correspond to the currently running run, then the cache is bundled somewhere (git repo or docker image).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":97.0,
        "Tool":"Polyaxon",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no data in dashboard\/artifacts"
    },
    {
        "Answerer_created_time":1449207605092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Manila, NCR, Philippines",
        "Answerer_reputation_count":185.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":92.0650375,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Challenge_closed_time":1632103988632,
        "Challenge_comment_count":2,
        "Challenge_created_time":1631772554497,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that will be running on a serverless manner through Vertex AI\/Pipelines. The user is facing challenges in running the pipeline on the latest accelerator type, Tesla A100, as it requires a special machine type, which is at least an a2-highgpu-1g. The user tried to follow the method for older GPUs, but it throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69203143",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.8,
        "Challenge_reading_time":21.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":92.0650375,
        "Challenge_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":473.0,
        "Challenge_word_count":197,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449207605092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Manila, NCR, Philippines",
        "Poster_reputation_count":185.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":4.2,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to specify machine type and GPU"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":61.4723369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Challenge_closed_time":1559329920120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559108619707,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"InternalServerError\" error while performing binary classification on time series using Sagemaker. The transform job works fine when the user inputs a few samples, but it returns an error when the user inputs more samples for prediction, which is greater than 100MB. The user tried to use both SingleRecord and MultiRecord parameters when deploying the model, but the result was the same. The user suspects that the jsonlines file isn't being split by Line and is too big to be processed in one batch, which throws the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56353814",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":64.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":61.4723369444,
        "Challenge_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1826.0,
        "Challenge_word_count":483,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559099281007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.3,
        "Solution_reading_time":23.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":191.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"InternalServerError on large input"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4416666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi,\r\nI was trying to follow this documentation: https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/noaa-integrated-surface-data\/ (Go to \"Data access\" tab)to use opendatasets module to access historical weather data. But it gives me the error message `No name 'opendatasets' in module 'azureml'`. \r\nI tried `pip install azureml-sdk[opendatasets]` as well, it shows `WARNING: azureml-sdk 1.0.55 does not provide the extra 'opendatasets'`.\r\nDo you know how to use the opendatasets module in azureml?\r\n\r\nThanks!",
        "Challenge_closed_time":1565217619000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1565216029000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a 502 Bad Gateway error after running a Tesla K80 compute instance on Azure ML and attempting to connect with VSCode on day 2. The error message suggests that the target may not be in a running state.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/518",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4416666667,
        "Challenge_title":"No name 'opendatasets' in module 'azureml' Error",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Discussion_body":"Find the solution, maybe because `opendatasets` is a preview module, so it is not included in azureml sdk yet. You can download through pip `pip install azureml-opendatasets` in your env. > pip install azureml-opendatasets\r\n\r\nThanks, was looking for the solution, this worked !! However, I had another error \" [WinError 5] Access is denied:\" This was solved by adding --user at the end of your command.",
        "Discussion_score_count":10.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"502 Bad Gateway error"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":182.1773088889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Someone should add \"net#\" as a tag. I'm trying to improve my neural network in Azure Machine Learning Studio by turning it into a convolution neural net using this tutorial:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2\" rel=\"noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2<\/a><\/p>\n\n<p>The differences between mine and the tutorial is I'm doing regression with 35 features and 1 label and they're doing classification with 28x28 features and 10 labels. <\/p>\n\n<p>I start with the basic and 2nd example and get them working with:<\/p>\n\n<pre><code>input Data [35];\n\nhidden H1 [100]\n    from Data all;\n\nhidden H2 [100]\n    from H1 all;\n\noutput Result [1] linear\n    from H2 all;\n<\/code><\/pre>\n\n<p>Now the transformation to convolution I misunderstand. In the tutorial and documentation here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide<\/a> it doesn't mention how the node tuple values are calculated for the hidden layers. The tutorial says:<\/p>\n\n<pre><code>hidden C1 [5, 12, 12]\n  from Picture convolve {\n    InputShape  = [28, 28];\n    KernelShape = [ 5,  5];\n    Stride      = [ 2,  2];\n    MapCount = 5;\n  }\n\nhidden C2 [50, 4, 4]\n   from C1 convolve {\n     InputShape  = [ 5, 12, 12];\n     KernelShape = [ 1,  5,  5];\n     Stride      = [ 1,  2,  2];\n     Sharing     = [ F,  T,  T];\n     MapCount = 10;\n  }\n<\/code><\/pre>\n\n<p>Seems like the [5, 12, 12] and [50,4,4] pop out of no where along with the KernalShape, Stride, and MapCount. How do I know what values are valid for my example? I tried using the same values, but it didn't work and I have a feeling since he has a [28,28] input and I have a [35], I need tuples with 2 integers not 3. <\/p>\n\n<p>I just tried with random values that seem to correlate with the tutorial:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 23]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [200, 6]\n   from C1 convolve {\n     InputShape  = [ 7, 23];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>Right now it seems impossible to debug because the only error code Azure Machine Learning Studio ever gives is:<\/p>\n\n<pre><code>Exception\":{\"ErrorId\":\"LibraryException\",\"ErrorCode\":\"1000\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\",\"Exception\":{\"Library\":\"TLC\",\"ExceptionType\":\"LibraryException\",\"Message\":\"Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\"}}}Error: Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown. Process exited with error code -2\n<\/code><\/pre>\n\n<p>Lastly my setup is <a href=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" alt=\"Azure Machine Learning Setup\"><\/a> <\/p>\n\n<p>Thanks for the help!<\/p>",
        "Challenge_closed_time":1503342256368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1502257048253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to build a convolution neural net in Azure Machine Learning Studio using a tutorial, but is facing challenges in understanding how to transform the basic and second example into convolution. The tutorial does not mention how the node tuple values are calculated for the hidden layers, and the user is unsure of what values are valid for their example. The user has tried using random values that seem to correlate with the tutorial, but is unable to debug due to the error code \"Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\"",
        "Challenge_last_edit_time":1502686418056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45582412",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":41.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":301.4466986111,
        "Challenge_title":"How to build a Convolution Neural Net in Azure Machine Learning?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1268.0,
        "Challenge_word_count":393,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1418505926276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Missouri",
        "Poster_reputation_count":1454.0,
        "Poster_view_count":328.0,
        "Solution_body":"<p>The correct network definition for 35-column length input with given kernels and strides would be following:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 15]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [14, 7, 5]\n   from C1 convolve {\n     InputShape  = [ 7, 15];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>First, the C1 = [7,15]. The first dimension is simply the MapCount. For the second dimension, the kernel shape defines the length of the \"window\" that's used to scan the input columns, and the stride defines how much it moves at each step. So the kernel windows would cover columns 1-7, 3-9, 5-11,...,29-35, yielding the second dimension of 15 when you tally the windows.<\/p>\n\n<p>Next, the C2 = [14,7,5]. The first dimension is again the MapCount. For the second and third dimension, the 1-by-7 kernel \"window\" has to cover the input size of 7-by-15, using steps of 1 and 2 along corresponding dimensions. <\/p>\n\n<p>Note that you could specify C2 hidden layer shape of [98,5] or even [490], if you wanted to flatten the outputs. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":14.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":199.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to transform examples"
    },
    {
        "Answerer_created_time":1396956946647,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tel Aviv, Israel",
        "Answerer_reputation_count":72196.0,
        "Answerer_view_count":7104.0,
        "Challenge_adjusted_solved_time":0.0607519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to create sparse matrix for one hot encoded features from data frame <code>df<\/code>. But I am getting memory issue for code given below. Shape of <code>sparse_onehot<\/code> is  (450138, 1508)<\/p>\n<pre><code>sp_features = ['id', 'video_id', 'genre']\nsparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features)\nimport scipy\nX = scipy.sparse.csr_matrix(sparse_onehot.values)\n<\/code><\/pre>\n<p>I get memory error as shown below.<\/p>\n<pre><code>MemoryError: Unable to allocate 647. MiB for an array with shape (1508, 450138) and data type uint8\n<\/code><\/pre>\n<p>I have tried <code>scipy.sparse.lil_matrix<\/code> and get same error as above.<\/p>\n<p>Is there any efficient way of handling this?\nThanks in advance<\/p>",
        "Challenge_closed_time":1597911539563,
        "Challenge_comment_count":2,
        "Challenge_created_time":1597908911857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing memory issues while creating a sparse matrix for one hot encoded features from a data frame. The shape of the resulting sparse matrix is (450138, 1508), and the user is getting a memory error while using pd.get_dummies() and scipy.sparse.csr_matrix(). The user has also tried using scipy.sparse.lil_matrix() but is still facing the same error. The user is seeking an efficient way to handle this issue.",
        "Challenge_last_edit_time":1597911320856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63500377",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.1,
        "Challenge_reading_time":9.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.7299183334,
        "Challenge_title":"memory issues for sparse one hot encoded features",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487135761367,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":911.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>Try setting to <code>True<\/code> the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html\" rel=\"nofollow noreferrer\"><code>sparse<\/code> parameter<\/a>:<\/p>\n<blockquote>\n<p>sparsebool, default False\nWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).<\/p>\n<\/blockquote>\n<pre><code>sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = True)\n<\/code><\/pre>\n<p>This will use a much more memory efficient (but somewhat slower) representation than the default one.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.6,
        "Solution_reading_time":7.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"memory error in sparse matrix creation"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.1167675,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi, I am using Azure Machine Learning studio.  <\/p>\n<p>I trained my deep learning model on computing cluster, and then outputs images onto 'outputs' directory.  <br \/>\nHowever, in Experiment &gt; Run &gt; outputs + logs tab, directories was displayed instead of my images.  <\/p>\n<p>Is it possible to get my images from 'outputs' directory?  <\/p>\n<p>After investigation, I recognized that images which have filename containing square brackets is not shown in the tab.  <br \/>\nFor instance, if I created 'outputs\/test[0].jpg', then 'outputs\/test' directory was shown in the tab.  <\/p>\n<p>[Sample code]  <\/p>\n<pre><code>import cv2\nimport numpy as np\nfrom pathlib import Path\n\n# Check whether output directory exists or not\noutput_dir = Path('.\/outputs\/')\nif not output_dir.exists():\n    output_dir.mkdir()\n\n# Generate test image\nIMAGE_SIZE = (28, 28)\n\ntest_image = np.zeros(IMAGE_SIZE)\n\n# Save image with filename which contained \/ not contained square brackets\n# 'test.jpg' and 'test' directory will be shown in the 'outputs' directory, however 'test[].jpg' and 'test[0].jpg' are disappeared.\ncv2.imwrite(str(output_dir \/ 'test.jpg'), test_image)\ncv2.imwrite(str(output_dir \/ 'test[].jpg'), test_image)\ncv2.imwrite(str(output_dir \/ 'test[0].jpg'), test_image)\n<\/code><\/pre>",
        "Challenge_closed_time":1610646282360,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610588261997,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning studio where images with filenames containing square brackets are not displayed in the outputs folder. Instead, only the directories are shown in the Experiment > Run > outputs + logs tab. The user has provided sample code to demonstrate the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/229808\/azure-machine-learning-studio-images-which-have-fi",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.3,
        "Challenge_reading_time":17.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":16.1167675,
        "Challenge_title":"Azure Machine Learning studio - Images which have filename containing square brackets is not shown in outputs folder.",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":175,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, it probably recognizes the square bracket as wildcard. Try using a double-backticks to escape the brackets, otherwise, I recommend you rename without the brackets and train again.  <\/p>\n<pre><code>cv2.imwrite(str(output_dir \/ 'test``[``].jpg'), test_image)\n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":3.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"square bracket filename display issue"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.5002777778,
        "Challenge_answer_count":0,
        "Challenge_body":"I've installed and updated the sdk yet when attempting to import the module for the ExplainationDashboard i keep getting the error that the interpret module does not exist.\r\ni am running build 1.0.72 and following the sample in 'how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb \r\nthe failing line in the sample notebook is:\r\n**from azureml.contrib.interpret.visualize import ExplanationDashboard**\r\n**\"ModuleNotFoundError: No module named 'azureml.contrib.interpret' \"**\r\nthis is the update command i ran:\r\npip install --upgrade azureml-sdk[explain,automl,contrib] \r\n(the install ran fine - no errors)\r\njim",
        "Challenge_closed_time":1573060395000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1572788594000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while using Azureml Automl, specifically in the azureml.PipelineStep automl step. The error message is \"Error: Null\" and the user is unsure how to interpret it. The error occurs consistently when the dataset has more than 1200 features, but works fine with fewer features. The user is questioning if there is a limitation with the number of features allowed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/639",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.78,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":75.5002777778,
        "Challenge_title":"azureml.contrib.interpret - ModuleNotFoundError - build 1.0.72",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Discussion_body":"azureml-contrib-interpret is not a part of azureml-sdk contrib extras. Please install it separately",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error with many features"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.2,
        "Challenge_answer_count":4,
        "Challenge_body":"Hi, I'm using Google Colab +pro and unfortunately I`m getting several Ram calls and have not been able to move forward or train some models\n\nWhich is the next tool that I should get in order to be able to run the Google Colab models without the Ram calls?\n\nShould I get a Google Compute Engine and try to connect the google colab files to it?\n\nShould I up load the model to vertex AI?\n\nWhat characteristics should I need to take into consideration before I select any of the different tools?",
        "Challenge_closed_time":1652442120000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652171400000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues with RAM calls while using Google Colab +pro and is seeking advice on the next tool to use in order to run models without encountering these issues. They are considering options such as Google Compute Engine and Vertex AI and are seeking guidance on the characteristics to consider before selecting a tool.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Next-Step-from-Google-Colab-Pro\/m-p\/421797#M322",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.5,
        "Challenge_reading_time":6.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":75.2,
        "Challenge_title":"Next Step from Google Colab +Pro",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":387.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps:\/\/stackoverflow.com\/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps:\/\/towardsdatascience.com\/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps:\/\/neptune.ai\/blog\/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.7,
        "Solution_reading_time":7.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Vertex AI",
        "Challenge_type":"inquiry",
        "Challenge_summary":"selecting ML tool"
    },
    {
        "Answerer_created_time":1637238945423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jakarta, Indonesia",
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":164.8295133333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm importing a text dataset to Google Vertex AI and got the following error:<\/p>\n<pre><code>Hello Vertex AI Customer,\n\nDue to an error, Vertex AI was unable to import data into \ndataset [dataset_name].\nAdditional Details:\nOperation State: Failed with errors\nResource Name: [resoure_link]\nError Messages: There are too many rows in the jsonl\/csv file. Currently we \nonly support 1000000 lines. Please cut your files to smaller size and run \nmultiple import data pipelines to import.\n<\/code><\/pre>\n<p>I checked my dataset which I generated from pandas and the actual CSV file, it only have 600k lines.<\/p>\n<p>Anyone got similar errors?<\/p>",
        "Challenge_closed_time":1639027160676,
        "Challenge_comment_count":4,
        "Challenge_created_time":1637835565680,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while importing a text dataset to Google Vertex AI. The error message stated that the dataset had too many rows, with a maximum limit of 1 million lines, while the user's dataset only had 600k lines. The user is seeking help from others who may have encountered a similar issue.",
        "Challenge_last_edit_time":1638435090680,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70109346",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.8,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":330.99861,
        "Challenge_title":"Vertex AI was unable to import data into dataset. It says maximum 1M lines while my dataset only have 600k",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":212.0,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1637238945423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jakarta, Indonesia",
        "Poster_reputation_count":31.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>So it turns out to be an error in my CSV formatting.<\/p>\n<p>I forgot to trim newlines and extra whitespaces in my text dataset. This solved the 1M lines count. But after doing that, I then get error telling me I have too much labels while it was only 2.<\/p>\n<pre><code>Error Messages: There are too many AnnotationSpecs in the dataset. Up to \n5000 AnnotationSpecs are allowed in one Dataset.\n<\/code><\/pre>\n<p>And this is because I created the text dataset using to_csv() method in Pandas dataframe. Creating a CSV file this way, it will automatically put quotes when your text include a &quot;,&quot; (comma character) only. So the CSV file will look like:<\/p>\n<pre><code>&quot;this is a sentence, with a comma&quot;, 0\nthis is a sentence without a comma, 1\n<\/code><\/pre>\n<p>Meanwhile, what Vertex AutoML Text wants the CSV is to look like this:<\/p>\n<pre><code>&quot;this is a sentence, with a comma&quot;, 0\n&quot;this is a sentence without a comma&quot;, 1\n<\/code><\/pre>\n<p>i.e. you have to put quotes on every line.<\/p>\n<p>Which you can achieve by writing your own CSV formatter, or if you insist on using Pandas to_csv(), you can pass csv.QUOTE_ALL to the quoting parameter. It will look like this:<\/p>\n<pre><code>import csv\ndf.to_csv(&quot;file.csv&quot;, index=False, quoting=csv.QUOTE_ALL, header=False)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1639028476928,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":16.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":206.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"dataset row limit error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2697222222,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to use SageMaker, but doesn't know how to get started with instance sizes or how to forecast the cost for it. I've looked at the SageMaker TCO PDF we have online, but that appears more marketing than helpful, i.e. more price *comparison* than guidance.\n\nI know that the SageMaker cost is really the underlying EC2 and storage pieces, not SageMaker itself. However, I feel it is incorrect to say that they start with (say) t3.medium and see if that fits and scale up if they need more power behind it. As well, that doesn't help them to forecast either.\n\nAny thoughts here?",
        "Challenge_closed_time":1603286522000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603285551000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to help a customer get started with SageMaker instance sizing and cost forecasting. They have looked at the SageMaker TCO PDF but found it to be more marketing than helpful. The user is unsure about starting with a specific instance size and scaling up if needed and is looking for suggestions.",
        "Challenge_last_edit_time":1667926579047,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUq-Kaj1bLStK6Bs2gCUZ1Iw\/where-can-i-find-guidance-for-getting-a-customer-started-with-sagemaker-sizing-and-cost",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.1,
        "Challenge_reading_time":8.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2697222222,
        "Challenge_title":"Where can I find guidance for getting a customer started with SageMaker sizing and cost?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":82.0,
        "Challenge_word_count":119,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"See the **performance efficiency** and **cost optimization** pillars in [Machine Learning Lens][1].\nAdditionally this is an [EC2 based right sizing best practices guide][2].  \nOverall, it's better to start small, then increase instance size as needed (as those that start large, never bother reduce the size), or apply auto scaling for SageMaker hosting.  \nAssuming a CPU ML predictions: When choosing ml.t2.medium instances the customer will need to keep an eye on the instance CPU credits. If they lack the knowledge, just start with M5.\n\n\n  [1]: https:\/\/d1.awsstatic.com\/whitepapers\/architecture\/wellarchitected-Machine-Learning-Lens.pdf\n  [2]: https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/cost-optimization-right-sizing\/tips-for-right-sizing-your-workloads.html",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1667925560088,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"SageMaker instance sizing guidance"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.9419444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen using MLflow logger, log_param() function require `run_id`\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-23-d048545e1854> in <module>\r\n      9 trainer.fit(model=experiment, \r\n     10            train_dataloader=train_dl,\r\n---> 11            val_dataloaders=test_dl)\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    452         self.call_hook('on_fit_start')\r\n    453 \r\n--> 454         results = self.accelerator_backend.train()\r\n    455         self.accelerator_backend.teardown()\r\n    456 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in train(self)\r\n     51 \r\n     52         # train or test\r\n---> 53         results = self.train_or_test()\r\n     54         return results\r\n     55 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/base_accelerator.py in train_or_test(self)\r\n     48             results = self.trainer.run_test()\r\n     49         else:\r\n---> 50             results = self.trainer.train()\r\n     51         return results\r\n     52 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in train(self)\r\n    499 \r\n    500                 # run train epoch\r\n--> 501                 self.train_loop.run_training_epoch()\r\n    502 \r\n    503                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_epoch(self)\r\n    525             # TRAINING_STEP + TRAINING_STEP_END\r\n    526             # ------------------------------------\r\n--> 527             batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    528 \r\n    529             # when returning -1 from train_step, we end epoch early\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    660                     opt_idx,\r\n    661                     optimizer,\r\n--> 662                     self.trainer.hiddens\r\n    663                 )\r\n    664 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    739         \"\"\"\r\n    740         # lightning module hook\r\n--> 741         result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    742 \r\n    743         if result is None:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    300         with self.trainer.profiler.profile('model_forward'):\r\n    301             args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens)\r\n--> 302             training_step_output = self.trainer.accelerator_backend.training_step(args)\r\n    303             training_step_output = self.trainer.call_hook('training_step_end', training_step_output)\r\n    304 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in training_step(self, args)\r\n     59                 output = self.__training_step(args)\r\n     60         else:\r\n---> 61             output = self.__training_step(args)\r\n     62 \r\n     63         return output\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in __training_step(self, args)\r\n     67         batch = self.to_device(batch)\r\n     68         args[0] = batch\r\n---> 69         output = self.trainer.model.training_step(*args)\r\n     70         return output\r\n     71 \r\n\r\n<ipython-input-21-31b6dc3ffd67> in training_step(self, batch, batch_idx, optimizer_idx)\r\n     28         for key, val in train_loss.items():\r\n     29             self.log(key, val.item())\r\n---> 30             self.logger.experiment.log_param(key=key, value=val.item())\r\n     31 \r\n     32         return train_loss\r\n\r\nTypeError: log_param() missing 1 required positional argument: 'run_id'\r\n```\r\n#### Expected behavior\r\nThe MlflowLogger should behave the same as the mlflow api where only key and value argment is needed for log_param() function\r\n\r\n#### Code sample\r\n```python\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name='test',\r\n    tracking_uri=\"file:.\/ml-runs\"\r\n)\r\n\r\nCllass VAEexperiment(LightningModule):\r\n...\r\n    def training_step(self, batch, batch_idx, optimizer_idx = 0):\r\n        ....\r\n        for key, val in train_loss.items():\r\n            self.logger.experiment.log_param(key=key, value=val.item())\r\n       ....\r\n       return train_loss\r\n\r\ntrainer = Trainer(logger=mlf_logger,\r\n                  default_root_dir='..\/logs',\r\n                  early_stop_callback=False,\r\n                  gpus=1, \r\n                  auto_select_gpus=True,\r\n                  max_epochs=40)\r\n\r\ntrainer.fit(model=experiment, \r\n           train_dataloader=train_dl, \r\n           val_dataloaders=test_dl)\r\n```\r\n\r\n\r\n### Environment\r\n\r\npytorch-lightning==0.10.0\r\ntorch==1.6.0\r\ntorchsummary==1.5.1\r\ntorchvision==0.7.0\r\n\r\n\r\n",
        "Challenge_closed_time":1602141183000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1602116192000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug with the MLFlowLogger in PyTorch Lightning, where calling `t.logger.experiment_id` throws a `JSONDecodeError` exception. The user has provided a code sample and environment details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3964",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":18.0,
        "Challenge_reading_time":59.98,
        "Challenge_repo_contributor_count":444.0,
        "Challenge_repo_fork_count":2922.0,
        "Challenge_repo_issue_count":15116.0,
        "Challenge_repo_star_count":23576.0,
        "Challenge_repo_watch_count":234.0,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":6.9419444444,
        "Challenge_title":"mlflow logger complains about missing run_id",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":304,
        "Discussion_body":"Hi! thanks for your contribution!, great first issue! Here, mlflow logger is actually an MlflowClient object. so you ll need to use the function calls specified in this doc - https:\/\/www.mlflow.org\/docs\/latest\/_modules\/mlflow\/tracking\/client.html . These functions needs run_id as first argument which can be accessed as self.logger.run_id @nazim1021 thx for clarification! @qianyu-berkeley feel free to reopen if needed... Thanks! Same problem here, working with @nazim1021 suggestion. What about adding it to the doc? > What about adding it to the doc?\r\n\r\ngood idea, mind send a PR? :]",
        "Discussion_score_count":4.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"JSONDecodeError with MLFlowLogger"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.1426305556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>while running pipeline creation python script facing the following error.\n&quot;AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The specified job configuration exceeds the max allowed size of 32768 characters. Please reduce the size of the job's command line arguments and environment settings&quot;<\/p>",
        "Challenge_closed_time":1632803094283,
        "Challenge_comment_count":1,
        "Challenge_created_time":1632798980813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while running a pipeline creation python script in Azure, stating that the job configuration exceeds the maximum allowed size of 32768 characters. The user needs to reduce the size of the job's command line arguments and environment settings.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69355385",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.7,
        "Challenge_reading_time":4.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.1426305556,
        "Challenge_title":"Size of the input \/ output parameters in the pipeline step in Azure",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":208.0,
        "Challenge_word_count":50,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632461310820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":107.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>When we tried to pass a quite lengthy content as argument value to a Pipeline. You can try to upload file to blob, optionally create a dataset, then pass on dataset name or file path to AML pipeline as parameter. The pipeline step will read content of the file from the blob.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"job config too large"
    },
    {
        "Answerer_created_time":1569423384323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":8089.9413483334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Im working on sentence classification using in-build  blazing text algorithm, while invoking endpoint inside lambda function it throughs the content type mismatching error. <\/p>\n\n<p>-- For blazing text it support only application\/jsonlines or application\/json but while invoking , it throughs the error like , it accepts only byte or bytearray<\/p>\n\n<pre><code>input format . application\/json\nevent={\n  \"features\": [\n    \"sensor_subtype Thermostats Thermal Switches product_features Hermetically sealed n Tight tolerances n Tight differentials n Logic level contacts n applications Computers n Medical electronics n Power supplies n Industrial controls n Test equipment n Infotech n description Technical Specifications technical_specs CloseTolerance 2 8 C 5 F DielectricStrength MIL STD 202 Method 301 1250 Vac 60 Hz Terminal to Case ContactResistance MIL STD\"\n  ]\n}\n<\/code><\/pre>\n\n<p>and also i tried application\/jsonlines<\/p>\n\n<p>My code looks like this>>>>>>>>>>>>>>>>>>>>>>>><\/p>\n\n<pre><code>def transform_data(data):\n    try:\n        features = data.copy()\n\n        return features\n\n    except Exception as err:\n        print('Error when transforming: {0},{1}'.format(data,err))\n        raise Exception('Error when transforming: {0},{1}'.format(data,err))\n\n\ndef lambda_handler(event, context):\n    try:    \n        print(\"Received event: \" + json.dumps(event, indent=2))\n\n        request = json.loads(json.dumps(event))\n\n        transformed_data = str(transform_data(request['features'])) #for instance in request['features'])\n        print(ENDPOINT_NAME, \"-------&gt;&gt;&gt;&gt;\")\n        payload=transformed_data\n        result = client.invoke_endpoint(EndpointName=ENDPOINT_NAME, \n                              Body=(payload.encode('utf-8')),\n                              ContentType='application\/json')\n        return result\n<\/code><\/pre>\n\n<pre><code>  \"statusCode\": 400,\n  \"isBase64Encoded\": false,\n  \"body\": \"Call Failed An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (406) from model with message \\\"Invalid payload format\\\".\n_______________LOGS__________________________________\n\uf141\n11:35:22\n[08\/18\/2019 11:35:22 ERROR 140074862942016] Customer Error: Unable to decode payload: Incorrect data format. (caused by ValueError)\n\uf141\n11:35:22\nCaused by: No JSON object could be decoded\n\uf141\n11:35:22\nTraceback (most recent call last): File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/blazingtext\/serve.py\", line 317, in invocations data = json.loads(payload.decode(\"utf-8\")) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/__init__.py\", line 339, in loads return _default_decoder.decode(s) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/decoder.py\", line 364, in decode obj, end = self.\n\uf141\n11:35:22\nValueError: No JSON object could be decoded\n<\/code><\/pre>\n\n<p>I need to predict the sentence in realtime using invoke_endpoint option but it shows invalid payload format <\/p>\n\n<p>I tried with byte format and apllication\/jsonlines format.<\/p>",
        "Challenge_closed_time":1595256539003,
        "Challenge_comment_count":1,
        "Challenge_created_time":1566128809927,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while invoking the endpoint of the in-built Blazing text algorithm inside a lambda function for sentence classification. The error is due to a content type mismatch, as the algorithm only supports application\/jsonlines or application\/json, but the endpoint only accepts byte or bytearray. The user has tried using both application\/json and application\/jsonlines formats, but the error persists. The user needs to predict sentences in real-time using the invoke_endpoint option, but the invalid payload format error is preventing this.",
        "Challenge_last_edit_time":1566133069936,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57544237",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":15.8,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":8091.0358544445,
        "Challenge_title":"Inside lambda function - Blazing text algorithm invoke endpoint doesn't support the input content type",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":493.0,
        "Challenge_word_count":305,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541220234400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":13.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I encountered the same problem when trying to predict on text classification with a BlazingText container. What worked for me was simply changing the key in the payload while keeping the ContentType as application\/json:<\/p>\n<pre><code>sentence = &quot;I'm selling my PS4, practically brand new&quot;\n\npayload = {&quot;instances&quot;: [sentence]}\n\nresponse = client.invoke_endpoint(\n        EndpointName=&quot;text_classification&quot;,\n        Body=json.dumps(payload),\n        ContentType='application\/json'\n        \n    )\n<\/code><\/pre>\n<p>After playing around a little with the payload it seems that blazing text models only accept payloads as a dictionary with &quot;instances&quot; as its key and a list containing your data you want to predict on as its value.<\/p>\n<p>To get to your predictions simply :<\/p>\n<pre><code>print(&quot;ResponseMetadata:&quot;, response[&quot;ResponseMetadata&quot;])\nprint()\nprint(&quot;Body:&quot;, response['Body'].read())\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595256858790,
        "Solution_link_count":0.0,
        "Solution_readability":16.0,
        "Solution_reading_time":12.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":103.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"content type mismatch"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":195.8666666667,
        "Challenge_answer_count":3,
        "Challenge_body":"I am trying to call an API to inference from a model I have uploaded to vertex AI.\n\nI have tried three methods, and none worked so far.\n\nAt first, I was following a youtube from standford university,\u00a0https:\/\/www.youtube.com\/watch?v=fw6NMQrYc6w&t=3876s\u00a0which uses ai platform.\n\n1. I also tried that, but I think google is trying to get rid of AI platform, and although I succesfully uploaded the model, it doesn't allow me to make a new version, basically allows me nothing.\n\n2. I tried to work this tutorial,\u00a0https:\/\/codelabs.developers.google.com\/vertex-p2p-predictions#5\u00a0\n\nand it keeps complaining that my payload is above 1.5MB limit, but my image is only 49KB, so it's ridiculous. maybe something happened in this code, but it's from the tutorial, so the tutorial must be wrong then.\n\n\u00a0\n\nIMAGE_PATH = \"test-image.jpg\"\nim = Image.open(IMAGE_PATH)\nx_test = np.asarray(im).astype(np.float32).tolist()\nendpoint.predict(instances=x_test).predictions\n\n\u00a0\n\n3. Last, I've been trying to call the API from the sample code,\n\nhttps:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predic...\n\nbut it gives me a json format error.\n\nI have referenced from this website to get the json format.\n\nhttps:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/prediction_service\/predic...\u00a0\n\nThe error I am getting is as is :\u00a0\n\n400 { \"error\": \"Failed to process element: 0 key: instances of 'instances' list. Error: Invalid argument: JSON object: does not have named input: instances\" }\n\u00a0\nThe code that I have used is :\n\u00a0\n\n\u00a0\n\n    encoded_content = base64.b64encode(image).decode(\"utf-8\")\n    instances = {\"instances\": {\"image\": {\"b64\": encoded_content}}, \"key\": \"0\"}\n\n\u00a0\n\nand the 400 error comes from API and the log from vertex AI is not very useful when debugging.\n\nHonestly I have been struggling with this issue for days and in my opinion, this should not be this difficult. My experience with GCP and vertex AI is very disappointing and I'm considering to explore other options. Please let me know if any of you have any advices. Thanks",
        "Challenge_closed_time":1682031900000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681326780000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering errors while trying to call an API to inference from a custom trained model uploaded to Vertex AI. They have tried three methods, including following a tutorial from Stanford University, but none have worked so far. The user is facing issues with the payload limit, JSON format, and the log from Vertex AI is not helpful in debugging. They are disappointed with their experience with GCP and Vertex AI and are considering exploring other options.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-from-Vertex-AI-Getting-predictions-from-custom-trained\/m-p\/543292#M1647",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":9.9,
        "Challenge_reading_time":26.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":195.8666666667,
        "Challenge_title":"Error from Vertex AI Getting predictions from custom trained models",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":179.0,
        "Challenge_word_count":291,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I actually solved this error by applying this document.\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-predict-image-classification-sample.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":23.9,
        "Solution_reading_time":2.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":15.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"API inference errors"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0275,
        "Challenge_answer_count":0,
        "Challenge_body":"We are trying to save a model using log_model_ref and add a name to it, i.e. best_auc. Then we want to be able to retrieve this model from the latest run.\nHowever, if we use RunClient.client.runs_v1.get_runs_artifacts_lineage this returns all the artifacts ever generated for that project. And if we use RunClient.get_artifacts_tree, we do have more control about which run we are looking at, but we lose the name information we set when using log_model_ref?",
        "Challenge_closed_time":1649410238000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649410139000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to save a model using log_model_ref and add a name to it, but is facing challenges in retrieving the model from the latest run as the available methods either return all artifacts ever generated for the project or lose the name information set when using log_model_ref.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1485",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0275,
        "Challenge_title":"How to get model references logged by a specific run?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To get the logged model refs:\n\nfrom polyaxon.client import RunClient\n\nrun_client = RunClient(project=\"PROJECT_NAME\", run_uuid=\"RUN_UUID\")\n\n# Query the lineage information\nlineages = run_client.get_artifacts_lineage(query=\"kind: model\").results\n\n# Download the lineage assets\nfor lineage in lineages:\n    run_client.download_artifact_for_lineage(lineage=lineage)\n\nYou can restrict the ref to specific lineage by filtering further by name:\n\nlineages = run_client.get_artifacts_lineage(query=\"kind: model, name: best_auc\").results",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.6,
        "Solution_reading_time":6.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":47.0,
        "Tool":"Polyaxon",
        "Challenge_type":"anomaly",
        "Challenge_summary":"challenges retrieving named model"
    },
    {
        "Answerer_created_time":1433761344910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Aruppukkottai, India",
        "Answerer_reputation_count":802.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":0.9015666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a blazingText model and followed this guide.\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html<\/a><\/p>\n\n<p>\"Sample JSON request\" The Invoke end point is working perfectly. So I switched to,\nBatch Transform Job with \"content-type: application\/jsonlines\" and created a file in S3 with the following format data:<\/p>\n\n<pre><code>{\"source\": \"source_0\"}\n<\/code><\/pre>\n\n<p>The job ran success. But the output did not sent to S3. Also In the cloud logs,<\/p>\n\n<pre><code>\" [79] [INFO] Booting worker with pid: 79\"\n<\/code><\/pre>\n\n<p>This is is the last response. Did anyone know what went wrong?<\/p>",
        "Challenge_closed_time":1543477871070,
        "Challenge_comment_count":2,
        "Challenge_created_time":1543474625430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a BlazingText model on AWS Sagemaker and successfully tested the Invoke endpoint. However, when attempting to use Batch Transform Job with a JSONlines file in S3, the job ran successfully but did not send output to S3. The last response in the cloud logs was \"Booting worker with pid: 79\". The user is seeking assistance in identifying the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53533434",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.3,
        "Challenge_reading_time":10.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.9015666667,
        "Challenge_title":"AWS Sagemaker - Blazingtext BatchTransform no output",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":317.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1433761344910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Aruppukkottai, India",
        "Poster_reputation_count":802.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>I have found the issue. The batchtransform select the folder as input and the s3 source should be S3Prefix instead of manifest.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no output in S3"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.3457913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello everyone,    <\/p>\n<p>I am tring to deploy R script as a web service using Azure Machine Learning. I created pipeline as below.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/143603-001.png?platform=QnA\" alt=\"143603-001.png\" \/>    <\/p>\n<p>I can deploy the model and endpoint from [Deploy] button but I cannot control some properties: i.e. resource name, dns name.    <\/p>\n<p>It seems that the <code>az ml model deploy<\/code> command can be used to deploy the endpoint.     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#using-the-azure-cli\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#using-the-azure-cli<\/a>    <\/p>\n<p>I have no information for <code>inferenceconfig.json<\/code>. How to write <code>score.py<\/code> to execute R script? Is it any example?    <\/p>",
        "Challenge_closed_time":1635283098256,
        "Challenge_comment_count":2,
        "Challenge_created_time":1635213453407,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to deploy an R script as a web service using Azure Machine Learning and has created a pipeline. They are able to deploy the model and endpoint from the \"Deploy\" button but cannot control some properties such as resource name and DNS name. The user is looking for information on how to use the \"az ml model deploy\" command to deploy the endpoint and how to write \"score.py\" to execute the R script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/603664\/how-to-deploy-r-script-web-service-via-azure-cli",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":12.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.3457913889,
        "Challenge_title":"How to deploy R script web service via Azure CLI",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, the following document describes how to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli#define-an-inference-configuration\">define an inference configuration<\/a>.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.5,
        "Solution_reading_time":4.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy R script web service"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.7569444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nI've just trained a churn prediction model with XGBoost algorithm, based on the SageMaker example notebooks.  I've created SageMaker batch transformation jobs using this model using input from CSV file with multiple records, however the output file is a single record CSV containing all the inferences in a single comma separated row.  The result is that I'm not able to use the \"Join source\" feature with \"Input  - Merge input data with job output\" since the input and output files must match the number of records. I've tried with different batch job configurations but I always get the same single line output file.\n\nDo you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? Is this a restriction from the XGBoost algorithm built-in implementation?",
        "Challenge_closed_time":1599791910000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599771185000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a churn prediction model with XGBoost algorithm and created SageMaker batch transformation jobs using this model. However, the output file is a single record CSV containing all the inferences in a single comma separated row, which is causing difficulty in using the \"Join source\" feature with \"Input - Merge input data with job output\". The user is seeking a configuration that allows merging input and output to have a direct association between an input column with its inference result and is unsure if this is a restriction from the XGBoost algorithm built-in implementation.",
        "Challenge_last_edit_time":1668594259996,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUYz7Bz_5sTmG0uBaqlt7J_g\/xgboost-sagemaker-batch-transform-job-output-in-multiple-lines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":11.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":5.7569444444,
        "Challenge_title":"xgboost sagemaker batch transform job output in multiple lines",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":152,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Sounds like a configuration issue, this algorithm should be able to output proper output CSVs.\n\nAre you using `accept=\"text\/csv\"` and `assemble_with=\"Line\"` on your `Transformer`? Is your `strategy` set to `SingleRecord` or `MultiRecord`?\n\nAnd `split_type=\"Line\"`, `content_type=\"text\/csv\"` on the `.transform()` call?\n\nI have had custom algorithms accidentally output row vectors instead of column vectors for multi-record batches in the past (because they gave a 1D output which the default serializer interpreted as a row), but not built-in algorithms.\n\nDropping to `SingleRecord` could be a last resort (forcing Batch Transform itself to handle the serialization), but would decrease efficiency\/speed.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1607690229838,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":8.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":96.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"single row output file"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":4.1983277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do I delete projects in my workspace ?when I click to delete a project the delete button is inactive. Tried clearing cache and whatnot but cannot delete the project from studio.azureml.net... how do I do this ? <\/p>",
        "Challenge_closed_time":1526865758888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526851287060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty deleting a project in their Azure ML Studio workspace as the delete button is inactive. They have attempted to clear their cache but have not been successful in deleting the project.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50439489",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":2.0,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4.0199522222,
        "Challenge_title":"Delete Project Azure ML Studio ( Web App)",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":576.0,
        "Challenge_word_count":44,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506435894640,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Southeast Asia",
        "Poster_reputation_count":1922.0,
        "Poster_view_count":404.0,
        "Solution_body":"<p>I have reproduced your issue. Try to go to your project -> EDIT ->remove the <strong>ASSETS<\/strong> of your project. Then the delete button will be able.<\/p>\n\n<p>You could follow the screenshot.<\/p>\n\n<ol>\n<li>The <strong>DELETE<\/strong> button is disable.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/E850F.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E850F.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>2.Go to <strong>EDIT<\/strong> and remove the <strong>ASSETS<\/strong>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PbEZC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PbEZC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2kBgO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2kBgO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>3.Then the <strong>DELETE<\/strong> button will be able<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/08lOW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/08lOW.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1526866401040,
        "Solution_link_count":8.0,
        "Solution_readability":12.4,
        "Solution_reading_time":14.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"inactive delete button"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":9.6420455556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Challenge_closed_time":1565705412680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565670597500,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to use GPU to train a neural network model in Azure Machine Learning Service using P100-NC6s-V2 compute. The training is happening on the CPU and the logs show that it's not able to find CUDA. The user has mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. The warning log shows that it fails to use the GPU and is currently using the CPU. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1565670701316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57471129",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":57.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":9.6708833333,
        "Challenge_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1402.0,
        "Challenge_word_count":396,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408145271463,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.2,
        "Solution_reading_time":11.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to use GPU"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":31.5388561111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code class=\"lang-auto\">for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> shows logs only for the first file in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code class=\"lang-auto\">wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code class=\"lang-auto\">def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Challenge_closed_time":1650552651672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650439111790,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while fine-tuning multiple models using a for loop with Huggingface Trainer and Wandb. Although the code is training and saving models for all files in the data directory, Wandb is only showing logs for the first file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-for-huggingface-trainer-saves-only-first-model\/2270",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":17.1,
        "Challenge_reading_time":17.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":31.5388561111,
        "Challenge_title":"Wandb for Huggingface Trainer saves only first model",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":207.0,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code class=\"lang-auto\">\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" class=\"inline-onebox\">Launch Experiments with wandb.init - Documentation<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.4,
        "Solution_reading_time":7.73,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Wandb logs only show first file"
    },
    {
        "Answerer_created_time":1439975952067,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":84.5929313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to connect to redshift using sagemaker notebook instances. I want to run Unload commands to unload data from redshift to s3 using IAM role and schedule the sagemaker notebook.\nI want to know how I can import db credentials in sagemaker without hardcoding.<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1618908321396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618603786843,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to connect to Redshift using Sagemaker notebook instances and run Unload commands to unload data from Redshift to S3 using IAM role. They are looking for a way to import db credentials in Sagemaker without hardcoding.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67131617",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":84.5929313889,
        "Challenge_title":"Connect to redshift using sagemaker notebook instances",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1504.0,
        "Challenge_word_count":53,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468599558956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":107.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/secretsmanager\/latest\/userguide\/intro.html\" rel=\"nofollow noreferrer\">AWS Secrets Manager<\/a> to store and access credentials. Your Sagemaker execution role should have permission to read from Secrets Manager (AFAIK AWS managed-role does have it). This is the same mechanism that's used by Sagemaker notebooks to get access to github repo, for example<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":5.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"import Redshift credentials"
    },
    {
        "Answerer_created_time":1592412555903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":166.3580597222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running into the following error when I try to run Automated ML through the studio on a GPU compute cluster:<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/uLyxr.png\" alt=\"Azure ML error message\" \/><\/p>\n<blockquote>\n<p>Error: AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The\nspecified job configuration exceeds the max allowed size of 32768\ncharacters. Please reduce the size of the job's command line arguments\nand environment settings<\/p>\n<\/blockquote>\n<p>The attempted run is on a registered tabulated dataset in filestore and is a simple regression case. Strangely, it works just fine with the CPU compute instance I use for my other pipelines. I have been able to run it a few times using that and wanted to upgrade to a cluster only to be hit by this error. I found online that it could be a case of having the following setting: AZUREML_COMPUTE_USE_COMMON_RUNTIME:false; but I am not sure where to put this in when just running from the web studio.<\/p>",
        "Challenge_closed_time":1639585163672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638986274657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running Automated ML through the studio on a GPU compute cluster. The error message indicates that the job configuration exceeds the maximum allowed size of 32768 characters. The user has tried to run the same on a CPU compute instance and it worked fine. The user is unsure where to put the setting \"AZUREML_COMPUTE_USE_COMMON_RUNTIME:false\" when running from the web studio.",
        "Challenge_last_edit_time":1641204588636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70279636",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":13.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":166.3580597222,
        "Challenge_title":"Azure Auto ML JobConfigurationMaxSizeExceeded error when using a cluster",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":171.0,
        "Challenge_word_count":160,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592412555903,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>It looks like the bug was fixed. I just ran it on a cluster without changing any of the parameters. Thank you Yutong for the help!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.1,
        "Solution_reading_time":1.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"job config size exceeds limit"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":139.4886111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThere are some errors: https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3402182291\/jobs\/5657762171#step:3:1022\r\n\r\n```\r\n=========================== short test summary info ============================\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\n======================== 48 warnings, 3 errors in 3.79s ========================\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nINFO:submit_groupwise_azureml_pytest.py:Test execution completed!\r\n\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1668591607000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1668089448000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with the SASRec integration test taking an unusually long time on an AzureML compute cluster triggered using a GitHub workflow. The runtime varies significantly from the ADO pipeline, and both machines are of the same type and use the same CUDA and CuDNN versions. The user needs to investigate why this is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1841",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2749.0,
        "Challenge_repo_issue_count":1927.0,
        "Challenge_repo_star_count":15795.0,
        "Challenge_repo_watch_count":264.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":139.4886111111,
        "Challenge_title":"[BUG] Error in some of the AzureML tests",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Discussion_body":"@pradnyeshjoshi any thoughts for this error?",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"long SASRec integration test"
    },
    {
        "Answerer_created_time":1365101584443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":7203.0,
        "Answerer_view_count":445.0,
        "Challenge_adjusted_solved_time":0.5689572222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a stepfunction, the definition for this statemachine below (<code>step-function.json<\/code>) is used in terraform (using the syntax in this page:<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html<\/a>)<\/p>\n<p>The first time if I execute this statemachine, it will create a SageMaker batch transform job named <code>example-jobname<\/code>, but I need to exeucute this statemachine everyday, then it will give me error <code>&quot;error&quot;: &quot;SageMaker.ResourceInUseException&quot;, &quot;cause&quot;: &quot;Job name must be unique within an AWS account and region, and a job with this name already exists <\/code>.<\/p>\n<p>The cause is because the job name is hard-coded as <code>example-jobname<\/code> so if the state machine gets executed after the first time, since the job name needs to be unique, the task will fail, just wondering how I can add a string (something like ExecutionId at the end of the job name). Here's what I have tried:<\/p>\n<ol>\n<li><p>I added <code>&quot;executionId.$&quot;: &quot;States.Format('somestring {}', $$.Execution.Id)&quot;<\/code> in the <code>Parameters<\/code> section in the json file, but when I execute the task I got error <code> &quot;error&quot;: &quot;States.Runtime&quot;, &quot;cause&quot;: &quot;An error occurred while executing the state 'SageMaker CreateTransformJob' (entered at the event id #2). The Parameters '{\\&quot;BatchStrategy\\&quot;:\\&quot;SingleRecord\\&quot;,..............\\&quot;executionId\\&quot;:\\&quot;somestring arn:aws:states:us-east-1:xxxxx:execution:xxxxx-state-machine:xxxxxxxx72950\\&quot;}' could not be used to start the Task: [The field \\&quot;executionId\\&quot; is not supported by Step Functions]&quot;}<\/code><\/p>\n<\/li>\n<li><p>I modified the jobname in the json file to  <code>&quot;TransformJobName&quot;: &quot;example-jobname-States.Format('somestring {}', $$.Execution.Id)&quot;,<\/code>, when I execute the statemachine, it gave me error: <code>&quot;error&quot;: &quot;SageMaker.AmazonSageMakerException&quot;, &quot;cause&quot;: &quot;2 validation errors detected: Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}; Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must have length less than or equal to 63<\/code><\/p>\n<\/li>\n<\/ol>\n<p>I really run out of ideas, can someone help please? Many thanks.<\/p>",
        "Challenge_closed_time":1610637215396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610635167150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with creating a unique SageMaker batch transform job name when executing a step function daily. The job name is hard-coded as \"example-jobname\" and causes an error when executed after the first time. The user has tried adding a string to the job name using the execution ID, but it resulted in an error. Modifying the job name in the JSON file also resulted in an error due to a regular expression pattern constraint. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1611071837132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65721061",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":16.2,
        "Challenge_reading_time":36.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.5689572222,
        "Challenge_title":"How to parse stepfunction executionId to SageMaker batch transform job name?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1209.0,
        "Challenge_word_count":288,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>So as per the <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-train-model.html#sample-train-model-code-examples\" rel=\"nofollow noreferrer\">documentation<\/a>, we should be passing the parameters in the following format<\/p>\n<pre><code>        &quot;Parameters&quot;: {\n            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  \n            ....\n        },\n<\/code><\/pre>\n<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:<\/p>\n<p>either<\/p>\n<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;\n<\/code><\/pre>\n<p>full State machine definition:<\/p>\n<pre><code>    {\n        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,\n        &quot;StartAt&quot;: &quot;Generate Random String&quot;,\n        &quot;States&quot;: {\n            &quot;Generate Random String&quot;: {\n                &quot;Type&quot;: &quot;Task&quot;,\n                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,\n                &quot;ResultPath&quot;: &quot;$.executionid&quot;,\n                &quot;Parameters&quot;: {\n                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;\n                },\n                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;\n            },\n        &quot;SageMaker CreateTransformJob&quot;: {\n            &quot;Type&quot;: &quot;Task&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,\n            &quot;Parameters&quot;: {\n            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n            &quot;DataProcessing&quot;: {\n                &quot;InputFilter&quot;: &quot;$&quot;,\n                &quot;JoinSource&quot;: &quot;Input&quot;,\n                &quot;OutputFilter&quot;: &quot;xxx&quot;\n            },\n            &quot;Environment&quot;: {\n                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;\n            },\n            &quot;MaxConcurrentTransforms&quot;: 100,\n            &quot;MaxPayloadInMB&quot;: 1,\n            &quot;ModelName&quot;: &quot;${model_name}&quot;,\n            &quot;TransformInput&quot;: {\n                &quot;DataSource&quot;: {\n                    &quot;S3DataSource&quot;: {\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;\n                    }\n                },\n                &quot;ContentType&quot;: &quot;application\/jsonlines&quot;,\n                &quot;CompressionType&quot;: &quot;Gzip&quot;,\n                &quot;SplitType&quot;: &quot;Line&quot;\n            },\n            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,\n            &quot;TransformOutput&quot;: {\n                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,\n                &quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n                &quot;AssembleWith&quot;: &quot;Line&quot;\n            },    \n            &quot;TransformResources&quot;: {\n                &quot;InstanceType&quot;: &quot;xxx&quot;,\n                &quot;InstanceCount&quot;: 1\n            }\n        },\n            &quot;End&quot;: true\n        }\n        }\n    }\n<\/code><\/pre>\n<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:<\/p>\n<pre><code> def lambda_handler(event, context):\n    return(event.get('executionId').split(':')[-1])\n<\/code><\/pre>\n<p>Or if you dont wanna pass the execution id , it can simply return the random string like<\/p>\n<pre><code> import string\n def lambda_handler(event, context):\n    return(string.ascii_uppercase + string.digits)\n<\/code><\/pre>\n<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.<\/p>",
        "Solution_comment_count":23.0,
        "Solution_last_edit_time":1610641519768,
        "Solution_link_count":1.0,
        "Solution_readability":24.6,
        "Solution_reading_time":43.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":220.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"hard-coded job name error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7230.9875,
        "Challenge_answer_count":0,
        "Challenge_body":"- [ X] I have checked that this bug exists on the latest stable version of AutoGluon\r\n- [ ] and\/or I have checked that this bug exists on the latest mainline of AutoGluon via source installation\r\n\r\n**Describe the bug**\r\nAutogluon 0.4.0 TextPredictor training on p3.8xl 4-GPU instance in Sagemaker Notebook terminal, with `env.num_gpus: 4` setting.  I get an error in spawning multiprocessing.  When I train with everything the same, but only on a single GPU within the same instance and setup, it trains without a problem.\r\n\r\n**Expected behavior**\r\nTrain across all 4 GPUs in the p3.8xl instance with no errors.\r\n\r\n**To Reproduce**\r\n* SageMaker Notebook p3.8xl instance\r\n* python 3.7.12.  \r\n* pip install torch==1.10.0 autogluon.text==0.4.0 awswrangler pandas autofluon.features==0.4.0\r\n* python train.py\r\n\r\nCode:\r\nin `train.py` file\r\n```from argparse import Namespace\r\n\r\nimport pandas as pd\r\nimport awswrangler as wr\r\nfrom autogluon.text import TextPredictor\r\n\r\nargs = Namespace(\r\n    train_filename = \"s3:\/\/ccds-asin-drc\/eu\/modeling-data\/mf2_no_emb\/train\/0\/train.parquet\",\r\n)\r\n\r\nmodel_config = {\r\n    \"eval_metric\": \"accuracy\",\r\n    \"time_limit\": 60*60*3,\r\n    \"features\": ['label', 'item_name_orig']\r\n}\r\n\r\nhyperparameters = {\r\n    'model.hf_text.checkpoint_name': 'microsoft\/mdeberta-v3-base',\r\n    'optimization.top_k': 1,\r\n    'optimization.lr_decay': 0.9,\r\n    'optimization.learning_rate': 1e-4,\r\n    'env.precision': 32,\r\n    'env.per_gpu_batch_size': 4,\r\n    'env.num_gpus': 4\r\n}\r\n\r\ntrain_df = wr.s3.read_parquet(args.full_train_filename)\r\nprint(train_df.info())\r\n\r\npredictor = TextPredictor(\r\n    label='label',\r\n    eval_metric=model_config['eval_metric']\r\n)\r\n\r\npredictor.fit(\r\n    train_data=train_df[model_config['features']],\r\n    hyperparameters=hyperparameters,\r\n    time_limit=model_config['time_limit']\r\n)\r\n\r\n```\r\n\r\n**Screenshots**\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 114, in _main\r\n    prepare(preparation_data)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 225, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 277, in _fixup_main_from_path\r\n    run_name=\"__mp_main__\")\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/ec2-user\/SageMaker\/rubinome_labs\/lab\/202203_drc_multilingual\/train_textonly.py\", line 46, in <module>\r\n    time_limit=model_config['time_limit']\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/text_prediction\/predictor.py\", line 248, in fit\r\n    seed=seed,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 410, in fit\r\n    enable_progress_bar=self._enable_progress_bar,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 561, in _fit\r\n    ckpt_path=self._ckpt_path,  # this is to resume training that was broken accidentally\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 741, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 173, in start_training\r\n    self.spawn(self.new_process, trainer, self.mp_queue, return_result=False)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 201, in spawn\r\n    mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 143, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 136, in _check_not_importing_main\r\n    is not going to be frozen to produce an executable.''')\r\nRuntimeError: \r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\n```\r\n\r\n**Installed Versions**\r\nWhich version of AutoGluon are you are using?  \r\nIf you are using 0.4.0 and newer, please run the following code snippet:\r\n<details>\r\n\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ndate                 : 2022-04-06\r\ntime                 : 15:22:16.975165\r\npython               : 3.7.12.final.0\r\nOS                   : Linux\r\nOS-release           : 4.14.252-131.483.amzn1.x86_64\r\nVersion              : #1 SMP Mon Nov 1 20:48:11 UTC 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 32\r\ncpu_ram_mb           : 245845\r\ncuda version         : 11.450.142.00\r\nnum_gpus             : 4\r\ngpu_ram_mb           : [8404, 8476, 16160, 16160]\r\navail_disk_size_mb   : 11391\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon_contrib_nlp: None\r\nboto3                : 1.21.34\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nmatplotlib           : 3.5.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.21.5\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\nPIL                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : None\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : None\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorchmetrics         : 0.7.3\r\ntqdm                 : 4.64.0\r\ntransformers         : 4.16.2\r\n```\r\n\r\n<\/details>\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1675290130000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1649258575000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the SageMaker endpoint is unable to find or open the model file, resulting in a server error. The user suspects that this may be related to the transition from MXNet to Pytorch and the way their artifacts are stored. The user is attempting to adapt the example model trained in the Multi-Modal documentation to deploy a multi-modal model and pass image paths to the SageMaker endpoint. The user is using AutoGluon version 0.4.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1650",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":15.2,
        "Challenge_reading_time":104.31,
        "Challenge_repo_contributor_count":96.0,
        "Challenge_repo_fork_count":743.0,
        "Challenge_repo_issue_count":2972.0,
        "Challenge_repo_star_count":5806.0,
        "Challenge_repo_watch_count":90.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":7230.9875,
        "Challenge_title":"[BUG] Unable to train on multiple GPUs in Sagemaker Notebook Terminal",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":636,
        "Discussion_body":"@zhiqiangdon we may need to take a look. There seems to be some issues in ddp_spawn in SageMaker Notebook\/Studio environments. I see a similar issue while training the text predictor 0.4 on an instance with multiple GPUs on SageMaker Studio.  @Abhinavb08 @melrubino  We need to write the code like follows (protect `.fit()` with `if __name__ == \"__main__\":`):\r\n\r\n```python\r\nimport pandas as pd\r\nimport awswrangler as wr\r\nfrom autogluon.text import TextPredictor\r\n\r\nargs = Namespace(\r\n    train_filename = \"s3:\/\/ccds-asin-drc\/eu\/modeling-data\/mf2_no_emb\/train\/0\/train.parquet\",\r\n)\r\n\r\nmodel_config = {\r\n    \"eval_metric\": \"accuracy\",\r\n    \"time_limit\": 60*60*3,\r\n    \"features\": ['label', 'item_name_orig']\r\n}\r\n\r\nhyperparameters = {\r\n    'model.hf_text.checkpoint_name': 'microsoft\/mdeberta-v3-base',\r\n    'optimization.top_k': 1,\r\n    'optimization.lr_decay': 0.9,\r\n    'optimization.learning_rate': 1e-4,\r\n    'env.precision': 32,\r\n    'env.per_gpu_batch_size': 4,\r\n    'env.num_gpus': 4\r\n}\r\n\r\ntrain_df = wr.s3.read_parquet(args.full_train_filename)\r\nprint(train_df.info())\r\n\r\nif __name__ == \"__main__\":\r\n    predictor = TextPredictor(\r\n        label='label',\r\n        eval_metric=model_config['eval_metric']\r\n    )\r\n\r\n    predictor.fit(\r\n        train_data=train_df[model_config['features']],\r\n        hyperparameters=hyperparameters,\r\n        time_limit=model_config['time_limit']\r\n    )\r\n\r\n``` Marking as complete, please create a new issue if issue persists in v0.7 release \/ pre-release when using autogluon.multimodal.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"model file not found"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.0084252778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How to get the existing AKS from the notebook that I have already added to AML.\nCreate the cluster<\/p>\n\n<pre><code>attach_config = AksCompute.attach_configuration(resource_id=resource_id)\naks_target = ComputeTarget.attach(workspace=ws, name=create_name, attach_configuration=attach_config)\naks_target.wait_for_completion(True)\n<\/code><\/pre>",
        "Challenge_closed_time":1585037443208,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585033812877,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to retrieve an existing AKS from a notebook that has already been added to AML. They have provided a code snippet for creating a cluster.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60826366",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":5.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.0084252778,
        "Challenge_title":"How to get the existing AKS using compute target",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":279.0,
        "Challenge_word_count":35,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>List all ComputeTarget objects within the workspace:\nPlease follow the below link.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.compute.computetarget?view=azure-ml-py#list-workspace-\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.compute.computetarget?view=azure-ml-py#list-workspace-<\/a><\/p>\n\n<p>you can do like as shown below.<\/p>\n\n<pre><code>from azureml.core.compute import AksCompute, ComputeTarget\naks_name = 'YOUR_EXISTING_CLUSTER_NAME\u2019\naks_target =AksCompute(ws, aks_name)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":23.6,
        "Solution_reading_time":8.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"retrieve existing AKS"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":30.3172266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train a ResNet50 model using keras with tensorflow backend. I'm using a sagemaker GPU instance <strong>ml.p3.2xlarge<\/strong> but my training time is extremely long. I am using conda_tensorflow_p36 kernel and I have verified that I have tensorflow-gpu installed.<\/p>\n<p>When inspecting the output of nvidia-smi I see the process is on the GPU, but the utilization is never above <strong>0%<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Tensorflow also recognizes the GPU.\n<a href=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Screenshot of training time.\n<a href=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is sagemaker in fact using the GPU even though the usage is <strong>0%?<\/strong>\nCould the long epoch training time be caused by another issue?<\/p>",
        "Challenge_closed_time":1650008050343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649867231580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing slow training time while using a ResNet50 model with keras and tensorflow backend on an AWS Sagemaker GPU instance. The user has verified that tensorflow-gpu is installed and the GPU is recognized by tensorflow, but the GPU utilization is never above 0%. The user is unsure if the GPU is being used and is also questioning if there could be another issue causing the long epoch training time.",
        "Challenge_last_edit_time":1649898908327,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71860839",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":15.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":39.1163230556,
        "Challenge_title":"Slow ResNet50 training time using AWS Sagemaker GPU instance",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":120.0,
        "Challenge_word_count":133,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540782143880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Looks like you've completed 8 steps and it just takes very long. What's your step time?<br \/>\nIt might be due to data loading. Where ia data stored? Try to take data loading out of the picture by caching and feeding a single image to the DNN repeatedly and see if that helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.8,
        "Solution_reading_time":3.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"slow training, GPU not utilized"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.8202777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nCurrently the example DAG for sagemaker just uses access key and secret key. We need to use a temporary  access token\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1666960895000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666950742000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug where the XCom return value of `SageMakerTransformOperatorAsync` and `SageMakerTrainingOperatorAsync` does not produce the expected output. The issue seems to be that some keys do not match the non-async operator output. The user has tried to reproduce the issue by running a DAG with traditional operators and then running the same DAG with async operators, and comparing the outputs. The expected behavior is for the XCom keys and values to match whatever the traditional non-async version of the operators output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/738",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.6,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":23.0,
        "Challenge_repo_issue_count":1165.0,
        "Challenge_repo_star_count":110.0,
        "Challenge_repo_watch_count":34.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2.8202777778,
        "Challenge_title":"Sagemaker example DAG to use aws session token",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"mismatched XCom keys\/values"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":55.4034008333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In the table view of a project, is it possible to show only the columns that have different values among runs? This would be very useful to quickly explore how changing parameters affect the model.<\/p>",
        "Challenge_closed_time":1661383337880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661183885637,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to display only the columns in a project's table view that have different values among runs, in order to quickly analyze how changing parameters affect the model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/show-only-columns-with-different-values-in-experiments-table\/2972",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":3.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":55.4034008333,
        "Challenge_title":"Show only columns with different values in experiments table",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":43,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/enajx\">@enajx<\/a> , would the run comparer table work for your use case, see <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/run-comparer\">here<\/a>.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.4,
        "Solution_reading_time":2.59,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"display different columns in table view"
    },
    {
        "Answerer_created_time":1444940402827,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Oakland, CA, USA",
        "Answerer_reputation_count":8474.0,
        "Answerer_view_count":1313.0,
        "Challenge_adjusted_solved_time":10.0860825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running some projects with H2o AutoML using Sagemaker notebook instances, and I would like to know if H2o AutoML can benefit from a GPU Sagemaker instance, if so, how should I configure the notebook? <\/p>",
        "Challenge_closed_time":1567727347088,
        "Challenge_comment_count":4,
        "Challenge_created_time":1567711533230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether H2o AutoML can benefit from a GPU Sagemaker instance and how to configure the notebook if so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57811873",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.3927383333,
        "Challenge_title":"Can H2o AutoML benefit from a GPU instance on Sagemaker platform?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":462.0,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p><a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\" rel=\"nofollow noreferrer\">H2O AutoML<\/a> contains a handful of algorithms and one of them is <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/xgboost.html\" rel=\"nofollow noreferrer\">XGBoost<\/a>, which has been part of H2O AutoML since H2O version 3.22.0.1.  XGBoost is the only GPU-capable algorithm inside of H2O AutoML, however, a lot of the models that are trained in AutoML are XGBoost models, so it still can be useful to utilize a GPU. Keep in mind that you must use H2O 3.22 or above to use this feature.<\/p>\n\n<p>My suggestion is to test it on a GPU-enabled instance and compare the results to a non-GPU instance and see if it's worth the extra cost.  <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1567747843127,
        "Solution_link_count":2.0,
        "Solution_readability":10.2,
        "Solution_reading_time":9.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"GPU for H2o AutoML"
    },
    {
        "Answerer_created_time":1401922736652,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brisbane",
        "Answerer_reputation_count":748.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":510.2445766667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Challenge_closed_time":1523591814356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521754623920,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in making hyperparameters available to the serving endpoint in SageMaker Tensorflow. While the training instances have access to input parameters using hyperparameters, there is no way to pass in parameters to control data processing in the input function when the endpoint is deployed. The user is seeking advice on the best way to pass parameters to the serving instance.",
        "Challenge_last_edit_time":1521754933880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":510.3306766667,
        "Challenge_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1426.0,
        "Challenge_word_count":108,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":12.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"passing parameters to serving instance"
    },
    {
        "Answerer_created_time":1537462795807,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":99.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":473.2640808334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created a custom model and deployed it on sagemaker. I am invoking the endpoint using batch transform jobs. It works if the input file is small, i.e, number of rows in the csv file is less. If I upload a file with around 200000 rows, I am getting this error in the cloudwatch logs.<\/p>\n\n<pre><code>2018-11-21 09:11:52.666476: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113]\nAllocation of 2878368000 exceeds 10% of system memory.\n2018-11-21 09:11:53.166493: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113] \nAllocation of 2878368000 exceeds 10% of system memory.\n[2018-11-21 09:12:02,544] ERROR in serving: &lt;_Rendezvous of RPC that \nterminated with:\n#011status = StatusCode.DEADLINE_EXCEEDED\n#011details = \"Deadline Exceeded\"\n#011debug_error_string = \"\n{\n\"created\": \"@1542791522.543282048\",\n\"description\": \"Error received from peer\",\n\"file\": \"src\/core\/lib\/surface\/call.cc\",\n\"file_line\": 1017,\n\"grpc_message\": \"Deadline Exceeded\",\n\"grpc_status\": 4\n}\n\"\n<\/code><\/pre>\n\n<p>Any ideas what might be going wrong. This is the transform function which I am using to create the transform job.<\/p>\n\n<pre><code>transformer =sagemaker.transformer.Transformer(\nbase_transform_job_name='Batch-Transform',\nmodel_name='sagemaker-tensorflow-2018-11-21-07-58-15-887',\ninstance_count=1,\ninstance_type='ml.m4.xlarge',\noutput_path='s3:\/\/2-n2m-sagemaker-json-output\/out_files\/'\n\n)\ninput_location = 's3:\/\/1-n2m-n2g-csv-input\/smal_sagemaker_sample.csv'\ntransformer.transform(input_location, content_type='text\/csv', split_type='Line')\n<\/code><\/pre>\n\n<p>The .csv file contains 2 columns for first and last name of customer, which I am then preprocessing it in the sagemaker itself using input_fn().<\/p>",
        "Challenge_closed_time":1544503147007,
        "Challenge_comment_count":0,
        "Challenge_created_time":1542792620897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using Amazon SageMaker's batch transform jobs to invoke an endpoint with a large CSV file containing around 200,000 rows. The error message indicates that the allocation of memory exceeds the system limit. The user is seeking advice on how to resolve this issue.",
        "Challenge_last_edit_time":1542799396316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53408927",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":23.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":475.1461416667,
        "Challenge_title":"How to pass a bigger .csv files to amazon sagemaker for predictions using batch transform jobs",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1941.0,
        "Challenge_word_count":187,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444454434270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":140.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>The error looks to be coming from a GRPC client closing the connection before the server is able to respond. (There looks to be an existing feature request for the sagemaker tensorflow container on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46<\/a> to make this timeout configurable)<\/p>\n\n<p>You could try out a few things with the sagemaker Transformer to limit the size of each individual request so that it fits within the timeout:<\/p>\n\n<ul>\n<li>Set a <code>max_payload<\/code> to a smaller value, say 2-3 MB (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">the default is 6 MB<\/a>)<\/li>\n<li>If your instance metrics indicate it has compute \/ memory resources to spare, try <code>max_concurrent_transforms<\/code> > 1 to make use of multiple workers<\/li>\n<li>Split up your csv file into multiple input files. With a bigger dataset, you could also increase the instance count to fan out processing<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.7,
        "Solution_reading_time":14.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"memory allocation limit exceeded"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":968.2322455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to make transfer learning method on MXNet on Sagemaker instance. Train and serve start locally without any problem and I'm using that python code to predict:<\/p>\n\n<pre><code>def predict_mx(net, fname):\n    with open(fname, 'rb') as f:\n      img = image.imdecode(f.read())\n      plt.imshow(img.asnumpy())\n      plt.show()\n    data = transform(img, -1, test_augs)\n    plt.imshow(data.transpose((1,2,0)).asnumpy()\/255)\n    plt.show()\n    data = data.expand_dims(axis=0)\n    return net.predict(data.asnumpy().tolist())\n<\/code><\/pre>\n\n<p>I checked <code>data.asnumpy().tolist()<\/code> that is ok and pyplot draw images (firts is the original image, the second is the resized image). But <code>net.predict<\/code> raise an error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\n&lt;ipython-input-171-ea0f1f5bdc72&gt; in &lt;module&gt;()\n----&gt; 1 predict_mx(predictor.predict, '.\/data2\/burgers-imgnet\/00103785.jpg')\n\n&lt;ipython-input-170-150a72b14997&gt; in predict_mx(net, fname)\n     30     plt.show()\n     31     data = data.expand_dims(axis=0)\n---&gt; 32     return net(data.asnumpy().tolist())\n     33 \n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data)\n     89         if self.deserializer is not None:\n     90             # It's the deserializer's responsibility to close the stream\n---&gt; 91             return self.deserializer(response_body, response['ContentType'])\n     92         data = response_body.read()\n     93         response_body.close()\n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in __call__(self, stream, content_type)\n    290         \"\"\"\n    291         try:\n--&gt; 292             return json.load(codecs.getreader('utf-8')(stream))\n    293         finally:\n    294             stream.close()\n\n\/usr\/lib64\/python3.6\/json\/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    297         cls=cls, object_hook=object_hook,\n    298         parse_float=parse_float, parse_int=parse_int,\n--&gt; 299         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n    300 \n    301 \n\n\/usr\/lib64\/python3.6\/json\/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    352             parse_int is None and parse_float is None and\n    353             parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354         return _default_decoder.decode(s)\n    355     if cls is None:\n    356         cls = JSONDecoder\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in decode(self, s, _w)\n    337 \n    338         \"\"\"\n--&gt; 339         obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    340         end = _w(s, end).end()\n    341         if end != len(s):\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n    355             obj, end = self.scan_once(s, idx)\n    356         except StopIteration as err:\n--&gt; 357             raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    358         return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n\n<p>I tried to json.dumps my data, and there is no problem with that.<\/p>\n\n<p>Note that I didn't deployed the service on AWS yet, I want to be able to test the model and prediction locally before to make a larger train and to serve it later.<\/p>\n\n<p>Thanks for your help<\/p>",
        "Challenge_closed_time":1531584172427,
        "Challenge_comment_count":4,
        "Challenge_created_time":1528098536343,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to predict using transfer learning method on MXNet on Sagemaker instance. The error is raised by net.predict and is related to JSONDecodeError. The user has checked data.asnumpy().tolist() and it is okay. The user has also tried to json.dumps the data, and there is no problem with that. The user has not deployed the service on AWS yet and wants to test the model and prediction locally before making a larger train and serving it later.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50675708",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.9,
        "Challenge_reading_time":41.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":968.2322455556,
        "Challenge_title":"Sagemaker Predict on local instance, JSON Error",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1358.0,
        "Challenge_word_count":332,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340280616088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Laval, France",
        "Poster_reputation_count":2835.0,
        "Poster_view_count":281.0,
        "Solution_body":"<p>The call to <strong>net.predict<\/strong> is working fine. <\/p>\n\n<p>It seems that you are using the SageMaker Python SDK <strong>predict_fn<\/strong> for hosting. After the <strong>predict_fn<\/strong> is invoked, the MXNet container will try to serialize your prediction to JSON before sending it back to the client. You can see code that does that here: <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132<\/a><\/p>\n\n<p>The container is failing to serialize because <strong>net.predict<\/strong> does not return a serializable object. You can solve this issue by returning a list instead:<\/p>\n\n<pre><code>return net.predict(data.asnumpy().tolist()).asnumpy().tolist()\n<\/code><\/pre>\n\n<p>Another alternative is to use a <strong>transform_fn<\/strong> instead of <strong>prediction_fn<\/strong> so you can handle the output serialization yourself. You can see an example of a <strong>transform_fn<\/strong> here <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.4,
        "Solution_reading_time":18.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"JSONDecodeError in net.predict"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":62.4823258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Good morning,  <br \/>\nI have a a dataset that consist of 99000 (256 x 256 pixels) images. I am trying to use this dataset to training a generative advesarial network (GAN) for at least a 1,000 epoch.   <br \/>\nCurrently, I am using a standard_NC24r (24 cores, 224 GB RAM, 1440 GB disk) GPU  (4 x NVIDIA Tesla K80) cluster but the training is slow. It takes about 3000 seconds to train 1 epoch. This implies it would take at least a month to complete training.  <br \/>\nIs a cluster that I can used to speed up training?  <\/p>\n<p>Thanks for your help in advance  <\/p>\n<p>Many thanks  <\/p>\n<p>Roland<\/p>",
        "Challenge_closed_time":1633928472660,
        "Challenge_comment_count":1,
        "Challenge_created_time":1633703536287,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has a dataset of 99,000 images and is trying to train a GAN for at least 1,000 epochs. They are currently using a standard_NC24r GPU cluster but the training is slow, taking 3000 seconds to train 1 epoch. The user is seeking advice on a faster cluster to speed up the training process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/583349\/best-compute-cluster-for-training-large-image-data",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.4,
        "Challenge_reading_time":7.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":62.4823258333,
        "Challenge_title":"Best compute cluster for training large image datasets !",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=5554be0b-de3f-4849-9c04-ab53140c4523\">@Okwen, Roland T  <\/a> Thanks, Instead of bigger machines with more memory, there are techniques to be used with Aml Compute for larger datasets. The <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/machine-learning-pipelines\/parallel-run\">Parallel Run<\/a> Step is an AzureML Pipeline Step which enables parallel processing or data partitions across multiple workers on multiple nodes. PRS (ParallelRunStep) is designed for embarrassingly parallel workload, e.g. train many models, batch inference, etc.    <\/p>\n<p>Also look into using some of the curated images provided for compute clusters.    <br \/>\nSpecifically look into the DASK image.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/resource-curated-environments\">Curated environments - Azure Machine Learning | Microsoft Learn<\/a>    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":12.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"faster GPU cluster"
    },
    {
        "Answerer_created_time":1483444144907,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hoth",
        "Answerer_reputation_count":312.0,
        "Answerer_view_count":65.0,
        "Challenge_adjusted_solved_time":138.6800833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Crowd HTML Elements to perform bounding box annotation, but when I attempt to load some of my images, I get this error in the dev tools console:<\/p>\n<pre><code>crowd-html-elements.js:1 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements.js:1\nerror (async)\ne @ crowd-html-elements.js:1\ne @ crowd-html-elements.js:1\n.\/src\/crowd-html-elements-loader.ts @ crowd-html-elements.js:1\ns @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:6282\nerror (async)\ne @ crowd-html-elements-without-ce-polyfill.js:6282\ne @ crowd-html-elements-without-ce-polyfill.js:6282\n.\/src\/index.ts @ crowd-html-elements-without-ce-polyfill.js:6282\nr @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 Uncaught Error: Unexpected image dimensions during normalization\n    at Function.normalizeHeight (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Function.normalizeDimensions (crowd-html-elements-without-ce-polyfill.js:6282)\n    at new a (crowd-html-elements-without-ce-polyfill.js:6282)\n    at ie.handleTargetImageLoaded (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Image.r.onload (crowd-html-elements-without-ce-polyfill.js:6282)\nnormalizeHeight @ crowd-html-elements-without-ce-polyfill.js:6282\nnormalizeDimensions @ crowd-html-elements-without-ce-polyfill.js:6282\na @ crowd-html-elements-without-ce-polyfill.js:6282\nhandleTargetImageLoaded @ crowd-html-elements-without-ce-polyfill.js:6282\nr.onload @ crowd-html-elements-without-ce-polyfill.js:6282\nload (async)\nsetBackgroundImage @ crowd-html-elements-without-ce-polyfill.js:6282\nrenderImageSrcChange @ crowd-html-elements-without-ce-polyfill.js:6282\nshouldComponentUpdate @ crowd-html-elements-without-ce-polyfill.js:6282\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\nS @ crowd-html-elements-without-ce-polyfill.js:6278\ne.reactMount @ crowd-html-elements-without-ce-polyfill.js:3\ne.updateRegion @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\ne.reactBatchUpdate @ crowd-html-elements-without-ce-polyfill.js:3\ni @ crowd-html-elements-without-ce-polyfill.js:3\nf.componentDidUpdate @ crowd-html-elements-without-ce-polyfill.js:3\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\n_renderReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\n_updateReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\nY @ crowd-html-elements-without-ce-polyfill.js:5984\nC @ crowd-html-elements-without-ce-polyfill.js:5984\nk @ crowd-html-elements-without-ce-polyfill.js:5984\n_propertiesChanged @ crowd-html-elements-without-ce-polyfill.js:5984\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5954\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_invalidateProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_setProperty @ crowd-html-elements-without-ce-polyfill.js:5984\nObject.defineProperty.set @ crowd-html-elements-without-ce-polyfill.js:5954\n(anonymous) @ labeling.html:199\nasync function (async)\n(anonymous) @ labeling.html:198\nPromise.then (async)\n(anonymous) @ labeling.html:196\n<\/code><\/pre>\n<p>The <strong>Unexpected image dimensions during normalization<\/strong> portion seems like the issue, but I've found nothing with regard to troubleshooting.  Can someone explain what expected image dimensions are and why some are failing?<\/p>\n<p>Here's a snippet of the code that's throwing the error.<\/p>\n<pre><code>            static normalizeHeight(e) {\n                if (e.height === e.naturalHeight)\n                    return e.height;\n                if (e.height === e.naturalWidth)\n                    return e.height;\n                if (Math.abs(e.height - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                if (Math.abs(e.height - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n            }\n            static normalizeWidth(e) {\n                if (e.width === e.naturalWidth)\n                    return e.width;\n                if (e.width === e.naturalHeight)\n                    return e.width;\n                if (Math.abs(e.width - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                if (Math.abs(e.width - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n<\/code><\/pre>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1603367517400,
        "Challenge_comment_count":6,
        "Challenge_created_time":1601930941820,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while using Crowd HTML Elements for bounding box annotation. The error message \"Unexpected image dimensions during normalization\" is displayed in the console. The user is seeking an explanation of expected image dimensions and why some images are failing. The provided code snippet shows the function that is throwing the error.",
        "Challenge_last_edit_time":1602868269100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64215998",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":25.9,
        "Challenge_reading_time":94.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":108,
        "Challenge_solved_time":399.0487722222,
        "Challenge_title":"Why is Crowd HTML breaking this image?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":168.0,
        "Challenge_word_count":380,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483444144907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hoth",
        "Poster_reputation_count":312.0,
        "Poster_view_count":65.0,
        "Solution_body":"<p>The issue turned out to be related to the css styling that was being applied to the canvas portion of my site that was loading the labeling tools.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":1.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unexpected image dimensions"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":31.7955766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In colab, whenever we need GPU, we simply click <code>change runtime type<\/code> and change hardware accelarator to <code>GPU<\/code><\/p>\n<p>and cuda becomes available, <code>torch.cuda.is_available()<\/code> is <code>True<\/code><\/p>\n<p>How to do this is AWS sagemaker, i.e. turning on cuda.\nI am new to AWS and trying to train model using pytorch in aws sagemaker, where Pytorch code is first tested in colab environment.<\/p>\n<p>my sagemaker notebook insatnce is <code>ml.t2.medium<\/code><\/p>",
        "Challenge_closed_time":1617464162903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617348836210,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to AWS and is trying to train a Pytorch model in an AWS Sagemaker notebook instance. They are looking for guidance on how to turn on CUDA, similar to how it is done in Colab, where they can simply change the runtime type to GPU. They are currently using a ml.t2.medium instance.",
        "Challenge_last_edit_time":1617349698827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66915920",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":32.0351925,
        "Challenge_title":"Using pytorch cuda in AWS sagemaker notebook instance",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1608.0,
        "Challenge_word_count":73,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567880532003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":137.0,
        "Poster_view_count":100.0,
        "Solution_body":"<p>Using AWS Sagemaker you don't need to worry about the GPU, you simply select an instance type with GPU ans Sagemaker will use it. Specifically <code>ml.t2.medium<\/code> doesn't have a GPU but it's anyway not the right way to train a model.\nBasically you have 2 canonical ways to use Sagemaker (look at the documentation and examples please), the first is to use a notebook with a limited computing resource to spin up a training job using a prebuilt image, in that case when you call the estimator you simply specify what <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">instance type<\/a> you want (you'll choose one with GPU, looking at the costs). The second way is to use your own container, push it to ECR and launch a training job from the console, where you specify the instance type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":10.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"enable CUDA in Sagemaker"
    },
    {
        "Answerer_created_time":1297232105368,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":27164.0,
        "Answerer_view_count":3016.0,
        "Challenge_adjusted_solved_time":0.6067841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I have this code to write out a json file to be connected to via endpoint. The file is pretty standard in that it has the location of how to connect to the endpoint as well as some data.<\/p>\n<pre><code>%%writefile default-pred.json\n{   PROJECT_ID:&quot;msds434-gcp&quot;,\n    REGION:&quot;us-central1&quot;,\n    ENDPOINT_ID:&quot;2857701089334001664&quot;,\n    INPUT_DATA_FILE:&quot;INPUT-JSON&quot;,\n    &quot;instances&quot;: [\n    {&quot;age&quot;: 39,\n    &quot;bill_amt_1&quot;: 47174,\n    &quot;bill_amt_2&quot;: 47974,\n    &quot;bill_amt_3&quot;: 48630,\n    &quot;bill_amt_4&quot;: 50803,\n    &quot;bill_amt_5&quot;: 30789,\n    &quot;bill_amt_6&quot;: 15874,\n    &quot;education_level&quot;: &quot;1&quot;,\n    &quot;limit_balance&quot;: 50000,\n    &quot;marital_status&quot;: &quot;2&quot;,\n    &quot;pay_0&quot;: 0,\n    &quot;pay_2&quot;:0,\n    &quot;pay_3&quot;: 0,\n    &quot;pay_4&quot;: 0,\n    &quot;pay_5&quot;: &quot;0&quot;,\n    &quot;pay_6&quot;: &quot;0&quot;,\n    &quot;pay_amt_1&quot;: 1800,\n    &quot;pay_amt_2&quot;: 2000,\n    &quot;pay_amt_3&quot;: 3000,\n    &quot;pay_amt_4&quot;: 2000,\n    &quot;pay_amt_5&quot;: 2000,\n    &quot;pay_amt_6&quot;: 2000,\n    &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n    }\n<\/code><\/pre>\n<p>Then I have this trying to connect to the file and then taking the information to connect to the end point in question. I know the information is right as it's the exact code from GCP.<\/p>\n<pre><code>!curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n-d &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>So from the information I have I would expect it to parse the information I have and connect to the endpoint, but obviously I have my file wrong somehow. Any idea what it is?<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;,\n    &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;,\n    &quot;details&quot;: [\n      {\n        &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.BadRequest&quot;,\n        &quot;fieldViolations&quot;: [\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;\n          }\n        ]\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>What am I missing here?<\/p>",
        "Challenge_closed_time":1653863593763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653862009460,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to write a JSON file to be connected to via endpoint, but encounters an error message stating that the JSON payload received is invalid and cannot find the specified fields. The user is seeking assistance in identifying the issue with the file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72427594",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":40.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":0.4400841667,
        "Challenge_title":"Questions on json and GCP",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":281,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632597456472,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>The data file should only include the data.<\/p>\n<p>You've included <code>PROJECT_ID<\/code>, <code>REGION<\/code>, <code>ENDPOINT<\/code> and should not.<\/p>\n<p>These need to be set in the (bash) environment <strong>before<\/strong> you issue the <code>curl<\/code> command:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>PROJECT_ID=&quot;msds434-gcp&quot;\nREGION=&quot;us-central1&quot;\nENDPOINT_ID=&quot;2857701089334001664&quot;\n\ncurl \\\n--request POST \\\n--header &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n--header &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n--data &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>The file <code>default-pred.json<\/code> should <strike>probably (I can never find this service's methods in <a href=\"https:\/\/developers.google.com\/apis-explorer\" rel=\"nofollow noreferrer\">APIs Explorer<\/a>!<\/strike>) just be:<\/p>\n<pre><code>{\n  instances&quot;: [\n    { &quot;age&quot;: 39,\n      &quot;bill_amt_1&quot;: 47174,\n      &quot;bill_amt_2&quot;: 47974,\n      &quot;bill_amt_3&quot;: 48630,\n      &quot;bill_amt_4&quot;: 50803,\n      &quot;bill_amt_5&quot;: 30789,\n      &quot;bill_amt_6&quot;: 15874,\n      &quot;education_level&quot;: &quot;1&quot;,\n      &quot;limit_balance&quot;: 50000,\n      &quot;marital_status&quot;: &quot;2&quot;,\n      &quot;pay_0&quot;: 0,\n      &quot;pay_2&quot;:0,\n      &quot;pay_3&quot;: 0,\n      &quot;pay_4&quot;: 0,\n      &quot;pay_5&quot;: &quot;0&quot;,\n      &quot;pay_6&quot;: &quot;0&quot;,\n      &quot;pay_amt_1&quot;: 1800,\n      &quot;pay_amt_2&quot;: 2000,\n      &quot;pay_amt_3&quot;: 3000,\n      &quot;pay_amt_4&quot;: 2000,\n      &quot;pay_amt_5&quot;: 2000,\n      &quot;pay_amt_6&quot;: 2000,\n      &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>See the documentation for the <code>aiplatform<\/code> <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/reference\/rest\/v1\/projects\/predict\" rel=\"nofollow noreferrer\"><code>predict<\/code><\/a> method as this explains this.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653864193883,
        "Solution_link_count":3.0,
        "Solution_readability":19.8,
        "Solution_reading_time":27.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":135.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid JSON payload"
    },
    {
        "Answerer_created_time":1639972619152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":958.0,
        "Answerer_view_count":995.0,
        "Challenge_adjusted_solved_time":19.0550916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running custom training jobs using Google cloud Vertex AI. But when I enter a custom training job page, the GPU utilization display is not shown, instead, there is a message saying &quot;you don't have access to this data.&quot;\n<a href=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" alt=\"enter image description here\" \/><\/a>\nI would appreciate help finding the right IAM role which will allow me to view the GPU utilization.\nThanks!<\/p>",
        "Challenge_closed_time":1660101916403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660033318073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while running custom training jobs using Google cloud Vertex AI as the GPU utilization display is not shown and a message saying \"you don't have access to this data\" is displayed. The user is seeking help to find the right IAM role to view the GPU utilization.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73288631",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":19.0550916667,
        "Challenge_title":"Allowing users to view GPU utilization in GCP Vertex AI training jobs",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":80,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1379514677176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":597.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/cloud.google.com\/monitoring\/access-control#mon_roles_desc\" rel=\"nofollow noreferrer\"><code>monitoring.viewer<\/code><\/a> IAM role to display both CPU and GPU utilization in GCP Vertex AI training jobs on top of <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\"><code>aiplatform.viewer<\/code><\/a> IAM role.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.3,
        "Solution_reading_time":7.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":41.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"GPU utilization not displayed"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4397.9938172222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Challenge_closed_time":1617119678127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616764992750,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on a Machine Learning proof of concept on AWS with SageMaker and is confused about the differences between sagemaker and sagemaker_pyspark libraries. They are unsure if using sagemaker is equivalent to running a training job without taking advantage of the distributed computing capabilities of AWS. They are also experiencing slow training times with sagemaker_pyspark and are wondering if there is a way to reuse the same endpoint with different training jobs. The user is also unsure if sagemaker_pyspark supports custom algorithms and hyperparameter optimization. They are confused about the concept of entry_point and container\/image_name and are wondering if model containers are necessary. The user is also considering using a combination of Step Functions, Glue processes",
        "Challenge_last_edit_time":1618398204670,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66817781",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":47.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":98.5237158334,
        "Challenge_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":242.0,
        "Challenge_word_count":473,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376052312528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Australia",
        "Poster_reputation_count":135.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1634230982412,
        "Solution_link_count":2.0,
        "Solution_readability":10.5,
        "Solution_reading_time":41.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":433.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"clarify sagemaker_pyspark differences"
    },
    {
        "Answerer_created_time":1360164540016,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH",
        "Answerer_reputation_count":11190.0,
        "Answerer_view_count":365.0,
        "Challenge_adjusted_solved_time":49.8260258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is a hard situation to describe.<\/p>\n\n<p>I have a python model train script at:<\/p>\n\n<p><code>myproject\/opt\/program\/train<\/code><\/p>\n\n<p>This gets a file at <code>.\/opt\/ml\/input\/data\/external\/train.csv<\/code><\/p>\n\n<p>When I do <code>python3 opt\/program\/train<\/code> the training runs fine locally.<\/p>\n\n<p>Then I containerize the project and copy <code>opt<\/code> to <code>\/opt<\/code> in my Dockerfile.<\/p>\n\n<p>Now when I run <code>docker run &lt;image name&gt; train<\/code> it also trains fine.<\/p>\n\n<p>Then I deploy the image to SageMaker, create an estimator, and call <code>model.fit(my_data)<\/code> I get:<\/p>\n\n<p><code>Exception during training: [Errno 2] File b'.\/opt\/ml\/input\/data\/external\/train.csv' does not exist<\/code><\/p>\n\n<p>It's definitely there, I was able to train by running the container myself.  Also running the container and exploring the file system I can find the file.<\/p>\n\n<p>So I think I have some filesystem misunderstanding.  From the root of the container, all of these seem to have equivalent outputs.<\/p>\n\n<pre><code>root@798ffe7364c6:\/# ls opt\nml  program\nroot@798ffe7364c6:\/# ls \/opt\nml  program\nroot@798ffe7364c6:\/# ls .\/opt\nml  program\n<\/code><\/pre>\n\n<p>I'm trying to come up with a way to have one path that will work locally, in the container, and on AWS.<\/p>",
        "Challenge_closed_time":1574100008956,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573920322537,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sagemaker where the model training script is unable to find a file at the specified path, even though the file exists and the training runs fine locally and in the container. The user suspects a filesystem misunderstanding and is trying to find a way to have one path that will work locally, in the container, and on AWS.",
        "Challenge_last_edit_time":1573920635263,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58892606",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":16.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":49.9128941667,
        "Challenge_title":"Sagemaker can't find paths in container",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2448.0,
        "Challenge_word_count":175,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360164540016,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH",
        "Poster_reputation_count":11190.0,
        "Poster_view_count":365.0,
        "Solution_body":"<p>I was missing the fact that SageMaker looks for your data channels in S3 and copies those to your container at <code>\/opt\/ml\/input\/data<\/code><\/p>\n\n<p>By default it seems to use <code>training<\/code> and <code>validation<\/code> as the channel names.  Therefore, in my example above, it would have never copied data from my <code>external<\/code> folder on S3 to the right <code>external<\/code> folder in my container.  In fact, I discovered it was copying it instead to <code>\/opt\/ml\/input\/data\/training\/external\/train.csv<\/code>.<\/p>\n\n<p>To resolve this, I would have either had to change my folder names, or use <code>InputDataConfig<\/code> to define other channels.  I chose the later and was able to get it working.<\/p>\n\n<p>More info on <code>InputDataConfig<\/code> here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.2,
        "Solution_reading_time":12.56,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"file not found in Sagemaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.3284980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using .NII file format which represents, the neuroimaging dataset. I need to use Auto ML to label the dataset of images that are nearly 2GB in size per patient. The main issue is with using Auto ML to label the dataset of images with.NII file extension and classify whether the patient is having dementia or not.<\/p>\n<p><strong>Requirement:<\/strong> Forget about the problem domain of implementation like dementia. I would like to know about the procedure of using Auto ML for Computer vision applications through ML studio to use.NII file format dataset images.<\/p>\n<p>Any help would be thankful.<\/p>",
        "Challenge_closed_time":1652091549923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652090367330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in using AutoML to label and classify MRI images in .NII file format for dementia diagnosis. The main issue is with implementing AutoML for computer vision applications through ML studio to use the .NII file format dataset images. The user is seeking guidance on the procedure for using AutoML in this context.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72170169",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":8.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3284980556,
        "Challenge_title":"Implementing Computer Vison on AutoML to classify dementia using MRI images in .NII file format",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>The requirement of using .nii or other file formats in Azure auto ML is a challenging task. Unfortunately, Auto ML image input format will be using in only JSON format. Kindly check the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-automl-images-schema\" rel=\"nofollow noreferrer\">document<\/a><\/p>\n<p>Answering regarding requirement of .nii format of dataset, there are different file format convertors available like &quot;<em><strong><a href=\"http:\/\/medicalimageconverter.com\/?msclkid=403b597ecf8111ecac65b3422c7b95b5\" rel=\"nofollow noreferrer\">Medical Image Convertor<\/a><\/strong><\/em>&quot;. This software is commercial and can be used for 10days for free. Convert .nii file formats into JPG and proceed with the general documentation provided in the top of the answer.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":10.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use AutoML for MRI classification"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.7666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"I am doing a comparative analysis of predictive analytics software for my Project. I am looking for approximate lines of code for the Google Vertex AI product.",
        "Challenge_closed_time":1638190740000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638115980000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is conducting a comparative analysis of predictive analytics software for their project and is seeking information on the approximate lines of code for Google Vertex AI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-anyone-tell-what-is-the-approximate-SLOC-of-Google-Vertex-AI\/m-p\/176629#M96",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":20.7666666667,
        "Challenge_title":"Can anyone tell what is the approximate SLOC of Google Vertex AI? For my comparative analysis study.",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":43,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,\u00a0\n\nSLOC will be different depending on your specific use case, and you should be in the best position to figure out the approximate SLOC for your scenarios. That being said, you might look into the sample code and notebooks for Vertex AI, the end-to-end machine learning platform on Google Cloud at [1].\n\n[1]\u00a0https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":4.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Vertex AI",
        "Challenge_type":"inquiry",
        "Challenge_summary":"code lines for Vertex AI"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":6.7239763889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an AKS cluster using Azure Machine Learning SDK extension and I attached to the workspace created. When the cluster is created and attached, I doesn't show any error. When I am trying to detach it from workspace, it is not accepting the operations.<\/p>\n<p>I would like to detach the existing AKS cluster from workspace either by program manner, using CLI or even using Azure portal.<\/p>",
        "Challenge_closed_time":1659333006088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659308799773,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to detach an AKS cluster from a workspace created using Azure Machine Learning SDK extension. The cluster was created and attached without any issues, but detaching it is not working through program, CLI, or Azure portal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73187536",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.7239763889,
        "Challenge_title":"Error while detaching AKS cluster through Azure ML SDK extension",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":76,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>If we are using any <strong>extensions of SDK or Azure CLI<\/strong> for machine learning to detach AKS cluster, it <strong>will not work<\/strong> and it will not get deleted or detached. Instead, we need to use <strong>Azure CLI with AKS<\/strong>. There are two types of implementations we can perform.<\/p>\n<p><strong>Python:<\/strong><\/p>\n<pre><code>Aks_target.detach()\n<\/code><\/pre>\n<p><strong>Azure CLI:<\/strong><\/p>\n<p>Before performing this step, we need to get the details of the working AKS cluster name attached to our workspace. Resource Group details and workspace name<\/p>\n<pre><code>az ml computertarget detach -n youraksname -g yourresourcegroup -w yourworkspacename\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":8.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to detach AKS cluster"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":10.2829988889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm moving my first steps in <code>amazon sagemaker<\/code>. I'm using script mode to train a classification algorithm. Training is fine, however I'm not able to do incremental training. I want to train again the same model with new data. Here what I did. This is my script:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker import get_execution_role\n\nbucket = 'sagemaker-blablabla'\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\ntf_estimator = TensorFlow(entry_point='main.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          framework_version='1.12', \n                          py_version='py3',\n                          output_path=s3_output_location)\n\ninputs = {'train': train_data, 'test': validation_data}\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>The entry point is my custom keras code, which I adapted to receive arguments from the script.\nNow the training is successfully completed and I have in my s3 bucket the model.tar.gz. I want to train again, but it's not clear to me how to do it.. I tried this<\/p>\n\n<pre><code>trained_model = 's3:\/\/sagemaker-blablabla\/sagemaker-tensorflow-scriptmode-2019-11-27-12-01-42-300\/output\/model.tar.gz'\n\ntf_estimator = sagemaker.estimator.Estimator(image_name='blablabla-west-1.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12-gpu-py3', \n                                              role=get_execution_role(),\n                                              train_instance_count=1, \n                                              train_instance_type='ml.p2.xlarge',\n                                              output_path=s3_output_location,\n                                              model_uri = trained_model)\n\ninputs = {'train': train_data, 'test': validation_data}\n\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>Doesn't work. Firstly, I don't know how to retrieve the training image name (for this I looked for it in the <code>aws<\/code> console, but I guess there should be a smarter solution), second this code throws an exception about the entry point but it is my understanding that I shouldn't need it when I do incremental learning with a ready image.\nI'm surely missing something important, any help? Thank you!<\/p>",
        "Challenge_closed_time":1574981388936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574941643827,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with incremental training on custom code in Amazon SageMaker. They have successfully trained a classification algorithm using script mode but are unable to train the same model with new data. The user has tried to use the trained model and a new estimator but is encountering an exception about the entry point. They are seeking guidance on how to retrieve the training image name and how to perform incremental learning with a ready image.",
        "Challenge_last_edit_time":1574944370140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59088199",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":27.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":11.0403080556,
        "Challenge_title":"incremental training on custom code in amazon sagemaker",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":621.0,
        "Challenge_word_count":224,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>Incremental training is a native feature for the built-in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/now-easily-perform-incremental-learning-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Image Classifier and Object Detector<\/a>. For custom code, it is the developer responsibility to write the incremental training logic and to verify its validity. Here is a possible path:<\/p>\n\n<ol>\n<li>use one of the data channels passed in the <code>fit<\/code> to load a model state (artifact to fine-tune)<\/li>\n<li>in your code, check if the model state channel is filled\nwith artifacts. If it is, instantiate a model from that state\nand continue training. This is framework specific and you may to take\nnecessary precautions to avoid forgetting previous learnings.<\/li>\n<\/ol>\n\n<p>Some frameworks provide better support for incremental learning that others. For example some sklearn models provide an <a href=\"https:\/\/scikit-learn.org\/0.15\/modules\/scaling_strategies.html#incremental-learning\" rel=\"nofollow noreferrer\">incremental_fit<\/a> method. For DL frameworks it is technically very easy to continue training from a checkpoint, but if new data is very different from previously-seen data this may lead your model to forget previous learnings.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":16.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"exception on incremental training"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4389.5575297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Challenge_closed_time":1638396070903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638395443060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about SageMaker pipelines and has several questions about them. They are unsure if it is a stand-alone service, how it integrates with codepipeline, and how the Python SDK differs from CDK and CloudFormation. They are also having trouble finding examples besides the Python SDK usage and are seeking clarification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1744008334,
        "Challenge_title":"What are SageMaker pipelines actually?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":716.0,
        "Challenge_word_count":131,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578250359256,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam",
        "Poster_reputation_count":197.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1654197850167,
        "Solution_link_count":8.0,
        "Solution_readability":18.8,
        "Solution_reading_time":68.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":402.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"clarify SageMaker pipelines"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4752777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Enchanter v0.7.0 raise `COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)` when using Context API\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: v0.7.0\r\n- Python version: ?\r\n- OS: Linux\r\n- (Optional) Other libraries and their versions: Google Colab with GPU\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\nrunner = ClassificationRunner(\r\n    net, optimizer, criterion, Experiment()\r\n)\r\n\r\nwith runner:\r\n    runner.scaler = torch.cuda.amp.GradScaler()\r\n\r\n    runner.add_loader(\"train\", trainloader)\r\n    runner.add_loader(\"test\", testloader)\r\n    runner.train_config(epochs=20)\r\n\r\n    runner.run()\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600153381000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1600151670000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug in DVC evaluation which crashes due to \"int64 not JSON serializable\" error. The bug was introduced by #348 and the error traceback is provided in the post. The user has also provided steps to reproduce the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/129",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":211.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.4752777778,
        "Challenge_title":"COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":109,
        "Discussion_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_type":"anomaly",
        "Challenge_summary":"int64 serialization error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11799.0105555556,
        "Challenge_answer_count":0,
        "Challenge_body":"For uploading data to AWS Neptune we use `NeptuneCSVPublisher`, which internally uses `NeptuneBulkLoaderApi`. The current configuration uses config key `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME`, which provides name of IAM role for the loader to be able to use S3 and Neptune. The issue is that `NeptuneBulkLoaderApi` constructs IAM role ARN from name as follows: \r\n\r\n```python\r\naccount_id = self.session.client('sts').get_caller_identity()['Account']\r\nself.iam_role_arn = f'arn:aws:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nwhereas, [second element of ARN aka partition](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws-arns-and-namespaces.html) can be currently:\r\n* `aws` -AWS Regions\r\n* `aws-cn` - China Regions\r\n* `aws-us-gov` - AWS GovCloud (US) Regions\r\n\r\nSince we use Amundsen also in AWS China, the above ARN is not valid. \r\n\r\n## Expected Behavior\r\n\r\nIAM role ARN either takes into account AWS partition or there is a possibility of passing IAM role ARN instead of name directly.\r\n\r\n## Current Behavior\r\n\r\nIAM role ARN is constructed incorrectly outside of AWS Global.\r\n\r\n## Possible Solutions\r\n\r\nIAM role ARN should take partition into account. There are two solutions:\r\n1. Add partition into current code\r\n2. Add option of passing IAM role ARN directly which supersedes IAM role name \r\n\r\n### Solution 1\r\n\r\nSince I didn't know or found any good way to get the AWS partition, we can use caller identity and ARN there to get the partition, e.g.:\r\n\r\n```python\r\nidentity = self.session.client('sts').get_caller_identity()\r\naccount_id = identity['Account']\r\npartition = identity['Arn'].split(':')[1]\r\nself.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nThis is smaller fix but it is a bit hacky and I'm not sure it'll work in all situation, but it should I guess.\r\n\r\n### Solution 2\r\n\r\nAdd config key `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN` which either supersedes `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` in a way that in constructor we would have something like:\r\n\r\n```python\r\nif iam_role_arn:\r\n    self.iam_role_arn = iam_role_arn\r\nelse:\r\n   ...\r\n   self.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nOr even replace `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` with `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN`, which is IMO cleaner, but would be not backward compatible. \r\n\r\n## Steps to Reproduce\r\nDeploy Amundsen in AWS China with Neptune and try to use Databuilder to upload CSV data from S3. \r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\nCurrently we are unable to load data into Neptune as the IAM role ARN setting is hidden and we get an error:\r\n\r\n```\r\n[ERROR] Exception: Failed to load csv. Response: {'detailedMessage': \"Failed to start new load from the source s3:\/\/amundsenBucket\/amundsen\/2021_08_10_01_01_28. Couldn't find the aws credential for iam_role_arn: arn:aws:iam::111111111:role\/RoleForNeptune111111-2222\", 'code': 'InvalidParameterException', 'requestId': 'xxx'}\r\nTraceback (most recent call last):\r\n\u00a0\u00a0File \"\/var\/task\/ctw\/jobs\/synchronize_redshift_metadata.py\", line 49, in lambda_handler\r\n\u00a0\u00a0\u00a0\u00a0redshift_to_neptune_job.launch()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 76, in launch\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 72, in launch\r\n\u00a0\u00a0\u00a0\u00a0self.publisher.publish()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n\u00a0\u00a0\u00a0\u00a0self.publish_impl()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/neptune_csv_publisher.py\", line 109, in publish_impl\r\n\u00a0\u00a0\u00a0\u00a0raise Exception(\"Failed to load csv. Response: {0}\".format(str(bulk_upload_response)))\r\n```\r\n\r\n## Your Environment\r\n* Amunsen version used: `amundsen-databuilder==4.3.1`\r\n* Data warehouse stores: AWS Neptune\r\n* Deployment (k8s or native): AWS Step Functions (k8s for backend but unrelated for now)\r\n* Link to your fork or repository:",
        "Challenge_closed_time":1671067447000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1628591009000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The Neptune_ML widget is encountering an error in version 2.0.9 where the json values being passed in are resulting in a JSONDecodeError. This issue occurs when running through the 01-Introduction-to-Node-Classification-Gremlin notebook during the export step. The problem is not present in version 2.0.7.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1430",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":49.7,
        "Challenge_repo_contributor_count":209.0,
        "Challenge_repo_fork_count":928.0,
        "Challenge_repo_issue_count":2115.0,
        "Challenge_repo_star_count":3927.0,
        "Challenge_repo_watch_count":244.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":11799.0105555556,
        "Challenge_title":"Databuilder `NeptuneBulkLoaderApi` constructs wrong IAM role ARN for AWS other than global",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":441,
        "Discussion_body":"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune",
        "Challenge_type":"anomaly",
        "Challenge_summary":"JSONDecodeError in widget"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":24.9859916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How is the &quot;GpuUtilization&quot; metric computed for an Azure Machine Learning (AML) workspace? What are the inputs and what is the equation used to compute GpuUtilization?<\/p>\n<p>The &quot;metrics&quot; tab in the AML web portal shows a chart of the GpuUtilization over a specified time period, along with the average GpuUtilization for that time period. However, I have found that average GpuUtilization does not appear to accurately reflect the data shown in the chart for some of my organization's AML workspaces.<\/p>\n<p>For example, the following screenshot shows the GpuUtilization for July 1-31, with the average GpuUtilization reported as 54.06. This is clearly much higher than what is shown in the chart. When I download the data from the chart (Share -&gt; Download to Excel), I compute the average GpuUtilization to be ~11% in Excel. Why is there such a discrepancy?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/igqST.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/igqST.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have found similar discrepancies for other AML workspaces as well. However, the average GpuUtilization appears to be more accurate for the August 1-25 time period than it is for July 1-31. I wish to better understand how AML computes the average GpuUtilization over a time period so we can accurately account for my organization's AML GPU usage on a per-workspace basis.<\/p>",
        "Challenge_closed_time":1598627114363,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598537164793,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on how the \"GpuUtilization\" metric is computed for an Azure Machine Learning (AML) workspace, as the average GpuUtilization reported in the metrics tab does not accurately reflect the data shown in the chart for some workspaces. The user has found discrepancies in the reported average GpuUtilization and wishes to understand how AML computes the average GpuUtilization over a time period.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63617788",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":18.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":24.9859916667,
        "Challenge_title":"How is Azure Machine Learning's average GpuUtilization metric computed?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":126.0,
        "Challenge_word_count":219,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369863777596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cambridge, MA",
        "Poster_reputation_count":335.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>The 54.06 is likely the average over time when GPU VM was allocated. If the VM gets deallocated, the Azure Monitor gets no data. These missing values get interpolated as zeros on the chart.<\/p>\n<p>To get a better estimate of utilization, you could check when the VM was stopped, and exclude that time interval from the average.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":4.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"compute GpuUtilization metric"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":92.6310866667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to perform <strong>distributed training<\/strong> on <strong>Amazon SageMaker<\/strong>. The code is written with <strong>TensorFlow<\/strong> and similar to the following code where I think CPU instance should be enough:\u00a0\n<a href=\"https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py<\/a><\/p>\n<p>Can <strong>Horovod with TensorFlow<\/strong> work on <strong>non-GPU<\/strong> instances in Amazon SageMaker?<\/p>",
        "Challenge_closed_time":1663198075692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662864603780,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to perform distributed training on Amazon SageMaker using TensorFlow and is wondering if Horovod with TensorFlow can work on non-GPU instances. The user believes that a CPU instance should be sufficient for the task.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73676483",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":8.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":92.6310866667,
        "Challenge_title":"Can Horovod with TensorFlow work on non-GPU instances in Amazon SageMaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":17.0,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>Yeah you should be able to use both CPU's and GPU's with Horovod on Amazon SageMaker. Please follow the below example for the same<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.3,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"Horovod on non-GPU instances"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":326.3584541667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried to follow the normal (non-studio) documentation on mounting an EFS file system, as can be found <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/mount-an-efs-file-system-to-an-amazon-sagemaker-notebook-with-lifecycle-configurations\/\" rel=\"nofollow noreferrer\">here<\/a>, however, these steps don't work in a studio notebook. Specifically, the <code>sudo mount -t nfs ...<\/code> does not work in both the Image terminal and the system terminal.<\/p>\n<p>How do I mount an EFS file system that already exists to amazon Sagemaker, so I can access the data\/ datasets I stored in them?<\/p>",
        "Challenge_closed_time":1596816839976,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596816839977,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in mounting an EFS volume on AWS Sagemaker Studio. They have tried following the normal documentation but the steps don't work in a studio notebook. The user is seeking guidance on how to mount an EFS file system that already exists to Amazon Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63305569",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to mount an EFS volume on AWS Sagemaker Studio",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2331.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>Update: I spoke to an AWS Solutions Architect, and he confirms that EFS is not supported on Sagemaker Studio.<\/p>\n<hr \/>\n<p><strong>Workaround:<\/strong><\/p>\n<p>Instead of mounting your old EFS, you can mount the SageMaker studio EFS onto an EC2 instance, and copy over the data manually. You would need the correct EFS storage volume id, and you'll find your newly copied data available in Sagemaker Studio. <em>I have not actually done this though.<\/em><\/p>\n<p>To find the EFS id, look at the section &quot;Manage your storage volume&quot; <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks.html#manage-your-storage-volume\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1597991730412,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":8.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"mount EFS on Sagemaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":153.0623633333,
        "Challenge_answer_count":1,
        "Challenge_body":"1. Can we make a new code through the sagemaker studio?\n2. In my computer, GPU is GTX2080ti model, so if I use AWS sagemaker for paid service, can I get better performance?\n3. How much GPU performance can you improve compared to before?\n4. I want to proceed with object segmentation through AWS sagemaker, can I use the code I used through sagemaker studio?",
        "Challenge_closed_time":1661427331608,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660876307100,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is seeking information about using AWS SageMaker for coding and improving GPU performance. They also want to know if they can use their existing code for object segmentation in SageMaker Studio.",
        "Challenge_last_edit_time":1667925848878,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCKKplP7ES22DuZf8QJ38JA\/ask-aws-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":4.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":153.0623633333,
        "Challenge_title":"Ask AWS SageMaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":50.0,
        "Challenge_word_count":67,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"My apologies, I am not fully sure on all the questions. But let me still make an attempt to respond to see if it helps.\n\n1. Yes, you can write your own custom code through SageMaker studio.\n\n2. This may not be an apple to apple comparison. The main advantage in this context, is your able to scale out your training to multiple nodes and cores (if your underlying model supports that). Likewise you can scale out the deployment as well. Typically the studio notebook is backed by a lightweight EC2 instance, but there are a large range of EC2 instances for training on SageMaker. Please refer to the following links for further assistance. 1. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-instance-types.html 2. https:\/\/aws.amazon.com\/ec2\/instance-types\/\n\n3. Please refer to the response above for question # 2.\n4. Did you mean semantic segmentation? If yes, the answer is yes too.\n\nHope that helps!\n\nRegards,\nPunya",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1661427331608,
        "Solution_link_count":2.0,
        "Solution_readability":6.9,
        "Solution_reading_time":11.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":146.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"AWS SageMaker for GPU and object segmentation"
    },
    {
        "Answerer_created_time":1393021961043,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":838.0,
        "Answerer_view_count":193.0,
        "Challenge_adjusted_solved_time":4512.6045455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm following this example notebook to learn SageMaker's processing jobs API: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb<\/a><\/p>\n<p>I'm trying to modify their code to avoid using the default S3 bucket, namely: <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code><\/p>\n<p>For their data processing step with the <code>.run<\/code> method:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[ProcessingInput(source=input_data, destination=&quot;\/opt\/ml\/processing\/input&quot;)],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>I was able to modify it to use my own S3 bucket by using the <code>destination<\/code> parameter like this:<\/p>\n<pre><code>sklearn_processor.run( \n    code=output_bucket_uri + &quot;preprocessing.py&quot;, \n    inputs=[ProcessingInput( \n        source=input_bucket_uri + &quot;census-income.csv&quot;, \n        destination=path+&quot;input\/&quot;, \n    )], \n    outputs=[ \n        ProcessingOutput( \n            output_name=&quot;train_data&quot;, \n            source=path+&quot;train\/&quot;, \n            destination=output_bucket_uri + &quot;train\/&quot;, \n        ), \n        ProcessingOutput( \n            output_name=&quot;test_data&quot;, \n            source=path+&quot;test\/&quot;, \n            destination=output_bucket_uri + &quot;test\/&quot;, \n        ), \n    ], \n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;], \n)\n<\/code><\/pre>\n<p>But for the <code>.fit<\/code> method:<\/p>\n<pre><code>sklearn.fit({&quot;train&quot;: preprocessed_training_data})\n<\/code><\/pre>\n<p>I have not been able to find a parameter to pass it so that the output artifacts are saved to a S3 bucket that I specify instead of the default s3 bucket <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code>.<\/p>",
        "Challenge_closed_time":1648557908696,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632276763933,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to modify the code in an example notebook to use their own S3 bucket instead of the default S3 bucket for SageMaker's processing jobs API. They were able to modify the code for the data processing step, but are unable to find a parameter to specify the S3 bucket for the output artifacts in the fit method.",
        "Challenge_last_edit_time":1632312532332,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69277390",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":26.2,
        "Challenge_reading_time":32.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":4522.5402119444,
        "Challenge_title":"Can I specify S3 bucket for sagemaker.sklearn.estimator's SKLearn?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":144,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1329161697310,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4154.0,
        "Poster_view_count":314.0,
        "Solution_body":"<p>For SKLearnProcessor, the ideal way to specify default bucket is by creating a sagemaker session with that bucket, and sending that as sagemaker_session parameter. Example:<\/p>\n<pre><code>from sagemaker.session import Session    \nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role='&lt;arn-role&gt;',\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1,\n                                     sagemaker_session=Session(default_bucket='&lt;s3-bucket-name&gt;'))\n<\/code><\/pre>\n<p>I know this is not your exact question but you have added an alternative to this in your question details. So I am adding it here as a cleaner approach.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":7.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"cannot specify output bucket"
    },
    {
        "Answerer_created_time":1388584380832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":396.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":91.8788925,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I could not find a way yet of setting the runs name after the first start_run for that run (we can pass a name there). <\/p>\n\n<p>I Know we can use tags but that is not the same thing. I would like to add a run relevant name, but very often we know the name only after run evaluation or while we're running the run interactively in notebook for example.<\/p>",
        "Challenge_closed_time":1564380155503,
        "Challenge_comment_count":0,
        "Challenge_created_time":1564049391490,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to set or change the name of an mlflow run after the initial creation, as they often only know the relevant name during or after the run evaluation. They have not found a way to do this yet, and while they know they can use tags, it is not the same thing.",
        "Challenge_last_edit_time":1564533527096,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57199472",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":11,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":91.8788925,
        "Challenge_title":"Is it possible to set\/change mlflow run name after run initial creation?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":7689.0,
        "Challenge_word_count":81,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1393062808772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Portugal",
        "Poster_reputation_count":133.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>It is possible to edit run names from the MLflow UI. First, click into the run whose name you'd like to edit.<\/p>\n\n<p>Then, edit the run name by clicking the dropdown next the run name (i.e. the downward-pointing caret in this image):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sl6Qs.png\" alt=\"Rename run dropdown\"><\/a><\/p>\n\n<p>There's currently no stable public API for setting run names - however, you can programmatically set\/edit run names by setting the tag with key <code>mlflow.runName<\/code>, which is what the UI (currently) does under the hood.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.9,
        "Solution_reading_time":7.92,
        "Solution_score_count":9.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":83.0,
        "Tool":"MLflow",
        "Challenge_type":"inquiry",
        "Challenge_summary":"change mlflow run name"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":0.5572202778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to train a model using Amazon Sagemaker (xgboost: eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'). But I always get the same error message shortly after starting the training job:<\/p>\n\n<blockquote>\n  <p>\"ClientError: Hidden file found in the data path! Remove that before\n  training.\"<\/p>\n<\/blockquote>\n\n<p>The S3 console shows that output path is empty (I also tried to create a new directory to no avail). Versioning is not enabled for the bucket.<\/p>\n\n<p>Surprisingly, google finds nothing under this error message.<\/p>\n\n<p>I have configured the input and outputs as follows:<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/train\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/validation\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        }\n    ],\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": \"s3:\/\/{}\/{}-xgboost-output\".format(s3_utils.bucket, LABEL)        },\n<\/code><\/pre>\n\n<p>The field<\/p>\n\n<pre><code>    \"RoleArn\": role,\n<\/code><\/pre>\n\n<p>where role comes from<\/p>\n\n<pre><code>    from sagemaker import get_execution_role\n    role = get_execution_role()\n<\/code><\/pre>\n\n<p>and is<\/p>\n\n<pre><code>    arn:aws:iam::&lt;ACCOUNT&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;HIDDEN&gt;\n<\/code><\/pre>\n\n<p>Here is a screenshot showing the data-path:\n<a href=\"https:\/\/i.stack.imgur.com\/bs8kl.png\" rel=\"nofollow noreferrer\">S3 dashboard view of data-path<\/a>. The two csv files is all there is. In particular, there is no empty \"directory\" which might be what \"hidden file\" could mean.<\/p>",
        "Challenge_closed_time":1531417705336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531339613883,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error message while trying to train a model using Amazon Sagemaker. The error message states that a hidden file has been found in the data path and needs to be removed before training. The user has checked the S3 console and found that the output path is empty, and versioning is not enabled for the bucket. The user has configured the input and output data paths correctly, and the role is also set up correctly. However, the error persists, and the user is unable to find any information about this error message online.",
        "Challenge_last_edit_time":1531415699343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51293471",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.0,
        "Challenge_reading_time":24.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":21.6920702778,
        "Challenge_title":"AWS Sagemaker - \"Hidden file found in the data path! Remove that before training.\"",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1578.0,
        "Challenge_word_count":191,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531338135003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Ok, the prefix you set in the <code>S3Uri<\/code> matters here. Based on your screenshot I think your bucket looks something like this (in tree form):<\/p>\n\n<pre><code>s3:\/\/bucket\n\u2514\u2500\u2500 LABEL-inputdata\n    \u251c\u2500\u2500 train.csv\n    \u2514\u2500\u2500 validation.csv\n<\/code><\/pre>\n\n<p>Based on your <code>InputDataConfig<\/code> above, SageMaker has to download it to folders on the filesystem for the <code>xgboost<\/code> training algorithm to run. It does so based on the channel names and on the <code>S3Uri<\/code> prefix you provided. The prefix is chopped off to determine the name of the folder\/file to download to. So, in your example, the <code>train<\/code> channel gets downloaded as:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/train\/.csv\n<\/code><\/pre>\n\n<p>Finally, the <code>xgboost<\/code> implementation sees the <code>.csv<\/code> file as a hidden file and complains about it.<\/p>\n\n<p>To get it to work you could rearrange your data in s3 like so...<\/p>\n\n<pre><code>s3:bucket\n\u2514\u2500\u2500 LABEL-inputdata\n    \u251c\u2500\u2500 train\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 data.csv\n    \u2514\u2500\u2500 validation\n        \u2514\u2500\u2500 data.csv\n<\/code><\/pre>\n\n<p>.. and change your input data config to:<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/train\/\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/validation\/\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":20.04,
        "Solution_score_count":5.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"hidden file in data path"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.1972222222,
        "Challenge_answer_count":1,
        "Challenge_body":"How does Amazon SageMaker batch transform handle failures? Is there a way to automate failure handling and retries built into the service?",
        "Challenge_closed_time":1593617691000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593595381000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how Amazon SageMaker batch transform handles failures and if there is a way to automate failure handling and retries within the service.",
        "Challenge_last_edit_time":1668082995576,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUE10OtSwDRCiB-0pP6wflYQ\/is-there-a-way-to-automate-failure-handling-and-retries-when-using-amazon-sagemaker-batch-transform",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.1972222222,
        "Challenge_title":"Is there a way to automate failure handling and retries when using Amazon SageMaker batch transform?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":37,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":" You can use the [ModelClientConfig](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ModelClientConfig.html#sagemaker-Type-ModelClientConfig-InvocationsMaxRetries) API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925593260,
        "Solution_link_count":1.0,
        "Solution_readability":23.6,
        "Solution_reading_time":4.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"automate batch transform retries"
    },
    {
        "Answerer_created_time":1542402402780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":186.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":56.0515805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy an ALS model trained using PySpark on Azure ML service. I am providing a score.py file that loads the trained model using ALSModel.load() function. Following is the code of my score.py file.<\/p>\n<pre><code>import os\nfrom azureml.core.model import Model\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, StringType\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\nsqlContext = SQLContext(sc)\nspark = sqlContext.sparkSession\n\ninput_schema = StructType([StructField(&quot;UserId&quot;, StringType())])\nreader = spark.read\nreader.schema(input_schema)\n\n\ndef init():\n    global model\n    # note here &quot;iris.model&quot; is the name of the model registered under the workspace\n    # this call should return the path to the model.pkl file on the local disk.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), &quot;recommendation-model&quot;)\n    # Load the model file back into a LogisticRegression model\n    model = ALSModel.load(model_path)\n    \n\ndef run(data):\n    try:\n        input_df = reader.json(sc.parallelize([data]))\n        input_df = indexer.transform(input_df)\n        \n        res = model.recommendForUserSubset(input_df[['UserId_index']], 10)\n\n        # you can return any datatype as long as it is JSON-serializable\n        return result.collect()[0]['recommendations']\n    except Exception as e:\n        traceback.print_exc()\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>Following is the error I get when I deploy it as LocalWebService using Model.deploy function in Azure ML service<\/p>\n<pre><code>Generating Docker build context.\nPackage creation Succeeded\nLogging into Docker registry viennaglobal.azurecr.io\nLogging into Docker registry viennaglobal.azurecr.io\nBuilding Docker image from Dockerfile...\nStep 1\/5 : FROM viennaglobal.azurecr.io\/azureml\/azureml_43542b56c5ec3e8d0f68e1556558411f\n ---&gt; 5b3bb174ca5f\nStep 2\/5 : COPY azureml-app \/var\/azureml-app\n ---&gt; 8e540c0746f7\nStep 3\/5 : RUN mkdir -p '\/var\/azureml-app' &amp;&amp; echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjNkN2M1ZjM4LTI1ODEtNGUxNi05NTdhLWEzOTU1OGI1ZjBiMyIsInJlc291cmNlR3JvdXBOYW1lIjoiZGV2LW9tbmljeC10ZnMtYWkiLCJhY2NvdW50TmFtZSI6ImRldi10ZnMtYWktd29ya3NwYWNlIiwid29ya3NwYWNlSWQiOiI1NjkzNGMzNC1iZmYzLTQ3OWUtODRkMy01OGI4YTc3ZTI4ZjEifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode &gt; \/var\/azureml-app\/model_config_map.json\n ---&gt; Running in 502ad8edf91e\n ---&gt; a1bc5e0283d0\nStep 4\/5 : RUN mv '\/var\/azureml-app\/tmpvxhomyin.py' \/var\/azureml-app\/main.py\n ---&gt; Running in eb4ec1a0b702\n ---&gt; 6a3296fe6420\nStep 5\/5 : CMD [&quot;runsvdir&quot;,&quot;\/var\/runit&quot;]\n ---&gt; Running in 834fd746afef\n ---&gt; 5b9f8be538c0\nSuccessfully built 5b9f8be538c0\nSuccessfully tagged recommend-service:latest\nContainer (name:musing_borg, id:0f3163692f5119685eee5ed59c8e00aa96cd472f765e7db67653f1a6ce852e83) cannot be killed.\nContainer has been successfully cleaned up.\nImage sha256:0f146f4752878bbbc0e876f4477cc2877ff12a366fca18c986f9a9c2949d028b successfully removed.\nStarting Docker container...\nDocker container running.\nChecking container health...\nERROR - Error: Container has crashed. Did your init method fail?\n\n\nContainer Logs:\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:00,312735664+00:00 - rsyslog\/run \n2020-07-30T11:57:00,312768364+00:00 - gunicorn\/run \n2020-07-30T11:57:00,313017966+00:00 - iot-server\/run \nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\n2020-07-30T11:57:00,313969073+00:00 - nginx\/run \n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:00,597835804+00:00 - iot-server\/finish 1 0\n2020-07-30T11:57:00,598826211+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 19.9.0\nListening at: http:\/\/127.0.0.1:31311 (10)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 41\nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\nIvy Default Cache set to: \/root\/.ivy2\/cache\nThe jars for the packages stored in: \/root\/.ivy2\/jars\n:: loading settings :: url = jar:file:\/home\/mmlspark\/lib\/spark\/jars\/ivy-2.4.0.jar!\/org\/apache\/ivy\/core\/settings\/ivysettings.xml\ncom.microsoft.ml.spark#mmlspark_2.11 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-e07358bb-d354-4f41-aa4c-f0aa73bb0156;1.0\n    confs: [default]\n    found com.microsoft.ml.spark#mmlspark_2.11;0.15 in spark-list\n    found io.spray#spray-json_2.11;1.3.2 in central\n    found com.microsoft.cntk#cntk;2.4 in central\n    found org.openpnp#opencv;3.2.0-1 in central\n    found com.jcraft#jsch;0.1.54 in central\n    found org.apache.httpcomponents#httpclient;4.5.6 in central\n    found org.apache.httpcomponents#httpcore;4.4.10 in central\n    found commons-logging#commons-logging;1.2 in central\n    found commons-codec#commons-codec;1.10 in central\n    found com.microsoft.ml.lightgbm#lightgbmlib;2.1.250 in central\n:: resolution report :: resolve 318ms :: artifacts dl 11ms\n    :: modules in use:\n    com.jcraft#jsch;0.1.54 from central in [default]\n    com.microsoft.cntk#cntk;2.4 from central in [default]\n    com.microsoft.ml.lightgbm#lightgbmlib;2.1.250 from central in [default]\n    com.microsoft.ml.spark#mmlspark_2.11;0.15 from spark-list in [default]\n    commons-codec#commons-codec;1.10 from central in [default]\n    commons-logging#commons-logging;1.2 from central in [default]\n    io.spray#spray-json_2.11;1.3.2 from central in [default]\n    org.apache.httpcomponents#httpclient;4.5.6 from central in [default]\n    org.apache.httpcomponents#httpcore;4.4.10 from central in [default]\n    org.openpnp#opencv;3.2.0-1 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n    ---------------------------------------------------------------------\n\n:: problems summary ::\n:::: ERRORS\n    unknown resolver repo-1\n\n    unknown resolver repo-1\n\n\n:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n:: retrieving :: org.apache.spark#spark-submit-parent-e07358bb-d354-4f41-aa4c-f0aa73bb0156\n    confs: [default]\n    0 artifacts copied, 10 already retrieved (0kB\/7ms)\n2020-07-30 11:57:02 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nInitialized PySpark session.\nInitializing logger\n2020-07-30 11:57:09,464 | root | INFO | Starting up app insights client\nStarting up app insights client\n2020-07-30 11:57:09,464 | root | INFO | Starting up request id generator\nStarting up request id generator\n2020-07-30 11:57:09,464 | root | INFO | Starting up app insight hooks\nStarting up app insight hooks\n2020-07-30 11:57:09,464 | root | INFO | Invoking user's init function\nInvoking user's init function\n2020-07-30 11:57:19,652 | root | ERROR | User's init function failed\nUser's init function failed\n2020-07-30 11:57:19,656 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 163, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/main.py&quot;, line 44, in init\n    model = ALSModel.load(model_path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 362, in load\n    return cls.read().load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 300, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/java_gateway.py&quot;, line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/sql\/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/protocol.py&quot;, line 328, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o64.load.\n: java.util.NoSuchElementException: Param blockSize does not exist.\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n    at org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:591)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:589)\n    at scala.collection.immutable.List.foreach(List.scala:392)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.setParams(ReadWrite.scala:589)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.getAndSetParams(ReadWrite.scala:572)\n    at org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:533)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n\n\nEncountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 163, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/main.py&quot;, line 44, in init\n    model = ALSModel.load(model_path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 362, in load\n    return cls.read().load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 300, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/java_gateway.py&quot;, line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/sql\/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/protocol.py&quot;, line 328, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o64.load.\n: java.util.NoSuchElementException: Param blockSize does not exist.\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n    at org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:591)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:589)\n    at scala.collection.immutable.List.foreach(List.scala:392)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.setParams(ReadWrite.scala:589)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.getAndSetParams(ReadWrite.scala:572)\n    at org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:533)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n\n\nWorker exiting (pid: 41)\nShutting down: Master\nReason: Worker failed to boot.\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:19,833136837+00:00 - gunicorn\/finish 3 0\n2020-07-30T11:57:19,834216245+00:00 - Exit code 3 is not normal. Killing image.\n\n---------------------------------------------------------------------------\nWebserviceException                       Traceback (most recent call last)\n&lt;ipython-input-43-d0992ae9d1c9&gt; in &lt;module&gt;\n      6 local_service = Model.deploy(workspace, &quot;recommend-service&quot;, [register_model], inference_config, deployment_config)\n      7 \n----&gt; 8 local_service.wait_for_deployment()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/local.py in decorated(self, *args, **kwargs)\n     69                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n     70                                           logger=module_logger)\n---&gt; 71             return func(self, *args, **kwargs)\n     72         return decorated\n     73     return decorator\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/local.py in wait_for_deployment(self, show_output)\n    601                                    self._container,\n    602                                    health_url=self._internal_base_url,\n--&gt; 603                                    cleanup_if_failed=False)\n    604 \n    605             self.state = LocalWebservice.STATE_RUNNING\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_model_management\/_util.py in container_health_check(docker_port, container, health_url, cleanup_if_failed)\n    745             # The container has started and crashed.\n    746             _raise_for_container_failure(container, cleanup_if_failed,\n--&gt; 747                                          'Error: Container has crashed. Did your init method fail?')\n    748 \n    749         # The container hasn't crashed, so try to ping the health endpoint.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_model_management\/_util.py in _raise_for_container_failure(container, cleanup, message)\n   1258         cleanup_container(container)\n   1259 \n-&gt; 1260     raise WebserviceException(message, logger=module_logger)\n   1261 \n   1262 \n\nWebserviceException: WebserviceException:\n    Message: Error: Container has crashed. Did your init method fail?\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Error: Container has crashed. Did your init method fail?&quot;\n    }\n}\n<\/code><\/pre>\n<p>However, the ALSModel.load() works fine when executed in a Jupyter notebook.<\/p>",
        "Challenge_closed_time":1596478571823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596276786133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering an error when attempting to deploy an als model trained using pyspark on a service, with the error being a java.util.nosuchelementexception.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63204081",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.7,
        "Challenge_reading_time":224.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":179,
        "Challenge_solved_time":56.0515805556,
        "Challenge_title":"PySpark ALSModel load fails in deployment over Azure ML service with error java.util.NoSuchElementException: Param blockSize does not exist",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":598.0,
        "Challenge_word_count":1195,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596274712792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chandigarh, India",
        "Poster_reputation_count":13.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>A couple of things to check:<\/p>\n<ol>\n<li>Is your model registered in the workspace? AZUREML_MODEL_DIR only works for registered models. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">this link<\/a> for information about registering a model<\/li>\n<li>Are you specifying the same version of pyspark.ml.recommendation in your InferenceConfig as you use locally? This kind of error might be due to a difference in versions<\/li>\n<li>Have you looked at the output of <code>print(service.get_logs())<\/code>? Check out our <a href=\"https:\/\/review.docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?branch=pr-en-us-124666\" rel=\"nofollow noreferrer\">troubleshoot and debugging documentation here<\/a> for other things you can try<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.2,
        "Solution_reading_time":15.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"NoSuchElementException during deployment"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.9563108333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>\u63b2\u984c\u306e\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u73fe\u5728Machine Learning\u3092\u4f7f\u7528\u3057\u3066\u6a5f\u68b0\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002  <br \/>\n\u305d\u3053\u3067\u8cea\u554f\u306b\u306a\u308b\u306e\u3067\u3059\u304c\u3001\u30c7\u30b6\u30a4\u30ca\u30fc\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u5b66\u7fd2\u7d50\u679c\u3092CSV\u3067\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001  <br \/>\nExport Data\u30e2\u30c7\u30eb\u3067CSV\u5f62\u5f0f\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u3066\u3082CSV\u3067\u306f\u306a\u3044\u5f62\u5f0f\u3067\u5171\u6709\u305b\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u304c\u3001\u539f\u56e0\u304c\u308f\u304b\u3089\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002  <br \/>\n\u3054\u6559\u793a\u306e\u307b\u3069\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002<\/p>",
        "Challenge_closed_time":1631268910236,
        "Challenge_comment_count":1,
        "Challenge_created_time":1631251067517,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is currently using Machine Learning and attempting to export the learning results in CSV format using the designer function. However, even though the Export Data model is set to CSV format, the shared file is not in CSV format and the user is unsure of the cause of the issue. The user is seeking guidance on how to resolve this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/546760\/machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":3.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":4.9563108333,
        "Challenge_title":"Machine Learning\u306b\u3064\u3044\u3066\u306e\u8cea\u554f",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":10,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8f940edc-4c98-48e4-8a54-287e99830334\">@\u6817\u7530\u771f\u5b5d  <\/a> Are you referring to the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/export-data\">export data module<\/a> of the designer from ml.azure.com?    <br \/>\nI think I understand the issue, Are you seeing that the .csv format of file is not listed on the blob storage?    <\/p>\n<p>Since the input is a dataframe directory to export module the output format selected should still be the format you selected, in this case CSV. The file name extension only might be missing. You can still open the csv file in excel and it will recognize the delimiters and headers so you can convert it into excel files.     <\/p>\n<p>You can also avoid this by providing the .csv extension in the path itself in export settings and file will be exported as a csv file directly.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/131128-image.png?platform=QnA\" alt=\"131128-image.png\" \/>    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":12.7,
        "Solution_reading_time":26.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":226.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"exported file not in CSV"
    },
    {
        "Answerer_created_time":1635045129020,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":53.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":250.6002630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Challenge_closed_time":1636075476147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635046349497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a simple recommender system using TfidfVectorizer and linear_kernel, but is facing challenges in deploying it through Sagemaker with an endpoint. They are unsure if the output from tf.fit_transform(ds['content']) is considered the model or the output from linear_kernel(tfidf_matrix, tfidf_matrix).",
        "Challenge_last_edit_time":1635173315200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":21.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":285.8685138889,
        "Challenge_title":"How to Deploy ML Recommender System on AWS",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":164,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635045129020,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":4.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unclear model output"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0858333333,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi Team,  \nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.  \nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e..  deployment package size is 50 MB.  \nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.  \nsample code for this api :   \n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.  \n  \nimport sagemaker  \nfrom sagemaker.amazon.amazon_estimator import get_image_uri  \ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')  \n  \nAny reference would be of great help. Thank you.",
        "Challenge_closed_time":1568642184000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568641875000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in adding sagemaker dependencies as external dependencies to upload to lambda due to the max size limit error. The package size is more than the allowed size limit of 50 MB. The user is trying to access the 'get_image_uri' API, which is not accessible with boto3. The user has provided sample code for the API and is seeking any reference to resolve the issue.",
        "Challenge_last_edit_time":1668590352780,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0858333333,
        "Challenge_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":338.0,
        "Challenge_word_count":104,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Could you explain in more detail why do you want to have sagemaker inside of a lambda please?",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1568642184000,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"max size limit error"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.02458,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I currently work with Kedro (from quantum black <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/01_introduction\/01_introduction.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/01_introduction\/01_introduction.html<\/a>) as a framework for deployment oriented framework to code collaboratively. It is a great framework to develop machine learning in a team.<\/p>\n<p>I am looking for an R equivalent.<\/p>\n<p>My main issue is that I have teams of data scientists that develop in R, but each team is developing in different formats.<\/p>\n<p>I wanted to make them follow a common framework to develop deployment ready R code, easy to work on in 2 or 3-people teams.<\/p>\n<p>Any suggestions are welcome<\/p>",
        "Challenge_closed_time":1639581358648,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639580284993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for an R package that can serve as a collaborative framework for developing deployment-ready R code. They currently use Kedro for machine learning development in teams, but are struggling with different formats used by different teams of data scientists.",
        "Challenge_last_edit_time":1639581270160,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70365836",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":10.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2982375,
        "Challenge_title":"Is there a package in R that mimics KEDRO as a modular collaborative framework for development?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":107,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460063521156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo - SP, Brasil",
        "Poster_reputation_count":2472.0,
        "Poster_view_count":186.0,
        "Solution_body":"<p>member of the Kedro team here. We've heard good things about the <a href=\"https:\/\/github.com\/ropensci\/targets\" rel=\"nofollow noreferrer\">Targets<\/a> library doing similar things in the R world.<\/p>\n<p>It would be remiss for me to not try and covert you and your team to the dark side too :)<\/p>\n<p>Before Kedro our teams internally were writing a mix of Python, SQL, Scala and R. Part of the drive to write the framework was to get our teams internally speaking the same language. Python felt like the best compromise available at the time and I'd argue this still holds. We also had trouble productionising R projects and felt Python is more manageable in that respect.<\/p>\n<p>Whilst not officially documented - I've also seen some people on the Kedro <a href=\"https:\/\/discord.com\/channels\/778216384475693066\/778998585454755870\/901111920290070588\" rel=\"nofollow noreferrer\">Discord play with r2py<\/a> so that they can use specific R functionality within their Python pipelines.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.2,
        "Solution_reading_time":12.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":141.0,
        "Tool":"Kedro",
        "Challenge_type":"inquiry",
        "Challenge_summary":"collaborative R package"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.2657813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey!    <br \/>\nThe VS code integration in the AML ecosystem is great.    <br \/>\nIs it possible to debug using the synapse spark pool as it attached compute (like I am running notebook on it)    <br \/>\nCurrently from the VS code only compute instance is supported .    <br \/>\nAm I missing something?    <\/p>\n<p>Thanks,    <br \/>\nMaya<\/p>",
        "Challenge_closed_time":1669871984343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669791827530,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"Maya is using VS code integration in the AML ecosystem and wants to know if it is possible to debug using the synapse spark pool as an attached compute. Currently, only compute instance is supported in VS code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1109976\/aml-vs-code-integration-with-synapse-spark-pool-as",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":4.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":22.2657813889,
        "Challenge_title":"AML VS Code integration with synapse spark pool as attached computes",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ce96d2b0-635e-4ed7-b685-15e43ebb5007\">@Maya Shauli  <\/a>,    <\/p>\n<p>Thanks for the question and using MS platform.    <\/p>\n<blockquote>\n<p>Unfortunately,  synapse spark pool as it attached compute on Visual Code is no longer supported.    <\/p>\n<\/blockquote>\n<p>Appreciate if you could share the feedback on our <a href=\"https:\/\/feedback.azure.com\/d365community\/idea\/184e6a07-14fa-ec11-a81b-6045bd853c94\">feedback channel<\/a>. Which would be open for the user community to upvote &amp; comment on. This allows our product teams to effectively prioritize your request against our existing feature backlog and gives insight into the potential impact of implementing the suggested feature.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is jhow you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/support\/community-champions-program\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.5,
        "Solution_reading_time":24.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":193.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"debug with Synapse Spark"
    },
    {
        "Answerer_created_time":1589738451347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":99.0917625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to provision an AKS cluster that is connected to a vnet and has an internal load balancer on Azure. I am using code from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet?tabs=python\" rel=\"nofollow noreferrer\">here<\/a> that looks like this:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Verify that cluster does not exist already\ntry:\n    aks_target = AksCompute(workspace=ws, name=aks_cluster_name)\n    print(&quot;Found existing aks cluster&quot;)\n\nexcept:\n    print(&quot;Creating new aks cluster&quot;)\n\n    # Subnet to use for AKS\n    subnet_name = &quot;default&quot;\n    # Create AKS configuration\n    prov_config=AksCompute.provisioning_configuration(load_balancer_type=&quot;InternalLoadBalancer&quot;)\n    # Set info for existing virtual network to create the cluster in\n    prov_config.vnet_resourcegroup_name = &quot;myvnetresourcegroup&quot;\n    prov_config.vnet_name = &quot;myvnetname&quot;\n    prov_config.service_cidr = &quot;10.0.0.0\/16&quot;\n    prov_config.dns_service_ip = &quot;10.0.0.10&quot;\n    prov_config.subnet_name = subnet_name\n    prov_config.docker_bridge_cidr = &quot;172.17.0.1\/16&quot;\n\n    # Create compute target\n    aks_target = ComputeTarget.create(workspace = ws, name = &quot;myaks&quot;, provisioning_configuration = prov_config)\n    # Wait for the operation to complete\n    aks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>However, I get the following error<\/p>\n<pre><code>K8s failed to assign an IP for Load Balancer after waiting for an hour.\n<\/code><\/pre>\n<p>Is this because the AKS cluster does not yet have a 'network contributor' role for the vnet resource group? Is the only way to get this to work to first create AKS outside of AMLS, grant the network contributor role to the vnet resource group, then attach the AKS cluster to AMLS and configure the internal load balancer afterwards?<\/p>",
        "Challenge_closed_time":1604359053332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603997735143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to provision an AKS cluster with an internal load balancer on Azure using code from a Microsoft documentation page. However, they are encountering an error where K8s failed to assign an IP for Load Balancer after waiting for an hour. The user suspects that this is because the AKS cluster does not have a 'network contributor' role for the vnet resource group. They are wondering if the only solution is to create AKS outside of AMLS, grant the network contributor role to the vnet resource group, and then attach the AKS cluster to AMLS and configure the internal load balancer afterwards.",
        "Challenge_last_edit_time":1604002322987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64597526",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":25.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":100.3661636111,
        "Challenge_title":"Provision AKS with internal load balancer from AMLS on Azure",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":352.0,
        "Challenge_word_count":204,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I was able to get this to work by first creating an AKS resource without an internal load balancer, then separately updating the load balancer following this code:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute.aks import AksUpdateConfiguration\nfrom azureml.core.compute import AksCompute\n\n# ws = workspace object. Creation not shown in this snippet\naks_target = AksCompute(ws,&quot;myaks&quot;)\n\n# Change to the name of the subnet that contains AKS\nsubnet_name = &quot;default&quot;\n# Update AKS configuration to use an internal load balancer\nupdate_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)\naks_target.update(update_config)\n# Wait for the operation to complete\naks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>No network contributor role was required.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":10.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed to assign IP"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.4512016667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello Microsoft Q&amp;A Team,    <\/p>\n<p>I get the error     <\/p>\n<blockquote>\n<p>AssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available    <\/p>\n<\/blockquote>\n<p> when executing the following command:    <\/p>\n<blockquote>\n<p>pipeline_job = ml_client.jobs.create_or_update(    <br \/>\n    pipeline_job, experiment_name=&quot;data_preparation&quot;    <br \/>\n)    <br \/>\npipeline_job    <\/p>\n<\/blockquote>\n<p>Yesterday the command worked without an error. I did not make any changes. So I have no idea, what the problem is.    <\/p>\n<p>Thanks for helping me out.    <\/p>\n<p>Cheers    <\/p>\n<p>Lukas    <\/p>",
        "Challenge_closed_time":1667605270036,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667553245710,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an AssetException error with code \"Can't connect to HTTPS URL because the SSL module is not available\" while executing a command using ml_client.jobs. The error occurred suddenly without any changes made by the user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1075753\/aml-assetexception-error-with-code-cant-connect-to",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":9.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":14.4512016667,
        "Challenge_title":"AML - AssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available.",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=733bec54-8d30-4052-8297-64b100f6e3d4\">@Lukas  <\/a> Thanks for your question. Can you please add more details about the document\/sample that you are trying.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":2.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"SSL module not available"
    },
    {
        "Answerer_created_time":1462800865783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":316.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":1192.9984197222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We were using kedro version 0.15.8 and we were loading one specific item from the catalog this way:<\/p>\n<pre><code>from kedro.context import load_context\nget_context().catalog.datasets.__dict__[key]\n<\/code><\/pre>\n<p>Now, we are changing to kedro 0.17.0 and trying to load the catalogs datasets the same way(using the framework context):<\/p>\n<pre><code>from kedro.framework.context import load_context\nget_context().catalog.datasets.__dict__[key]\n<\/code><\/pre>\n<p>And now we get the error:<\/p>\n<blockquote>\n<p>kedro.framework.context.context.KedroContextError: Expected an instance of <code>ConfigLoader<\/code>, got <code>NoneType<\/code> instead.<\/p>\n<\/blockquote>\n<p>It's because the hook register_config_loader from the project it's not being used by the hook_manager that calls the function.<\/p>\n<p>The project hooks are the defined the following way:<\/p>\n<pre><code>class ProjectHooks:\n\n    @hook_impl\n\n    def register_pipelines(self) -&gt; Dict[str, Pipeline]:\n\n        &quot;&quot;&quot;Register the project's pipeline.\n\n        Returns:\n\n            A mapping from a pipeline name to a ``Pipeline`` object.\n\n        &quot;&quot;&quot;\n\n        pm = pre_master.create_pipeline()\n\n        return {\n\n            &quot;pre_master&quot;: pm,\n\n            &quot;__default__&quot;: pm\n\n        }\n\n    @hook_impl\n\n    def register_config_loader(self, conf_paths: Iterable[str]) -&gt; ConfigLoader:\n\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n\n    def register_catalog(\n\n        self,\n\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n\n        credentials: Dict[str, Dict[str, Any]],\n\n        load_versions: Dict[str, str],\n\n        save_version: str,\n\n        journal: Journal,\n\n    ) -&gt; DataCatalog:\n\n        return DataCatalog.from_config(\n\n            catalog, credentials, load_versions, save_version, journal\n\n        )\n\nproject_hooks = ProjectHooks()\n<\/code><\/pre>\n<p>And the settings are called the following way:\n&quot;&quot;&quot;Project settings.&quot;&quot;&quot;<\/p>\n<pre><code>from price_based_trading.hooks import ProjectHooks\n\n\nHOOKS = (ProjectHooks(),)\n<\/code><\/pre>\n<p>How can we configure that in a way that the hooks are used calling the method load_context(_working_dir).catalog.datasets ?<\/p>\n<p>I posted the same question in the kedro community: <a href=\"https:\/\/discourse.kedro.community\/t\/how-to-load-a-specific-catalog-item-in-kedro-0-17-0\/310\" rel=\"nofollow noreferrer\">https:\/\/discourse.kedro.community\/t\/how-to-load-a-specific-catalog-item-in-kedro-0-17-0\/310<\/a><\/p>",
        "Challenge_closed_time":1612452830192,
        "Challenge_comment_count":3,
        "Challenge_created_time":1612267210303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to load a specific item from the catalog in Kedro 0.17.0 using the framework context. They were previously using Kedro version 0.15.8 and loading the item using a different method. The error message suggests that the hook register_config_loader from the project is not being used by the hook_manager that calls the function. The user has posted the same question in the Kedro community seeking help to configure the hooks in a way that they can be used to call the method load_context(_working_dir).catalog.datasets.",
        "Challenge_last_edit_time":1612461225272,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66009324",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":15.0,
        "Challenge_reading_time":31.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":51.5610802778,
        "Challenge_title":"How to load a specific catalog dataset instance in kedro 0.17.0?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1298.0,
        "Challenge_word_count":224,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462800865783,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":316.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>It was a silly mistake because I was not creating the Kedro session. To load an item of the catalog it can be done with the following code:<\/p>\n<pre><code>from kedro.framework.session import get_current_session\nfrom kedro.framework.session import KedroSession\n\nKedroSession.create(&quot;name_of_proyect&quot;) as session:\n    key = &quot;item_of_catalog&quot;\n    session = get_current_session()\n    context = session.load_context()\n    kedro_connector = context.catalog.datasets.__dict__[key] \n    \/\/ or kedro_connector = context.catalog._get_datasets(key)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1616756019583,
        "Solution_link_count":0.0,
        "Solution_readability":15.6,
        "Solution_reading_time":7.29,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":51.0,
        "Tool":"Kedro",
        "Challenge_type":"anomaly",
        "Challenge_summary":"hook not being used"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.4279386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Difficulty in understanding<\/h1>\n<p>Q2) How to download a file from S3?<\/p>\n<p><strong>From<\/strong>  <a href=\"https:\/\/medium.com\/akeneo-labs\/machine-learning-workflow-with-sagemaker-b83b293337ff\" rel=\"nofollow noreferrer\">The Machine Learning Workflow with SageMaker<\/a><\/p>\n<p>And also why are we using this piece of code?<\/p>\n<p><code>estimator.fit(train_data_location)<\/code><\/p>",
        "Challenge_closed_time":1568928522076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568926981497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty understanding how to download a file from S3 and why the code \"estimator.fit(train_data_location)\" is being used in the blog post \"The Machine Learning Workflow with SageMaker\".",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58018893",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":6.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4279386111,
        "Challenge_title":"How do S3 file download and estimator.fit() work in this blog post?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":43,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556451987416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1309.0,
        "Poster_view_count":288.0,
        "Solution_body":"<h2>Downloading a file from S3:<\/h2>\n\n<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()<\/code> method.<\/p>\n\n<pre><code>def download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<h2>Estimator.fit() explanation:<\/h2>\n\n<p>The <code>estimator.fit(train_data_location)<\/code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location<\/code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. <\/p>\n\n<p>You can find the result of this training job in the SageMaker console.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":19.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":166.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"difficulty downloading S3 file"
    },
    {
        "Answerer_created_time":1420546067812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":876.6783925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My <em>PySpark<\/em> dataset contains categorical data.<\/p>\n<p>To train a model on this data, I followed this <a href=\"https:\/\/docs.databricks.com\/_static\/notebooks\/binary-classification.html\" rel=\"nofollow noreferrer\">example notebook<\/a>. Especially, see the <em>Preprocess Data<\/em> section for the encoding part.<\/p>\n<p>I now need to use this model somewhere else; hence, I followed <em>Databricks<\/em> recommendation to save and load this model.<\/p>\n<p>It's working fine with <em>Pandas<\/em> (cf. code below).<\/p>\n<pre><code>logged_model = 'runs:\/e905f5759d434a1391bbe1e54a2b\/best-model'\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n# Predict on a Pandas DataFrame.\nimport pandas as pd\nloaded_model.predict(pd.DataFrame(data))\n<\/code><\/pre>\n<p>However the dataframe is to big to be converted to <em>Pandas<\/em>. Hence I need to make it work in <em>Spark<\/em>:<\/p>\n<pre><code>import mlflow\nlogged_model = 'runs:\/e905f5759d434a131bbe1e54a2b\/best-model'\n\n# Load model as a Spark UDF.\nloaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)\n\n# Predict on a Spark DataFrame.\ndf.withColumn('predictions', loaded_model(*columns)).collect()\n<\/code><\/pre>\n<p>But this snippet is producing:<\/p>\n<pre><code>java.lang.UnsupportedOperationException: Unsupported data type: struct&amp;lt;type:tinyint,size:int,indices:array&amp;lt;int&amp;gt;,values:array&amp;lt;double&amp;gt;&amp;gt;\n<\/code><\/pre>\n<p>My feeling is that the udf doesn't accept this type of data as input.\nIs there a way to fix it ?\nAnother solution ?<\/p>",
        "Challenge_closed_time":1629378460768,
        "Challenge_comment_count":1,
        "Challenge_created_time":1627291388767,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue while trying to use mlflow.pyfunc.spark_udf to predict on a Spark DataFrame containing categorical data. The code is producing an error message stating that the struct type of the data is unsupported. The user is seeking a solution to fix this issue or an alternative solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68527422",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":21.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":579.7422225,
        "Challenge_title":"mlflow.pyfunc.spark_udf and vector struct type",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":790.0,
        "Challenge_word_count":159,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480398962230,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":440.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Have you tried using the <code>mlflow.spark.load_model<\/code>?<\/p>\n<p>I'm having a very similar issue over here, but but using the spark method. I tried using the <code>mlflow.spark.load_model('runs:\/run-id\/my-model')<\/code> method and I got this weird error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/dbfs\/tmp\/mlflow\/weird-id-folder'\n<\/code><\/pre>\n<p>Searching for the docs, I see the problem that we are facing (which seems to be different), seems to be a signature problem.<\/p>\n<p>According with other part of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-signature\" rel=\"nofollow noreferrer\">docs<\/a> we have that the signature logged with the model will help to define what type of input the model has. The problem for me here is that my input is a Spark Sparse Vector -- which is not supported... Right now I'm trying to convert that into a column-based signature.<\/p>\n<p>Have you tried something like this?<\/p>\n<hr \/>\n<p>UPDATE:<\/p>\n<p>I would like to add that in my case adding the signature did solve the problem. All I did was ignore the vectors and consider only the input data and output data.<\/p>\n<p>I took a look into the notebook, but haven't seen any mlflow logs, anyway, I do suppose you are logging your experiment according to <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/tracking.html#log-runs-to-a-notebook-or-workspace-experiment\" rel=\"nofollow noreferrer\">this<\/a> and using the <code>mlflow.spark<\/code> flavor.<\/p>\n<p>If so, consider using all your data transformation and model fit in the same pipeline, using <code>from pyspark.ml import Pipeline<\/code>. Before logging the model, consider going under signature and registering the model schema.<\/p>\n<pre><code>import mlflow.spark\nfrom mlflow.models.signature import infer_signature\n\nwith mlflow.start_run():\n    [...]\n    # executing train &amp; test pipelines:\n    model = pipeline.fit(train_features) # training model\n    predictions = model.transform(test_features) # testing model\n    train_signature = train_features.select('input_data') # ignores all other features created on the pipeline\n    prediction_signature = predictions.select('input_data', 'prediction') # ignores all other features created on the training pipeline \n    signature = infer_signature(train_signature, prediction_signature) # register model schema\n    mlflow.spark.log_model(model, 'transactions-classification', signature=signature) # logging model to mlflow\n    [...]\n<\/code><\/pre>\n<p>After logging the model to the experiment, in a different notebook, you can use the load_model function as:<\/p>\n<pre><code># importing model\nimport mlflow.spark\nmodel_path = 'runs:\/run-id'\nmodel = mlflow.spark.load_model(model_path)\n<\/code><\/pre>\n<p>And it will work! :D<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1630447430980,
        "Solution_link_count":2.0,
        "Solution_readability":11.3,
        "Solution_reading_time":35.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":320.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported struct type"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1339.2047222222,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nStarting in version 2.0.9 the neptune_ml widget is having an issue where the json values being passed in are getting the following error \r\n```\r\n{'error': JSONDecodeError('Expecting value: line 1 column 1 (char 0)',)}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run through the 01-Introduction-to-Node-Classification-Gremlin notebook\r\n2. When you get to the export step the error occurs\r\n\r\n**Additional context**\r\nThis is not a problem in version 2.0.7",
        "Challenge_closed_time":1620330541000,
        "Challenge_comment_count":25,
        "Challenge_created_time":1615509404000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug with AWS Neptune templates for p3.2 and p3.16, which fail to start and stall out during the formation process, resulting in auto-deletion. The issue is being investigated, and it is suspected that it may be related to V100 GPU services.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/81",
        "Challenge_link_count":0,
        "Challenge_participation_count":25,
        "Challenge_readability":10.2,
        "Challenge_reading_time":6.48,
        "Challenge_repo_contributor_count":25.0,
        "Challenge_repo_fork_count":129.0,
        "Challenge_repo_issue_count":493.0,
        "Challenge_repo_star_count":546.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1339.2047222222,
        "Challenge_title":"[BUG] Neptune_ML widget error in 2.0.9",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Discussion_body":"This appears to be an issue with the versions of `ipython` that SageMaker is using.  If you update the Lifecycle start script by putting the following code at the bottom (just before EOF) and stopping and starting the notebook.\r\n```\r\nsource activate JupyterSystemEnv\r\npip install --upgrade ipython==7.16.1\r\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\r\n``` Hi, i have updated the Lifecycle scripts as suggested and that works - but then it fails on the training:\r\n\r\n`\"status\": \"Failed\",\r\n    \"failureReason\": \"ClientError: Failed to download data`\r\n\r\n...\r\npreloading-2021-04-05-17-33-3910000\/preloading-output\/graph.bin has an illegal char sub-sequence '\/\/' in it\"`\r\n\r\ni just used the movie lens database and steps in the notebook. it adds an extra '\\' in the \"outputLocation\"...?\r\n\r\ncan you help?  \r\n Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?  > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\n\r\n<img width=\"1103\" alt=\"error_train_screen\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n > > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n> \r\n> <img alt=\"error_train_screen\" width=\"1103\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n\r\n<img width=\"1117\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705546-73cdce00-96d5-11eb-81fa-633c14942847.png\">\r\n > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\nhi @austinkline  - thanks for helping me out. So as you can see from the screenshots, it fails to download the data and seems to be adding an extra slash...\r\n\r\nso I changed the script: `--s3-processed-uri {str(s3_bucket_uri)}preloading \"\"\"` \r\nand it then ran fine.... perhaps you want to correct that in the notebook?\r\n\r\nbut when making the prediction I am getting:\r\n\r\n<img width=\"1120\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113713442-42f29680-96df-11eb-8dc8-131e8377fa4c.png\">\r\n\r\n\r\nso Toy Story comes up as 'Thriller\" and not 'Comedy' as  per the notebook\r\n\r\n\r\nhow can I see which actual model the classification is using? Is it a graph convolutional network, I recall seeing that in the notebooks in the repository. It would be good to see the actual DGL model & code. \r\n\r\nThanks!!\r\n Thanks for the info. I'll spend some time reproducing and get back to you I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n\r\n@Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore > I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n> \r\n> ![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n> \r\n> @Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore\r\n\r\nthank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?  > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n\r\nChecked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console. \r\n > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> \r\n> Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n\r\nyeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network > > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> > \r\n> > \r\n> > Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n> \r\n> yeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network\r\n\r\nyes. that's correct. Hi @Kristof-Neys and updates? Did recreating work for you? Hi @austinkline - thanks for reaching out. I have been caught up in another project but was just about to look at it. I'll update you guys probably tomorrow.  hi @austinkline & Team, i am finally getting around to this. I started everything new but now I cannot export the configuration any more, I get the following error:\r\n`{'error': ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='none', port=443): Max retries exceeded with url: \/neptune-export (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f28f5e81748>: Failed to establish a new connection: [Errno -2] Name or service not known',))\",),)}`\r\n\r\nUPdate: when I re-started everything and used the notebook of last week... i get\r\n\r\n`403 \"Missing Authentication Token\" `\r\n\r\n\r\n\r\nany ideas? Thanks!!\r\n     Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n\r\nCan you provide your notebook version and configuration by running the following:\r\n\r\n1. What cell did you execute that gave you the above mentioned error?\r\n\r\n2. What version of `graph-notebook` are you running?\r\n```\r\n%graph_notebook_version\r\n```\r\n\r\n3. What is your configuration? Really we just care about the authentication setting\r\n**NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n\r\n```\r\n%graph_notebook_config\r\n```\r\n > Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n> \r\n> Can you provide your notebook version and configuration by running the following:\r\n> \r\n>     1. What cell did you execute that gave you the above mentioned error?\r\n> \r\n>     2. What version of `graph-notebook` are you running?\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_version\r\n> ```\r\n> \r\n>     1. What is your configuration? Really we just care about the authentication setting\r\n>        **NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_config\r\n> ```\r\n\r\n@austinkline thank you! Very much appreciate taking time & effort. Ok, so these are the detail:\r\n\r\ncell that I am running:\r\n`%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}`\r\n=> this gives me error: \r\n`{\r\n  \"message\": \"Missing Authentication Token\"\r\n}`\r\n\r\n\r\nVersion graph-notebook: 2.1.0\r\n\r\n%graph_notebook_config:\r\n`{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}`\r\n\r\nThe strange thing is that all worked well two weeks ago, altho I did get wrong predictions, but at least the export worked and I could train model and get predictions etc. Now I cannot get beyond the export.... \r\n\r\nthank you again\r\n\r\n @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n\r\n```\r\n%%graph_notebook_config\r\n{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"IAM\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}\r\n```\r\n\r\nNote that we're changing the auth mode to IAM > @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n> \r\n> ```\r\n> %%graph_notebook_config\r\n> {\r\n>   \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n>   \"port\": 8182,\r\n>   \"auth_mode\": \"IAM\",\r\n>   \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n>   \"ssl\": true,\r\n>   \"aws_region\": \"us-east-1\",\r\n>   \"sparql\": {\r\n>     \"path\": \"sparql\"\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> Note that we're changing the auth mode to IAM\r\nhey @austinkline  - that worked!, export and training went fine....but still predicting the wrong genre.... - how can this be??\r\n\r\n<img width=\"904\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/115867858-82d1b180-a433-11eb-809c-a1aa733e5d90.png\">\r\n\r\n\r\n @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect.  Drama is what is coming back from the model that is generated .  I have created an issue to track this https:\/\/github.com\/aws\/graph-notebook\/issues\/116 and will address this with the additional feedback on those notebooks in the near future.  > @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect. Drama is what is coming back from the model that is generated . I have created an issue to track this #116 and will address this with the additional feedback on those notebooks in the near future.\r\n\r\nok understood - thank you\r\n Closing this out since we're tracking the reported issue of notebooks being out of date in #116. Please cut us a new ticket if you run into any further issues! Hi guys, I'm facing a similar issue, I applied your fix(setting \"auth_mode\": \"IAM\") but did not work, any suggestions? Hi @llealgt , is this referring to the same issue mentioned at https:\/\/github.com\/aws\/graph-notebook\/issues\/445#issuecomment-1426192856? Hi @michaelnchin, nope, it's not the same, this happens when running notebook \r\nNeptune-ML-01-Introduction-to-Node-Classification-Gremlin\r\nThe other errors happen in notebook \r\nNeptune-ML-00-Getting-Started-with-Neptune-ML-Gremlin\r\nI guess it is related but they are different errors in different notebooks.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Neptune templates fail to start"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":40.4833333333,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nGetting errors with the new Sagemaker Async Operators that I don't get with the traditional ones. I'm using a personal Access Key, Secret, and Session Token as I did with the non async operators for auth.\r\n\r\n```\r\nbotocore.exceptions.ClientError: An error occurred (UnrecognizedClientException) when calling the DescribeTrainingJob operation: The security token included in the request is invalid.\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nUse the SageMaker async operators with user Access Key, Secret, and Session Token\r\n\r\n**Expected behavior**\r\nExpect it to not have auth\/token errors.\r\n\r\n\r\n**Additional context**\r\nWhen I switch back to the traditional operators in the same dag with the same auth creds it works fine.\r\n\r\n\r\n@kentdanas also had similar issues and her auth was setup a little different.",
        "Challenge_closed_time":1666859588000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1666713848000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing errors while trying to use Sagemaker Debugger with HPO and is getting a \"FileNotFoundError\" related to the debughookconfig.json file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/725",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.75,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":23.0,
        "Challenge_repo_issue_count":1165.0,
        "Challenge_repo_star_count":110.0,
        "Challenge_repo_watch_count":34.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":40.4833333333,
        "Challenge_title":"Token error with Sagemaker Async Operators",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":127,
        "Discussion_body":"The issue is with the session token is not considered while the secrete and access key is given in the connection proper field, not in the extra config",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"FileNotFoundError with Debugger"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":5.6856088889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Sagmaker, and I have created a pipeline from the SageMaker notebook, consisting of training and deployment components.\nIn the training script, we can upload the model to s3 via <strong>SM_MODEL_DIR<\/strong>. But now, I want to upload the classification report to s3. I tried this code. But It shows this is not a proper s3 bucket.<\/p>\n<pre><code>df_classification_report = pd.DataFrame(class_report).transpose()\nclassification_report_file_name = os.path.join(args.output_data_dir,\n                                               f&quot;{args.eval_model_name}_classification_report.csv&quot;)\ndf_classification_report.to_csv(classification_report_file_name)\n# instantiate S3 client and upload to s3\n\n# save classification report to s3\ns3 = boto3.resource('s3')\nprint(f&quot;classification_report is being uploaded to s3- {args.model_dir}&quot;)\ns3.meta.client.upload_file(classification_report_file_name, args.model_dir,\n                            f&quot;{args.eval_model_name}_classification_report.csv&quot;)\n<\/code><\/pre>\n<p>And the error<\/p>\n<pre><code>Invalid bucket name &quot;\/opt\/ml\/output\/data&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>Can anybody help? I really appreciate any help you can provide.<\/p>",
        "Challenge_closed_time":1649187284408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649158713513,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to upload a custom file to S3 from the training script in the training component of AWS SageMaker Pipeline. The user is able to upload the model to S3 via SM_MODEL_DIR but is unable to upload the classification report to S3. The error message indicates that the bucket name is not in the correct format. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1649166816216,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71751105",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":20.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.9363597222,
        "Challenge_title":"Upload custom file to s3 from training script in training component of AWS SageMaker Pipeline",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":458.0,
        "Challenge_word_count":138,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383025927423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dhaka, Bangladesh",
        "Poster_reputation_count":647.0,
        "Poster_view_count":105.0,
        "Solution_body":"<p>SageMaker Training Jobs will compress any files located in <code>\/opt\/ml\/model<\/code> which is the value of <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir\" rel=\"nofollow noreferrer\"><code>SM_MODEL_DIR<\/code><\/a> and upload it to S3 automatically. You could look at saving your file to <code>SM_MODEL_DIR<\/code> (Your classification report will thus be uploaded to S3 in the model tar ball).<\/p>\n<p>The <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\"><code>upload_file()<\/code><\/a> function requires you to pass an S3 bucket.\nYou could also look at manually specify an S3 bucket in your code to upload the file to.<\/p>\n<pre><code>s3.meta.client.upload_file(classification_report_file_name, &lt;YourS3Bucket&gt;,\n                            f&quot;{args.eval_model_name}_classification_report.csv&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.6,
        "Solution_reading_time":12.68,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect bucket name format"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":467.3672177778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am able to submit jobs to Azure ML services using a compute cluster. It works well, and the autoscaling combined with good flexibility for custom environments seems to be exactly what I need. However, so far all these jobs seem to only use one compute node of the cluster. Ideally I would like to use multiple nodes for a computation, but all methods that I see rely on rather deep integration with azure ML services.<\/p>\n\n<p>My modelling case is a bit atypical. From previous experiments I identified a group of architectures (pipelines of preprocessing steps + estimators in Scikit-learn) that worked well. \nHyperparameter tuning for one of these estimators can be performed reasonably fast (couple of minutes) with <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">RandomizedSearchCV<\/a>. So it seems less effective to parallelize this step.<\/p>\n\n<p>Now I want to tune and train this entire list of architectures.\nThis should be very easily to parallelize since all architectures can be trained independently. <\/p>\n\n<p>Ideally I would like something like (in pseudocode)<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tuned = AzurePool.map(tune_model, [model1, model2,...])\n<\/code><\/pre>\n\n<p>However, I could not find any resources on how I could achieve this with an Azure ML Compute cluster.\nAn acceptable alternative would come in the form of a plug-and-play substitute for sklearn's CV-tuning methods, similar to the ones provided in <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">dask<\/a> or <a href=\"https:\/\/databricks.github.io\/spark-sklearn-docs\/#spark_sklearn.GridSearchCV\" rel=\"nofollow noreferrer\">spark<\/a>.<\/p>",
        "Challenge_closed_time":1565990325968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565966913733,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in parallelizing work on an Azure ML Service Compute cluster. They have been able to submit jobs to the cluster, but all jobs seem to use only one compute node. The user wants to train a list of architectures in parallel, but they could not find any resources on how to achieve this with an Azure ML Compute cluster. They are looking for a plug-and-play substitute for sklearn's CV-tuning methods.",
        "Challenge_last_edit_time":1565987551452,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57526707",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":25.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.5033986111,
        "Challenge_title":"How to parallelize work on an Azure ML Service Compute cluster?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":818.0,
        "Challenge_word_count":233,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525187747288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":1466.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>There are a number of ways you could tackle this with AzureML. The simplest would be to just launch a number of jobs using the AzureML Python SDK (the underlying example is taken from <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training\/train-hyperparameter-tune-deploy-with-sklearn\/train-hyperparameter-tune-deploy-with-sklearn.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\n\nruns = []\n\nfor kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n    for penalty in [0.5, 1, 1.5]:\n        print ('submitting run for kernel', kernel, 'penalty', penalty)\n        script_params = {\n            '--kernel': kernel,\n            '--penalty': penalty,\n        }\n\n        estimator = SKLearn(source_directory=project_folder, \n                            script_params=script_params,\n                            compute_target=compute_target,\n                            entry_script='train_iris.py',\n                            pip_packages=['joblib==0.13.2'])\n\n        runs.append(experiment.submit(estimator))\n<\/code><\/pre>\n\n<p>The above requires you to factor your training out into a script (or a set of scripts in a folder) along with the python packages required. The above estimator is a convenience wrapper for using Scikit Learn. There are also estimators for Tensorflow, Pytorch, Chainer and a generic one (<code>azureml.train.estimator.Estimator<\/code>) -- they all differ in the Python packages and base docker they use.<\/p>\n\n<p>A second option, if you are actually tuning parameters, is to use the HyperDrive service like so (using the same <code>SKLearn<\/code> Estimator as above):<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\nfrom azureml.train.hyperdrive.runconfig import HyperDriveConfig\nfrom azureml.train.hyperdrive.sampling import RandomParameterSampling\nfrom azureml.train.hyperdrive.run import PrimaryMetricGoal\nfrom azureml.train.hyperdrive.parameter_expressions import choice\n\nestimator = SKLearn(source_directory=project_folder, \n                    script_params=script_params,\n                    compute_target=compute_target,\n                    entry_script='train_iris.py',\n                    pip_packages=['joblib==0.13.2'])\n\nparam_sampling = RandomParameterSampling( {\n    \"--kernel\": choice('linear', 'rbf', 'poly', 'sigmoid'),\n    \"--penalty\": choice(0.5, 1, 1.5)\n    }\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                                         hyperparameter_sampling=param_sampling, \n                                         primary_metric_name='Accuracy',\n                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                         max_total_runs=12,\n                                         max_concurrent_runs=4)\n\nhyperdrive_run = experiment.submit(hyperdrive_run_config)\n<\/code><\/pre>\n\n<p>Or you could use DASK to schedule the work as you were mentioning. Here is a sample of how to set up DASK on and AzureML Compute Cluster so you can do interactive work on it: <a href=\"https:\/\/github.com\/danielsc\/azureml-and-dask\" rel=\"nofollow noreferrer\">https:\/\/github.com\/danielsc\/azureml-and-dask<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1567670073436,
        "Solution_link_count":3.0,
        "Solution_readability":17.0,
        "Solution_reading_time":36.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":250.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"jobs using only one node"
    },
    {
        "Answerer_created_time":1508520702036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":21.2055166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>when running an AML pipeline on AML compute, I get this kind of error : <\/p>\n\n<p>I can try rebooting the cluster, but that may not fix the problem (if storage gets accumulated no the nodes, that should be cleaned.<\/p>\n\n<pre><code>Session ID: 933fc468-7a22-425d-aa1b-94eba5784faa\n{\"error\":{\"code\":\"ServiceError\",\"message\":\"Job preparation failed: [Errno 28] No space left on device\",\"detailsUri\":null,\"target\":null,\"details\":[],\"innerError\":null,\"debugInfo\":{\"type\":\"OSError\",\"message\":\"[Errno 28] No space left on device\",\"stackTrace\":\" File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 126, in &lt;module&gt;\\n invoke()\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 97, in invoke\\n extract_project(project_dir, options.project_zip, options.snapshots)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 60, in extract_project\\n project_fetcher.fetch_project_snapshot(snapshot[\\\"Id\\\"], snapshot[\\\"PathStack\\\"])\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 72, in fetch_project_snapshot\\n _download_tree(sas_tree, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 98, in _download_tree\\n fh.write(response.read())\\n\",\"innerException\":null,\"data\":null,\"errorResponse\":null}},\"correlation\":null,\"environment\":null,\"location\":null,\"time\":\"0001-01-01T00:00:00+00:00\"}\n<\/code><\/pre>\n\n<p>I would expect the job to run as it should. And in fact, I've checked on the node and the node do have lots of available harddrive space :<\/p>\n\n<pre><code>root@4f57957ac829466a86bad4d4dc51fadd000001:~# df -kh                                                                                               Filesystem      Size  Used Avail Use% Mounted on\nudev             28G     0   28G   0% \/dev\ntmpfs           5.6G  9.0M  5.5G   1% \/run\n\/dev\/sda1       125G  2.8G  122G   3% \/\ntmpfs            28G     0   28G   0% \/dev\/shm\ntmpfs           5.0M     0  5.0M   0% \/run\/lock\ntmpfs            28G     0   28G   0% \/sys\/fs\/cgroup\n\/dev\/sdb1       335G  6.7G  311G   3% \/mnt\ntmpfs           5.6G     0  5.6G   0% \/run\/user\/1002\n<\/code><\/pre>\n\n<p>Suggestions on what I should check?<\/p>",
        "Challenge_closed_time":1568309206067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568232866207,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error message stating \"No space left on device\" when running an AML pipeline on AML compute. The user has checked the node and found that there is enough available hard drive space. The user is seeking suggestions on what to check next.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57896195",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.5,
        "Challenge_reading_time":42.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":21.2055166667,
        "Challenge_title":"Out of disk space",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":738.0,
        "Challenge_word_count":210,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538275960603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Montreal, QC, Canada",
        "Poster_reputation_count":381.0,
        "Poster_view_count":50.0,
        "Solution_body":"<p>Seems like you've run into Azure file share constraints. You can use the following sample code to change your runs to use blob storage which can scale to large number of jobs running in parallel:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.0,
        "Solution_reading_time":6.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no space left on device"
    },
    {
        "Answerer_created_time":1595690872796,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Deutschland",
        "Answerer_reputation_count":699.0,
        "Answerer_view_count":46.0,
        "Challenge_adjusted_solved_time":78.6193025,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For a project I am working on, which uses annual financial reports data (of multiple categories) from companies which have been successful or gone bust\/into liquidation, I previously created a (fairly well performing) model on AWS Sagemaker using a multiple linear regression algorithm (specifically, the AWS stock algorithm for logistic regression\/classification problems - the 'Linear Learner' algorithm)<\/p>\n<p>This model just produces a simple &quot;company is in good health&quot; or &quot;company looks like it will go bust&quot; binary prediction, based on one set of annual data fed in; e.g.<\/p>\n<pre><code>query input: {data:[{\n&quot;Gross Revenue&quot;: -4000,\n&quot;Balance Sheet&quot;: 10000,\n&quot;Creditors&quot;: 4000,\n&quot;Debts&quot;: 1000000 \n}]}\n\ninference output: &quot;in good health&quot; \/ &quot;in bad health&quot;\n<\/code><\/pre>\n<p>I trained this model by just ignoring what year for each company the values were from and pilling in all of the annual financial reports data (i.e. one years financial data for one company = one input line) for the training, along with the label of &quot;good&quot; or &quot;bad&quot; - a good company was one which has existed for a while, but hasn't gone bust, a bad company is one which was found to have eventually gone bust; e.g.:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue<\/th>\n<th>Balance Sheet<\/th>\n<th>Creditors<\/th>\n<th>Debts<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>0<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I hence used these multiple features (gross revenue, balance sheet...) along with the label (good\/bad) in my training input, to create my first model.<\/p>\n<p>I would like to use the same features as before as input (gross revenue, balance sheet..) but over multiple years; e.g take the values from 2020 &amp; 2019 and use these (along with the eventual company status of &quot;good&quot; or &quot;bad&quot;) as the singular input for my new model. However I'm unsure of the following:<\/p>\n<ul>\n<li>is this an inappropriate use of logistic regression Machine learning? i.e. is there a more suitable algorithm I should consider?<\/li>\n<li>is it fine, or terribly wrong to try and just use the same technique as before, but combine the data for both years into one input line like:<\/li>\n<\/ul>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue(2019)<\/th>\n<th>Balance Sheet(2019)<\/th>\n<th>Creditors(2019)<\/th>\n<th>Debts(2019)<\/th>\n<th>Gross Revenue(2020)<\/th>\n<th>Balance Sheet(2020)<\/th>\n<th>Creditors(2020)<\/th>\n<th>Debts(2020)<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<td>30000<\/td>\n<td>10000<\/td>\n<td>40<\/td>\n<td>500<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>100<\/td>\n<td>50<\/td>\n<td>200<\/td>\n<td>50000<\/td>\n<td>100<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>5000<\/td>\n<td>0<\/td>\n<td>2000<\/td>\n<td>800000<\/td>\n<td>2000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I would personally expect that a company which has gotten worse over time (i.e. companies finances are worse in 2020 than in 2019) should be more likely to be found to be a &quot;bad&quot;\/likely to go bust, so I would hope that, if I feed in data like in the above example (i.e. earlier years data comes before later years data, on an input line) my training job ends up creating a model which gives greater weighting to the earlier years data, when making predictions<\/p>\n<p>Any advice or tips would be greatly appreciated - I'm pretty new to machine learning and would like to learn more<\/p>\n<p>UPDATE:<\/p>\n<p>Using Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNN) is one potential route I think I could try taking, but this seems to commonly just be used with multivariate data over many dates; my data only has 2 or 3 dates worth of multivariate data, per company. I would want to try using the data I have for all the companies, over the few dates worth of data there are, in training<\/p>",
        "Challenge_closed_time":1614450076540,
        "Challenge_comment_count":1,
        "Challenge_created_time":1613315731557,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has previously created a model on AWS Sagemaker using a multiple linear regression algorithm for binary prediction of a company's financial health based on one set of annual data fed in. The user now wants to use the same features as before but over multiple years and is unsure if this is an appropriate use of logistic regression machine learning or if there is a more suitable algorithm. The user is also unsure if it is fine to combine the data for both years into one input line and hopes that the model gives greater weighting to the earlier years' data when making predictions. The user is considering using Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNN) but is unsure if it is suitable for their data.",
        "Challenge_last_edit_time":1614168553343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66196815",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.6,
        "Challenge_reading_time":54.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":315.0958286111,
        "Challenge_title":"Using Logistic Regression For Timeseries Data in Amazon SageMaker",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":597,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1566839876032,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>I once developed a so called Genetic Time Series in R. I used a Genetic Algorithm which sorted out the best solutions from multivariate data, which were fitted on a VAR in differences or a VECM. Your data seems more macro economic or financial than user-centric and VAR or VECM seems appropriate. (Surely it is possible to treat time-series data in the same way so that we can use LSTM or other approaches, but these are very common) However, I do not know if VAR in differences or VECM works with binary classified labels. Perhaps if you would calculate a metric outcome, which you later label encode to a categorical feature (or label it first to a categorical) than VAR or VECM may also be appropriate.<\/p>\n<p>However you may add all yearly data points to one data points per firm to forecast its survival, but you would loose a lot of insight. If you are interested in time series ML which works a little bit different than for neural networks or elastic net (which could also be used with time series) let me know. And we can work something out. Or I'll paste you some sources.<\/p>\n<p>Summary:\n1.)\nIt is possible to use LSTM, elastic NEt (time points may be dummies or treated as cross sectional panel) or you use VAR in differences and VECM with a slightly different out come variable<\/p>\n<p>2.)\nIt is possible but you will loose information over time.<\/p>\n<p>All the best,\nPatrick<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1614451582832,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":16.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":247.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"suitable algorithm for multi-year data"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2159.1657988889,
        "Challenge_answer_count":2,
        "Challenge_body":"Is there a link that shows how much GPU memory is available on the following GPU instances on AWS?\n\n1. g4-series instances (NVidia T4)\n2. g5-series instances (NVidia A10)\n3. p3d-series instances (NVidia V100)\n4. p4d-series instances (NVidia A100)\n\nUpdate: the information is available for the [p3d series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/) and [g5 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g5\/), though not for the [g4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g4\/) or the [p4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p4\/) instances. Is it possible to retrieve the information for the latter two instances anywhere (without having to launch the instances)?",
        "Challenge_closed_time":1676386194668,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643029354176,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for information on the amount of GPU memory available on AWS instances, specifically the g4, g5, p3d, and p4d series. The user has found information for the p3d and g5 series, but not for the g4 and p4d series. The user is asking if there is a way to retrieve this information without having to launch the instances.",
        "Challenge_last_edit_time":1668613197792,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUvPdBv2rwTEiYKKHDPLUTWA\/how-much-gpu-memory-are-available-on-the-g4-g5-p3d-and-p4d-series-instances",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":9.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":9265.7890255556,
        "Challenge_title":"How much GPU memory are available on the g4, g5, p3d, and p4d series instances?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2195.0,
        "Challenge_word_count":95,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This link has a table that compares instances' GPU memory\nhttps:\/\/docs.amazonaws.cn\/en_us\/AmazonECS\/latest\/developerguide\/ecs-gpu.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1676386194668,
        "Solution_link_count":1.0,
        "Solution_readability":24.1,
        "Solution_reading_time":1.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"GPU memory on AWS instances"
    },
    {
        "Answerer_created_time":1397184643572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":74.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":91.3691802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy my model (container) on AWS SageMaker. I've pushed the container to AWS ECR.\nThen I use an AWS Lambda that basically runs <code>create_training_job()<\/code> via the <code>boto3<\/code> SageMaker client. It runs the container in <strong>train<\/strong> mode and puts the generated artifact to the S3. Like that:<\/p>\n<pre><code>sm = boto3.client('sagemaker')\n\nsm.create_training_job(\n        TrainingJobName=full_job_name,\n        HyperParameters={\n            'general': json.dumps(\n                {\n                    'environment': ENVIRONMENT,\n                    'region': REGION,\n                    'version': date_suffix,\n                    'hyperparameter_tuning': training_params.get('hyperparameter_tuning', False),\n                    'basket_analysis': training_params.get('basket_analysis', True),\n                    'init_inventory_cache': training_params.get('init_inventory_cache', True),\n                }\n            ),\n            'aws_profile': '***-dev',\n            'db_config': json.dumps(database_mapping),\n            'model_server_params': json.dumps(training_params.get('model_server_params', {}))\n        },\n\n        AlgorithmSpecification={\n            'TrainingImage': training_image,\n            'TrainingInputMode': 'File',\n        },\n        RoleArn=ROLE_ARN,\n        OutputDataConfig={\n            'S3OutputPath': S3_OUTPUT_PATH\n        },\n        ResourceConfig={\n            'InstanceType': INSTANCE_TYPE,\n            'InstanceCount': 1,\n            'VolumeSizeInGB': 20,\n        },\n        # VpcConfig={\n        #     'SecurityGroupIds': SECURITY_GROUPS.split(','),\n        #     'Subnets': SUBNETS.split(',')\n        # },\n        StoppingCondition={\n            'MaxRuntimeInSeconds': int(MAX_RUNTIME_SEC),\n            #        'MaxWaitTimeInSeconds': 1800\n        },\n        Tags=[ ],\n        EnableNetworkIsolation=False,\n        EnableInterContainerTrafficEncryption=False,\n        EnableManagedSpotTraining=False,\n    )\n<\/code><\/pre>\n<p>I have a logger inside the container that says that <code>opt\/ml\/input\/config\/hyperparameters.json<\/code> now exists. It has been added by SageMaker. Fine.<\/p>\n<p>But then, when I try to run the same container in <code>serve<\/code> mode (so basically to deploy it) I encounter that <code>opt\/ml\/input\/config\/hyperparameters.json<\/code> doesn't exist anymore. I deploy it this way:<\/p>\n<pre><code>     sm.create_model(\n        ModelName=model_name,\n        PrimaryContainer={\n            'Image': training_image,\n            'ModelDataUrl': model_artifact,\n            'Environment': {\n                'version': version\n            }\n        },\n        ExecutionRoleArn=role_arn,\n        Tags=[ ],\n        # VpcConfig = {\n        #     'SecurityGroupIds': os.environ['security_groups'].split(','),\n        #     'Subnets': os.environ['subnets'].split(',')\n        # }\n    )\n\n    sm.create_endpoint_config(\n        EndpointConfigName=config_name,\n        ProductionVariants=[\n            {\n                'VariantName': variant_name,\n                'ModelName': model_name,\n                'InitialInstanceCount': instance_count,\n                'InstanceType': instance_type,\n                'InitialVariantWeight': 1\n            },\n        ],\n        Tags=[ ],\n    )\n\n    existing_endpoints = sm.list_endpoints(NameContains=endpoint_name)\n\n    scaling_resource_id = f'endpoint\/{endpoint_name}\/variant\/{variant_name}'\n\n    if not existing_endpoints['Endpoints']:\n        sm.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=config_name\n        )\n    else:\n        if aas.describe_scalable_targets(\n                ServiceNamespace='sagemaker',\n                ResourceIds=[scaling_resource_id],\n                ScalableDimension='sagemaker:variant:DesiredInstanceCount')['ScalableTargets']:\n            aas.deregister_scalable_target(\n                ServiceNamespace='sagemaker',\n                ResourceId=scaling_resource_id,\n                ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n            )\n\n        sm.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=config_name\n        )\n<\/code><\/pre>\n<p>It is important since it seemed to be a convenient way to pass some parameters inside the container from outside (like management console).<\/p>\n<p>I thought that this file\/directory will still exist after the <strong>train<\/strong>. Any ideas?<\/p>",
        "Challenge_closed_time":1629579807236,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629250878187,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with AWS SageMaker where the directory that SageMaker was supposed to create automatically is not present when trying to run the container in serve mode. The user is using an AWS Lambda to run create_training_job() via the boto3 SageMaker client and has a logger inside the container that says that opt\/ml\/input\/config\/hyperparameters.json now exists. However, when trying to run the same container in serve mode, the user encounters that opt\/ml\/input\/config\/hyperparameters.json doesn't exist anymore. The user is looking for ideas on why this file\/directory is not present after the train.",
        "Challenge_last_edit_time":1629669900607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68825648",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":22.1,
        "Challenge_reading_time":47.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":91.3691802778,
        "Challenge_title":"Why is there no specific directory that SageMaker was supposed to create automatically?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":262,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1496401724783,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":165.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>tl;dr: two options:<\/p>\n<ul>\n<li>Copy the <code>hyperparameters.json<\/code> file to <code>\/opt\/ml\/model<\/code> in the training logic and it will be packed with the model artifacts;<\/li>\n<li>Pass whatever parameters you want through the <code>PrimaryContainer<\/code> parameter's <code>Environment<\/code> property.<\/li>\n<\/ul>\n<p>Long version:<\/p>\n<p>That file, <code>opt\/ml\/input\/config\/hyperparameters.json<\/code>, (in fact the whole <code>\/opt\/ml\/input<\/code> folder) is mounted on the <strong>training<\/strong> container when it is created. It is provided by SageMaker, based on information you provide, <em>only<\/em> for training purposes. SageMaker does not change your container in any way, and it doesn't preserve this or any configuration file it passes to the training job once training is done. If you want to pass parameters to the inference endpoint, that is not the way.<\/p>\n<p>You <em>could<\/em> copy the <code>hyperparameters.json<\/code> file to the <code>\/opt\/ml\/model<\/code> folder, and it'd be packed with the model in the <code>model.tar.gz<\/code> tarball. Your infrence code could then use that - but that's not the prescribed way to pass parameters to an endpoint, and it cause problems with your framework.<\/p>\n<p>The generally prescribed way to pass parameters to SageMaker endpoints is through the <strong>environment<\/strong>. If you check the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">boto3 docs for create_model<\/a>, you'll see that there's an <code>Environment<\/code> key within the <code>PrimaryContainer<\/code> parameter (also for each of the <code>Containers<\/code> parameter). In fact, your code above already uses that to pass a <code>version<\/code> parameter. You should use that to pass any parameters to your model and, from there, to the endpoint based on it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.0,
        "Solution_reading_time":24.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":236.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing hyperparameters.json directory"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4346333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi , i would like to know if it is possible to change the location of AzureML workspace after creating it ?  <br \/>\nRight now i do not find any option to change it manually on the UI. We want to move the server location to a different country.  <br \/>\nAny leads would be helpful. Thanks<\/p>",
        "Challenge_closed_time":1643796022067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643794457387,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on whether it is possible to change the location of an Azure ML workspace after it has been created, as they want to move the server location to a different country. They are unable to find an option to change it manually on the UI and are seeking assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/719406\/change-location-of-azure-ml-workspace",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4346333334,
        "Challenge_title":"change location of Azure ML workspace?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Based on the below document, ML workspace can't be moved across region. Probably, you will have to create a new resource in target region and move artifacts \/ pipelines \/ child resources to it (not so familiar with ML)     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/move-support-resources#microsoftmachinelearning\">https:\/\/learn.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/move-support-resources#microsoftmachinelearning<\/a>    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-workspace#limitations\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-workspace#limitations<\/a>    <\/p>\n<p>----------    <\/p>\n<p>Please don't forget to <strong>Accept Answer<\/strong> and <strong>Up-vote<\/strong> if the response helped -- Vaibhav<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.9,
        "Solution_reading_time":11.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":59.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"change workspace location"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":245.3151869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting the following error from the Evaluate Model module in Azure Machine Learning Designer:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/101409-screenshot-2021-06-01-at-100708-pm.png?platform=QnA\" alt=\"101409-screenshot-2021-06-01-at-100708-pm.png\" \/>    <\/p>\n<p>When I open the Assigned Data to Clusters module everything seems fine. I downloaded the output for Assigned Data to Clusters and played with cluster number 31 and there doesn't seem to be any issue. Additionally, I am using Azure Modules, so I am confused as to why this is failing. Please provide some clarity into this issue. This is a part of my pipeline:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/101399-screenshot-2021-06-01-at-104512-pm.png?platform=QnA\" alt=\"101399-screenshot-2021-06-01-at-104512-pm.png\" \/>    <\/p>\n<p>Additionally, it seems unless I successfully run the Evaluate Model module, I cannot create an inference pipeline. If this is untrue, please help me out here as well. There is no option for me to 'Create an Inference Pipeline' which shown in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">this tutorial; step 1.<\/a>    <\/p>\n<p>Please let me know if you need any other information.    <\/p>\n<p>Thanks in advance.    <\/p>",
        "Challenge_closed_time":1623450957476,
        "Challenge_comment_count":3,
        "Challenge_created_time":1622567822803,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an ambiguous error in the Azure Machine Learning Designer's 'Evaluate Model' module. They have checked the output for the 'Assigned Data to Clusters' module and found no issues. The user is also confused about the inability to create an inference pipeline unless the 'Evaluate Model' module runs successfully. They are seeking clarity on the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/418016\/ambiguous-error-in-azure-machine-learning-designer",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.7,
        "Challenge_reading_time":18.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":245.3151869444,
        "Challenge_title":"Ambiguous error in Azure Machine Learning Designer 'Evaluate Model' Module",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":163,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Can you please check if the Assignment cluster 31 has NaN value? The <strong>Assign Data to Clusters<\/strong> leverages SKlearn, and from the error message, seems the Assignment column had NaN value which resulted in an error. If that's the case, let us know, so we can enable <strong>Evaluate<\/strong> Module module to deal with NaN values, and in the meantime, here's a short-term workaround:    <\/p>\n<ul>\n<li> Connect <strong>Clean Missing Data<\/strong> module to Assign Data to Cluster module, to clean the missing values.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/104943-image.png?platform=QnA\" alt=\"104943-image.png\" \/>    <\/li>\n<li> Use <strong>Edit Metadata<\/strong> module to convert Assignment to Integer and categorical type, this is because if Assignment column has NaN value before and its column type was double, we need to convert it to integer.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/104888-image.png?platform=QnA\" alt=\"104888-image.png\" \/>    <\/li>\n<li> Connect <strong>Edit Metadata<\/strong> to Evaluate Model module.    <\/li>\n<\/ul>\n<p>Hope this help!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":14.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":142.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ambiguous error in Evaluate Model"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":14.6893280556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\">IO section of the kedro API docs<\/a> I could not find functionality w.r.t. storing trained models (e.g. <code>.pkl<\/code>, <code>.joblib<\/code>, <code>ONNX<\/code>, <code>PMML<\/code>)? Have I missed something?<\/p>",
        "Challenge_closed_time":1589225919163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589224779627,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the availability of IO functionality to store trained models in kedro, as they could not find any related information in the kedro API documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61737613",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":4.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.3165377778,
        "Challenge_title":"Is there IO functionality to store trained models in kedro?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":795.0,
        "Challenge_word_count":39,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441627039648,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Augsburg, Germany",
        "Poster_reputation_count":3635.0,
        "Poster_view_count":383.0,
        "Solution_body":"<p>There is the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PickleLocalDataSet.html#kedro.io.PickleLocalDataSet\" rel=\"nofollow noreferrer\"><code>pickle<\/code><\/a> dataset in <code>kedro.io<\/code>, that you can use to save trained models and\/or anything you want to pickle and is serialisable (models being a common object). It accepts a <code>backend<\/code> that defaults to <code>pickle<\/code> but can be set to <code>joblib<\/code> if you want to use <code>joblib<\/code> instead.<\/p>\n\n<p>I'm just going to quickly note that Kedro is moving to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\"><code>kedro.extras.datasets<\/code><\/a> for its datasets and moving away from having non-core datasets in <code>kedro.io<\/code>. You might want to look at <code>kedro.extras.datasets<\/code> and in Kedro 0.16 onwards <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.extras.datasets.pickle.PickleDataSet.html#kedro.extras.datasets.pickle.PickleDataSet\" rel=\"nofollow noreferrer\"><code>pickle.PickleDataSet<\/code><\/a> with <code>joblib<\/code> support.<\/p>\n\n<p>The Kedro <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/02_tutorial_template.html\" rel=\"nofollow noreferrer\"><code>spaceflights<\/code><\/a> tutorial in the documentation actually saves the trained linear regression model using the <code>pickle<\/code> dataset if you want to see an example of it. The relevant section is <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html#working-with-multiple-pipelines\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1589277661208,
        "Solution_link_count":5.0,
        "Solution_readability":16.1,
        "Solution_reading_time":21.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":137.0,
        "Tool":"Kedro",
        "Challenge_type":"inquiry",
        "Challenge_summary":"store models in kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.0508991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>we have also found this example of using <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-use-databricks-as-compute-target.ipynb\">Databricks as a Compute Target for an Azure Machine Learning Pipeline<\/a>.  <\/p>\n<p>However, we want to use an existing Databricks Cluster as compute target within Azure Machine Learning Studio for our Azure Machine Learning Pipeline.  <br \/>\nCould you help us in accomplishing this, please?  <\/p>\n<p>With best regards  <br \/>\nAlex  <\/p>",
        "Challenge_closed_time":1654664851167,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654614267930,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking assistance in connecting an existing Databricks Cluster as a compute target within Azure Machine Learning Studio for their Azure Machine Learning Pipeline. They have found an example of using Databricks as a Compute Target for an Azure Machine Learning Pipeline but require guidance on how to use an existing cluster.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/880189\/connecting-to-an-existing-databricks-cluster-in-am",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.0508991667,
        "Challenge_title":"Connecting to an existing Databricks Cluster in AMLS",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@AlexanderPakakis-0994 Are you looking at adding the cluster from the UI of ML studio rather than using the SDK as mentioned in the notebook you referenced?    <br \/>\nIf Yes, you need to add the same attached compute.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209283-image.png?platform=QnA\" alt=\"209283-image.png\" \/>    <\/p>\n<p>Once you select Azure Databricks the following option to add the existing databricks workspace is seen.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209260-image.png?platform=QnA\" alt=\"209260-image.png\" \/>    <\/p>\n<p>I hope this helps!!    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":10.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.2,
        "Solution_reading_time":13.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"connect existing Databricks cluster"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":1.0551408334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to save Kedro memory dataset in azure as a file and still want to have it in memory as my pipeline will be using this later in the pipeline. Is this possible in Kedro. I tried to look at Transcoding datasets but looks like not possible. Is there any other way to acheive this?<\/p>",
        "Challenge_closed_time":1642520713270,
        "Challenge_comment_count":2,
        "Challenge_created_time":1642516914763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to save a Kedro memory dataset in Azure as a file while still keeping it in memory for later use in their pipeline. They have looked into transcoding datasets but have not found a solution. They are seeking advice on alternative methods to achieve this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70757448",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.1,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.0551408334,
        "Challenge_title":"How to save kedro dataset in azure and still have it in memory",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":231.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>This may be a good opportunity to use <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\">CachedDataSet<\/a> this allows you to wrap any other dataset, but once it's read into memory - make it available to downstream nodes without re-performing the IO operations.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":4.16,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":37.0,
        "Tool":"Kedro",
        "Challenge_type":"inquiry",
        "Challenge_summary":"save memory dataset in Azure"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":23.0099397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running local unsupervised learning (predominantly clustering) on a large, single node with GPU.<\/p>\n<p>Does SageMaker support <strong>distributed unsupervised learning<\/strong> using <strong>clustering<\/strong>?<\/p>\n<p>If yes, please provide the relevant example (preferably non-TensorFlow).<\/p>",
        "Challenge_closed_time":1663485400223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663402564440,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if SageMaker supports distributed unsupervised learning using clustering and is seeking an example, preferably non-TensorFlow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73753271",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":23.0099397222,
        "Challenge_title":"Distributed Unsupervised Learning in SageMaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":14.0,
        "Challenge_word_count":36,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>SageMaker Training allow you to bring your own training scripts, and supports various forms of distributed training, like data\/model parallel, and frameworks like PyTorch DDP, Horovod, DeepSpeed, etc.\nAdditionally, if you want to bring your data, but not code, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algorithms-unsupervised.html\" rel=\"nofollow noreferrer\">SageMaker training offers various unsupervised built-in algorithms<\/a>, some of which are parallelizable.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.2,
        "Solution_reading_time":6.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"distributed unsupervised clustering example"
    },
    {
        "Answerer_created_time":1459917054448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Frankfurt, Germany",
        "Answerer_reputation_count":9168.0,
        "Answerer_view_count":675.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a time series predictor with Keras and  Dockerized the model with with Flash and Gunicorn as per AWS docs. I am loading the serialized model with this code.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n    return cls.model\n<\/code><\/pre>\n\n<p>Then I used the predict method to produce the results , the dockerized container is working perfectly in the local environment , but when I try to host the model in sagemaker it produces this error.<\/p>\n\n<pre><code>ValueError: Tensor Tensor(\"dense_1\/BiasAdd:0\", shape=(?, 1), dtype=float32) is not an element of this graph.\n<\/code><\/pre>\n\n<p>So how can I resolve this issue ?<\/p>",
        "Challenge_closed_time":1541480239328,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541480239330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a ValueError while hosting a time series predictor model in Sagemaker with Gunicorn and Flask and Keras. The error message states that the Tensor is not an element of this graph. The model works fine in the local environment but produces an error when hosted in Sagemaker. The user is seeking a solution to resolve this issue.",
        "Challenge_last_edit_time":1541480860787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53165953",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"ValueError: Tensor is not an element of this graph, when hosting a model in Sagemaker with Gunicorn and Flask and Keras",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":223.0,
        "Challenge_word_count":117,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459917054448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Frankfurt, Germany",
        "Poster_reputation_count":9168.0,
        "Poster_view_count":675.0,
        "Solution_body":"<p>The issue was resolved by calling _make_predict_function() method in the model load phase.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n        cls.model._make_predict_function()\n    return cls.model\n<\/code><\/pre>\n\n<p>Bug Reference : <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/6462\" rel=\"nofollow noreferrer\">https:\/\/github.com\/keras-team\/keras\/issues\/6462<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.3,
        "Solution_reading_time":5.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Tensor not in graph"
    }
]