[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.2563888889,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to control a Spark cluster (using SparkR) from a Sagemaker notebook. I followed these instructions closely: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/  and got it to work.\n\nToday when I try to run the SageMaker notebook (using the exact same code as before) I inexplicably get this error:  \n\n    An error was encountered:\n    [1] \"Error in callJMethod(sparkSession, \\\"read\\\"): Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed.\"\n\nDoes anyone know why this is? I terminated the SparkR kernel and am still getting this error.\n",
        "Challenge_closed_time":1591041375000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591029652000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to control a Spark cluster using SparkR from a Sagemaker notebook and followed the instructions provided by AWS. However, when trying to run the notebook again, the user encountered an error message stating that the Spark operations need to be re-executed. The user terminated the SparkR kernel but is still getting the same error.",
        "Challenge_last_edit_time":1667926595812,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqyiKb_XvRhGxm1RxwjXhJQ\/sparkr-not-working",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.2563888889,
        "Challenge_title":"SparkR not working",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":75.0,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You cannot have multiple SparkContexts in one JVM. The issue is resolved as WON'T FIX. You have to stop the spark session which spawned the sparkcontext (which you have already done). \n\n`sparkR.session.stop()`\n\nhttps:\/\/issues.apache.org\/jira\/browse\/SPARK-2243",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925571654,
        "Solution_link_count":1.0,
        "Solution_readability":7.6,
        "Solution_reading_time":3.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":36.3344202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to predict from my past data which has around 20 attribute columns and a label. Out of those 20, only 4 are significant for prediction. But i also want to know that if a row falls into one of the classified categories, what other important correlated columns apart from those 4 and what are their weight. I want to get that result from my deployed web service on Azure.<\/p>",
        "Challenge_closed_time":1461985174656,
        "Challenge_comment_count":1,
        "Challenge_created_time":1461854370743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to predict using past data with 20 attribute columns and a label, but only 4 columns are significant for prediction. The user wants to know the weight of other important correlated columns apart from those 4, and wants to get the result from the deployed web service on Azure.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36917948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":5.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":36.3344202778,
        "Challenge_title":"Feature weightage from Azure Machine Learning Deployed Web Service",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":303.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461853741067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You can use permutation feature importance module but that will give importance of the features across the sample set. Retrieving the weights on per call basis is not available in Azure ML.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":2.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1645796339972,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":29.3345611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have thousands of training jobs that I want to run on sagemaker. Basically I have a list of hyperparameters and I want to train the model for <em>all<\/em> of those hyperparmeters in parallel (not a standard hyperparameter tuning where we just want to optimize the hyperparameter, here we want to train for all of the hyperparameters). I have searched the docs quite extensively but it surprises me that I couldn't find any info about this, even though it seems like a pretty basic functionality.<\/p>\n<p>For example, let's say I have 10,000 training jobs, and my quota is 20 instances, what is the best way to run these jobs utilizing all my available instances? In particular,<\/p>\n<ul>\n<li>Is there a &quot;queue manager&quot; functionality that takes the list of hyperparameters and runs the training jobs in batches of 20 until they are all done (even better if it could keep track of failed\/completed jobs).<\/li>\n<li>Is it best practice to run a single training job per instance? If that's the case do I need to ask for a much higher quota on the number of instance?<\/li>\n<li>If this functionality does not exist in sagemaker, is it worth using EC2 instead since it's a bit cheaper?<\/li>\n<\/ul>",
        "Challenge_closed_time":1650614502720,
        "Challenge_comment_count":2,
        "Challenge_created_time":1650506611153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to run thousands of training jobs on SageMaker for all hyperparameters in parallel, but cannot find any information on how to do so. They are looking for a queue manager functionality to run the jobs in batches and keep track of failed\/completed jobs. They are also unsure if it is best practice to run a single training job per instance and if they need a higher instance quota. The user is considering using EC2 instead if this functionality does not exist in SageMaker.",
        "Challenge_last_edit_time":1650508898300,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71948090",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":15.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":29.9698797222,
        "Challenge_title":"Best way to run 1000s of training jobs on sagemaker",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":269.0,
        "Challenge_word_count":213,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622388181327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":103.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Your question is very broad and the best way forward would depend on other details of your use-case, so we will have to make some assumptions.<\/p>\n<p>[Queue manager]\nSageMaker does <em>not<\/em> have a queue manager. If at the end you decide you need a queue manager, I would suggest looking towards AWS Batch.<\/p>\n<p>[Single vs multiple training jobs]\nSince you need to run 10s of thousands job I assume you are training fairly lightweight models, so to save on time, you would be better off reusing instances for multiple training jobs. (Otherwise, with 20 instances limit, you need 500 rounds of training, with a 3 min start time - depending on instance type - you need 25 hours just for the wait time. Depending on the complexity of each individual model, this 25hours might be significant or totally acceptable).<\/p>\n<p>[Instance limit increase]\nYou can always ask for a limit increase, but going from a limit of 20 to 10k at once is likely that will not be accepted by the AWS support team, unless you are part of an organisation with a track record of usage on AWS, in which case this might be fine.<\/p>\n<p>[One possible option] (Assuming multiple lightweight models)\nYou could create a single training job, with instance count, the number of instances available to you.\nInside the training job, your code can run a for loop and perform all the individual training jobs you need.<\/p>\n<p>In this case, you will need to know which which instance is which so you can make the split of the HPOs. SageMaker writes this information on the file: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-dist-training\" rel=\"nofollow noreferrer\">\/opt\/ml\/input\/config\/resourceconfig.json<\/a> so using that you can easily have each instance run a subset of the trainings required.<\/p>\n<p>Another thing to think of, is if you need to save the generated models (which you probably need). You can either save everything in the output model directory - standard SM approach- but this would zip all models in a model.tar.gz file.\nIf you don't want this, and prefer to have each model individually saved, I'd suggest using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">checkpoints<\/a> directory that will sync anything written there to your s3 location.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.5,
        "Solution_reading_time":30.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":359.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341273154903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":5996.0,
        "Answerer_view_count":666.0,
        "Challenge_adjusted_solved_time":0.1159838889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Challenge_closed_time":1659257407332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655350561587,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running an ML pipeline's training component\/step. The error message states that the return type is unknown and must be one of the specified types. The user has provided the code for the training component and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1659256989790,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":44.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":1085.2349291667,
        "Challenge_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":278,
        "Platform":"Stack Overflow",
        "Poster_created_time":1621620820567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":28.3,
        "Solution_reading_time":27.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":107.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":66.9054508334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We know that Horovod is suppported. Is there an example script which uses DistributedDataParallel and Pytorch estimator?<\/p>",
        "Challenge_closed_time":1574100412503,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573859552880,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking an example script that uses DistributedDataParallel with PyTorch estimator, as they are aware that Horovod is supported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58885873",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":66.9054508334,
        "Challenge_title":"Is it possible to use DistributedDataParallel with PyTorch Estimator",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":98.0,
        "Challenge_word_count":25,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488866265607,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>You should be able to specify nccl or gloo as distributed data parallel backend, in addition to MPI with Horovod. See the <em>distributed_training<\/em> parameter of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.pytorch?view=azure-ml-py\" rel=\"nofollow noreferrer\">PyTorch Estimator<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.5,
        "Solution_reading_time":4.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":0.2453072222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my Azure ML pipeline I've got a PythonScriptStep that is crunching some data. I need to access the Azure ML Logger to track metrics in the step, so I'm trying to import get_azureml_logger but that's bombing out. I'm not sure what dependency I need to install via pip. <\/p>\n\n<p><code>from azureml.logging import get_azureml_logger<\/code><\/p>\n\n<p><code>ModuleNotFoundError: No module named 'azureml.logging'<\/code><\/p>\n\n<p>I came across a similar <a href=\"https:\/\/stackoverflow.com\/questions\/49438358\/azureml-logging-module-not-found\">post<\/a> but it deals with Azure Notebooks. Anyway, I tried adding that blob to my pip dependency, but it's failing with an Auth error.   <\/p>\n\n<pre><code>Collecting azureml.logging==1.0.79 [91m  ERROR: HTTP error 403 while getting\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n[0m91m  ERROR: Could not install requirement azureml.logging==1.0.79 from\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n(from -r \/azureml-environment-setup\/condaenv.g4q7suee.requirements.txt\n(line 3)) because of error 403 Client Error:\nServer failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. for url:\nhttps:\/\/azuremldownloads.blob.core.windows.net\/wheels\/latest\/azureml.logging-1.0.79-py3-none-any.whl?sv=2016-05-31&amp;si=ro-2017&amp;sr=c&amp;sig=xnUdTm0B%2F%2FfknhTaRInBXyu2QTTt8wA3OsXwGVgU%2BJk%3D\n<\/code><\/pre>\n\n<p>I'm not sure how to move on this, all I need to do is to log metrics in the step.  <\/p>",
        "Challenge_closed_time":1587756432003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587755548897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing the Azure ML Logger to track metrics in a PythonScriptStep of their Azure ML pipeline. They are trying to import 'get_azureml_logger' but are encountering a 'ModuleNotFoundError'. They tried to add a dependency via pip but it failed with an Auth error. The user is unsure how to proceed and needs to log metrics in the step.",
        "Challenge_last_edit_time":1587809992912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61415793",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":24.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":0.2453072222,
        "Challenge_title":"Log metrics in PythonScriptStep",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>Check out the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-track-experiments#option-2-use-scriptrunconfig\" rel=\"nofollow noreferrer\">ScriptRunConfig Section of the Monitor Azure ML experiment runs and metrics<\/a>. <code>ScriptRunConfig<\/code> works effectively the same as a <code>PythonScriptStep<\/code>.<\/p>\n\n<p>The idiom is generally to have the following in your the script of your <code>PythonScriptStep<\/code>:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nrun.log('foo_score', \"bar\")\n<\/code><\/pre>\n\n<p>Side note: You don't need to change your environment dependencies to use this because <code>PythonScriptStep<\/code>s have <code>azureml-defaults<\/code> installed automatically as a dependency.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.2,
        "Solution_reading_time":10.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":0.2048830556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I execute code without parallel computation, <code>n_trials<\/code> in the <code>optimize<\/code> function means how many trials the program runs. When executed via parallel computation (following the tutorial <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/10_key_features\/004_distributed.html\" rel=\"nofollow noreferrer\">here<\/a> via launching it again in another console), it does <code>n_trials<\/code> for each process, not for all the sum of processes like I would like.<\/p>\n<p>Is there a way to make sure that the sum of all parallel processes' trials are equal to a fixed number, regardless of how many process I launch?<\/p>",
        "Challenge_closed_time":1629887227052,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629886489473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with setting the n_trials parameter for multiple processes when using parallelization. When executing code via parallel computation, n_trials is executed for each process instead of the sum of all processes. The user is seeking a way to ensure that the total number of trials is fixed regardless of the number of processes launched.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68920952",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2048830556,
        "Challenge_title":"How to set n_trials for multiple processes when using parallelization?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529092998780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":788.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Yes, <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.MaxTrialsCallback.html#optuna.study.MaxTrialsCallback\" rel=\"nofollow noreferrer\"><code>MaxTrialsCallback<\/code><\/a> is the exact feature for such a situation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":38.9,
        "Solution_reading_time":3.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.7569444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\nI've just trained a churn prediction model with XGBoost algorithm, based on the SageMaker example notebooks.  I've created SageMaker batch transformation jobs using this model using input from CSV file with multiple records, however the output file is a single record CSV containing all the inferences in a single comma separated row.  The result is that I'm not able to use the \"Join source\" feature with \"Input  - Merge input data with job output\" since the input and output files must match the number of records. I've tried with different batch job configurations but I always get the same single line output file.\n\nDo you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? Is this a restriction from the XGBoost algorithm built-in implementation?",
        "Challenge_closed_time":1599791910000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599771185000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a churn prediction model with XGBoost algorithm and created SageMaker batch transformation jobs using this model. However, the output file is a single record CSV containing all the inferences in a single comma separated row, which is causing difficulty in using the \"Join source\" feature with \"Input - Merge input data with job output\". The user is seeking a configuration that allows merging input and output to have a direct association between an input column with its inference result and is unsure if this is a restriction from the XGBoost algorithm built-in implementation.",
        "Challenge_last_edit_time":1668594259996,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUYz7Bz_5sTmG0uBaqlt7J_g\/xgboost-sagemaker-batch-transform-job-output-in-multiple-lines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":11.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":5.7569444444,
        "Challenge_title":"xgboost sagemaker batch transform job output in multiple lines",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Sounds like a configuration issue, this algorithm should be able to output proper output CSVs.\n\nAre you using `accept=\"text\/csv\"` and `assemble_with=\"Line\"` on your `Transformer`? Is your `strategy` set to `SingleRecord` or `MultiRecord`?\n\nAnd `split_type=\"Line\"`, `content_type=\"text\/csv\"` on the `.transform()` call?\n\nI have had custom algorithms accidentally output row vectors instead of column vectors for multi-record batches in the past (because they gave a 1D output which the default serializer interpreted as a row), but not built-in algorithms.\n\nDropping to `SingleRecord` could be a last resort (forcing Batch Transform itself to handle the serialization), but would decrease efficiency\/speed.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1607690229838,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":8.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":96.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":24.8830416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a webservice which exposes a predictive model. It has been deployed with Auzure ML Studio. Since the last model re-training and webservice deployment, in circa 1% of the cases in production, I get the following out-of-memory (possibly correlated) errors:<\/p>\n<p>1) &quot;The model consumed more memory than was appropriated for it. Maximum allowed memory for the model is 2560 MB. Please check your model for issues.&quot;  <br \/>\n2) &quot;The following error occurred during evaluation of R script: R_tryEval: return error: Error: cannot allocate vector of size 57.6 Mb&quot;<\/p>\n<p>Please note that these errors occur exclusively while trying to consume the webservice, and not while model training, evaluation and deployment.<\/p>\n<p>Also, consuming the webservice in batch mode, as suggested <a href=\"https:\/\/social.microsoft.com\/Forums\/azure\/he-IL\/ccf4c683-f904-4117-8a4e-3258a56515f9\/azureml-execure-r-script-cannot-allocate-vector-of-size-818-mb?forum=MachineLearning\">here<\/a>, is not a viable option for our business use case.<\/p>\n<p>Is there a way to increase the memory limit for Azure webservices?<\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":1592916913067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592827334117,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered out-of-memory errors while consuming a webservice that exposes a predictive model deployed with Azure ML Studio. The errors occur in approximately 1% of the cases in production and are related to memory allocation. The user is seeking a solution to increase the memory limit for Azure webservices as consuming the webservice in batch mode is not a viable option for their business use case.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/38509\/out-of-memory-error-webservice-deployed-with-azure",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":15.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":24.8830416667,
        "Challenge_title":"Out-of-memory error webservice deployed with Azure ML Studio",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for reaching out. Currently, there's no way to increase memory limit in Classic Studio. We encourage customers to try <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-designer\">Azure Machine Learning designer (preview)<\/a>, which provides similar drag and drop ML modules plus scalability, version control, and enterprise security. Furthermore, with Designer, the endpoints are deployed to AKS where no limit other than cluster resource is imposed.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.5,
        "Solution_reading_time":6.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":60.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2830.1302777778,
        "Challenge_answer_count":0,
        "Challenge_body":"At the moment, an mlflow byom predictor with arbitrary URLs can be created. We should first check whether an actual mlflow model is served at that URL before creating\/linking said model.",
        "Challenge_closed_time":1656947080000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1646758611000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug in pycaret where started runs in MlflowLogger are never ended, resulting in all runs shown in MLflow dashboard being nested recursively. This bug is problematic as it affects the display of deeply nested runs. The user has provided a reproducible example and expected behavior, but the actual display is not as expected. The installed version of pycaret is 2.3.10.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/mindsdb\/mindsdb\/issues\/2043",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":241.0,
        "Challenge_repo_fork_count":1404.0,
        "Challenge_repo_issue_count":4035.0,
        "Challenge_repo_star_count":12007.0,
        "Challenge_repo_watch_count":327.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2830.1302777778,
        "Challenge_title":"[ BYOM MLflow ] Check valid URL when creating predictor",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Can we close this @paxcema and @ea-rus  I think we need to merge the above PR after checking there are no conflicts (because it's a bit outdated by now), but once merged we can close this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":2.31,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1566583092316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":479.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":8.3681258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Yesterday I have install tensorflow module from iPython notebook from Azure machine learning studio (classic) version. The import worked well after installing the module using (!pip install tensorflow). But today when tried to import this module got this \"missing module\" error and when I tried reinstalling the module it works well. Am I missing anything here? \nDo I need to install the module each and everyday, before using it? Can someone please explain?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rI7hE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rI7hE.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1578608056696,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578516766930,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user installed the tensorflow module on iPython notebook from Azure machine learning studio (classic) version and it worked well. However, the next day when trying to import the module, they received a \"missing module\" error. Reinstalling the module fixed the issue, but the user is unsure if they need to reinstall the module every day before using it.",
        "Challenge_last_edit_time":1578577931443,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59653641",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":8.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":25.3582683333,
        "Challenge_title":"Missing module tensorflow on iPython azure machine learning (Classic)",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":90,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500744375327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":255.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>For Azure Machine Learning (Classic) Studio notebooks, you need to install Tensorflow. Furthermore, the notebook server session times out after a period of inactivity, hence, you need to re-install Tensorflow once the server shuts down or after starting a new session. Thanks.<\/p>\n\n<p>Here are some references:<\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/help\/jupyter-notebooks\/timeouts\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/help\/jupyter-notebooks\/timeouts<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/notebooks\/install-packages-jupyter-notebook\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/notebooks\/install-packages-jupyter-notebook<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.2,
        "Solution_reading_time":9.45,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433761344910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Aruppukkottai, India",
        "Answerer_reputation_count":802.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":0.9015666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a blazingText model and followed this guide.\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html<\/a><\/p>\n\n<p>\"Sample JSON request\" The Invoke end point is working perfectly. So I switched to,\nBatch Transform Job with \"content-type: application\/jsonlines\" and created a file in S3 with the following format data:<\/p>\n\n<pre><code>{\"source\": \"source_0\"}\n<\/code><\/pre>\n\n<p>The job ran success. But the output did not sent to S3. Also In the cloud logs,<\/p>\n\n<pre><code>\" [79] [INFO] Booting worker with pid: 79\"\n<\/code><\/pre>\n\n<p>This is is the last response. Did anyone know what went wrong?<\/p>",
        "Challenge_closed_time":1543477871070,
        "Challenge_comment_count":2,
        "Challenge_created_time":1543474625430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a BlazingText model on AWS Sagemaker and successfully tested the Invoke endpoint. However, when attempting to use Batch Transform Job with a JSONlines file in S3, the job ran successfully but did not send output to S3. The last response in the cloud logs was \"Booting worker with pid: 79\". The user is seeking assistance in identifying the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53533434",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.3,
        "Challenge_reading_time":10.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.9015666667,
        "Challenge_title":"AWS Sagemaker - Blazingtext BatchTransform no output",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":317.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1433761344910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Aruppukkottai, India",
        "Poster_reputation_count":802.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>I have found the issue. The batchtransform select the folder as input and the s3 source should be S3Prefix instead of manifest.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":34.1587602778,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<br>\nMe and a colleague are sharing a remote server with 8 GPUs. We split them, 4 GPUs each. In the system panel at the WANDB page I currently see data of all 8 GPUs. Is it possible to filter some of those curves, so I\u2019ll only see the GPUs I\u2019m using?<\/p>\n<p>Many thanks<\/p>",
        "Challenge_closed_time":1663689312779,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663566341242,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user and their colleague are sharing a remote server with 8 GPUs, split between them. They want to know if it's possible to filter the GPU curves in the system panel at the WANDB page to only see the GPUs they are using.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/filter-gpu-curves-in-the-system-panel\/3152",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":3.7,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":34.1587602778,
        "Challenge_title":"Filter GPU curves in the system panel",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/yonatan-shimoni\">@yonatan-shimoni<\/a> thank you for writing in! Just to clarify here, is this for the System view that you can access from the left panel of an individual Run, or is it at the System panels section in your Project\u2019s Workspace? You can select which GPUs to visualise there by editing the Chart (pencil icon) as in the attached screenshot. Would this help?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818.png\" data-download-href=\"\/uploads\/short-url\/uFNI8njAc6A8srPIjwcR86x4AZi.png?dl=1\" title=\"Screenshot 2022-09-20 at 16.50.40\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_476x500.png\" alt=\"Screenshot 2022-09-20 at 16.50.40\" data-base62-sha1=\"uFNI8njAc6A8srPIjwcR86x4AZi\" width=\"476\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_476x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_714x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_952x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-09-20 at 16.50.40<\/span><span class=\"informations\">1098\u00d71153 76.4 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":22.0,
        "Solution_reading_time":26.25,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":113.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1543010383463,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":404.8255119445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I send a TensorFlow training job to a SageMaker instance, what is the typical way to view training progress? Can I access TensorBoard for this launched EC2 instance? Is there some other alternative? What I'm looking for specifically are things like graphs of current training epoch and mAP.<\/p>",
        "Challenge_closed_time":1543010856183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541553484340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for ways to view the training progress of a TensorFlow job sent to a SageMaker instance, specifically graphs of current training epoch and mAP. They are asking if they can access TensorBoard or if there is another alternative.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53182436",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":4.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":404.8255119445,
        "Challenge_title":"SageMaker: visualizing training statistics",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>you can now specify metrics(metricName, Regex) that you want to track by using AWS management console or Amazon SageMaker Python SDK APIs. After the model training starts, Amazon SageMaker will automatically monitor and stream the specified metrics in real time to the Amazon CloudWatch console for visualizing time-series curves. <\/p>\n\n<p>Ref: \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":7.11,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1420546067812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":876.6783925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My <em>PySpark<\/em> dataset contains categorical data.<\/p>\n<p>To train a model on this data, I followed this <a href=\"https:\/\/docs.databricks.com\/_static\/notebooks\/binary-classification.html\" rel=\"nofollow noreferrer\">example notebook<\/a>. Especially, see the <em>Preprocess Data<\/em> section for the encoding part.<\/p>\n<p>I now need to use this model somewhere else; hence, I followed <em>Databricks<\/em> recommendation to save and load this model.<\/p>\n<p>It's working fine with <em>Pandas<\/em> (cf. code below).<\/p>\n<pre><code>logged_model = 'runs:\/e905f5759d434a1391bbe1e54a2b\/best-model'\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n# Predict on a Pandas DataFrame.\nimport pandas as pd\nloaded_model.predict(pd.DataFrame(data))\n<\/code><\/pre>\n<p>However the dataframe is to big to be converted to <em>Pandas<\/em>. Hence I need to make it work in <em>Spark<\/em>:<\/p>\n<pre><code>import mlflow\nlogged_model = 'runs:\/e905f5759d434a131bbe1e54a2b\/best-model'\n\n# Load model as a Spark UDF.\nloaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)\n\n# Predict on a Spark DataFrame.\ndf.withColumn('predictions', loaded_model(*columns)).collect()\n<\/code><\/pre>\n<p>But this snippet is producing:<\/p>\n<pre><code>java.lang.UnsupportedOperationException: Unsupported data type: struct&amp;lt;type:tinyint,size:int,indices:array&amp;lt;int&amp;gt;,values:array&amp;lt;double&amp;gt;&amp;gt;\n<\/code><\/pre>\n<p>My feeling is that the udf doesn't accept this type of data as input.\nIs there a way to fix it ?\nAnother solution ?<\/p>",
        "Challenge_closed_time":1629378460768,
        "Challenge_comment_count":1,
        "Challenge_created_time":1627291388767,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue while trying to use mlflow.pyfunc.spark_udf to predict on a Spark DataFrame containing categorical data. The code is producing an error message stating that the struct type of the data is unsupported. The user is seeking a solution to fix this issue or an alternative solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68527422",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":21.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":579.7422225,
        "Challenge_title":"mlflow.pyfunc.spark_udf and vector struct type",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":790.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480398962230,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":440.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Have you tried using the <code>mlflow.spark.load_model<\/code>?<\/p>\n<p>I'm having a very similar issue over here, but but using the spark method. I tried using the <code>mlflow.spark.load_model('runs:\/run-id\/my-model')<\/code> method and I got this weird error:<\/p>\n<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\/dbfs\/tmp\/mlflow\/weird-id-folder'\n<\/code><\/pre>\n<p>Searching for the docs, I see the problem that we are facing (which seems to be different), seems to be a signature problem.<\/p>\n<p>According with other part of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-signature\" rel=\"nofollow noreferrer\">docs<\/a> we have that the signature logged with the model will help to define what type of input the model has. The problem for me here is that my input is a Spark Sparse Vector -- which is not supported... Right now I'm trying to convert that into a column-based signature.<\/p>\n<p>Have you tried something like this?<\/p>\n<hr \/>\n<p>UPDATE:<\/p>\n<p>I would like to add that in my case adding the signature did solve the problem. All I did was ignore the vectors and consider only the input data and output data.<\/p>\n<p>I took a look into the notebook, but haven't seen any mlflow logs, anyway, I do suppose you are logging your experiment according to <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/tracking.html#log-runs-to-a-notebook-or-workspace-experiment\" rel=\"nofollow noreferrer\">this<\/a> and using the <code>mlflow.spark<\/code> flavor.<\/p>\n<p>If so, consider using all your data transformation and model fit in the same pipeline, using <code>from pyspark.ml import Pipeline<\/code>. Before logging the model, consider going under signature and registering the model schema.<\/p>\n<pre><code>import mlflow.spark\nfrom mlflow.models.signature import infer_signature\n\nwith mlflow.start_run():\n    [...]\n    # executing train &amp; test pipelines:\n    model = pipeline.fit(train_features) # training model\n    predictions = model.transform(test_features) # testing model\n    train_signature = train_features.select('input_data') # ignores all other features created on the pipeline\n    prediction_signature = predictions.select('input_data', 'prediction') # ignores all other features created on the training pipeline \n    signature = infer_signature(train_signature, prediction_signature) # register model schema\n    mlflow.spark.log_model(model, 'transactions-classification', signature=signature) # logging model to mlflow\n    [...]\n<\/code><\/pre>\n<p>After logging the model to the experiment, in a different notebook, you can use the load_model function as:<\/p>\n<pre><code># importing model\nimport mlflow.spark\nmodel_path = 'runs:\/run-id'\nmodel = mlflow.spark.load_model(model_path)\n<\/code><\/pre>\n<p>And it will work! :D<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1630447430980,
        "Solution_link_count":2.0,
        "Solution_readability":11.3,
        "Solution_reading_time":35.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":320.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1386279656143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":1352.0,
        "Answerer_view_count":229.0,
        "Challenge_adjusted_solved_time":1.6610233334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>DVC uses git commits to save the experiments and navigate between experiments.<\/p>\n<p>Is it possible to avoid making auto-commits in CI\/CD (to save data artifacts after <code>dvc repro<\/code> in CI\/CD side).<\/p>",
        "Challenge_closed_time":1587029761967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587023782283,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning the necessity of committing DVC files from their CI pipelines and is seeking a way to avoid auto-commits in CI\/CD while still saving data artifacts after running \"dvc repro\".",
        "Challenge_last_edit_time":1594640021920,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61245284",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":3.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.6610233334,
        "Challenge_title":"Is it necessary to commit DVC files from our CI pipelines?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1047.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558529684192,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":79.0,
        "Poster_view_count":14.0,
        "Solution_body":"<blockquote>\n  <p>will you make it part of CI pipeline<\/p>\n<\/blockquote>\n\n<p>DVC often serves as a part of MLOps infrastructure. There is a popular <a href=\"https:\/\/martinfowler.com\/articles\/cd4ml.html\" rel=\"noreferrer\">blog post about CI\/CD for ML<\/a> where DVC is used under the hood. <a href=\"https:\/\/blog.codecentric.de\/en\/2020\/01\/remote-training-gitlab-ci-dvc\/\" rel=\"noreferrer\">Another example<\/a> but with GitLab CI\/CD.<\/p>\n\n<blockquote>\n  <p>scenario where you will integrate dvc commit command with CI\n  pipelines?<\/p>\n<\/blockquote>\n\n<p>If you mean <code>git commit<\/code> of DVC files (not <code>dvc commit<\/code>) then yes, you need to commit dvc-files into Git during CI\/CD process. Auto-commit is not the best practice.<\/p>\n\n<p>How to avoid Git commit in CI\/CD:<\/p>\n\n<ol>\n<li>After ML model training in CI\/CD, save changed dvc-files in external storage (for example GitLab artifact\/releases), then get the files to a developer machine and commit there. Users usually write scripts to automate it.<\/li>\n<li>Wait for DVC 1.0 release when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1234\" rel=\"noreferrer\">run-cache (like build-cache)<\/a> will be implemented. Run-cache makes dvc-files ephemeral and no additional Git commits will be required. Technically, run-cache is an associative storage <code>repo state --&gt; run results<\/code> outside of Git repo (in data remote).<\/li>\n<\/ol>\n\n<p>Disclaimer: I'm one of the creators of DVC.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.2,
        "Solution_reading_time":18.47,
        "Solution_score_count":6.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":183.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1291793452900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berkeley, CA, United States",
        "Answerer_reputation_count":752.0,
        "Answerer_view_count":120.0,
        "Challenge_adjusted_solved_time":3886.8446766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Short form:\nI am trying to figure out how can I run the hyperparam within a <strong>training step<\/strong> (i.e. train_step = PythonScriptStep(...)) in the pipeline, I am not sure where shall I put the &quot;config=hyperdrive&quot;<\/p>\n<p>Long form:<\/p>\n<p>General:<\/p>\n<pre><code># Register the environment \ndiabetes_env.register(workspace=ws)\nregistered_env = Environment.get(ws, 'diabetes-pipeline-env')\n\n# Create a new runconfig object for the pipeline\nrun_config = RunConfiguration()\n\n# Use the compute you created above. \nrun_config.target = ComputerTarget_Crea\n\n# Assign the environment to the run configuration\nrun_config.environment = registered_env\n<\/code><\/pre>\n<p>Hyperparam:<\/p>\n<pre><code>script_config = ScriptRunConfig(source_directory=experiment_folder,\n                                script='diabetes_training.py',\n                                # Add non-hyperparameter arguments -in this case, the training dataset\n                                arguments = ['--input-data', diabetes_ds.as_named_input('training_data')],\n                                environment=sklearn_env,\n                                compute_target = training_cluster)\n\n# Sample a range of parameter values\nparams = GridParameterSampling(\n    {\n        # Hyperdrive will try 6 combinations, adding these as script arguments\n        '--learning_rate': choice(0.01, 0.1, 1.0),\n        '--n_estimators' : choice(10, 100)\n    }\n)\n\n# Configure hyperdrive settings\nhyperdrive = HyperDriveConfig(run_config=script_config, \n                          hyperparameter_sampling=params, \n                          policy=None, # No early stopping policy\n                          primary_metric_name='AUC', # Find the highest AUC metric\n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=6, # Restict the experiment to 6 iterations\n                          max_concurrent_runs=2) # Run up to 2 iterations in parallel\n\n# Run the experiment if I only want to run hyperparam alone without the pipeline\n#experiment = Experiment(workspace=ws, name='mslearn-diabetes-hyperdrive')\n#run = experiment.submit(**config=hyperdrive**)\n<\/code><\/pre>\n<p>PipeLine:<\/p>\n<pre><code>prep_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;prep_diabetes.py&quot;,\n                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n                                             '--prepped-data', prepped_data_folder],\n                                outputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n\n# Step 2, run the training script\ntrain_step = PythonScriptStep(name = &quot;Train and Register Model&quot;,\n                                source_directory = experiment_folder,\n                                script_name = &quot;train_diabetes.py&quot;,\n                                arguments = ['--training-folder', prepped_data_folder],\n                                inputs=[prepped_data_folder],\n                                compute_target = ComputerTarget_Crea,\n                                runconfig = run_config,\n                                allow_reuse = True)\n# Construct the pipeline\npipeline_steps = [prep_step, train_step]\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\nprint(&quot;Pipeline is built.&quot;)\n\n# Create an experiment and run the pipeline\n**#How do I need to change these below lines to use hyperdrive????**\nexperiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\npipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n<\/code><\/pre>\n<p>Not sure where I need to put <strong>config=hyperdrive<\/strong> in the Pipeline section?<\/p>",
        "Challenge_closed_time":1633922581063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619929277080,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to figure out how to run hyperparameters within a training step in a pipeline using AzureML SDK. They have already created a hyperdrive configuration and a pipeline with two steps, but they are unsure where to put \"config=hyperdrive\" in the pipeline section to use hyperdrive.",
        "Challenge_last_edit_time":1619929940227,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67352949",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":20.0,
        "Challenge_reading_time":41.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":3887.0288841667,
        "Challenge_title":"How to combine pipeline and hyperparameter in AzureML SDK in the training step",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":276.0,
        "Challenge_word_count":280,
        "Platform":"Stack Overflow",
        "Poster_created_time":1375186444008,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Auckland, New Zealand",
        "Poster_reputation_count":912.0,
        "Poster_view_count":288.0,
        "Solution_body":"<p>here's how to combine hyperparameters with an AML pipeline: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.hyperdrivestep?view=azure-ml-py<\/a><\/p>\n<p>Alternatively, here's a sample notebook: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":57.4,
        "Solution_reading_time":11.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":245.3151869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting the following error from the Evaluate Model module in Azure Machine Learning Designer:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/101409-screenshot-2021-06-01-at-100708-pm.png?platform=QnA\" alt=\"101409-screenshot-2021-06-01-at-100708-pm.png\" \/>    <\/p>\n<p>When I open the Assigned Data to Clusters module everything seems fine. I downloaded the output for Assigned Data to Clusters and played with cluster number 31 and there doesn't seem to be any issue. Additionally, I am using Azure Modules, so I am confused as to why this is failing. Please provide some clarity into this issue. This is a part of my pipeline:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/101399-screenshot-2021-06-01-at-104512-pm.png?platform=QnA\" alt=\"101399-screenshot-2021-06-01-at-104512-pm.png\" \/>    <\/p>\n<p>Additionally, it seems unless I successfully run the Evaluate Model module, I cannot create an inference pipeline. If this is untrue, please help me out here as well. There is no option for me to 'Create an Inference Pipeline' which shown in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">this tutorial; step 1.<\/a>    <\/p>\n<p>Please let me know if you need any other information.    <\/p>\n<p>Thanks in advance.    <\/p>",
        "Challenge_closed_time":1623450957476,
        "Challenge_comment_count":3,
        "Challenge_created_time":1622567822803,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an ambiguous error in the Azure Machine Learning Designer's 'Evaluate Model' module. They have checked the output for the 'Assigned Data to Clusters' module and found no issues. The user is also confused about the inability to create an inference pipeline unless the 'Evaluate Model' module runs successfully. They are seeking clarity on the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/418016\/ambiguous-error-in-azure-machine-learning-designer",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.7,
        "Challenge_reading_time":18.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":245.3151869444,
        "Challenge_title":"Ambiguous error in Azure Machine Learning Designer 'Evaluate Model' Module",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":163,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Can you please check if the Assignment cluster 31 has NaN value? The <strong>Assign Data to Clusters<\/strong> leverages SKlearn, and from the error message, seems the Assignment column had NaN value which resulted in an error. If that's the case, let us know, so we can enable <strong>Evaluate<\/strong> Module module to deal with NaN values, and in the meantime, here's a short-term workaround:    <\/p>\n<ul>\n<li> Connect <strong>Clean Missing Data<\/strong> module to Assign Data to Cluster module, to clean the missing values.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/104943-image.png?platform=QnA\" alt=\"104943-image.png\" \/>    <\/li>\n<li> Use <strong>Edit Metadata<\/strong> module to convert Assignment to Integer and categorical type, this is because if Assignment column has NaN value before and its column type was double, we need to convert it to integer.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/104888-image.png?platform=QnA\" alt=\"104888-image.png\" \/>    <\/li>\n<li> Connect <strong>Edit Metadata<\/strong> to Evaluate Model module.    <\/li>\n<\/ul>\n<p>Hope this help!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":14.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":142.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.0674716667,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to do some inference on some CSV file saved on s3 using BatchTransform with `strategy ='MultiRecord'` and `assemble_with='Line'`. The same system works with `strategy ='SingleRecord'`, however I need it to be as efficient as possible. The main issue comes when I switch to MultiRecord with a small csv composed of two columns, both texts.\nWith a `max_payload = 6`, the process is succesfull with a CSV of 181 samples (609.7KB), but with the same CSV with 182 samples (621.6KB), the process fails with a `\"message\": \"Worker died.\"`. I imagined it had something to do with the memory limit of the instance I am using, so I switched to a `ml.m5.2xlarge` with 32GB of memory. \nWhen I switch to a `max_payload = 7`, suddenly the process works with the 182 samples of CSV (621.6KB), but it fails with anything bigger than that. Any ideas of what could be causing this?\n\nThe logs look like this\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,218 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 0.00\/8.68M [00:00<?, ?B\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,327 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 20.0k\/8.68M [00:00<00:47, 191kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,434 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 1%| | 100k\/8.68M [00:00<00:17, 526kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,543 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 3%|\u258e | 228k\/8.68M [00:00<00:10, 841kB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,651 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 6%|\u258c | 509k\/8.68M [00:00<00:05, 1.56MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,759 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 12%|\u2588\u258f | 1.04M\/8.68M [00:00<00:02, 2.91MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,867 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 25%|\u2588\u2588\u258d | 2.14M\/8.68M [00:00<00:01, 5.54MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,975 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 50%|\u2588\u2588\u2588\u2588\u2589 | 4.31M\/8.68M [00:00<00:00, 10.6MB\/s]\n\n2023-05-19T12:49:51.989+03:00\t2023-05-19T09:49:51,976 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 8.65M\/8.68M [00:00<00:00, 20.7MB\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,129 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.68M\/8.68M [00:00<00:00, 10.5MB\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,129 [WARN ] W-9007-model_1.0-stderr MODEL_LOG -\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,131 [WARN ] W-9007-model_1.0-stderr MODEL_LOG - Downloading: 0%| | 0.00\/615 [00:00<?, ?B\/s]\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,894 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13647\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,895 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13459\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,895 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:15108|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,896 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:56|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,897 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489793897\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,898 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13705\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,898 [INFO ] W-9007-model_1.0 TS_METRICS - W-9007-model_1.0.ms:15111|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,899 [INFO ] W-9007-model_1.0 TS_METRICS - WorkerThreadTime.ms:42|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,896 [INFO ] W-9005-model_1.0 TS_METRICS - W-9005-model_1.0.ms:15111|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,902 [INFO ] W-9005-model_1.0 TS_METRICS - WorkerThreadTime.ms:108|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,906 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13399\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,906 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:15148|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,907 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:73|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:53.990+03:00\t2023-05-19T09:49:53,907 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Backend received inference at: 1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,989 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13784\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,991 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:15231|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:53,992 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:56|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489793\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,009 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13663\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,009 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:15249|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,010 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:103|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,038 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13293\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,038 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:15277|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,039 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:52|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,187 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13751\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,187 [INFO ] W-9004-model_1.0 TS_METRICS - W-9004-model_1.0.ms:15426|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:49:54.991+03:00\t2023-05-19T09:49:54,188 [INFO ] W-9004-model_1.0 TS_METRICS - WorkerThreadTime.ms:87|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489794\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,971 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,972 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:46.74238204956055|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,972 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:9.122749328613281|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,973 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:16.3|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,973 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:17504.14453125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,974 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:13734.4453125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:40.004+03:00\t2023-05-19T09:50:39,974 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:44.8|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489839\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,909 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60000\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,910 [ERROR] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 1\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,910 [ERROR] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\n\n2023-05-19T12:50:54.009+03:00\torg.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time\n\n2023-05-19T12:50:54.009+03:00\t#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:199) [model-server.jar:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t#011at java.lang.Thread.run(Thread.java:829) [?:?]\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,945 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_MODEL_LOADED\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,946 [INFO ] W-9006-model_1.0 ACCESS_LOG - \/169.254.255.130:54478 \"POST \/invocations HTTP\/1.1\" 500 72597\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,947 [INFO ] W-9006-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489779\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,947 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stderr\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,948 [WARN ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-model_1.0-stdout\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,949 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,995 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489853995\n\n2023-05-19T12:50:54.009+03:00\t2023-05-19T09:50:53,997 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Backend received inference at: 1684489853\n\n2023-05-19T12:50:55.010+03:00\t2023-05-19T09:50:54,024 [INFO ] W-9006-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stdout\n\n2023-05-19T12:50:55.010+03:00\t2023-05-19T09:50:54,024 [INFO ] W-9006-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-model_1.0-stderr\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,227 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: \/home\/model-server\/tmp\/.ts.sock.9006\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Successfully loaded \/opt\/conda\/lib\/python3.8\/site-packages\/ts\/configs\/metrics.yaml.\n\n2023-05-19T12:50:57.010+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]428\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,236 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: \/home\/model-server\/tmp\/.ts.sock.9006\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,235 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started.\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,237 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,238 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: \/home\/model-server\/tmp\/.ts.sock.9006.\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,238 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1684489856238\n\n2023-05-19T12:50:57.011+03:00\t2023-05-19T09:50:56,269 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,351 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 10083\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,351 [INFO ] W-9006-model_1.0 TS_METRICS - W-9006-model_1.0.ms:87564|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489866\n\n2023-05-19T12:51:07.014+03:00\t2023-05-19T09:51:06,352 [INFO ] W-9006-model_1.0 TS_METRICS - WorkerThreadTime.ms:30|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489866\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,971 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:46.74235534667969|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:9.12277603149414|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,972 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:16.3|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.023+03:00\t2023-05-19T09:51:39,973 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:17432.91796875|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.024+03:00\t2023-05-19T09:51:39,974 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:13805.67578125|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:40.024+03:00\t2023-05-19T09:51:39,974 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:45.0|#Level:Host|#hostname:d9e41fe4df61,timestamp:1684489899\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,996 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60000\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,997 [ERROR] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 2\n\n2023-05-19T12:51:54.028+03:00\t2023-05-19T09:51:53,997 [ERROR] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\n\n2023-05-19T12:51:54.028+03:00\torg.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time\n\n2023-05-19T12:51:54.028+03:00\t#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:199) [model-server.jar:?]\n\n2023-05-19T12:51:54.028+03:00\t#011at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)",
        "Challenge_closed_time":1684555351304,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684490308406,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while using SageMaker Batch Transform with `strategy ='MultiRecord'` and `assemble_with='Line'` to perform inference on CSV files saved on S3. The process works with `strategy ='SingleRecord'`, but fails with a `\"message\": \"Worker died.\"` error when using `strategy ='MultiRecord'` with a CSV file of 182 samples (621.6KB) and a `max_payload` of 6. The user suspects that the issue is related to the memory limit of the instance being used and switches to a `ml.m5.2xlarge` instance with 32GB of memory. However, the process only works with a `max_payload` of 7 and fails with anything bigger than that. The user is seeking ideas on what could be causing this issue.",
        "Challenge_last_edit_time":1684836402615,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUAI7z2SCLT06bUOu1ULKV-Q\/sagemaker-batch-transform-multirecord-fail-with-csv-as-input",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":205.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":183,
        "Challenge_solved_time":18.0674716667,
        "Challenge_title":"SageMaker Batch Transform MultiRecord Fail with CSV as Input",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":837,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"There could be two possible reasons for the error message : \"message\": \"Worker died.\".\n\n1. This is commonly noticed when the instance exhausts the Memory usage. (This can be verified if you can check the Cloudwatch Metrics for the job run.)\n2. Model server timeouts during the job. \n\nYou can either use a larger instance for the job or try to set the higher value of following environment variables - worker and timeout in your script. \n\n\tmodel_server_workers = int(os.environ.get(_params.MODEL_SERVER_WORKERS_ENV, num_cpus())) \n\tmodel_server_timeout = int(os.environ.get(_params.MODEL_SERVER_TIMEOUT_ENV, <Enter value here>))",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1684555351304,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":7.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.4247222222,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n    @pytest.mark.gpu\r\n    @pytest.mark.notebooks\r\n    @pytest.mark.integration\r\n    @pytest.mark.parametrize(\r\n        \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n        [\r\n            (\r\n                15,\r\n                10,\r\n                ***\r\n                    \"res_syn\": ***\"auc\": 0.9716, \"logloss\": 0.699***,\r\n                    \"res_real\": ***\"auc\": 0.749, \"logloss\": 0.4926***,\r\n                ***,\r\n                42,\r\n            )\r\n        ],\r\n    )\r\n    def test_xdeepfm_integration(\r\n        notebooks,\r\n        output_notebook,\r\n        kernel_name,\r\n        syn_epochs,\r\n        criteo_epochs,\r\n        expected_values,\r\n        seed,\r\n    ):\r\n        notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            output_notebook,\r\n            kernel_name=kernel_name,\r\n            parameters=dict(\r\n                EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n                EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n                BATCH_SIZE_SYNTHETIC=1024,\r\n                BATCH_SIZE_CRITEO=1024,\r\n                RANDOM_SEED=seed,\r\n            ),\r\n        )\r\n        results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n            \"data\"\r\n        ]\r\n    \r\n        for key, value in expected_values.items():\r\n>           assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\nE           assert 0.5131 == 0.9716 \u00b1 9.7e-02\r\nE             comparison failed\r\nE             Obtained: 0.5131\r\nE             Expected: 0.9716 \u00b1 9.7e-02\r\n```\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3459763061\/jobs\/5775521889\r\n\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1668600473000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1668591744000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered errors in some of the AzureML tests, specifically in the test_notebooks_gpu.py file. The errors are related to not finding certain names in the module. The platform where the issue is happening is not specified, and there are no instructions on how to replicate the issue. The expected behavior is not mentioned.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1848",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":24.18,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.4247222222,
        "Challenge_title":"[BUG] xdeepfm error in AzureML test",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":162,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"```\r\n pytest tests\/integration\/examples\/test_notebooks_gpu.py::test_xdeepfm_integration --disable-warnings --durations 0\r\n```\r\nwith \r\n```\r\n@pytest.mark.gpu\r\n@pytest.mark.notebooks\r\n@pytest.mark.integration\r\n@pytest.mark.parametrize(\r\n    \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n    [\r\n        (\r\n            15,\r\n            10,\r\n            {\r\n                \"res_syn\": {\"auc\": 0.9716, \"logloss\": 0.699},\r\n                \"res_real\": {\"auc\": 0.749, \"logloss\": 0.4926},\r\n            },\r\n            42,\r\n        )\r\n    ],\r\n)\r\ndef test_xdeepfm_integration(\r\n    notebooks,\r\n    output_notebook,\r\n    kernel_name,\r\n    syn_epochs,\r\n    criteo_epochs,\r\n    expected_values,\r\n    seed,\r\n):\r\n    notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n    pm.execute_notebook(\r\n        notebook_path,\r\n        output_notebook,\r\n        kernel_name=kernel_name,\r\n        parameters=dict(\r\n            EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n            EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n            BATCH_SIZE_SYNTHETIC=1024,\r\n            BATCH_SIZE_CRITEO=1024,\r\n            RANDOM_SEED=seed,\r\n        ),\r\n    )\r\n    results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n        \"data\"\r\n    ]\r\n\r\n    for key, value in expected_values.items():\r\n        assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\n        assert results[key][\"logloss\"] == pytest.approx(\r\n            value[\"logloss\"], rel=TOL, abs=ABS_TOL\r\n        )\r\n```",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.8,
        "Solution_reading_time":15.13,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"I read somewhere that some Amazon SageMaker's built-in algorithms can *only* be trained using GPU, whereas some can use either GPU or CPU, and some can only be used on CPU.\n\nIs there any official documentation explicitly stating which algorithms can only use GPU or both? ",
        "Challenge_closed_time":1597251737000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597251203000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for official documentation that lists which Amazon SageMaker algorithms can only be trained using GPU, which can use either GPU or CPU, and which can only be used on CPU.",
        "Challenge_last_edit_time":1668607243320,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUdTLbPM2STGelSj1g3TIjpA\/which-amazon-sagemaker-algorithms-can-only-use-gpu-for-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":4.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.1483333333,
        "Challenge_title":"Which Amazon SageMaker algorithms can only use GPU for training?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":275.0,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":1.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Documentation for [Amazon SageMaker built-in algorithms][1]  provides recommendations around choice of Amazon EC2 instances and whether given algorithm supports GPU or CPU devices.\n\nLet's take [Image Classification][2] as an example. Here is a excerpt from online documentation:\n\n> For image classification, we support the following GPU instances for\n> training: ml.p2.xlarge, ml.p2.8xlarge, ml.p2.16xlarge, ml.p3.2xlarge,\n> ml.p3.8xlargeand ml.p3.16xlarge. We recommend using GPU instances with\n> more memory for training with large batch sizes. However, both CPU\n> (such as C4) and GPU (such as P2 and P3) instances can be used for the\n> inference. You can also run the algorithm on multi-GPU and\n> multi-machine settings for distributed training.\n\nFor more complex scenarios, such as [Script or BYO Container][3] modes, customers have flexibility to choose which device (GPU or CPU) to utilize for which operation. This is configured as part of their training scripts.\n\n\n  [1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\n  [2]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\n  [3]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612350091844,
        "Solution_link_count":3.0,
        "Solution_readability":12.4,
        "Solution_reading_time":15.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":143.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1464391892936,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Answerer_reputation_count":2243.0,
        "Answerer_view_count":148.0,
        "Challenge_adjusted_solved_time":48.6672925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Challenge_closed_time":1573653626700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573479066647,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a credit-fraud dataset on AWS Sagemaker and created an endpoint of the model. They want to share it with a friend who has some credit data and wants to know whether the transaction is fraud or not. The user is unsure of the correct way to share the endpoint and is considering sharing their ARN, but believes it may not be the right way. They are seeking advice on alternative methods.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58802366",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":6.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":48.4889036111,
        "Challenge_title":"Deploying the sagemaker endpoint created as a service",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568318861627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1573654268900,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.56,
        "Solution_score_count":6.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":467.3672177778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am able to submit jobs to Azure ML services using a compute cluster. It works well, and the autoscaling combined with good flexibility for custom environments seems to be exactly what I need. However, so far all these jobs seem to only use one compute node of the cluster. Ideally I would like to use multiple nodes for a computation, but all methods that I see rely on rather deep integration with azure ML services.<\/p>\n\n<p>My modelling case is a bit atypical. From previous experiments I identified a group of architectures (pipelines of preprocessing steps + estimators in Scikit-learn) that worked well. \nHyperparameter tuning for one of these estimators can be performed reasonably fast (couple of minutes) with <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">RandomizedSearchCV<\/a>. So it seems less effective to parallelize this step.<\/p>\n\n<p>Now I want to tune and train this entire list of architectures.\nThis should be very easily to parallelize since all architectures can be trained independently. <\/p>\n\n<p>Ideally I would like something like (in pseudocode)<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tuned = AzurePool.map(tune_model, [model1, model2,...])\n<\/code><\/pre>\n\n<p>However, I could not find any resources on how I could achieve this with an Azure ML Compute cluster.\nAn acceptable alternative would come in the form of a plug-and-play substitute for sklearn's CV-tuning methods, similar to the ones provided in <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">dask<\/a> or <a href=\"https:\/\/databricks.github.io\/spark-sklearn-docs\/#spark_sklearn.GridSearchCV\" rel=\"nofollow noreferrer\">spark<\/a>.<\/p>",
        "Challenge_closed_time":1565990325968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565966913733,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in parallelizing work on an Azure ML Service Compute cluster. They have been able to submit jobs to the cluster, but all jobs seem to use only one compute node. The user wants to train a list of architectures in parallel, but they could not find any resources on how to achieve this with an Azure ML Compute cluster. They are looking for a plug-and-play substitute for sklearn's CV-tuning methods.",
        "Challenge_last_edit_time":1565987551452,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57526707",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":25.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.5033986111,
        "Challenge_title":"How to parallelize work on an Azure ML Service Compute cluster?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":818.0,
        "Challenge_word_count":233,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525187747288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":1466.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>There are a number of ways you could tackle this with AzureML. The simplest would be to just launch a number of jobs using the AzureML Python SDK (the underlying example is taken from <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training\/train-hyperparameter-tune-deploy-with-sklearn\/train-hyperparameter-tune-deploy-with-sklearn.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\n\nruns = []\n\nfor kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n    for penalty in [0.5, 1, 1.5]:\n        print ('submitting run for kernel', kernel, 'penalty', penalty)\n        script_params = {\n            '--kernel': kernel,\n            '--penalty': penalty,\n        }\n\n        estimator = SKLearn(source_directory=project_folder, \n                            script_params=script_params,\n                            compute_target=compute_target,\n                            entry_script='train_iris.py',\n                            pip_packages=['joblib==0.13.2'])\n\n        runs.append(experiment.submit(estimator))\n<\/code><\/pre>\n\n<p>The above requires you to factor your training out into a script (or a set of scripts in a folder) along with the python packages required. The above estimator is a convenience wrapper for using Scikit Learn. There are also estimators for Tensorflow, Pytorch, Chainer and a generic one (<code>azureml.train.estimator.Estimator<\/code>) -- they all differ in the Python packages and base docker they use.<\/p>\n\n<p>A second option, if you are actually tuning parameters, is to use the HyperDrive service like so (using the same <code>SKLearn<\/code> Estimator as above):<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\nfrom azureml.train.hyperdrive.runconfig import HyperDriveConfig\nfrom azureml.train.hyperdrive.sampling import RandomParameterSampling\nfrom azureml.train.hyperdrive.run import PrimaryMetricGoal\nfrom azureml.train.hyperdrive.parameter_expressions import choice\n\nestimator = SKLearn(source_directory=project_folder, \n                    script_params=script_params,\n                    compute_target=compute_target,\n                    entry_script='train_iris.py',\n                    pip_packages=['joblib==0.13.2'])\n\nparam_sampling = RandomParameterSampling( {\n    \"--kernel\": choice('linear', 'rbf', 'poly', 'sigmoid'),\n    \"--penalty\": choice(0.5, 1, 1.5)\n    }\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                                         hyperparameter_sampling=param_sampling, \n                                         primary_metric_name='Accuracy',\n                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                         max_total_runs=12,\n                                         max_concurrent_runs=4)\n\nhyperdrive_run = experiment.submit(hyperdrive_run_config)\n<\/code><\/pre>\n\n<p>Or you could use DASK to schedule the work as you were mentioning. Here is a sample of how to set up DASK on and AzureML Compute Cluster so you can do interactive work on it: <a href=\"https:\/\/github.com\/danielsc\/azureml-and-dask\" rel=\"nofollow noreferrer\">https:\/\/github.com\/danielsc\/azureml-and-dask<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1567670073436,
        "Solution_link_count":3.0,
        "Solution_readability":17.0,
        "Solution_reading_time":36.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":250.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1336700249823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":885.0,
        "Answerer_view_count":127.0,
        "Challenge_adjusted_solved_time":2447.0429822222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When using <code>Sacred<\/code> it is necessary to pass all variables from the experiment config, into the main function, for example<\/p>\n\n<pre><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(C, gamma):\n  iris = datasets.load_iris()\n  per = permutation(iris.target.size)\n  iris.data = iris.data[per]\n  iris.target = iris.target[per]\n  clf = svm.SVC(C, 'rbf', gamma=gamma)\n  clf.fit(iris.data[:90],\n          iris.target[:90])\n  return clf.score(iris.data[90:],\n                   iris.target[90:])\n<\/code><\/pre>\n\n<p>As you can see, in this experiment there are 2 variables, <code>C<\/code> and <code>gamma<\/code>, and they are passed into the main function.<\/p>\n\n<p>In real scenarios, there are dozens of experiment variables, and the passing all of them into the main function gets really cluttered.\nIs there a way to pass them all as a dictionary? Or maybe as an object with attributes? <\/p>\n\n<p>A good solution will result in something like follows:<\/p>\n\n<pre><code>@ex.automain\ndef run(config):\n    config.C      # Option 1\n    config['C']   # Option 2 \n<\/code><\/pre>",
        "Challenge_closed_time":1551699854476,
        "Challenge_comment_count":2,
        "Challenge_created_time":1542890499740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge while using Sacred, where it is necessary to pass all variables from the experiment config into the main function. This becomes cluttered when there are dozens of experiment variables. The user is looking for a solution to pass all variables as a dictionary or an object with attributes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53431283",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.0,
        "Challenge_reading_time":13.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":2447.0429822222,
        "Challenge_title":"Sacred - pass all parameters as one",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":735.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443016881603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Israel",
        "Poster_reputation_count":9385.0,
        "Poster_view_count":1033.0,
        "Solution_body":"<p>Yes, you can use the <a href=\"https:\/\/sacred.readthedocs.io\/en\/latest\/configuration.html#special-values\" rel=\"nofollow noreferrer\">special value<\/a> <code>_config<\/code> value for that:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(_config):\n  C = _config['C']\n  gamma = _config['gamma']\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.5,
        "Solution_reading_time":5.35,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":33.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.7000047222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Thanks to <a href=\"https:\/\/www.dabeaz.com\/generators\/\" rel=\"nofollow noreferrer\">David Beazley's slides on Generators<\/a> I'm quite taken with using generators for data processing in order to keep memory consumption minimal. Now I'm working on my first kedro project, and my question is how I can use generators in kedro. When I have a node that yields a generator, and then run it with <code>kedro run --node=example_node<\/code>, I get the following error:<\/p>\n<pre><code>DataSetError: Failed while saving data to data set MemoryDataSet().\ncan't pickle generator objects\n<\/code><\/pre>\n<p>Am I supposed to always load all my data into memory when working with kedro?<\/p>",
        "Challenge_closed_time":1661267379900,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661264859883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use generators for data processing in their kedro project to minimize memory consumption. However, when they run a node that yields a generator, they encounter an error stating that generator objects cannot be pickled, resulting in a failed attempt to save data to the MemoryDataSet. The user is questioning whether they need to load all their data into memory when working with kedro.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73460511",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.7000047222,
        "Challenge_title":"How to use generators with kedro?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":101,
        "Platform":"Stack Overflow",
        "Poster_created_time":1481393494750,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":19.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Hi @ilja to do this you may need to change the type of <code>assignment<\/code> operation that <code>MemoryDataSet<\/code> applies.<\/p>\n<p>In your catalog, declare your datasets explicitly, change the <code>copy_mode<\/code> to one of <code>copy<\/code> or <code>assign<\/code>. I think <code>assign<\/code> may be your best bet here...<\/p>\n<p><a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.MemoryDataSet.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.MemoryDataSet.html<\/a><\/p>\n<p>I hope this works, but am not 100% sure.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.8,
        "Solution_reading_time":7.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":56.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1537550036732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":83.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":257.7548786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Why we need to ML Batch Execution and ML Update resource option in Data factory ? How this can be used to retrain machine learning when updating a blob file ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1539774604663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538738198597,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning the need for ML Batch Execution and ML Update resource option in Azure Data Factory and how it can be used to retrain machine learning when updating a blob file.",
        "Challenge_last_edit_time":1538846687100,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52664415",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":287.8905738889,
        "Challenge_title":"Why we need ML batch execution and update resource option in azure data factory",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":141.0,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538737895716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":78.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Ml Batch Execution- to call retraining experiment and get a .ilearner file as output.\nML Update Resource- Use the above .ilearner as input and call patch endpoint of predictive web service to Update resource.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":2.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1440024011336,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":39.2092791667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a Model to an ACI container and have an endpoint that I can hit in Postman or using python SDK. I use Python to hit the endpoint as well as Postman and I get a response and the Container Instance logging records the event. I now what to use the AZ ML CLI to run the service and pass in some hardcoded JSON:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/service?view=azure-cli-latest#ext-azure-cli-ml-az-ml-service-run\" rel=\"nofollow noreferrer\">From the Azure ML CLI docs<\/a>:  <\/p>\n\n<pre><code>az ml service run --name (-n) --input-data (-d)\n<\/code><\/pre>\n\n<p>I run this <\/p>\n\n<pre><code>az ml service run -n \"rj-aci-5\" -d {\\\"input_df\\\": [{\\\"width\\\": 50, \\\"shoe_size\\\": 28}]}\n<\/code><\/pre>\n\n<p>There is no output or error. The logs do not record any invocation. Has anyone used the Azure CLI ML extensions to run a service in the manner above?<\/p>",
        "Challenge_closed_time":1570637042412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570495889007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deployed a model to an ACI container and can hit the endpoint using Postman or Python SDK. They are trying to use the Azure ML CLI to run the service and pass in hardcoded JSON, but there is no output or error, and the logs do not record any invocation. The user is seeking help from anyone who has used the Azure CLI ML extensions to run a service in this manner.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58278844",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":11.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":39.2092791667,
        "Challenge_title":"Does the Azure CLI ML \"service run\" command work?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Poster_created_time":1256089885500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":4947.0,
        "Poster_view_count":531.0,
        "Solution_body":"<p>The az cli is likely failing to parse the provided data input. If I attempt to run the same command I see the following error:<\/p>\n\n<p><code>az: error: unrecognized arguments: [{\"width\": 50, \"shoe_size\": 28}]}<\/code><\/p>\n\n<p>You need to wrap the input in quotes for it to appropriately be taken as a single input parameter:<\/p>\n\n<p><code>az ml service run -n \"rj-aci-5\" -d \"{\\\"input_df\\\": [{\\\"width\\\": 50, \\\"shoe_size\\\": 28}]}\"<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":5.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":74.1955488889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run tensorboard from my Jupyter notebook in Sagemaker. The below is my code:<\/p>\n<pre><code>import tensorflow as tf\nimport datetime, os\n\n%load_ext tensorboard\nlogs_base_dir = &quot;.\/logs&quot;\nos.makedirs(logs_base_dir)\n\n!tensorboard --logdir=data\/ --host localhost --port=8080\n<\/code><\/pre>\n<p>The output I get looks fine:\n<code>TensorBoard 1.14.0 at http:\/\/localhost:8080\/ (Press CTRL+C to quit)<\/code>\nbut when I click on the link, I'm taken to a page with ERR_CONNECTION_REFUSED.<\/p>\n<p>Does anyone have suggestions about what to try next? Thanks so much!<\/p>\n<p>Tensorflow: 1.14\nPython: 2<\/p>",
        "Challenge_closed_time":1624263423263,
        "Challenge_comment_count":2,
        "Challenge_created_time":1623953190457,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run Tensorboard from their Jupyter notebook in Sagemaker, but when they click on the link, they are taken to a page with ERR_CONNECTION_REFUSED. They are seeking suggestions on what to try next.",
        "Challenge_last_edit_time":1623996319287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68024476",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":7.1,
        "Challenge_reading_time":8.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":86.1757794445,
        "Challenge_title":"Running Tensorboard in Jupyter in Sagemaker: this site can't be reached",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":286.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340392287000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1843.0,
        "Poster_view_count":87.0,
        "Solution_body":"<p>Based on the comments it seems to me that you are trying to open the wrong URL. If I understand you question, you are not running in a local environment, so you can not open <code>localhost<\/code>. The right URL for <code>sagemaker<\/code> from the docs is <code>https:\/\/&lt;notebook instance hostname&gt;\/proxy\/6006\/<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":4.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":23.3159841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run this on Amazon Sagemaker but I am getting this error while when I try to run it on my local machine, it works very fine.<\/p>\n<p>this is my code:<\/p>\n<pre><code>import tensorflow as tf\n\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\n    \ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)\n\ncontent_path = tf.keras.utils.get_file('YellowLabradorLooking_nw4.jpg', 'https:\/\/example.com\/IMG_20200216_163015.jpg')\n\n\nstyle_path = tf.keras.utils.get_file('kandinsky3.jpg','https:\/\/example.com\/download+(2).png')\n\n\ndef load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\ncontent_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\nimport tensorflow_hub as hub\nhub_module = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/1')\nstylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)\n\n\nfile_name = 'stylized-image5.png'\ntensor_to_image(stylized_image).save(file_name)\n<\/code><\/pre>\n<p>This is the exact error I get:<\/p>\n<pre><code>---------------------------------------------------------------------------\n<\/code><\/pre>\n<p>TypeError                                 Traceback (most recent call last)<\/p>\n<pre><code>&lt;ipython-input-24-c47a4db4880c&gt; in &lt;module&gt;()\n     53 \n     54 \n---&gt; 55 content_image = load_img(content_path)\n     56 style_image = load_img(style_path)\n     57 \n<\/code><\/pre>\n<p> in load_img(path_to_img)<\/p>\n<pre><code>     34 \n     35     shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n---&gt; 36     long_dim = max(shape)\n     37     scale = max_dim \/ long_dim\n     38 \n<\/code><\/pre>\n<p>~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/ops.py in <strong>iter<\/strong>(self)<\/p>\n<pre><code>    475     if not context.executing_eagerly():\n    476       raise TypeError(\n--&gt; 477           &quot;Tensor objects are only iterable when eager execution is &quot;\n    478           &quot;enabled. To iterate over this tensor use tf.map_fn.&quot;)\n    479     shape = self._shape_tuple()\n<\/code><\/pre>\n<p>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.<\/p>",
        "Challenge_closed_time":1594129639003,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594076057097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a Tensorflow error while trying to run a code on Amazon Sagemaker. The error message states that Tensor objects are only iterable when eager execution is enabled and suggests using tf.map_fn to iterate over the tensor. The code runs fine on the user's local machine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62765658",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":12.0,
        "Challenge_reading_time":41.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":14.8838627778,
        "Challenge_title":"Tensorflow error. TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1530.0,
        "Challenge_word_count":281,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565307837780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":570.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>Your error is being raised in this function <code>load_img<\/code>:<\/p>\n<pre><code>def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n<\/code><\/pre>\n<p>Specifically, this line:<\/p>\n<pre><code>    long_dim = max(shape)\n<\/code><\/pre>\n<p>You are passing a tensor to the <a href=\"https:\/\/docs.python.org\/3\/library\/functions.html#max\" rel=\"nofollow noreferrer\">built-in Python max function<\/a> in graph execution mode. You can only iterate through tensors in eager-execution mode. You probably want to use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/reduce_max\" rel=\"nofollow noreferrer\">tf.reduce_max<\/a> instead:<\/p>\n<pre><code>    long_dim = tf.reduce_max(shape)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1594159994640,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":13.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.3457913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello everyone,    <\/p>\n<p>I am tring to deploy R script as a web service using Azure Machine Learning. I created pipeline as below.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/143603-001.png?platform=QnA\" alt=\"143603-001.png\" \/>    <\/p>\n<p>I can deploy the model and endpoint from [Deploy] button but I cannot control some properties: i.e. resource name, dns name.    <\/p>\n<p>It seems that the <code>az ml model deploy<\/code> command can be used to deploy the endpoint.     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#using-the-azure-cli\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#using-the-azure-cli<\/a>    <\/p>\n<p>I have no information for <code>inferenceconfig.json<\/code>. How to write <code>score.py<\/code> to execute R script? Is it any example?    <\/p>",
        "Challenge_closed_time":1635283098256,
        "Challenge_comment_count":2,
        "Challenge_created_time":1635213453407,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to deploy an R script as a web service using Azure Machine Learning and has created a pipeline. They are able to deploy the model and endpoint from the \"Deploy\" button but cannot control some properties such as resource name and DNS name. The user is looking for information on how to use the \"az ml model deploy\" command to deploy the endpoint and how to write \"score.py\" to execute the R script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/603664\/how-to-deploy-r-script-web-service-via-azure-cli",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":12.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.3457913889,
        "Challenge_title":"How to deploy R script web service via Azure CLI",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, the following document describes how to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli#define-an-inference-configuration\">define an inference configuration<\/a>.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.5,
        "Solution_reading_time":4.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1574939238203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":350.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":15.0446438889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Pretty self-explanatory question. When should I use Azure ML Notebooks VS Azure Databricks? I feel there\u2019s a great overlap between the two products and one is definitely better marketed than the other.. <\/p>\n\n<p>I\u2019m mainly looking for information concerning datasets sizes and typical workflow. Why should I use Databricks over AzureML if I don\u2019t have a Spark oriented workflow ?<\/p>\n\n<p>Thanks !<\/p>",
        "Challenge_closed_time":1585823280848,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585769120130,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on when to use Azure ML Notebooks versus Azure Databricks, as they feel there is significant overlap between the two products. They are specifically interested in information regarding dataset sizes and typical workflows, and are questioning whether Databricks is necessary if they do not have a Spark-oriented workflow.",
        "Challenge_last_edit_time":1625712808263,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60978808",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.0446438889,
        "Challenge_title":"When should I use Azure ML Notebooks VS Azure Databricks? Both are competitor products in my opinion",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":3755.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447320137140,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":40.0,
        "Solution_body":"<p>@Nethim, from my pov these are the main difference:<br><\/p>\n\n<ol>\n<li><p>Data Distribution:<\/p>\n\n<ul>\n<li>Azure ML Notebooks are good when you are training with a limited data on single machine. Though Azure ML provides training clusters, the data distribution among the nodes is to be handled in the code.<\/li>\n<li>Azure Databricks with its RDDs are designed to handle data distributed on multiple nodes.This is advantageous when your data size is huge.When your data size is small and can fit in a scaled up single machine\/ you are using a pandas dataframe, then use of Azure databricks is a overkill<\/li>\n<\/ul><\/li>\n<li><p>Data Cleaning:\nDatabricks can support a lot of file formats natively and querying and cleaning huge datasets are easy where as this has to be handled custom in AzureML notebooks. This can be done with a aml notebooks but cleaning and writing to stores has to be handled.<\/p><\/li>\n<li>Training\nBoth has the capabilities if distributing the training, Databricks provides inbuilt ML algorithms that can act on chunk of data on that node and coordinate with other nodes. Though this can be done on both AzureMachineLearning and Databricks with tf,horovod etc.,<\/li>\n<\/ol>\n\n<p>In general(just my opinion), if the dataset is small, aml notebooks is good.If the data size is huge, then Azure databricks is easy for datacleanup and format conversions.Then the training can happen on AML or databricks.Though databricks has a learning curve whereas Azure ML can be easy with the python and pandas.<\/p>\n\n<p>Thanks.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":18.96,
        "Solution_score_count":6.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":243.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359884693920,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Challenge_adjusted_solved_time":1.5477430556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im trying to set comet (<a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a>) to track my Tensorflow experiment, after I create an Experiment and log the data set i dont get the accuracy in my report.<\/p>\n\n<p>my code:<\/p>\n\n<pre><code>mnist = get_data()\ntrain_step, cross_entropy, accuracy, x, y, y_ = build_model_graph(hyper_params)\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\nexperiment.log_multiple_params(hyper_params)\nexperiment.log_dataset_hash(mnist)\n<\/code><\/pre>\n\n<p>in the example account : <a href=\"https:\/\/www.comet.ml\/view\/Jon-Snow\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml\/view\/Jon-Snow<\/a> I see that accuracy is reported<\/p>",
        "Challenge_closed_time":1506100257932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506094686057,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to configure comet.ml to track their Tensorflow experiment, but after creating an experiment and logging the data set, they are not getting accuracy in their report. They have shared their code and noticed that accuracy is reported in an example account on comet.ml.",
        "Challenge_last_edit_time":1514341154200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46368389",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":9.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1.5477430556,
        "Challenge_title":"How to configure comet (comet.ml) to log Tensorflow?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":338.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506066897167,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>you can report accuracy using this method:<\/p>\n\n<ul>\n<li><code>experiment.log_accuracy(train_accuracy)<\/code><\/li>\n<\/ul>\n\n<p>take a look at the full Tensorflow example in our guide:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1513514205487,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":5.41,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":92.6310866667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to perform <strong>distributed training<\/strong> on <strong>Amazon SageMaker<\/strong>. The code is written with <strong>TensorFlow<\/strong> and similar to the following code where I think CPU instance should be enough:\u00a0\n<a href=\"https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/horovod\/horovod\/blob\/master\/examples\/tensorflow_word2vec.py<\/a><\/p>\n<p>Can <strong>Horovod with TensorFlow<\/strong> work on <strong>non-GPU<\/strong> instances in Amazon SageMaker?<\/p>",
        "Challenge_closed_time":1663198075692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662864603780,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to perform distributed training on Amazon SageMaker using TensorFlow and is wondering if Horovod with TensorFlow can work on non-GPU instances. The user believes that a CPU instance should be sufficient for the task.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73676483",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":8.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":92.6310866667,
        "Challenge_title":"Can Horovod with TensorFlow work on non-GPU instances in Amazon SageMaker?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":17.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>Yeah you should be able to use both CPU's and GPU's with Horovod on Amazon SageMaker. Please follow the below example for the same<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.3,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.0685027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi;  <\/p>\n<p>First off, where can I find the costs for all the different things I can run in Azure ML? Not just a compute, but editing a notebook, connecting to a datastore, splitting a datastore, etc. Basically where is the price list?  <\/p>\n<p>Second, where can I find what I will be charged for things I ran in the last hour? I want to see what I'm spending before a month is up and the charge is then 100x what I expected (and can afford).  <\/p>\n<p>thanks - dave<\/p>",
        "Challenge_closed_time":1634347958867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634318912257,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on where to find the cost of running various tasks in Azure ML, including compute, editing a notebook, connecting to a datastore, and splitting a datastore. They are also looking for a way to view charges for tasks run in the last hour to avoid unexpected charges at the end of the month.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/592299\/cost-of-running-a-compute-other-tasks",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":6.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.0685027778,
        "Challenge_title":"Cost of running a compute, other tasks",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, you can use <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/cost-management-billing-overview\">Azure Cost Management<\/a> to manage Azure costs, please review the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/costs\/quick-acm-cost-analysis\">quickstart<\/a> document. Also, the following document provides detailed information on how to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-plan-manage-cost\">plan and manage cost for AML<\/a>.<\/p>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":19.7,
        "Solution_reading_time":8.39,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":0.6262913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wonder if it's possible to run SageMaker Inference or Batch Transform job directly for a video input (.mp4 or another format)?<\/p>\n<p>If no could you please advice the best practice that might be used for pre-processing?<\/p>",
        "Challenge_closed_time":1654270724136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654268345127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of running SageMaker Inference or Batch Transform job directly for a video input (.mp4 or another format) and is seeking advice on the best practice that might be used for pre-processing if it is not possible.",
        "Challenge_last_edit_time":1654268469487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72491505",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.6608358333,
        "Challenge_title":"SageMaker Inference for a video input",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":93.0,
        "Challenge_word_count":42,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442786553536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kyiv",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>Asynchronous inference could be a good option for this use case. There is a blog published by AWS that talks about how you can do this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":21.6,
        "Solution_reading_time":6.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":184.4022222222,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n*Description:*\r\n\r\nAn apt-get error is seen in `sagemaker-local-test` builds as below. This is because `apt-get` process is already running and in active state.\r\n\r\n```\r\nE: Could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: Resource temporarily unavailable)\r\n--\r\n294 | E: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it?\r\n```\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1598551848000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597888000000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to train on multiple GPUs in Sagemaker Notebook Terminal using Autogluon 0.4.0 TextPredictor. The user gets an error in spawning multiprocessing. However, when the user trains on a single GPU within the same instance and setup, it trains without any problem. The user expects to train across all 4 GPUs in the p3.8xl instance with no errors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/517",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":184.4022222222,
        "Challenge_title":"[bug] apt-get failure in sagemaker-local-test builds",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.0150825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This error message is super confusing, what does it mean? <\/p>",
        "Challenge_closed_time":1661980770760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661977116463,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error message related to the creation of descriptors, but finds it confusing and is seeking clarification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/989409\/descriptors-cannot-not-be-created",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":1.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1.0150825,
        "Challenge_title":"Descriptors cannot not be created",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":15,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=6ec12307-1a06-4236-b249-3fd890a3f2a1\">@matsuoka  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform. This problem is caused by breaking changes introduced in protobuf 4.0.0. For more information, see <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates\">https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates<\/a>.    <\/p>\n<p>Please refer to this troubleshooting guidance - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-protobuf-descriptor-error\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-protobuf-descriptor-error<\/a>    <\/p>\n<p>I hope it helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.4,
        "Solution_reading_time":11.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":30.887275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After applying Principal Component Analysis PCA to my data set in order to achieve better model accuracy. The 13 features dimensions, I am reducing it to 10 features using PCA. Everything is fine till here.    <\/p>\n<p>After implementing the model in WebApp, it is building &amp; seems fine in the studio.    <\/p>\n<p>In the testing phase of model prediction, Instead of displaying 10 features as an input, the UI system is showing the original features which is 13 &amp; the output is showing 10 featu<a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/70351-01.pdf?platform=QnA\">70351-01.pdf<\/a>res which does not have any feature names for the newly generated features which are 10. And also prediction is not working at all after executing it.\\    <\/p>\n<p>Attached are the screenshots, Please refer.    <\/p>",
        "Challenge_closed_time":1613976829527,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613865635337,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user applied Principal Component Analysis (PCA) to their dataset to reduce 13 features to 10 for better model accuracy. The model implementation in a WebApp seemed fine, but during testing, the UI system displayed the original 13 features instead of the reduced 10, and the output did not have any feature names for the newly generated features. Additionally, the prediction was not working at all. Screenshots were attached for reference.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/281600\/error-webapp-implementation-with-principal-compone",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":30.887275,
        "Challenge_title":"Error - WebApp Implementation with Principal Component Analysis(PCA)",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":126,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@RakshitSidd-7739 After updating and running the training experiment did you try to update the prediction experiment.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/70512-image.png?platform=QnA\" alt=\"70512-image.png\" \/>    <\/p>\n<p>After the prediction experiment is updated you can update the web service and check if it shows up correctly.     <\/p>\n",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.5,
        "Solution_reading_time":4.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1489170718523,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Toronto, ON, Canada",
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":13.2178627778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I try to run the Sagemaker provided examples with PySpark in Sagemaker Studio<\/p>\n<pre><code>import os\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\n\nrole = get_execution_role()\n\n# Configure Spark to use the SageMaker Spark dependency jars\njars = sagemaker_pyspark.classpath_jars()\n\nclasspath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\n\n# See the SageMaker Spark Github repo under sagemaker-pyspark-sdk\n# to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\nspark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n    .master(&quot;local[*]&quot;).getOrCreate()\n<\/code><\/pre>\n<p>I get the following exception:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-6-c8f6fff0daaf&gt; in &lt;module&gt;\n     19 # to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\n     20 spark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n---&gt; 21     .master(&quot;local[*]&quot;).getOrCreate()\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    171                     for key, value in self._options.items():\n    172                         sparkConf.set(key, value)\n--&gt; 173                     sc = SparkContext.getOrCreate(sparkConf)\n    174                     # This SparkContext may be an existing one.\n    175                     for key, value in self._options.items():\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    361         with SparkContext._lock:\n    362             if SparkContext._active_spark_context is None:\n--&gt; 363                 SparkContext(conf=conf or SparkConf())\n    364             return SparkContext._active_spark_context\n    365 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    127                     &quot; note this option will be removed in Spark 3.0&quot;)\n    128 \n--&gt; 129         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    130         try:\n    131             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    310         with SparkContext._lock:\n    311             if not SparkContext._gateway:\n--&gt; 312                 SparkContext._gateway = gateway or launch_gateway(conf)\n    313                 SparkContext._jvm = SparkContext._gateway.jvm\n    314 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf)\n     44     :return: a JVM gateway\n     45     &quot;&quot;&quot;\n---&gt; 46     return _launch_gateway(conf)\n     47 \n     48 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in _launch_gateway(conf, insecure)\n    106 \n    107             if not os.path.isfile(conn_info_file):\n--&gt; 108                 raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    109 \n    110             with open(conn_info_file, &quot;rb&quot;) as info:\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Before running the example I installed pyspark and sagemaker_pyspark with pip from the notebook. I am also using SparkMagic kernel from the kernels library of SageMaker.<\/p>",
        "Challenge_closed_time":1611005573523,
        "Challenge_comment_count":2,
        "Challenge_created_time":1610957989217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an exception while trying to run Sagemaker provided examples with PySpark in Sagemaker Studio. The exception is related to the Java gateway process which exited before sending its port number. The user has installed pyspark and sagemaker_pyspark with pip and is using SparkMagic kernel from the kernels library of SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65770913",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":14.5,
        "Challenge_reading_time":43.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":13.2178627778,
        "Challenge_title":"Sagemaker Studio Pyspark example fails",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1827.0,
        "Challenge_word_count":316,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452190055592,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":247.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Maybe, you are having this issue because this notebook was designed to run when you have an EMR cluster. I suggest you start a notebook with conda_python3 kernel on Sagemaker instead of the SparkMagic kernel. You will need to install <code>pyspark<\/code> and <code>sagemaker_pyspark<\/code> using pip, but it should work with the code you posted.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":4.39,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":60.6923205556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm preparing for the Azure Machine Learning exam, and here is a question confuses me.<\/p>\n<blockquote>\n<p>You are designing an Azure Machine Learning workflow. You have a\ndataset that contains two million large digital photographs. You plan\nto detect the presence of trees in the photographs. You need to ensure\nthat your model supports the following:<\/p>\n<p>Solution: You create a Machine\nLearning experiment that implements the Multiclass Decision Jungle\nmodule. Does this meet the goal?<\/p>\n<p>Solution: You create a Machine Learning experiment that implements the\nMulticlass Neural Network module. Does this meet the goal?<\/p>\n<\/blockquote>\n<p>The answer for the first question is No while for second is Yes, but I cannot understand why Multiclass Decision Jungle doesn't meet the goal since it is a classifier. Can someone explain to me the reason?<\/p>",
        "Challenge_closed_time":1559895314607,
        "Challenge_comment_count":3,
        "Challenge_created_time":1559676822253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is preparing for the Azure Machine Learning exam and is confused about a question related to image classification. The user needs to detect the presence of trees in a dataset of two million large digital photographs and is unsure why the Multiclass Decision Jungle module does not meet the goal while the Multiclass Neural Network module does. The user is seeking an explanation for this.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56450223",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.2,
        "Challenge_reading_time":11.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":60.6923205556,
        "Challenge_title":"Image Classification in Azure Machine Learning",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":538.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449453613688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":388.0,
        "Poster_view_count":322.0,
        "Solution_body":"<p>I suppose that this is part of a series of questions that present the same scenario. And there should be definitely some constraints in the scenario. \nMoreover if you have a look on the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/multiclass-neural-network\" rel=\"nofollow noreferrer\">Azure documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>However, recent research has shown that deep neural networks (DNN)\n  with many layers can be very effective in complex tasks such as image\n  or speech recognition. The successive layers are used to model\n  increasing levels of semantic depth.<\/p>\n<\/blockquote>\n\n<p>Thus, Azure recommends using Neural Networks for image classification. Remember, that the goal of the exam is to test your capacity to design data science solution <strong>using Azure<\/strong> so better to use their official documentation as a reference.<\/p>\n\n<p>And comparing to the other solutions:<\/p>\n\n<ol>\n<li>You create an Azure notebook that supports the Microsoft Cognitive\nToolkit.<\/li>\n<li>You create a Machine Learning experiment that implements\nthe Multiclass Decision Jungle module.<\/li>\n<li>You create an endpoint to the\nComputer vision API. <\/li>\n<li>You create a Machine Learning experiment that\nimplements the Multiclass Neural Network module.<\/li>\n<li>You create an Azure\nnotebook that supports the Microsoft Cognitive Toolkit.<\/li>\n<\/ol>\n\n<p>There are only 2 Azure ML Studio modules, and as the question is about constructing a <strong>workflow<\/strong> I guess we can only choose between them. (CNTK is actually the best solution as it allows constructing a deep neural network with ReLU whereas AML Studio doesn't, and API call is not about data science at all). <\/p>\n\n<p>Finally, I do agree with the other contributors that the question is absurd. Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":22.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":255.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1506627612487,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Denver, CO, USA",
        "Answerer_reputation_count":582.0,
        "Answerer_view_count":75.0,
        "Challenge_adjusted_solved_time":3073.7993022222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am attempting to use collect_list to collect arrays (and maintain order) from two different data frames.<\/p>\n\n<p>Test_Data and Train_Data have the same format.<\/p>\n\n<pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql import Window\n\nw = Window.partitionBy('Group').orderBy('date')\n\n# Train_Data has 4 data points\n# Test_Data has 7 data points\n# desired target array:         [1, 1, 2, 3]\n# desired MarchMadInd array:    [0, 0, 0, 1, 0, 0, 1]\n\nsorted_list_diff_array_lens = train_data.withColumn('target', \nF.collect_list('target').over(w)\n                                  )\\\ntest_data.withColumn('MarchMadInd', F.collect_list('MarchMadInd').over(w))\\\n   .groupBy('Group')\\\n   .agg(F.max('target').alias('target'), \n    F.max('MarchMadInd').alias('MarchMadInd')\n)\n<\/code><\/pre>\n\n<p>I realize the syntax is incorrect with \"test_data.withColumn\", but I want to select the array for the <em>MarchMadInd<\/em> from the <strong>test_date<\/strong>, but the array for the <em>target<\/em> from the <strong>train_data<\/strong>. The desired output would look like the following:<\/p>\n\n<pre><code>{\"target\":[1, 1, 2, 3], \"MarchMadInd\":[0, 0, 0, 1, 0, 0, 1]}\n<\/code><\/pre>\n\n<p>Context: this is for a DeepAR time series model (using AWS) that requires dynamic features to include the prediction period, but the target should be historical data.<\/p>",
        "Challenge_closed_time":1555600483376,
        "Challenge_comment_count":1,
        "Challenge_created_time":1544134968250,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is attempting to use PySpark's collect_list function to collect arrays of varying length from two different data frames while maintaining order. They want to select the array for \"MarchMadInd\" from the \"test_data\" and the array for \"target\" from the \"train_data\". The desired output is a dictionary with \"target\" and \"MarchMadInd\" arrays. This is for a DeepAR time series model that requires dynamic features to include the prediction period, but the target should be historical data.",
        "Challenge_last_edit_time":1544534805888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53660590",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":17.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3184.8653127778,
        "Challenge_title":"PySpark Using collect_list to collect Arrays of Varying Length",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":345.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506627612487,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denver, CO, USA",
        "Poster_reputation_count":582.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>The solution involves using a join as recommended by pault. <\/p>\n\n<ol>\n<li>Create a dataframe with dynamic features of length equal to Training + Prediction period<\/li>\n<li>Create a dataframe with target values of length equal to just the Training period.<\/li>\n<li>Use a LEFT JOIN (with the dynamic feature data on LEFT) to bring these dataframes together<\/li>\n<\/ol>\n\n<p>Now, using collect_list will create the desired result.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":5.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.9002777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nAfter Sagemaker workspace stopped automatically, workspace env status is not updated.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Set sagemaker workspace config's AutoStopIdleTimeInMinutes as 10 minutes\r\n2. Create sagemaker workspace and wait for more than 10 minutes,\r\n3. Check sagemaker notebook instances to confirm the instance status is Stopped\r\n4. Check Service Workbench workspace status, it is still \"AVAILABLE\"\r\n\r\n**Expected behavior**\r\n1. Above step 4, workspace status should be \"STOPPED\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1657702977000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657451336000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is requesting to add three dependencies to the Sagemaker section, which are urllib3, PyYAML, and ipython, to train a Deepracer model locally with GPU support.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/44",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":69.9002777778,
        "Challenge_title":"[Bug] Sagemaker template, after auto stoped, workspace env status is not updated",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Fixed, refer [commit](https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/commit\/2339efe78d0f4705ef0cd6d2b1c5f06a810e6730) ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":52.8,
        "Solution_reading_time":1.81,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1363320186152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1352.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":9.9320377778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a minimal example of a neural network with a back-propagation trainer, testing it on the IRIS data set. I started of with 7 hidden nodes and it worked well.<\/p>\n\n<p>I lowered the number of nodes in the hidden layer to 1 (expecting it to fail), but was surprised to see that the accuracy went up.<\/p>\n\n<p>I set up the experiment in azure ml, just to validate that it wasn't my code. Same thing there, 98.3333% accuracy with a single hidden node.<\/p>\n\n<p>Can anyone explain to me what is happening here?<\/p>",
        "Challenge_closed_time":1462119764790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1462108714660,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a neural network with a back-propagation trainer and tested it on the IRIS dataset. They initially used 7 hidden nodes and achieved good accuracy. However, when they reduced the number of nodes to 1, they were surprised to see that the accuracy actually increased. The user is seeking an explanation for this unexpected result.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36967126",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":7.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.0694805556,
        "Challenge_title":"Why do I get good accuracy with IRIS dataset with a single hidden node?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":4488.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426502047332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":825.0,
        "Poster_view_count":82.0,
        "Solution_body":"<p>First, it has been well established that a variety of classification models yield incredibly good results on Iris (Iris is very predictable); see <a href=\"http:\/\/lab.fs.uni-lj.si\/lasin\/wp\/IMIT_files\/neural\/doc\/seminar8.pdf\" rel=\"noreferrer\">here<\/a>, for example.<\/p>\n\n<p>Secondly, we can observe that there are relatively few features in the Iris dataset. Moreover, if you look at the <a href=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.names\" rel=\"noreferrer\">dataset description<\/a> you can see that two of the features are very highly correlated with the class outcomes.<\/p>\n\n<p>These correlation values are linear, single-feature correlations, which indicates that one can most likely apply a linear model and observe good results. Neural nets are highly nonlinear; they become more and more complex and capture greater and greater nonlinear feature combinations as the number of hidden nodes and hidden layers is increased.<\/p>\n\n<p>Taking these facts into account, that (a) there are few features to begin with and (b) that there are high linear correlations with class, would all point to a less complex, linear function as being the appropriate predictive model-- by using a single hidden node, you are very nearly using a linear model.<\/p>\n\n<p>It can also be noted that, in the absence of any hidden layer (i.e., just input and output nodes), and when the logistic transfer function is used, this is equivalent to logistic regression.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1462144469996,
        "Solution_link_count":2.0,
        "Solution_readability":12.5,
        "Solution_reading_time":18.6,
        "Solution_score_count":6.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":206.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":148.3679175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have written a pipeline that I want to run on a remote compute cluster within Azure Machine Learning. My aim is to process a large amount of historical data, and to do this I will need to run the pipeline on a large number of input parameter combinations.<\/p>\n\n<p>Is there a way to restrict the number of nodes that the pipeline uses on the cluster? By default it will use all the nodes available to the cluster, and I would like to restrict it so that it only uses a pre-defined maximum. This allows me to leave the rest of the cluster free for other users.<\/p>\n\n<p>My current code to start the pipeline looks like this:<\/p>\n\n<pre><code># Setup the pipeline\nsteps = [data_import_step] # Contains PythonScriptStep\npipeline = Pipeline(workspace=ws, steps=steps)\npipeline.validate()\n\n# Big long list of historical dates that I want to process data for\ndts = pd.date_range('2019-01-01', '2020-01-01', freq='6H', closed='left')\n# Submit the pipeline job\nfor dt in dts:\n    pipeline_run = Experiment(ws, 'my-pipeline-run').submit(\n        pipeline,\n        pipeline_parameters={\n            'import_datetime': dt.strftime('%Y-%m-%dT%H:00'),\n        }\n    )\n<\/code><\/pre>",
        "Challenge_closed_time":1592597207007,
        "Challenge_comment_count":2,
        "Challenge_created_time":1592308824897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to run a pipeline on a remote compute cluster within Azure Machine Learning to process a large amount of historical data. However, the user wants to restrict the number of nodes that the pipeline uses on the cluster to a pre-defined maximum, leaving the rest of the cluster free for other users. The user is seeking a solution to limit the number of nodes used by the pipeline.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62407943",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.9,
        "Challenge_reading_time":14.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":80.1061416667,
        "Challenge_title":"Restrict the number of nodes used by an Azure Machine Learning pipeine",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":274.0,
        "Challenge_word_count":174,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403698315160,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":1534.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>For me, the killer feature of Azure ML is not having to worry about load balancing like this. Our team has a compute target with <code>max_nodes=100<\/code> for every feature branch and we have <code>Hyperdrive<\/code> pipelines that result in 130 runs for each pipeline.<\/p>\n<p>We can submit multiple <code>PipelineRun<\/code>s back-to-back and the orchestrator does the heavy lifting of queuing, submitting, all the runs so that the <code>PipelineRun<\/code>s execute in the serial order I submitted them, and that the cluster is never overloaded. This works without issue for us 99% of the time.<\/p>\n<p>If what you're looking for is that you'd like the <code>PipelineRun<\/code>s to be executed in parallel, then you should check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-parallel-run-step#build-and-run-the-pipeline-containing-parallelrunstep\" rel=\"nofollow noreferrer\"><code>ParallelRunStep<\/code><\/a>.<\/p>\n<p>Another option is to isolate your computes. You can have up to 200 <code>ComputeTarget<\/code>s per workspace. Two 50-node <code>ComputeTarget<\/code>s cost the same as one 100-node <code>ComputeTarget<\/code>.<\/p>\n<p>On our team, we use <a href=\"https:\/\/www.pygit2.org\/\" rel=\"nofollow noreferrer\"><code>pygit2<\/code><\/a> to have a <code>ComputeTarget<\/code> created for each feature branch, so that, as data scientists, we can be confident that we're not stepping on our coworkers' toes.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1592842949400,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":18.57,
        "Solution_score_count":2.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":8.7381941667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use the pipeline functionality of dvc in a git repository. The data is managed otherwise and should not be versioned by dvc. The only functionality which is needed is that dvc reproduces the needed steps of the pipeline when <code>dvc repro<\/code> is called. Checking out the repository on a new system should lead to an 'empty' repository, where none of the pipeline steps are stored.<\/p>\n<p>Thus, - if I understand correctly - there is no need to track the dvc.lock file in the repository. However, adding dvc.lock to the .gitginore file leads to an error message:<\/p>\n<pre><code>ERROR: 'dvc.lock' is git-ignored.\n<\/code><\/pre>\n<p>Is there any way to disable the dvc.lock in .gitignore check for this usecase?<\/p>",
        "Challenge_closed_time":1624393487732,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624362030233,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to use the pipeline functionality of dvc in a git repository without versioning the data by dvc. The user needs dvc to reproduce the pipeline steps when called, and checking out the repository on a new system should lead to an 'empty' repository. However, adding dvc.lock to the .gitignore file leads to an error message, and the user is looking for a way to disable the dvc.lock in .gitignore check for this use case.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68082912",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":9.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":8.7381941667,
        "Challenge_title":"git-ignore dvc.lock in repositories where only the DVC pipelines are used",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":493.0,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542537900087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":147.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). See also helper <code>dvc stage add -O<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#options\" rel=\"nofollow noreferrer\">big O<\/a>, alias of <code>--outs-no-cache<\/code>).<\/p>\n<p>And the same for initial data dependencies, you can <code>dvc add --no-commit<\/code> them (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#options\" rel=\"nofollow noreferrer\">ref<\/a>).<\/p>\n<p>You do want to track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc.lock<\/a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.<\/p>\n<p>You'll be responsible for placing the right data files\/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro<\/code> or <code>dvc exp run<\/code> to behave as expected. <code>dvc checkout<\/code> won't be able to help you.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.9,
        "Solution_reading_time":17.17,
        "Solution_score_count":3.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":141.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":62.4823258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Good morning,  <br \/>\nI have a a dataset that consist of 99000 (256 x 256 pixels) images. I am trying to use this dataset to training a generative advesarial network (GAN) for at least a 1,000 epoch.   <br \/>\nCurrently, I am using a standard_NC24r (24 cores, 224 GB RAM, 1440 GB disk) GPU  (4 x NVIDIA Tesla K80) cluster but the training is slow. It takes about 3000 seconds to train 1 epoch. This implies it would take at least a month to complete training.  <br \/>\nIs a cluster that I can used to speed up training?  <\/p>\n<p>Thanks for your help in advance  <\/p>\n<p>Many thanks  <\/p>\n<p>Roland<\/p>",
        "Challenge_closed_time":1633928472660,
        "Challenge_comment_count":1,
        "Challenge_created_time":1633703536287,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has a dataset of 99,000 images and is trying to train a GAN for at least 1,000 epochs. They are currently using a standard_NC24r GPU cluster but the training is slow, taking 3000 seconds to train 1 epoch. The user is seeking advice on a faster cluster to speed up the training process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/583349\/best-compute-cluster-for-training-large-image-data",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.4,
        "Challenge_reading_time":7.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":62.4823258333,
        "Challenge_title":"Best compute cluster for training large image datasets !",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=5554be0b-de3f-4849-9c04-ab53140c4523\">@Okwen, Roland T  <\/a> Thanks, Instead of bigger machines with more memory, there are techniques to be used with Aml Compute for larger datasets. The <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/machine-learning-pipelines\/parallel-run\">Parallel Run<\/a> Step is an AzureML Pipeline Step which enables parallel processing or data partitions across multiple workers on multiple nodes. PRS (ParallelRunStep) is designed for embarrassingly parallel workload, e.g. train many models, batch inference, etc.    <\/p>\n<p>Also look into using some of the curated images provided for compute clusters.    <br \/>\nSpecifically look into the DASK image.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/resource-curated-environments\">Curated environments - Azure Machine Learning | Microsoft Learn<\/a>    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":12.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0858333333,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi Team,  \nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.  \nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e..  deployment package size is 50 MB.  \nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.  \nsample code for this api :   \n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.  \n  \nimport sagemaker  \nfrom sagemaker.amazon.amazon_estimator import get_image_uri  \ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')  \n  \nAny reference would be of great help. Thank you.",
        "Challenge_closed_time":1568642184000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568641875000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in adding sagemaker dependencies as external dependencies to upload to lambda due to the max size limit error. The package size is more than the allowed size limit of 50 MB. The user is trying to access the 'get_image_uri' API, which is not accessible with boto3. The user has provided sample code for the API and is seeking any reference to resolve the issue.",
        "Challenge_last_edit_time":1668590352780,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0858333333,
        "Challenge_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":338.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Could you explain in more detail why do you want to have sagemaker inside of a lambda please?",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1568642184000,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.4512016667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello Microsoft Q&amp;A Team,    <\/p>\n<p>I get the error     <\/p>\n<blockquote>\n<p>AssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available    <\/p>\n<\/blockquote>\n<p> when executing the following command:    <\/p>\n<blockquote>\n<p>pipeline_job = ml_client.jobs.create_or_update(    <br \/>\n    pipeline_job, experiment_name=&quot;data_preparation&quot;    <br \/>\n)    <br \/>\npipeline_job    <\/p>\n<\/blockquote>\n<p>Yesterday the command worked without an error. I did not make any changes. So I have no idea, what the problem is.    <\/p>\n<p>Thanks for helping me out.    <\/p>\n<p>Cheers    <\/p>\n<p>Lukas    <\/p>",
        "Challenge_closed_time":1667605270036,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667553245710,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an AssetException error with code \"Can't connect to HTTPS URL because the SSL module is not available\" while executing a command using ml_client.jobs. The error occurred suddenly without any changes made by the user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1075753\/aml-assetexception-error-with-code-cant-connect-to",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":9.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":14.4512016667,
        "Challenge_title":"AML - AssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available.",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=733bec54-8d30-4052-8297-64b100f6e3d4\">@Lukas  <\/a> Thanks for your question. Can you please add more details about the document\/sample that you are trying.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":2.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":46.4007888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I just started MLflow today and fail to display the log result on MLflow ui interface.\nWill appreciate a lot if someone can give me some hint..<\/p>\n<p>tried the sample code below<\/p>\n<pre><code>import os\nfrom random import random, randint\nfrom mlflow import log_metric, log_param, log_artifacts\n\nif __name__ == &quot;__main__&quot;:\n    # Log a parameter (key-value pair)\n    log_param(&quot;param1&quot;, randint(0, 100))\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(&quot;foo&quot;, random())\n    log_metric(&quot;foo&quot;, random() + 1)\n    log_metric(&quot;foo&quot;, random() + 2)\n\n    # Log an artifact (output file)\n    if not os.path.exists(&quot;outputs&quot;):\n        os.makedirs(&quot;outputs&quot;)\n    with open(&quot;outputs\/test.txt&quot;, &quot;w&quot;) as f:\n        f.write(&quot;hello world!&quot;)\n    log_artifacts(&quot;outputs&quot;)\n<\/code><\/pre>\n<p>ran the script above for 3 times and it gave me the result in the following structure. 3 folders representing 3 runs separately:<\/p>\n<pre><code>file:\/\/\/home\/devuser\/project\/mlruns\/0\n0 - 0737fec7d4824384b6320070cd688b78\n    355d57e092a242b7aa263451d280b497 \n    ed2614ffe2fd4f2db991d5d7166635f8  \n    meta.yaml\n<\/code><\/pre>\n<p>with folders\/files <code>artifacts, meta.yaml, metrics, params, tags<\/code> in each folder separately.<\/p>\n<p>I ran <code>mlflow ui<\/code> under <code>file:\/\/\/home\/devuser\/project\/mlruns\/<\/code> but nothing was showed on the interface. tried to look this up but no one has come across this problem with this kind of simple code.<\/p>\n<p>Appreciate a lot if someone could kindly let me know how I can change my setting.. Thank you..<\/p>",
        "Challenge_closed_time":1622968783720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622801740880,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble displaying log results on the MLflow UI interface despite running a sample code successfully and generating three folders representing three runs separately. The user has tried running \"mlflow ui\" under the directory where the folders are located but nothing is displayed on the interface. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67835498",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":21.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":46.4007888889,
        "Challenge_title":"MLflow - How to point interface path to show the expected result",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":379.0,
        "Challenge_word_count":193,
        "Platform":"Stack Overflow",
        "Poster_created_time":1445157877636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":267.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>You need to run <code>mlflow ui<\/code> in the project directory itself, not inside the <code>mlruns<\/code> - if you look into the <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui\" rel=\"nofollow noreferrer\">documentation for <code>mlflow ui<\/code> command<\/a>, it says:<\/p>\n<blockquote>\n<p><code>--default-artifact-root &lt;URI&gt;<\/code><\/p>\n<p>Path to local directory to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. <strong>Default: .\/mlruns<\/strong><\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.1,
        "Solution_reading_time":7.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":123.815,
        "Challenge_answer_count":0,
        "Challenge_body":"Forgot to create an issue in recent days.\r\nWhen tested with ```resume``` argument in ```WandBCallbacks```, i encountered this error. Here's the log:\r\n```python\r\n\r\n[Errno 2] No such file or directory: 'main'\r\n\/content\/main\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:override:78 - Overriding configuration...\r\n2022-04-04 12:21:56 | INFO     | classification\/pipeline.py:__init__:51 - {\r\n    \"global\": {\r\n        \"exp_name\": null,\r\n        \"exist_ok\": false,\r\n        \"debug\": true,\r\n        \"cfg_transform\": \"configs\/classification\/transform.yaml\",\r\n        \"save_dir\": \"\/content\/main\/runs\",\r\n        \"device\": \"cuda:0\",\r\n        \"use_fp16\": true,\r\n        \"pretrained\": null,\r\n        \"resume\": null\r\n    },\r\n    \"trainer\": {\r\n        \"name\": \"SupervisedTrainer\",\r\n        \"args\": {\r\n            \"num_iterations\": 2000,\r\n            \"clip_grad\": 10.0,\r\n            \"evaluate_interval\": 1,\r\n            \"print_interval\": 20,\r\n            \"save_interval\": 500\r\n        }\r\n    },\r\n    \"model\": {\r\n        \"name\": \"BaseTimmModel\",\r\n        \"args\": {\r\n            \"name\": \"convnext_tiny\",\r\n            \"from_pretrained\": true,\r\n            \"num_classes\": 180\r\n        }\r\n    },\r\n    \"loss\": {\r\n        \"name\": \"FocalLoss\"\r\n    },\r\n    \"callbacks\": [\r\n        {\r\n            \"name\": \"LoggerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"CheckpointCallbacks\",\r\n            \"args\": {\r\n                \"best_key\": \"bl_acc\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"VisualizerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"TensorboardCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"WandbCallbacks\",\r\n            \"args\": {\r\n                \"username\": \"lannguyen\",\r\n                \"project_name\": \"theseus_classification\",\r\n                \"resume\": true\r\n            }\r\n        }\r\n    ],\r\n    \"metrics\": [\r\n        {\r\n            \"name\": \"Accuracy\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"BalancedAccuracyMetric\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"F1ScoreMetric\",\r\n            \"args\": {\r\n                \"average\": \"weighted\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"ConfusionMatrix\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"ErrorCases\",\r\n            \"args\": null\r\n        }\r\n    ],\r\n    \"optimizer\": {\r\n        \"name\": \"AdamW\",\r\n        \"args\": {\r\n            \"lr\": 0.001,\r\n            \"weight_decay\": 0.0005,\r\n            \"betas\": [\r\n                0.937,\r\n                0.999\r\n            ]\r\n        }\r\n    },\r\n    \"scheduler\": {\r\n        \"name\": \"SchedulerWrapper\",\r\n        \"args\": {\r\n            \"scheduler_name\": \"cosine2\",\r\n            \"t_initial\": 7,\r\n            \"t_mul\": 0.9,\r\n            \"eta_mul\": 0.9,\r\n            \"eta_min\": 1e-06\r\n        }\r\n    },\r\n    \"data\": {\r\n        \"dataset\": {\r\n            \"train\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/train\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/val\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            }\r\n        },\r\n        \"dataloader\": {\r\n            \"train\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": true,\r\n                    \"shuffle\": false,\r\n                    \"collate_fn\": {\r\n                        \"name\": \"MixupCutmixCollator\",\r\n                        \"args\": {\r\n                            \"mixup_alpha\": 0.4,\r\n                            \"cutmix_alpha\": 1.0,\r\n                            \"weight\": [\r\n                                0.2,\r\n                                0.2\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"sampler\": {\r\n                        \"name\": \"BalanceSampler\",\r\n                        \"args\": null\r\n                    }\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": false,\r\n                    \"shuffle\": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:load_yaml:36 - Loading config from configs\/classification\/transform.yaml...\r\n2022-04-04 12:21:57 | DEBUG    | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - Calculating class distribution...\r\nDownloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth\r\nTraceback (most recent call last):\r\n  File \"\/content\/main\/configs\/classification\/train.py\", line 9, in <module>\r\n    train_pipeline = Pipeline(opts)\r\n  File \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__\r\n    registry=CALLBACKS_REGISTRY\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in <listcomp>\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively\r\n    return registry.get(config['name'])(**args, **kwargs)\r\nTypeError: type object got multiple values for keyword argument 'resume'\r\n```\r\n\r\nI guess because of the ```resume``` arg is both repeated in ```global``` and ```WandBCallbacks```. Maybe it also happens with ```Tensorboard```.",
        "Challenge_closed_time":1649731891000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1649286157000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create an index name using wandb and is looking for a solution to modify the name to \"modelname + save_folder_name\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kaylode\/theseus\/issues\/33",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.5,
        "Challenge_reading_time":51.74,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":41.0,
        "Challenge_repo_star_count":24.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":123.815,
        "Challenge_title":"Resume error in WandB.",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":326,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I will look into this soon. Crazily busy at the moment. This is not a bug, this happended because WandbCallbacks were used in the wrong way\r\n\r\nIn `pipeline.yaml`\r\n```python\r\n\"name\": \"WandbCallbacks\",\r\n\"args\": {\r\n    \"username\": \"lannguyen\",\r\n    \"project_name\": \"theseus_classification\",\r\n    \"resume\": true # <----- you didnt have to specify this\r\n}\r\n```\r\n\r\nThe repo havent been fully-well documented therefore it will be confusing sometimes.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":5.24,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0501694445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Because Azure Machine Learning now provides rich, consolidated capabilities for model training and deploying,\u202fwe'll retire the older Machine Learning Studio (classic) service on 31 August 2024. Please transition to using Azure Machine Learning by that date. If you have a question, please post it in this thread.  <\/p>",
        "Challenge_closed_time":1630044774267,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630044593657,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The older Machine Learning Studio (classic) service will be retired on 31 August 2024, and users are advised to transition to Azure Machine Learning, which provides consolidated capabilities for model training and deploying. Users are encouraged to ask any questions they may have in the provided thread.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/530215\/retirement-announcement-transition-to-azure-machin",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":5.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.0501694445,
        "Challenge_title":"Retirement Announcement - Transition to Azure Machine Learning by 31 August 2024",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Follow these steps to transition using Azure Machine Learning before 31 August 2024. Pricing may be subject to change, please view pricing <a href=\"https:\/\/azure.microsoft.com\/pricing\/details\/machine-learning\/\">here<\/a>.    <\/p>\n<ul>\n<li> Steps to migrate <a href=\"https:\/\/learn.microsoft.com\/azure\/machine-learning\/migrate-overview\">link<\/a>    <\/li>\n<li> Price <a href=\"https:\/\/azure.microsoft.com\/pricing\/details\/machine-learning\/#:%7E:text=Consumed%20Azure%20resources%20%28e.g.%20compute%2C%20storage%29%20%28No%20Azure,%24-%20%2B%20per%20vCPU%20hour%20Edition%3A%20Basic%20Enterprise\">link<\/a>    <\/li>\n<\/ul>\n<p>If you have any additional queries regarding this retirement, please use comments on this thread to ask your specific queries and we will try our best to answer those queries.    <\/p>\n",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.5,
        "Solution_reading_time":10.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1447522907212,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":758.0,
        "Answerer_view_count":95.0,
        "Challenge_adjusted_solved_time":39.3668766667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to use optuna for searching hyper parameter spaces.<\/p>\n\n<p>In one particular scenario I train a model on a machine with a few GPUs.\nThe model and batch size allows me to run 1 training per 1 GPU.\nSo, ideally I would like to let optuna spread all trials across the available GPUs\nso that there is always 1 trial running on each GPU.<\/p>\n\n<p>In the <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#how-can-i-use-two-gpus-for-evaluating-two-trials-simultaneously\" rel=\"nofollow noreferrer\">docs<\/a> it says, I should just start one process per GPU in a separate terminal like:<\/p>\n\n<pre><code>CUDA_VISIBLE_DEVICES=0 optuna study optimize foo.py objective --study foo --storage sqlite:\/\/\/example.db\n<\/code><\/pre>\n\n<p>I want to avoid that because the whole hyper parameter search continues in multiple rounds after that. I don't want to always manually start a process per GPU, check when all are finished, then start the next round.<\/p>\n\n<p>I saw <code>study.optimize<\/code> has a <code>n_jobs<\/code> argument.\nAt first glance this seems to be perfect.\n<em>E.g.<\/em> I could do this:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial):\n    # the actual model would be trained here\n    # the trainer here would need to know which GPU\n    # it should be using\n    best_val_loss = trainer(**trial.params)\n    return best_val_loss\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100, n_jobs=8)\n<\/code><\/pre>\n\n<p>This starts multiple threads each starting a training.\nHowever, the trainer within <code>objective<\/code> somehow needs to know which GPU it should be using.\nIs there a trick to accomplish that?<\/p>",
        "Challenge_closed_time":1589464897583,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589323176827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use optuna for hyperparameter search and wants to spread all trials across available GPUs. The documentation suggests starting one process per GPU in a separate terminal, but the user wants to avoid that. The user has tried using the `n_jobs` argument in `study.optimize` to start multiple threads, but the trainer within `objective` needs to know which GPU to use. The user is looking for a way to pass arguments to multiple jobs in optuna.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61763206",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":21.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":39.3668766667,
        "Challenge_title":"Is there a way to pass arguments to multiple jobs in optuna?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2684.0,
        "Challenge_word_count":231,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447522907212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":758.0,
        "Poster_view_count":95.0,
        "Solution_body":"<p>After a few mental breakdowns I figured out that I can do what I want using a <code>multiprocessing.Queue<\/code>. To get it into the objective function I need to define it as a lambda function or as a class (I guess partial also works). <em>E.g.<\/em><\/p>\n\n<pre><code>from contextlib import contextmanager\nimport multiprocessing\nN_GPUS = 2\n\nclass GpuQueue:\n\n    def __init__(self):\n        self.queue = multiprocessing.Manager().Queue()\n        all_idxs = list(range(N_GPUS)) if N_GPUS &gt; 0 else [None]\n        for idx in all_idxs:\n            self.queue.put(idx)\n\n    @contextmanager\n    def one_gpu_per_process(self):\n        current_idx = self.queue.get()\n        yield current_idx\n        self.queue.put(current_idx)\n\n\nclass Objective:\n\n    def __init__(self, gpu_queue: GpuQueue):\n        self.gpu_queue = gpu_queue\n\n    def __call__(self, trial: Trial):\n        with self.gpu_queue.one_gpu_per_process() as gpu_i:\n            best_val_loss = trainer(**trial.params, gpu=gpu_i)\n            return best_val_loss\n\nif __name__ == '__main__':\n    study = optuna.create_study()\n    study.optimize(Objective(GpuQueue()), n_trials=100, n_jobs=8)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":13.34,
        "Solution_score_count":6.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":109.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1595690872796,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Deutschland",
        "Answerer_reputation_count":699.0,
        "Answerer_view_count":46.0,
        "Challenge_adjusted_solved_time":78.6193025,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For a project I am working on, which uses annual financial reports data (of multiple categories) from companies which have been successful or gone bust\/into liquidation, I previously created a (fairly well performing) model on AWS Sagemaker using a multiple linear regression algorithm (specifically, the AWS stock algorithm for logistic regression\/classification problems - the 'Linear Learner' algorithm)<\/p>\n<p>This model just produces a simple &quot;company is in good health&quot; or &quot;company looks like it will go bust&quot; binary prediction, based on one set of annual data fed in; e.g.<\/p>\n<pre><code>query input: {data:[{\n&quot;Gross Revenue&quot;: -4000,\n&quot;Balance Sheet&quot;: 10000,\n&quot;Creditors&quot;: 4000,\n&quot;Debts&quot;: 1000000 \n}]}\n\ninference output: &quot;in good health&quot; \/ &quot;in bad health&quot;\n<\/code><\/pre>\n<p>I trained this model by just ignoring what year for each company the values were from and pilling in all of the annual financial reports data (i.e. one years financial data for one company = one input line) for the training, along with the label of &quot;good&quot; or &quot;bad&quot; - a good company was one which has existed for a while, but hasn't gone bust, a bad company is one which was found to have eventually gone bust; e.g.:<\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue<\/th>\n<th>Balance Sheet<\/th>\n<th>Creditors<\/th>\n<th>Debts<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>0<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I hence used these multiple features (gross revenue, balance sheet...) along with the label (good\/bad) in my training input, to create my first model.<\/p>\n<p>I would like to use the same features as before as input (gross revenue, balance sheet..) but over multiple years; e.g take the values from 2020 &amp; 2019 and use these (along with the eventual company status of &quot;good&quot; or &quot;bad&quot;) as the singular input for my new model. However I'm unsure of the following:<\/p>\n<ul>\n<li>is this an inappropriate use of logistic regression Machine learning? i.e. is there a more suitable algorithm I should consider?<\/li>\n<li>is it fine, or terribly wrong to try and just use the same technique as before, but combine the data for both years into one input line like:<\/li>\n<\/ul>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>label<\/th>\n<th>Gross Revenue(2019)<\/th>\n<th>Balance Sheet(2019)<\/th>\n<th>Creditors(2019)<\/th>\n<th>Debts(2019)<\/th>\n<th>Gross Revenue(2020)<\/th>\n<th>Balance Sheet(2020)<\/th>\n<th>Creditors(2020)<\/th>\n<th>Debts(2020)<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>good<\/td>\n<td>10000<\/td>\n<td>20000<\/td>\n<td>0<\/td>\n<td>0<\/td>\n<td>30000<\/td>\n<td>10000<\/td>\n<td>40<\/td>\n<td>500<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>100<\/td>\n<td>50<\/td>\n<td>200<\/td>\n<td>50000<\/td>\n<td>100<\/td>\n<td>5<\/td>\n<td>100<\/td>\n<td>10000<\/td>\n<\/tr>\n<tr>\n<td>bad<\/td>\n<td>5000<\/td>\n<td>0<\/td>\n<td>2000<\/td>\n<td>800000<\/td>\n<td>2000<\/td>\n<td>0<\/td>\n<td>4<\/td>\n<td>100000000<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div>\n<p>I would personally expect that a company which has gotten worse over time (i.e. companies finances are worse in 2020 than in 2019) should be more likely to be found to be a &quot;bad&quot;\/likely to go bust, so I would hope that, if I feed in data like in the above example (i.e. earlier years data comes before later years data, on an input line) my training job ends up creating a model which gives greater weighting to the earlier years data, when making predictions<\/p>\n<p>Any advice or tips would be greatly appreciated - I'm pretty new to machine learning and would like to learn more<\/p>\n<p>UPDATE:<\/p>\n<p>Using Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNN) is one potential route I think I could try taking, but this seems to commonly just be used with multivariate data over many dates; my data only has 2 or 3 dates worth of multivariate data, per company. I would want to try using the data I have for all the companies, over the few dates worth of data there are, in training<\/p>",
        "Challenge_closed_time":1614450076540,
        "Challenge_comment_count":1,
        "Challenge_created_time":1613315731557,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has previously created a model on AWS Sagemaker using a multiple linear regression algorithm for binary prediction of a company's financial health based on one set of annual data fed in. The user now wants to use the same features as before but over multiple years and is unsure if this is an appropriate use of logistic regression machine learning or if there is a more suitable algorithm. The user is also unsure if it is fine to combine the data for both years into one input line and hopes that the model gives greater weighting to the earlier years' data when making predictions. The user is considering using Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNN) but is unsure if it is suitable for their data.",
        "Challenge_last_edit_time":1614168553343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66196815",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.6,
        "Challenge_reading_time":54.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":315.0958286111,
        "Challenge_title":"Using Logistic Regression For Timeseries Data in Amazon SageMaker",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":597,
        "Platform":"Stack Overflow",
        "Poster_created_time":1566839876032,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>I once developed a so called Genetic Time Series in R. I used a Genetic Algorithm which sorted out the best solutions from multivariate data, which were fitted on a VAR in differences or a VECM. Your data seems more macro economic or financial than user-centric and VAR or VECM seems appropriate. (Surely it is possible to treat time-series data in the same way so that we can use LSTM or other approaches, but these are very common) However, I do not know if VAR in differences or VECM works with binary classified labels. Perhaps if you would calculate a metric outcome, which you later label encode to a categorical feature (or label it first to a categorical) than VAR or VECM may also be appropriate.<\/p>\n<p>However you may add all yearly data points to one data points per firm to forecast its survival, but you would loose a lot of insight. If you are interested in time series ML which works a little bit different than for neural networks or elastic net (which could also be used with time series) let me know. And we can work something out. Or I'll paste you some sources.<\/p>\n<p>Summary:\n1.)\nIt is possible to use LSTM, elastic NEt (time points may be dummies or treated as cross sectional panel) or you use VAR in differences and VECM with a slightly different out come variable<\/p>\n<p>2.)\nIt is possible but you will loose information over time.<\/p>\n<p>All the best,\nPatrick<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1614451582832,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":16.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":247.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":6.7239763889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an AKS cluster using Azure Machine Learning SDK extension and I attached to the workspace created. When the cluster is created and attached, I doesn't show any error. When I am trying to detach it from workspace, it is not accepting the operations.<\/p>\n<p>I would like to detach the existing AKS cluster from workspace either by program manner, using CLI or even using Azure portal.<\/p>",
        "Challenge_closed_time":1659333006088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659308799773,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to detach an AKS cluster from a workspace created using Azure Machine Learning SDK extension. The cluster was created and attached without any issues, but detaching it is not working through program, CLI, or Azure portal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73187536",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":6.7239763889,
        "Challenge_title":"Error while detaching AKS cluster through Azure ML SDK extension",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>If we are using any <strong>extensions of SDK or Azure CLI<\/strong> for machine learning to detach AKS cluster, it <strong>will not work<\/strong> and it will not get deleted or detached. Instead, we need to use <strong>Azure CLI with AKS<\/strong>. There are two types of implementations we can perform.<\/p>\n<p><strong>Python:<\/strong><\/p>\n<pre><code>Aks_target.detach()\n<\/code><\/pre>\n<p><strong>Azure CLI:<\/strong><\/p>\n<p>Before performing this step, we need to get the details of the working AKS cluster name attached to our workspace. Resource Group details and workspace name<\/p>\n<pre><code>az ml computertarget detach -n youraksname -g yourresourcegroup -w yourworkspacename\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":8.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":171.7806,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed an ANN tool by using pycharm\/tensorflow on my own computer. I uploaded the h5 and json files to Amazon Sagemaker by creating a Notebook Instance. I was finally able to successfully create an endpoint and make it work. The following code in Notebook Instance -Jupyter works:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\nclient = boto3.client('runtime.sagemaker')\ndata = np.random.randn(1,6).tolist()\nendpoint_name = 'sagemaker-tensorflow-**********'\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>However, the problem occurs when I created a lambda function and call the endpoint from there. The input should be a row of 6 features -that is a 1-by-6 vector. I enter the following input into lambda {&quot;data&quot;:&quot;1,1,1,1,1,1&quot;} and it gives me the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 20, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n<\/code><\/pre>\n<p>I think the problem is that the input needs to be 1-by-6 instead of 6-by-1 and I don't know how to do that.<\/p>",
        "Challenge_closed_time":1599633575463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599015165303,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has developed an ANN tool using PyCharm\/TensorFlow and uploaded the h5 and json files to Amazon Sagemaker by creating a Notebook Instance. The endpoint works fine when called from the Notebook Instance, but when the user created a Lambda function and called the endpoint from there, an error occurred. The user suspects that the input needs to be 1-by-6 instead of 6-by-1, but is unsure how to fix it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63698011",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":20.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":171.7806,
        "Challenge_title":"AWS Notebook Instance is working but Lambda is not accepting the input",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":187,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369539919747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":311.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>I assume the content type you specified is <code>text\/csv<\/code>, so try out:<\/p>\n<pre><code>{&quot;data&quot;: [&quot;1,1,1,1,1,1&quot;]}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1383025927423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dhaka, Bangladesh",
        "Answerer_reputation_count":647.0,
        "Answerer_view_count":105.0,
        "Challenge_adjusted_solved_time":1006.4421325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Sagmaker. I am creating a pipeline in sagemaker where I initialize the number of epochs as a pipeline parameter. But when I upsert, it shows this error.\nCheck the following code for reference, please.<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\npipeline = Pipeline(\nname=f&quot;a_name&quot;,\nparameters=[\n    training_instance_type,\n    training_instance_count,\n    epoch_count,\n    hugging_face_model_name,\n    endpoint_instance_type,\n    endpoint_instance_type_alternate,\n],\nsteps=[step_train, step_register, step_deploy_lambda],\nsagemaker_session=sagemaker_session,\n<\/code><\/pre>\n<p>)<\/p>\n<p>Error - ---<\/p>\n<pre><code>---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-54-138a517611f0&gt; in &lt;module&gt;\n----&gt; 1 pipeline.upsert(role_arn=role)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in upsert(self, role_arn, description, tags, parallelism_config)\n    217         &quot;&quot;&quot;\n    218         try:\n--&gt; 219             response = self.create(role_arn, description, tags, parallelism_config)\n    220         except ClientError as e:\n    221             error = e.response[&quot;Error&quot;]\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in create(self, role_arn, description, tags, parallelism_config)\n    119             Tags=tags,\n    120         )\n--&gt; 121         return self.sagemaker_session.sagemaker_client.create_pipeline(**kwargs)\n    122 \n    123     def _create_args(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    389                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    390             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 391             return self._make_api_call(operation_name, kwargs)\n    392 \n    393         _api_call.__name__ = str(py_operation_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    717             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    718             error_class = self.exceptions.from_code(error_code)\n--&gt; 719             raise error_class(parsed_response, operation_name)\n    720         else:\n    721             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreatePipeline operation: Cannot assign property reference [Parameters.EpochCount] to argument of type [String]\n<\/code><\/pre>",
        "Challenge_closed_time":1645641627683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645534662633,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a ValidationException error while creating a pipeline in Sagemaker. The error occurs when the user initializes the number of epochs as a pipeline parameter and tries to upsert. The error message indicates that the property reference [Parameters.EpochCount] cannot be assigned to an argument of type [String].",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71221741",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":32.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":29.7125138889,
        "Challenge_title":"ValidationException in Sagemaker pipeline creation",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":608.0,
        "Challenge_word_count":195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383025927423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dhaka, Bangladesh",
        "Poster_reputation_count":647.0,
        "Poster_view_count":105.0,
        "Solution_body":"<p>I replace<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>epoch_count = ParameterString(name=&quot;EpochCount&quot;, default_value=&quot;1&quot;)\n<\/code><\/pre>\n<p>And it works. Maybe we can only use an integer in pipeline parameters from the sagemaker notebook. But epoch_count is being used in the docker container, which is not directly something of Sagemaker, and that's my understanding.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1649157854310,
        "Solution_link_count":0.0,
        "Solution_readability":16.6,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1285875105172,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Cruz, CA, United States",
        "Answerer_reputation_count":4051.0,
        "Answerer_view_count":461.0,
        "Challenge_adjusted_solved_time":147.4295269444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Sagemaker and have a bunch of model.tar.gz files that I need to unpack and load in sklearn. I've been testing using list_objects with delimiter to get to the tar.gz files:<\/p>\n\n<pre><code>response = s3.list_objects(\nBucket = bucket,\nPrefix = 'aleks-weekly\/models\/',\nDelimiter = '.csv'\n)\n\n\nfor i in response['Contents']:\n    print(i['Key'])\n<\/code><\/pre>\n\n<p>And then I plan to extract with<\/p>\n\n<pre><code>import tarfile\ntf = tarfile.open(model.read())\ntf.extractall()\n<\/code><\/pre>\n\n<p>But how do I get to the actual tar.gz file from s3 instead of a some boto3 object? <\/p>",
        "Challenge_closed_time":1566337639600,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565806893303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to unpack and load model.tar.gz files in sklearn using Sagemaker. They have used list_objects with delimiter to get to the tar.gz files and plan to extract them using tarfile. However, they are unsure how to access the actual tar.gz file from s3 instead of a boto3 object.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57500105",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":147.4295269444,
        "Challenge_title":"Python boto3 load model tar file from s3 and unpack it",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":4787.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>You can download objects to files using <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.download_file\" rel=\"nofollow noreferrer\"><code>s3.download_file()<\/code><\/a>. This will make your code look like:<\/p>\n\n<pre><code>s3 = boto3.client('s3')\nbucket = 'my-bukkit'\nprefix = 'aleks-weekly\/models\/'\n\n# List objects matching your criteria\nresponse = s3.list_objects(\n    Bucket = bucket,\n    Prefix = prefix,\n    Delimiter = '.csv'\n)\n\n# Iterate over each file found and download it\nfor i in response['Contents']:\n    key = i['Key']\n    dest = os.path.join('\/tmp',key)\n    print(\"Downloading file\",key,\"from bucket\",bucket)\n    s3.download_file(\n        Bucket = bucket,\n        Key = key,\n        Filename = dest\n    )\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.7,
        "Solution_reading_time":9.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.4099408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What are the types of Regression algorithm? Are there any kinds of regression called &quot;non-linear regression&quot;?<\/p>",
        "Challenge_closed_time":1608817848270,
        "Challenge_comment_count":2,
        "Challenge_created_time":1608719172483,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information about the types of regression algorithms and whether there is a type called \"non-linear regression.\"",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/208685\/types-of-regression-algorithm",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.3,
        "Challenge_reading_time":2.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":27.4099408334,
        "Challenge_title":"Types of Regression Algorithm",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":19,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=4bd5b942-e3cc-4aa2-8ef5-03d77bfd1721\">@Sanniddha Chakrabarti  <\/a> Please follow the below document for Regression.    <br \/>\n<a href=\"https:\/\/www.analyticsvidhya.com\/blog\/2015\/08\/comprehensive-guide-regression\/#:%7E:text=Regression%20analysis%20is%20a%20form,effect%20relationship%20between%20the%20variables\">https:\/\/www.analyticsvidhya.com\/blog\/2015\/08\/comprehensive-guide-regression\/#:~:text=Regression%20analysis%20is%20a%20form,effect%20relationship%20between%20the%20variables<\/a>.    <\/p>\n<p>Types of Regressions:    <br \/>\nLinear Regression    <br \/>\nLogistic Regression    <br \/>\nPolynomial Regression    <br \/>\nStepwise Regression    <br \/>\nRidge Regression    <br \/>\nLasso Regression    <br \/>\nElasticNet Regression    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":33.3,
        "Solution_reading_time":10.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.5982666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to load files as a dataset in the GUI of Azure ML Studio. These parquet files have been created through Spark.  <\/p>\n<p>In my folder, Spark creates files such as &quot;_SUCCESS&quot; or &quot;_committed_8998000&quot;.   <\/p>\n<p>Azure ML Studio is not able to read them or ignore them and tells me:  <\/p>\n<pre><code>The provided file(s) have invalid byte(s) for the specified file encoding.\n{\n  &quot;message&quot;: &quot; &quot;\n}\n<\/code><\/pre>\n<p>I selected &quot;Ignore unmatched files path&quot; and yet, it still does not work.  <\/p>\n<p>If I remove the &quot;_SUCCESS&quot; and other Spark files, it works.   <\/p>\n<p>Does anyone have an idea about a workaround?  <\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1601535069840,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601468116080,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in creating a dataset in Azure ML Studio from a parquet file created with Azure Spark. The GUI is unable to read the files due to invalid byte(s) for the specified file encoding. Even after selecting \"Ignore unmatched files path,\" the issue persists. The problem is resolved when the user removes the Spark files such as \"_SUCCESS.\" The user is seeking a workaround for this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/112778\/how-can-i-create-a-dataset-in-azure-ml-studio-(thr",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":9.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":18.5982666667,
        "Challenge_title":"How can I create a dataset in Azure ML studio (through the GUI) from a parquet file created with Azure Spark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":124,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I used &quot;path\/<em>\/<\/em>.parquet&quot; in the &quot;Path&quot; field and now it works.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":1.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1572457215103,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United Kingdom",
        "Answerer_reputation_count":42593.0,
        "Answerer_view_count":5761.0,
        "Challenge_adjusted_solved_time":0.3024208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In python I would like to check that a given function is called within a <code>with<\/code> statement of a given type<\/p>\n<pre><code>class Bar:\n def __init__(self, x):\n  self.x = x\n def __enter__(self):\n  return self\n def __exit__(self, *a, **k):\n  pass\n\ndef foo(x):\n # assert that the enclosing context is an instance of bar\n # assert isinstance('enclosed context', Bar)\n print(x*2)\n\nwith Bar(1) as bar:\n foo(bar.x)\n<\/code><\/pre>\n<p>I could do something like enforcing an arg passed into <code>foo<\/code> and wrapping functions in a decorator i.e.<\/p>\n<pre><code>class Bar:\n def __init__(self, x):\n  self.x = x\n def __enter__(self):\n  return self\n def __exit__(self, *a, **k):\n  pass\n\ndef assert_bar(func):\n def inner(bar, *a, **k):\n  assert isinstance(bar, Bar)\n  return func(*a, **k)\n return inner\n\n\n@assert_bar\ndef foo(x):\n print(x*2)\n\nwith Bar(1) as bar:\n foo(bar, bar.x)\n\n<\/code><\/pre>\n<p>but then I would have to pass around <code>bar<\/code> everywhere.<\/p>\n<p>As a result I'm trying to see if there's a way to access the <code>with<\/code> context<\/p>\n<p>Note: The real world application of this is ensuring that <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#mlflow.pyfunc.log_model\" rel=\"nofollow noreferrer\"><code>mlflow.pyfunc.log_model<\/code><\/a> is called within an <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.ActiveRun\" rel=\"nofollow noreferrer\"><code>mlflow.ActiveRun<\/code><\/a> context, or it leaves an <code>ActiveRun<\/code> open, causing problems later on<\/p>",
        "Challenge_closed_time":1644974017812,
        "Challenge_comment_count":6,
        "Challenge_created_time":1644973389773,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to check if a given function is called within a specific type of `with` statement in Python. They have tried enforcing an argument passed into the function and wrapping functions in a decorator, but they would have to pass around the `with` context everywhere. The user is looking for a way to access the `with` context to ensure that a specific function is called within it. The real-world application of this is to ensure that `mlflow.pyfunc.log_model` is called within an `mlflow.ActiveRun` context to avoid leaving an `ActiveRun` open and causing problems later on.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71135260",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":10.6,
        "Challenge_reading_time":20.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.1744552778,
        "Challenge_title":"Python assert a function is called within a certain `with` statement's context",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473193743196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":617.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>Here's an ugly way to do it: global state.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class Bar:\n    active = 0\n    def __init__(self, x):\n        self.x = x\n    def __enter__(self):\n        Bar.active += 1\n        return self\n    def __exit__(self, *a, **k):\n        Bar.active -= 1\n\nfrom functools import wraps\n\ndef assert_bar(func):\n    @wraps(func)\n    def wrapped(*vargs, **kwargs):\n        if Bar.active &lt;= 0:\n            # raises even if asserts are disabled\n            raise AssertionError()\n        return func(*vargs, **kwargs)\n    return wrapped\n<\/code><\/pre>\n<p>Unfortunately I don't think there is any non-ugly way to do it. If you aren't going to pass around a <code>Bar<\/code> instance yourself then you must rely on some state existing somewhere else to tell you that a <code>Bar<\/code> instance exists and is currently being used as a context manager.<\/p>\n<p>The only way you can avoid that global state is to store the state in the instance, which means the decorator needs to be an instance method and the instance needs to exist before the function is declared:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from functools import wraps\n\nclass Bar:\n    def __init__(self, x):\n        self.x = x\n        self.active = 0\n    def __enter__(self):\n        self.active += 1\n        return self\n    def __exit__(self, *a, **k):\n        self.active -= 1\n    def assert_this(self, func):\n        @wraps(func)\n        def wrapped(*vargs, **kwargs):\n            if self.active &lt;= 0:\n                raise AssertionError()\n            return func(*vargs, **kwargs)\n        return wrapped\n\nbar = Bar(1)\n\n@bar.assert_this\ndef foo(x):\n    print(x + 1)\n\nwith bar:\n    foo(1)\n<\/code><\/pre>\n<p>This is still &quot;global state&quot; in the sense that the function <code>foo<\/code> now holds a reference to the <code>Bar<\/code> instance that holds the state. But it may be more palatable if <code>foo<\/code> is only ever going to be a local function.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1644974478488,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":21.9,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":245.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":159.9731416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello :)     <\/p>\n<p>Do you have any kind of idea when Azure Machine Learning Python SDK V2 could support parallel computing? We are testing things out with the machine learning studio and we are in a bit confusing stage that should we go with the SDK V1 or V2, but seemingly the V2 is not yet supporting multiple nodes in compute clusters.    <\/p>\n<p>Best regards,    <br \/>\nTuomas<\/p>",
        "Challenge_closed_time":1657774403743,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657198500433,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about when Azure Machine Learning Python SDK V2 will support parallel computing as they are currently testing with the machine learning studio and are unsure whether to use SDK V1 or V2, as V2 does not yet support multiple nodes in compute clusters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/918129\/parallel-computing-with-python-sdk-v2",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":5.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":159.9731416667,
        "Challenge_title":"Parallel computing with Python SDK V2",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=618a88ea-c762-4907-9a08-ae41864a250e\">@Tuomas Partanen  <\/a>     <\/p>\n<p>I have a good news for you, we are testing Parallel Run Step NOW in private preview of V2.     <\/p>\n<p>For your scenario, v1 is stable and serving all production customers. v2 (through DPv2) is still in private preview, and there are some dependency on new dataset\/mltable implementation. So if you want to seriously put some production traffic, I suggest guide to v1; but if you just want to have some prototypes, v2 may be better, as v2 is growing but v1 will not. Also, V2 will have the feature you want - Parallel.     <\/p>\n<p>The estimate time is not confirmed but should be around October.    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/em>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":10.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":138.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.0508991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>we have also found this example of using <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-use-databricks-as-compute-target.ipynb\">Databricks as a Compute Target for an Azure Machine Learning Pipeline<\/a>.  <\/p>\n<p>However, we want to use an existing Databricks Cluster as compute target within Azure Machine Learning Studio for our Azure Machine Learning Pipeline.  <br \/>\nCould you help us in accomplishing this, please?  <\/p>\n<p>With best regards  <br \/>\nAlex  <\/p>",
        "Challenge_closed_time":1654664851167,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654614267930,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking assistance in connecting an existing Databricks Cluster as a compute target within Azure Machine Learning Studio for their Azure Machine Learning Pipeline. They have found an example of using Databricks as a Compute Target for an Azure Machine Learning Pipeline but require guidance on how to use an existing cluster.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/880189\/connecting-to-an-existing-databricks-cluster-in-am",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.0508991667,
        "Challenge_title":"Connecting to an existing Databricks Cluster in AMLS",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@AlexanderPakakis-0994 Are you looking at adding the cluster from the UI of ML studio rather than using the SDK as mentioned in the notebook you referenced?    <br \/>\nIf Yes, you need to add the same attached compute.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209283-image.png?platform=QnA\" alt=\"209283-image.png\" \/>    <\/p>\n<p>Once you select Azure Databricks the following option to add the existing databricks workspace is seen.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209260-image.png?platform=QnA\" alt=\"209260-image.png\" \/>    <\/p>\n<p>I hope this helps!!    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":10.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.2,
        "Solution_reading_time":13.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.6317019444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I trained a model with Designer, created a real-time inference pipeline which was succesfully submitted. When deploying to either ACI or AKS it fails and I get the error &quot;ModuleNotFoundError: No module named 'azureml.api'&quot;. I've had no problems deploying this model many times in the past and haven't changed anything. Even if I use one of the sample pipelines (automobiles basic), I get the same error when deploying to real-time. <\/p>",
        "Challenge_closed_time":1632968850120,
        "Challenge_comment_count":2,
        "Challenge_created_time":1632862175993,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues while deploying a model from Designer to ACI or AKS. The error message \"ModuleNotFoundError: No module named 'azureml.api'\" is displayed, even when using sample pipelines. The user has successfully deployed the model in the past and has not made any changes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/569925\/deployment-from-designer-fails-in-every-possible-w",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":29.6317019444,
        "Challenge_title":"Deployment from Designer fails in every possible way",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>It's an known issue caused by unexpected module version upgrade. It's been resolved by applying hotfix to all regions. For users, please rerun training pipeline by check on &quot;Regenerate Output&quot;, and run corresponding inference pipeline and try deployment again.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":3.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1580472638260,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":127.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":65.4279266667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm training a Seq2Seq model on Tensorflow on a ml.p3.2xlarge instance. When I tried running the code on google colab, the time per epoch was around 40 mins. However on the instance it's around 5 hours!<\/p>\n<p>This is my training code<\/p>\n<pre><code>def train_model(train_translator, dataset, path, num=8):\n\n  with tf.device(&quot;\/GPU:0&quot;):\n    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n                                                save_weights_only=True,\n                                                 verbose=1)\n    batch_loss = BatchLogs('batch_loss')\n    train_translator.fit(dataset, epochs=num,callbacks=[batch_loss,cp_callback])  \n\n  return train_translator\n<\/code><\/pre>\n<p>I have also tried without the <code>tf.device<\/code> command and I still get the same timing. Am I doing something wrong?<\/p>",
        "Challenge_closed_time":1628917656376,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628682115840,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with their Sagemaker instance not utilizing the GPU during training of a Seq2Seq model on Tensorflow. The training time per epoch on the instance is around 5 hours, while on Google Colab it was around 40 minutes. The user has tried using the <code>tf.device<\/code> command and without it, but the timing remains the same.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68741326",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":65.4279266667,
        "Challenge_title":"Sagemaker Instance not utilising GPU during training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":995.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580472638260,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":127.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>I had to force GPU use with the help of<\/p>\n<pre><code>with tf.device('\/device:GPU:0')\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":13.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":21.2124047222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I keep getting this error in sagemaker when iterating through pytorch dataloader batch cycles:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self._process_data(data), w_id)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1299, in _process_data\n    data.reraise()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/_utils.py&quot;, line 429, in reraise\n    raise self.exc_type(msg)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 84, in __init__\n    super(HTTPClientError, self).__init__(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 40, in __init__\n    msg = self.fmt.format(**kwargs)\nKeyError: 'error'\n\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-1-81655136a841&gt; in &lt;module&gt;\n     58                             py_version='py3')\n     59 \n---&gt; 60 pytorch_estimator.fit({'train': Runtime.dataset_path}, job_name=Runtime.job_name)\n     61 \n     62 #print(pytorch_estimator.latest_job_tensorboard_artifacts_path())\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    955         self.jobs.append(self.latest_training_job)\n    956         if wait:\n--&gt; 957             self.latest_training_job.wait(logs=logs)\n    958 \n    959     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1954         # If logs are requested, call logs_for_jobs.\n   1955         if logs != &quot;None&quot;:\n-&gt; 1956             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1957         else:\n   1958             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3751 \n   3752         if wait:\n-&gt; 3753             self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n   3754             if dot:\n   3755                 print()\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3304                 ),\n   3305                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3306                 actual_status=status,\n   3307             )\n   3308 \n\nUnexpectedStatusException: Error for Training job 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/opt\/conda\/bin\/python3.6 main.py --runtime_var dataset_name=U12239-2022-05-09-14-39-18,job_name=2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training,model_name=pix2pix&quot;\n\n  0%|          | 0\/248 [00:00&lt;?, ?it\/s]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\nTraceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self\n<\/code><\/pre>\n<p>Here is the code which results in the error:<\/p>\n<pre><code>def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce, g_scaler, d_scaler,runtime_log_folder,runtime_log_file_name):\n\n    total_output=''\n    \n    loop = tqdm(loader, leave=True)\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\n    print(&quot;Loop&quot;)\n    print(loop)\n    print(&quot;Length loop&quot;)\n    print(len(loop))\n    for idx, (x, y) in enumerate(loop): #&lt;--error happens here\n        print(&quot;Loop index&quot;)\n        print(idx)\n        print(&quot;Loop item&quot;)\n        print(x,y)\n        x = x.to(device)\n        y = y.to(device)\n        \n        # train discriminator\n        with torch.cuda.amp.autocast():\n            y_fake = gen(x)\n\n            D_real = disc(x, y)\n            D_fake = disc(x, y_fake.detach())\n            # use detach so as to avoid breaking computational graph when do optimizer.step on discriminator\n            # can use detach, or when do loss.backward put loss.backward(retain_graph = True)\n\n            D_real_loss = bce(D_real, torch.ones_like(D_real))\n            D_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            D_loss = (D_real_loss + D_fake_loss) \/ 2\n            \n            # log tensorboard\n            \n        disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n        \n        # train generator\n        with torch.cuda.amp.autocast():\n            \n            D_fake = disc(x, y_fake)\n\n            # compute fake loss\n            # trick discriminator to believe these are real, hence send in torch.oneslikedfake\n            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            # compute L1 loss\n            L1 = l1(y_fake, y) * args.l1_lambda\n\n            G_loss = G_fake_loss + L1\n            \n            # log tensorboard\n           \n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n        \n        # print epoch, generator loss, discriminator loss\n        print(f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]')\n        output = f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]\\n'\n        total_output+=output\n\n\n\n    runtime_log = get_json_file_from_s3(runtime_log_folder, runtime_log_file_name)\n    runtime_log += total_output\n    upload_json_file_to_s3(runtime_log_folder,runtime_log_file_name,json.dumps(runtime_log))\n\n\n\ndef __getitem__(self, index):\n    print(&quot;Index &quot;,index)\n    pair_key = self.list_files[index]\n    print(&quot;Pair key &quot;,pair_key)\n    pair = Boto.s3_client.list_objects(Bucket=Boto.bucket_name, Prefix=pair_key, Delimiter='\/')\n\n    input_image_key = pair.get('Contents')[1].get('Key')\n    input_image_path = f's3:\/\/{Boto.bucket_name}\/{input_image_key}'\n    print(&quot;Input image path &quot;,input_image_path)\n    input_image_s3_source = get_file_from_filepath(input_image_path)\n    input_image = np.array(Image.open(input_image_s3_source))\n\n    target_image_key = pair.get('Contents')[0].get('Key')\n    target_image_path = f's3:\/\/{Boto.bucket_name}\/{target_image_key}'\n    print(&quot;Target image path &quot;,target_image_path)\n    target_image_s3_source = get_file_from_filepath(target_image_path)\n    target_image = np.array(Image.open(target_image_s3_source))\n\n    augmentations = config.both_transform(image=input_image, image0=target_image)\n\n    # get input image and target image by doing augmentations of images\n    input_image, target_image = augmentations['image'], augmentations['image0']\n\n    input_image = config.transform_only_input(image=input_image)['image']\n    target_image = config.transform_only_mask(image=target_image)['image']\n    \n    print(&quot;Input image size &quot;,input_image.size())\n    print(&quot;Target image size &quot;,target_image.size())\n    \n    return input_image, target_image\n\n<\/code><\/pre>\n<p>I did multiple runs and here are the traces of the failure points<\/p>\n<pre><code>i) 2022-06-03-05-00-04-pix2pix-U12239-2022-05-09-14-39-18-training\nNo index shown\n[Epoch 0\/100 (b: 0)]\n\nii) 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niii) 2022-06-03-05-44-46-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niv) 2022-06-03-06-08-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 1\/100 (b: 0)]\n\nv) 2022-06-15-02-49-20-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P423712\/Pair_71\/\n[Epoch 0\/100 (b: 0)\n\nvi) 2022-06-15-02-59-43-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P425642\/Pair_27\/\n[Epoch 0\/100 (b: 247)]\n\nvii) 2022-06-15-04-49-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P415414\/Pair_124\/\nNo specific epoch \n<\/code><\/pre>\n<p>My batch size is 248, so as you can see it seems to fail either at the start of the batch (0) or at the end (247). Also there are some common Indexes in the get item which seems to cause it to fail, namely Index 64 and Index 160. However there doesn't seem to be a common data point in the dataset that causes it to fail, as can be seen from the pair key all 3 data points in the datasets are different.<\/p>\n<p>Does anyone have any idea why this error happens please?<\/p>",
        "Challenge_closed_time":1655379897027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655303532370,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error in Sagemaker while iterating through PyTorch dataloader batch cycles. The error occurs at either the start or end of the batch and there are some common indexes that seem to cause the failure. However, there is no common data point in the dataset that causes the error. The user is seeking help to understand why this error is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72633246",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":124.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":74,
        "Challenge_solved_time":21.2124047222,
        "Challenge_title":"Error in pytorch data loader batch cycles",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":745,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643118380396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Try to run the same training script outside of a SageMaker training job and see what happens.<br \/>\nIf the error doesn't happen on a standalone script, try to run it as a <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"nofollow noreferrer\">Local SageMaker training job<\/a>, so you can reproduce it in seconds instead of minutes, and potentially use a debugger to figure out what is the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1401922736652,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brisbane",
        "Answerer_reputation_count":748.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":510.2445766667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Challenge_closed_time":1523591814356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521754623920,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in making hyperparameters available to the serving endpoint in SageMaker Tensorflow. While the training instances have access to input parameters using hyperparameters, there is no way to pass in parameters to control data processing in the input function when the endpoint is deployed. The user is seeking advice on the best way to pass parameters to the serving instance.",
        "Challenge_last_edit_time":1521754933880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":510.3306766667,
        "Challenge_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1426.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":12.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1509012479112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belo Horizonte, MG, Brasil",
        "Answerer_reputation_count":97.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":11.9663136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use de AWS SageMaker Training Jobs console to train a model with H2o.AutoMl.<\/p>\n\n<p>I got stuck trying to set up Hyperparameters, specifically setting up the 'training' field.<\/p>\n\n<pre><code>{'classification': true, 'categorical_columns':'', 'target': 'label'}\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm trying to set up a classification training job (1\/0), and I believe that everything else on the setup page I can cope, but I don't know how to set up the 'training' field. My data is stored on S3 as a CSV file, as the algorithm requires.<\/p>\n\n<p>My data has around 250000 columns, 4 out of them are categorical, one of them is the target, and the remainder is continuous variables (800 MB)<\/p>\n\n<pre><code>target column name = 'y'\ncategorical columns name = 'SIT','HOL','CTH','YTT'\n<\/code><\/pre>\n\n<p>I hope someone could help me.<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1568727043932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568683965203,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in setting up hyperparameters for a classification training job using AWS SageMaker Training Jobs console with H2o.AutoMl. Specifically, the user is unsure how to set up the 'training' field and is seeking assistance. The user's data is stored on S3 as a CSV file with 250000 columns, 4 categorical columns, and one target column.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57966245",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":11.9663136111,
        "Challenge_title":"How to hyperparametrize Amazon SageMaker Training Jobs Console",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>After I asked I came across an explanation from <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/aws_marketplace\/using_algorithms\/automl\/AutoML_-_Train_multiple_models_in_parallel.ipynb\" rel=\"nofollow noreferrer\">SageMaker examples.<\/a><\/p>\n\n<p>{classification': 'true', 'categorical_columns': 'SIT','HOL','CTH','YTT','target': 'y'}.<\/p>\n\n<p>Problem solved!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.3,
        "Solution_reading_time":5.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":109.7675,
        "Challenge_answer_count":0,
        "Challenge_body":"It seems they both assume that the current working dir is where they can find the `.git` and `.dvc` dirs.\r\nWe should correctly detect those paths, as it affects all our logic to e.g. automatically dvc init on behalf of the user.\r\n\r\nRelevant resources:\r\n1. https:\/\/stackoverflow.com\/a\/957978\r\n2. https:\/\/dvc.org\/doc\/command-reference\/root",
        "Challenge_closed_time":1630227884000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629832721000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the DVC add prompt is always displayed, even when there is no selection to make since the list of files is empty. The prompt should only be displayed if there is something to add.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/92",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.02,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":109.7675,
        "Challenge_title":"DVC and Git services don't correctly detect the repo root directory",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":303.4149847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use  TensorFlow Hub in Azure ML Studio<\/p>\n<p>I am using the kernel Python 3.8 PT and TF<\/p>\n<p>And I installed  a few modules:<\/p>\n<pre><code>!pip install bert-for-tf2\n!pip install sentencepiece\n!pip install &quot;tensorflow&gt;=2.0.0&quot;\n!pip install --upgrade tensorflow-hub\n<\/code><\/pre>\n<p>With pip list, I can see they are installed:<\/p>\n<pre><code>tensorflow                              2.8.0\ntensorflow-estimator                    2.3.0\ntensorflow-gpu                          2.3.0\ntensorflow-hub                          0.12.0\ntensorflow-io-gcs-filesystem            0.24.0\n<\/code><\/pre>\n<p>However when I try to use it as per the documentation (<a href=\"https:\/\/www.tensorflow.org\/hub\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/hub<\/a>)<\/p>\n<p>Then I get the classic:<\/p>\n<pre><code>ModuleNotFoundError: No module named 'tensorflow_hub'\n<\/code><\/pre>",
        "Challenge_closed_time":1650950165912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649857871967,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to use TensorFlow Hub in Azure ML Studio. They have installed the required modules and verified their installation, but when they try to use it as per the documentation, they encounter a \"ModuleNotFoundError: No module named 'tensorflow_hub'\" error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71858668",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":303.4149847222,
        "Challenge_title":"How to use tensorflow hub in Azure ML",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>To resolve this <code>ModuleNotFoundError: No module named 'tensorflow_hub'<\/code>  error, try following ways:<\/p>\n<ul>\n<li>Try installing\/upgrading the latest version of <code>tensorflow<\/code> and <code>tensorflow-hub<\/code> and then import:<\/li>\n<\/ul>\n<pre><code>!pip install --upgrade tensorflow\n\n!pip install --upgrade tensorflow_hub\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n<\/code><\/pre>\n<ul>\n<li>Install the current environment as a new kernel:<\/li>\n<\/ul>\n<pre><code>python3 -m ipykernel install --user --name=testenvironment\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/stackoverflow.com\/questions\/63884339\/modulenotfounderror-no-module-named-tensorflow-hub\">ModuleNotFoundError: No module named 'tensorflow_hub', No module named 'tensorflow_hub'<\/a> and <a href=\"https:\/\/github.com\/tensorflow\/hub\/issues\/767\" rel=\"nofollow noreferrer\">How to use Tensorflow Hub Model?<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":12.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1522076554448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":4030.6928727778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.<\/p>\n<p>It seems that the <code>mlflow.pyfunc.load_model<\/code> can only do the <code>predict<\/code> method.<\/p>\n<p>There is an option to directly access the pickle?<\/p>\n<p>I also tried to use the path in the run using the <code>pickle.load(path)<\/code> (example of path: dbfs:\/databricks\/mlflow-tracking\/20526156406\/92f3ec23bf614c9d934dd0195\/artifacts\/model\/model.pkl).<\/p>",
        "Challenge_closed_time":1629748421287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628544411170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to retrieve the pickle file of a trained model from their experiments in Databricks. They have tried using mlflow.pyfunc.load_model but it only allows the predict method. They have also attempted to use the path in the run using pickle.load(path) but it did not work.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68718719",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":334.4472547222,
        "Challenge_title":"How can I retrive the model.pkl in the experiment in Databricks",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":3081.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522076554448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":96.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>I recently found the solution which can be done by the following two approaches:<\/p>\n<ol>\n<li>Use the customized predict function at the moment of saving the model (check <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">databricks<\/a> documentation for more details).<\/li>\n<\/ol>\n<p>example give by Databricks<\/p>\n<pre><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n# Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Load the model artefacts as we are downloading the artefact:<\/li>\n<\/ol>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ntmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model\/model.pkl')\n\nf = open(tmp_path,'rb')\n\nmodel = pickle.load(f)\n\nf.close()\n\n \n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)\n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643054905512,
        "Solution_link_count":1.0,
        "Solution_readability":16.0,
        "Solution_reading_time":18.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":109.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.7375,
        "Challenge_answer_count":0,
        "Challenge_body":"Invoke Endpoint response time out. \r\n\r\n### Reproduction Steps\r\n\r\n{\r\n  \"trainingJob\": {\r\n    \"hyperparameters\": {\r\n    \"n-hidden\": \"2\",\r\n    \"n-epochs\": \"100\",\r\n    \"lr\":\"1e-2\"\r\n    },\r\n    \"instanceType\": \"ml.c5.9xlarge\",\r\n    \"timeoutInSeconds\": 10800    \r\n  }\r\n}\r\n\r\n\r\n\r\n### Error Log\r\nIn Inference Lambda CloudWatch:\r\n\r\nTask timed out after 120.10 seconds\r\n\r\n\r\nIn Sagemaker Training CloudWatch:\r\n\r\n2021-04-09   04:53:46,902 [INFO ] main org.pytorch.serve.ModelServer - Loading initial   models: model.mar\r\n--\r\n2021-04-09 04:53:49,837 [INFO ] main   org.pytorch.serve.archive.ModelArchive - eTag   8ff2b3de4bed4fb1bc7fe969652117ff\r\n2021-04-09 04:53:49,847 [INFO ] main   org.pytorch.serve.wlm.ModelManager - Model model loaded.\r\n2021-04-09 04:53:49,865 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Inference server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Inference API bind to: http:\/\/0.0.0.0:8080\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Metrics server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,931 [INFO ] main   org.pytorch.serve.ModelServer - Metrics API bind to: http:\/\/127.0.0.1:8082\r\nModel server started.\r\n2021-04-09 04:53:49,957 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on   port: \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]55\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker   started.\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime:   3.6.13\r\n2021-04-09 04:53:49,963 [INFO ]   W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to:   \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,972 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection   accepted: \/home\/model-server\/tmp\/.ts.sock.9000.\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   CPUUtilization.Percent:33.3\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskAvailable.Gigabytes:19.622234344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUsage.Gigabytes:4.731609344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUtilization.Percent:19.4\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryAvailable.Megabytes:30089.12109375\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUsed.Megabytes:902.6953125\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUtilization.Percent:4.1\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Setting the   default backend to \"pytorch\". You can change it in the   ~\/.dgl\/config.json file or export the DGLBACKEND environment variable.\u00a0 Valid options are: pytorch, mxnet,   tensorflow (all lowercase)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   ------------------ Loading model -------------------\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker   process died.\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most   recent call last):\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 176, in <module>\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 worker.run_server()\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 148, in run_server\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self.handle_connection(cl_socket)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 112, in handle_connection\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service, result, code =   self.load_model(msg)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 85, in load_model\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service = model_loader.load(model_name,   model_dir, handler, gpu, batch_size)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_loader.py\", line   117, in load\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   model_service.initialize(service.context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/home\/model-server\/tmp\/models\/8ff2b3de4bed4fb1bc7fe969652117ff\/handler_service.py\",   line 51, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 super().initialize(context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/default_handler_service.py\",   line 66, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self._service.validate_and_initialize(model_dir=model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py\",   line 158, in validate_and_initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self._model = self._model_fn(model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/ml\/model\/code\/fd_sl_deployment_entry_point.py\", line 149, in   model_fn\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 rgcn_model.load_state_dict(stat_dict)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\",   line 1045, in load_state_dict\r\n2021-04-09   04:53:51,251 [INFO ] W-9000-model_1-stdout   org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self.__class__.__name__, \"     \\t\".join(error_msgs)))\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError:   Error(s) in loading state_dict for HeteroRGCN:\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.weight: copying a   param with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.bias: copying a   param with shape torch.Size([2]) from checkpoint, the shape in current model   is torch.Size([16]).\r\n\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** <!-- Output of `cdk version` -->\r\n  - **Framework Version:**\r\n  - **Node.js Version:** <!-- Version of Node.js (run the command `node -v`) -->\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\nCause of this bug:\r\n\r\nBackend worker process died.\r\nSagemaker Endpoint deployment code and model training code parameter conflict on n-hidden and hidden_size.\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":1618279737000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618212282000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sagemaker Multi-GPU distributed data training where \"model.generate\" is returning empty tensors. They are unsure if this is due to the feature not yet being supported on Sagemaker Multi-GPU or if there is an issue with their own modified scripts.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/57",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":17.5,
        "Challenge_reading_time":124.73,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":25.0,
        "Challenge_repo_issue_count":1008.0,
        "Challenge_repo_star_count":130.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":107,
        "Challenge_solved_time":18.7375,
        "Challenge_title":"sagemaker endpoint fail to deploy or time out server error(0) bug",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":650,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1604138895270,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Straubing, Deutschland",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":93.5123102778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running a python script in the command line with <code>python3 CTGAN_noscale.py --database_name CTGAN_noshift<\/code> and receive the following error (with faulthandler):<\/p>\n<pre><code>Fatal Python error: Segmentation fault\n\nCurrent thread 0x00007f57e97fe700 (most recent call first):\n&lt;no Python frame&gt;\n\nThread 0x00007f593db07740 (most recent call first):\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/torch\/autograd\/__init__.py&quot;, line 145 in backward\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/torch\/tensor.py&quot;, line 245 in backward\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/ctgan\/synthesizers\/ctgan.py&quot;, line 374 in fit\n  File &quot;CTGAN_noscale.py&quot;, line 140 in objective\n  File &quot;CTGAN_noscale.py&quot;, line 162 in &lt;lambda&gt;\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/optuna\/_optimize.py&quot;, line 216 in _run_trial\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/optuna\/_optimize.py&quot;, line 162 in _optimize_sequential\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/optuna\/_optimize.py&quot;, line 65 in _optimize\n  File &quot;\/usr\/local\/lib\/python3.8\/dist-packages\/optuna\/study.py&quot;, line 401 in optimize\n  File &quot;CTGAN_noscale.py&quot;, line 162 in run_CTGAN\n  File &quot;CTGAN_noscale.py&quot;, line 210 in &lt;module&gt;\nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>It seems that the problem is somehow with optuna.\nThe weird part is that everything worked fine on another server, after changing the server it crashed like this.<\/p>\n<p><strong>Update<\/strong><\/p>\n<p>I found out that the the problem doesn't occur when I don't use a docker container OR use a docker container without GPU.<\/p>",
        "Challenge_closed_time":1630389747607,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630053103290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"Segmentation fault (core dumped)\" error while running a Python script with optuna. The error seems to be related to the use of a docker container with GPU, as the problem doesn't occur when not using a container or using a container without GPU. The issue worked fine on another server, but after changing the server, it crashed.",
        "Challenge_last_edit_time":1659952934500,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68950349",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":22.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":93.5123102778,
        "Challenge_title":"Segmentation fault (core dumped) with optuna",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":170,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604138895270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Straubing, Deutschland",
        "Poster_reputation_count":11.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I solved the problem by rebuilding a new image and derived a container from this image. In this container somehow the error didn't appear anymore.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":1.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":61.8899861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to implement a self-service solution in Azure so users can run a Jupyter or PySpark notebook on-Demand\/automatically with the dataset they found a search in the Azure Data Catalog.  I visualize, once the user finds the data in a search, there will be a link that will take him\/her to a Notebook and the dataset can be used for analysis.  Any suggestion would be very much appreciated!<\/p>",
        "Challenge_closed_time":1619432530247,
        "Challenge_comment_count":1,
        "Challenge_created_time":1619209726297,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to create a self-service solution in Azure where users can run a Jupyter or PySpark notebook on-demand with a dataset they found in a search in the Azure Data Catalog. The user is looking for suggestions on how to implement this solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/369952\/azure-on-demand-ml-cluster-from-a-search-in-the-da",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":61.8899861111,
        "Challenge_title":"Azure On-Demand ML cluster from a search in the data catalog",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=072ff3f6-8045-41ef-849b-87cd092ffd6e\">@Jairo Melo  <\/a> Thanks for the question.Azure Purview can find, understand, and consume data sources. Please follow the Azure Purview documentation: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/purview\/\">https:\/\/learn.microsoft.com\/en-us\/azure\/purview\/<\/a>    <\/p>\n<p>and We have  Azure Open Datasets where you can download a Notebook for AML, Databricks or Synapse that explores the data: <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/\">Azure Open Datasets Catalog<\/a> | Microsoft Azure. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/open-datasets\/overview-what-are-open-datasets\">What are open datasets? Curated public datasets - Azure Open Datasets | Microsoft Learn.<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.6,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.9679186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have not seen any doc talking about this topic, is this supported in Microsoft Machine Learning? Is this a good plan if anyone has tried? <\/p>",
        "Challenge_closed_time":1661379528520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661358044013,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on whether time series training for anomaly detection is supported in Microsoft Machine Learning and if it is a good plan to try. They have not found any documentation on the topic.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/980372\/time-series-training-for-anomal-detect",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":2.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.9679186111,
        "Challenge_title":"Time series training for anomal detect",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=033f3419-75e1-402b-b1ac-8869dd655829\">@minhoo lee  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform, Azure Machine Learning Serivce support time series training - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast<\/a>    <\/p>\n<p>You can check above document to see how to set up a quick model.    <\/p>\n<p>But for Anomaly Dectection, I think Anomaly Detector API is a better choice for you - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/anomaly-detector\/\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/anomaly-detector\/<\/a>    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":11.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":81.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.8707033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a machine learning model in Databricks workspace using mlflow. Model is getting registered in databricks model registry and saved in databricks file share. Now I want to download model artifacts from workspace. Currently I am transferring model to azure machine learning workspace. There I am able to download all the artifacts. How to do it from databricks workspace? <\/p>",
        "Challenge_closed_time":1665051150292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664972415760,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has trained a machine learning model in Databricks workspace using mlflow and wants to download the model artifacts from the workspace to a local directory. The model is registered in the Databricks model registry and saved in the Databricks file share. The user is currently transferring the model to Azure machine learning workspace to download all the artifacts and is seeking guidance on how to download the artifacts directly from the Databricks workspace.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1036179\/how-to-download-mlflow-model-artifacts-from-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":6.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":21.8707033333,
        "Challenge_title":"How to download mlflow model artifacts from Azure Databricks workspace to local directory?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c18ee9ee-fe4d-4f61-a87a-4da34719e169\">@Sriram Reddy  <\/a> Thanks for the question. To download a model from Databricks workspace you need to do two things:    <\/p>\n<p>Set MLFlow tracking URI to databricks using python API    <\/p>\n<p>Setup databricks authentication. I prefer authenticating by setting the following environment variables, you can also use databricks CLI to authenticate:    <\/p>\n<p>DATABRICKS_HOST    <\/p>\n<p>DATABRICKS_TOKEN    <br \/>\nHere's a basic code snippet to download a model from Databricks workspace model registry:    <\/p>\n<pre><code>import os  \nimport mlflow  \nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository  \n  \nmodel_name = &quot;example-model-name&quot;  \nmodel_stage = &quot;Staging&quot;  # Should be either 'Staging' or 'Production'  \n  \nmlflow.set_tracking_uri(&quot;databricks&quot;)  \n  \nos.makedirs(&quot;model&quot;, exist_ok=True)  \nlocal_path = ModelsArtifactRepository(  \n    f'models:\/{model_name}\/{model_stage}').download_artifacts(&quot;&quot;, dst_path=&quot;model&quot;)  \n  \nprint(f'{model_stage} Model {model_name} is downloaded at {local_path}')  \n<\/code><\/pre>\n<p>Running above python script will download an ML model in the model directory.    <\/p>\n<p>Containerizing MLFlow model serving with Docker    <\/p>\n<p>For more information you can follow this <a href=\"https:\/\/dev.to\/itachiredhair\/downloading-mlflow-model-from-databricks-and-serving-with-docker-38ip\">article<\/a> from Akshay Milmile    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.2,
        "Solution_reading_time":19.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":139.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1542402402780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":186.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":56.0515805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy an ALS model trained using PySpark on Azure ML service. I am providing a score.py file that loads the trained model using ALSModel.load() function. Following is the code of my score.py file.<\/p>\n<pre><code>import os\nfrom azureml.core.model import Model\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, StringType\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\nsqlContext = SQLContext(sc)\nspark = sqlContext.sparkSession\n\ninput_schema = StructType([StructField(&quot;UserId&quot;, StringType())])\nreader = spark.read\nreader.schema(input_schema)\n\n\ndef init():\n    global model\n    # note here &quot;iris.model&quot; is the name of the model registered under the workspace\n    # this call should return the path to the model.pkl file on the local disk.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), &quot;recommendation-model&quot;)\n    # Load the model file back into a LogisticRegression model\n    model = ALSModel.load(model_path)\n    \n\ndef run(data):\n    try:\n        input_df = reader.json(sc.parallelize([data]))\n        input_df = indexer.transform(input_df)\n        \n        res = model.recommendForUserSubset(input_df[['UserId_index']], 10)\n\n        # you can return any datatype as long as it is JSON-serializable\n        return result.collect()[0]['recommendations']\n    except Exception as e:\n        traceback.print_exc()\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>Following is the error I get when I deploy it as LocalWebService using Model.deploy function in Azure ML service<\/p>\n<pre><code>Generating Docker build context.\nPackage creation Succeeded\nLogging into Docker registry viennaglobal.azurecr.io\nLogging into Docker registry viennaglobal.azurecr.io\nBuilding Docker image from Dockerfile...\nStep 1\/5 : FROM viennaglobal.azurecr.io\/azureml\/azureml_43542b56c5ec3e8d0f68e1556558411f\n ---&gt; 5b3bb174ca5f\nStep 2\/5 : COPY azureml-app \/var\/azureml-app\n ---&gt; 8e540c0746f7\nStep 3\/5 : RUN mkdir -p '\/var\/azureml-app' &amp;&amp; echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjNkN2M1ZjM4LTI1ODEtNGUxNi05NTdhLWEzOTU1OGI1ZjBiMyIsInJlc291cmNlR3JvdXBOYW1lIjoiZGV2LW9tbmljeC10ZnMtYWkiLCJhY2NvdW50TmFtZSI6ImRldi10ZnMtYWktd29ya3NwYWNlIiwid29ya3NwYWNlSWQiOiI1NjkzNGMzNC1iZmYzLTQ3OWUtODRkMy01OGI4YTc3ZTI4ZjEifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode &gt; \/var\/azureml-app\/model_config_map.json\n ---&gt; Running in 502ad8edf91e\n ---&gt; a1bc5e0283d0\nStep 4\/5 : RUN mv '\/var\/azureml-app\/tmpvxhomyin.py' \/var\/azureml-app\/main.py\n ---&gt; Running in eb4ec1a0b702\n ---&gt; 6a3296fe6420\nStep 5\/5 : CMD [&quot;runsvdir&quot;,&quot;\/var\/runit&quot;]\n ---&gt; Running in 834fd746afef\n ---&gt; 5b9f8be538c0\nSuccessfully built 5b9f8be538c0\nSuccessfully tagged recommend-service:latest\nContainer (name:musing_borg, id:0f3163692f5119685eee5ed59c8e00aa96cd472f765e7db67653f1a6ce852e83) cannot be killed.\nContainer has been successfully cleaned up.\nImage sha256:0f146f4752878bbbc0e876f4477cc2877ff12a366fca18c986f9a9c2949d028b successfully removed.\nStarting Docker container...\nDocker container running.\nChecking container health...\nERROR - Error: Container has crashed. Did your init method fail?\n\n\nContainer Logs:\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:00,312735664+00:00 - rsyslog\/run \n2020-07-30T11:57:00,312768364+00:00 - gunicorn\/run \n2020-07-30T11:57:00,313017966+00:00 - iot-server\/run \nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\n2020-07-30T11:57:00,313969073+00:00 - nginx\/run \n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:00,597835804+00:00 - iot-server\/finish 1 0\n2020-07-30T11:57:00,598826211+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 19.9.0\nListening at: http:\/\/127.0.0.1:31311 (10)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 41\nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\nIvy Default Cache set to: \/root\/.ivy2\/cache\nThe jars for the packages stored in: \/root\/.ivy2\/jars\n:: loading settings :: url = jar:file:\/home\/mmlspark\/lib\/spark\/jars\/ivy-2.4.0.jar!\/org\/apache\/ivy\/core\/settings\/ivysettings.xml\ncom.microsoft.ml.spark#mmlspark_2.11 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-e07358bb-d354-4f41-aa4c-f0aa73bb0156;1.0\n    confs: [default]\n    found com.microsoft.ml.spark#mmlspark_2.11;0.15 in spark-list\n    found io.spray#spray-json_2.11;1.3.2 in central\n    found com.microsoft.cntk#cntk;2.4 in central\n    found org.openpnp#opencv;3.2.0-1 in central\n    found com.jcraft#jsch;0.1.54 in central\n    found org.apache.httpcomponents#httpclient;4.5.6 in central\n    found org.apache.httpcomponents#httpcore;4.4.10 in central\n    found commons-logging#commons-logging;1.2 in central\n    found commons-codec#commons-codec;1.10 in central\n    found com.microsoft.ml.lightgbm#lightgbmlib;2.1.250 in central\n:: resolution report :: resolve 318ms :: artifacts dl 11ms\n    :: modules in use:\n    com.jcraft#jsch;0.1.54 from central in [default]\n    com.microsoft.cntk#cntk;2.4 from central in [default]\n    com.microsoft.ml.lightgbm#lightgbmlib;2.1.250 from central in [default]\n    com.microsoft.ml.spark#mmlspark_2.11;0.15 from spark-list in [default]\n    commons-codec#commons-codec;1.10 from central in [default]\n    commons-logging#commons-logging;1.2 from central in [default]\n    io.spray#spray-json_2.11;1.3.2 from central in [default]\n    org.apache.httpcomponents#httpclient;4.5.6 from central in [default]\n    org.apache.httpcomponents#httpcore;4.4.10 from central in [default]\n    org.openpnp#opencv;3.2.0-1 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n    ---------------------------------------------------------------------\n\n:: problems summary ::\n:::: ERRORS\n    unknown resolver repo-1\n\n    unknown resolver repo-1\n\n\n:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n:: retrieving :: org.apache.spark#spark-submit-parent-e07358bb-d354-4f41-aa4c-f0aa73bb0156\n    confs: [default]\n    0 artifacts copied, 10 already retrieved (0kB\/7ms)\n2020-07-30 11:57:02 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nInitialized PySpark session.\nInitializing logger\n2020-07-30 11:57:09,464 | root | INFO | Starting up app insights client\nStarting up app insights client\n2020-07-30 11:57:09,464 | root | INFO | Starting up request id generator\nStarting up request id generator\n2020-07-30 11:57:09,464 | root | INFO | Starting up app insight hooks\nStarting up app insight hooks\n2020-07-30 11:57:09,464 | root | INFO | Invoking user's init function\nInvoking user's init function\n2020-07-30 11:57:19,652 | root | ERROR | User's init function failed\nUser's init function failed\n2020-07-30 11:57:19,656 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 163, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/main.py&quot;, line 44, in init\n    model = ALSModel.load(model_path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 362, in load\n    return cls.read().load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 300, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/java_gateway.py&quot;, line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/sql\/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/protocol.py&quot;, line 328, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o64.load.\n: java.util.NoSuchElementException: Param blockSize does not exist.\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n    at org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:591)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:589)\n    at scala.collection.immutable.List.foreach(List.scala:392)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.setParams(ReadWrite.scala:589)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.getAndSetParams(ReadWrite.scala:572)\n    at org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:533)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n\n\nEncountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 163, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/main.py&quot;, line 44, in init\n    model = ALSModel.load(model_path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 362, in load\n    return cls.read().load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 300, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/java_gateway.py&quot;, line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/sql\/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/protocol.py&quot;, line 328, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o64.load.\n: java.util.NoSuchElementException: Param blockSize does not exist.\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n    at org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:591)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:589)\n    at scala.collection.immutable.List.foreach(List.scala:392)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.setParams(ReadWrite.scala:589)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.getAndSetParams(ReadWrite.scala:572)\n    at org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:533)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n\n\nWorker exiting (pid: 41)\nShutting down: Master\nReason: Worker failed to boot.\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:19,833136837+00:00 - gunicorn\/finish 3 0\n2020-07-30T11:57:19,834216245+00:00 - Exit code 3 is not normal. Killing image.\n\n---------------------------------------------------------------------------\nWebserviceException                       Traceback (most recent call last)\n&lt;ipython-input-43-d0992ae9d1c9&gt; in &lt;module&gt;\n      6 local_service = Model.deploy(workspace, &quot;recommend-service&quot;, [register_model], inference_config, deployment_config)\n      7 \n----&gt; 8 local_service.wait_for_deployment()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/local.py in decorated(self, *args, **kwargs)\n     69                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n     70                                           logger=module_logger)\n---&gt; 71             return func(self, *args, **kwargs)\n     72         return decorated\n     73     return decorator\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/local.py in wait_for_deployment(self, show_output)\n    601                                    self._container,\n    602                                    health_url=self._internal_base_url,\n--&gt; 603                                    cleanup_if_failed=False)\n    604 \n    605             self.state = LocalWebservice.STATE_RUNNING\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_model_management\/_util.py in container_health_check(docker_port, container, health_url, cleanup_if_failed)\n    745             # The container has started and crashed.\n    746             _raise_for_container_failure(container, cleanup_if_failed,\n--&gt; 747                                          'Error: Container has crashed. Did your init method fail?')\n    748 \n    749         # The container hasn't crashed, so try to ping the health endpoint.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_model_management\/_util.py in _raise_for_container_failure(container, cleanup, message)\n   1258         cleanup_container(container)\n   1259 \n-&gt; 1260     raise WebserviceException(message, logger=module_logger)\n   1261 \n   1262 \n\nWebserviceException: WebserviceException:\n    Message: Error: Container has crashed. Did your init method fail?\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Error: Container has crashed. Did your init method fail?&quot;\n    }\n}\n<\/code><\/pre>\n<p>However, the ALSModel.load() works fine when executed in a Jupyter notebook.<\/p>",
        "Challenge_closed_time":1596478571823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596276786133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering an error when attempting to deploy an als model trained using pyspark on a service, with the error being a java.util.nosuchelementexception.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63204081",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.7,
        "Challenge_reading_time":224.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":179,
        "Challenge_solved_time":56.0515805556,
        "Challenge_title":"PySpark ALSModel load fails in deployment over Azure ML service with error java.util.NoSuchElementException: Param blockSize does not exist",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":598.0,
        "Challenge_word_count":1195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596274712792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chandigarh, India",
        "Poster_reputation_count":13.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>A couple of things to check:<\/p>\n<ol>\n<li>Is your model registered in the workspace? AZUREML_MODEL_DIR only works for registered models. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">this link<\/a> for information about registering a model<\/li>\n<li>Are you specifying the same version of pyspark.ml.recommendation in your InferenceConfig as you use locally? This kind of error might be due to a difference in versions<\/li>\n<li>Have you looked at the output of <code>print(service.get_logs())<\/code>? Check out our <a href=\"https:\/\/review.docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?branch=pr-en-us-124666\" rel=\"nofollow noreferrer\">troubleshoot and debugging documentation here<\/a> for other things you can try<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.2,
        "Solution_reading_time":15.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.02458,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I currently work with Kedro (from quantum black <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/01_introduction\/01_introduction.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/01_introduction\/01_introduction.html<\/a>) as a framework for deployment oriented framework to code collaboratively. It is a great framework to develop machine learning in a team.<\/p>\n<p>I am looking for an R equivalent.<\/p>\n<p>My main issue is that I have teams of data scientists that develop in R, but each team is developing in different formats.<\/p>\n<p>I wanted to make them follow a common framework to develop deployment ready R code, easy to work on in 2 or 3-people teams.<\/p>\n<p>Any suggestions are welcome<\/p>",
        "Challenge_closed_time":1639581358648,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639580284993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for an R package that can serve as a collaborative framework for developing deployment-ready R code. They currently use Kedro for machine learning development in teams, but are struggling with different formats used by different teams of data scientists.",
        "Challenge_last_edit_time":1639581270160,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70365836",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":10.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2982375,
        "Challenge_title":"Is there a package in R that mimics KEDRO as a modular collaborative framework for development?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460063521156,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo - SP, Brasil",
        "Poster_reputation_count":2472.0,
        "Poster_view_count":186.0,
        "Solution_body":"<p>member of the Kedro team here. We've heard good things about the <a href=\"https:\/\/github.com\/ropensci\/targets\" rel=\"nofollow noreferrer\">Targets<\/a> library doing similar things in the R world.<\/p>\n<p>It would be remiss for me to not try and covert you and your team to the dark side too :)<\/p>\n<p>Before Kedro our teams internally were writing a mix of Python, SQL, Scala and R. Part of the drive to write the framework was to get our teams internally speaking the same language. Python felt like the best compromise available at the time and I'd argue this still holds. We also had trouble productionising R projects and felt Python is more manageable in that respect.<\/p>\n<p>Whilst not officially documented - I've also seen some people on the Kedro <a href=\"https:\/\/discord.com\/channels\/778216384475693066\/778998585454755870\/901111920290070588\" rel=\"nofollow noreferrer\">Discord play with r2py<\/a> so that they can use specific R functionality within their Python pipelines.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.2,
        "Solution_reading_time":12.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":141.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1324129118823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11756.0,
        "Answerer_view_count":517.0,
        "Challenge_adjusted_solved_time":0.30124,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm looking to deploy the SageMaker pipeline using CDK (<a href=\"https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cdk\/api\/v2\/docs\/aws-cdk-lib.aws_sagemaker.CfnPipeline.html<\/a>) but could not find any code examples. Any pointers?<\/p>",
        "Challenge_closed_time":1652283812907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652282728443,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to deploy an AWS SageMaker pipeline using the cloud development kit (CDK) and is looking for code examples.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72203674",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.3,
        "Challenge_reading_time":5.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.30124,
        "Challenge_title":"Deploy AWS SageMaker pipeline using the cloud development kit (CDK)",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":31,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489685785576,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Canada",
        "Poster_reputation_count":65.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>CDK L1 Constructs correspond 1:1 to a CloudFormation resource of the same name. The construct props match the resouce properties.  The go-to source is therefore the CloudFormation docs.<\/p>\n<p>The <code>AWS::SageMaker::Pipeline<\/code> <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html#aws-resource-sagemaker-pipeline--examples\" rel=\"nofollow noreferrer\">docs have a more complete example<\/a>.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.8,
        "Solution_reading_time":6.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":39.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":70.55,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\n\nI successfully trained and deployed a pipeline in Vertex AI using Kubeflow for a retrieval model, Two Towers.\n\nNow I want to schedule this pipeline run every 8 minutes. Here's my code:\n\n\u00a0\n\nfrom kfp.v2.google.client import AIPlatformClient\napi_client = AIPlatformClient(project_id='my-project', region='us-central1')\n\napi_client.create_schedule_from_job_spec(\n    job_spec_path='vacantes_pipeline.json',\n    schedule=\"\/8 * * * *\", # every 8 minutes\n    time_zone='America\/Sao_Paulo',\n    parameter_values={\n        \"epochs_\": 5,\n    \"embed_length\":768,  \n        \"maxsplit_\" : 130\n    }\n)\n\n\u00a0\n\nThe JSON is successfuly created, but the Scheduler Job fails immediately.\n\nLogging tells me the httpRequest has an error 503 plus:\n\n\u00a0\n\njsonPayload: {\n@type: \"type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished\"\njobName: \"projects\/my-project\/locations\/us-central1\/jobs\/pipeline_vacantes-pipeline-with-deployment_c7e98a8f_59-14-a-a-a\"\nstatus: \"UNAVAILABLE\"\ntargetType: \"HTTP\"\nurl: \"https:\/\/us-central1-bogotatrabaja.cloudfunctions.net\/templated_http_request-v1\"\n}\n\n\u00a0\n\nAny ideas on how to solve this issue ?",
        "Challenge_closed_time":1683971520000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683717540000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error 503 while trying to schedule a pipeline run every 8 minutes in Vertex AI using Kubeflow. The JSON is successfully created, but the Scheduler Job fails immediately with an \"UNAVAILABLE\" status. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Scheduling-Vertex-AI-Pipeline-Error-503\/m-p\/552124#M1849",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.8,
        "Challenge_reading_time":14.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":70.55,
        "Challenge_title":"Scheduling Vertex AI Pipeline - Error 503",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":98,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I solved with Compute Engine and cron jobs.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":0.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":326.3584541667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried to follow the normal (non-studio) documentation on mounting an EFS file system, as can be found <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/mount-an-efs-file-system-to-an-amazon-sagemaker-notebook-with-lifecycle-configurations\/\" rel=\"nofollow noreferrer\">here<\/a>, however, these steps don't work in a studio notebook. Specifically, the <code>sudo mount -t nfs ...<\/code> does not work in both the Image terminal and the system terminal.<\/p>\n<p>How do I mount an EFS file system that already exists to amazon Sagemaker, so I can access the data\/ datasets I stored in them?<\/p>",
        "Challenge_closed_time":1596816839976,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596816839977,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in mounting an EFS volume on AWS Sagemaker Studio. They have tried following the normal documentation but the steps don't work in a studio notebook. The user is seeking guidance on how to mount an EFS file system that already exists to Amazon Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63305569",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to mount an EFS volume on AWS Sagemaker Studio",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2331.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>Update: I spoke to an AWS Solutions Architect, and he confirms that EFS is not supported on Sagemaker Studio.<\/p>\n<hr \/>\n<p><strong>Workaround:<\/strong><\/p>\n<p>Instead of mounting your old EFS, you can mount the SageMaker studio EFS onto an EC2 instance, and copy over the data manually. You would need the correct EFS storage volume id, and you'll find your newly copied data available in Sagemaker Studio. <em>I have not actually done this though.<\/em><\/p>\n<p>To find the EFS id, look at the section &quot;Manage your storage volume&quot; <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks.html#manage-your-storage-volume\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1597991730412,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":8.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1310699693432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2889.0,
        "Answerer_view_count":62.0,
        "Challenge_adjusted_solved_time":139.2068527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to replicate <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb<\/a><\/p>\n\n<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:<\/p>\n\n<pre><code>%%time\nimport boto3\n\nregion = boto3.Session().region_name\nsaved_model = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/model\/resnet\/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)\n\nimport sagemaker\nfrom sagemaker.tensorflow.serving import Model\n\nrole = sagemaker.get_execution_role()\n\ntensorflow_model = Model(model_data=saved_model,\nrole=role,\nframework_version='1.14')\ntf_predictor = tensorflow_model.deploy(initial_instance_count=1,\ninstance_type='local',\naccelerator_type='local_sagemaker_notebook')\n<\/code><\/pre>\n\n<p>I am getting following log in the notebook:<\/p>\n\n<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1\nalgo-1-7ynb1_1 | INFO:main:starting services\nalgo-1-7ynb1_1 | INFO:main:using default model name: Servo\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving model config:\nalgo-1-7ynb1_1 | model_config_list: {\nalgo-1-7ynb1_1 | config: {\nalgo-1-7ynb1_1 | name: \"Servo\",\nalgo-1-7ynb1_1 | base_path: \"\/opt\/ml\/model\/export\/Servo\",\nalgo-1-7ynb1_1 | model_platform: \"tensorflow\"\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:nginx config:\nalgo-1-7ynb1_1 | load_module modules\/ngx_http_js_module.so;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_processes auto;\nalgo-1-7ynb1_1 | daemon off;\nalgo-1-7ynb1_1 | pid \/tmp\/nginx.pid;\nalgo-1-7ynb1_1 | error_log \/dev\/stderr error;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_rlimit_nofile 4096;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | events {\nalgo-1-7ynb1_1 | worker_connections 2048;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | http {\nalgo-1-7ynb1_1 | include \/etc\/nginx\/mime.types;\nalgo-1-7ynb1_1 | default_type application\/json;\nalgo-1-7ynb1_1 | access_log \/dev\/stdout combined;\nalgo-1-7ynb1_1 | js_include tensorflow-serving.js;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream tfs_upstream {\nalgo-1-7ynb1_1 | server localhost:8501;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream gunicorn_upstream {\nalgo-1-7ynb1_1 | server unix:\/tmp\/gunicorn.sock fail_timeout=1;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | server {\nalgo-1-7ynb1_1 | listen 8080 deferred;\nalgo-1-7ynb1_1 | client_max_body_size 0;\nalgo-1-7ynb1_1 | client_body_buffer_size 100m;\nalgo-1-7ynb1_1 | subrequest_output_buffer_size 100m;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | set $tfs_version 1.14;\nalgo-1-7ynb1_1 | set $default_tfs_model Servo;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/tfs {\nalgo-1-7ynb1_1 | rewrite ^\/tfs\/(.) \/$1 break;\nalgo-1-7ynb1_1 | proxy_redirect off;\nalgo-1-7ynb1_1 | proxy_pass_request_headers off;\nalgo-1-7ynb1_1 | proxy_set_header Content-Type 'application\/json';\nalgo-1-7ynb1_1 | proxy_set_header Accept 'application\/json';\nalgo-1-7ynb1_1 | proxy_pass http:\/\/tfs_upstream;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ping {\nalgo-1-7ynb1_1 | js_content ping;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/invocations {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location ~ ^\/models\/(.)\/invoke {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/models {\nalgo-1-7ynb1_1 | proxy_pass http:\/\/gunicorn_upstream\/models;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ {\nalgo-1-7ynb1_1 | return 404 '{\"error\": \"Not Found\"}';\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | keepalive_timeout 3;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)\nalgo-1-7ynb1_1 | INFO:main:nginx version info:\nalgo-1-7ynb1_1 | nginx version: nginx\/1.16.1\nalgo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)\nalgo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016\nalgo-1-7ynb1_1 | TLS SNI support enabled\nalgo-1-7ynb1_1 | configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\nalgo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\nalgo-1-7ynb1_1 |\n!algo-1-7ynb1_1 | 172.18.0.1 - - [17\/Jun\/2020:05:02:10 +0000] \"GET \/ping HTTP\/1.1\" 200 3 \"-\" \"-\"\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-7ynb1_1 | EI Error Code: [3, 16, 8]\nalgo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | EI Client Version: 1.5.3\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\n`\n<\/code><\/pre>\n\n<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.<\/p>\n\n<p>I can see the docker of the model running\n<a href=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I try to infer\/predict from that model, I get error:<\/p>\n\n<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]\n\nalgo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-iikpj_1 | EI Error Code: [3, 16, 8]\nalgo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-iikpj_1 | EI Client Version: 1.5.3\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-iikpj_1 | INFO:main:tensorflow version info:\nalgo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-iikpj_1 | TensorFlow Library: 1.14.0\nalgo-1-iikpj_1 | EI Version: EI-1.4\nalgo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 172.18.0.1 - - [17\/Jun\/2020:05:29:47 +0000] \"POST \/invocations HTTP\/1.1\" 502 157 \"-\" \"-\"\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nJSONDecodeError Traceback (most recent call last)\nin ()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/serving.py in predict(self, data, initial_args)\n116 args[\"CustomAttributes\"] = self._model_attributes\n117\n--&gt; 118 return super(Predictor, self).predict(data, args)\n119\n120\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model)\n109 request_args = self._create_request_args(data, initial_args, target_model)\n110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n--&gt; 111 return self._handle_response(response)\n112\n113 def _handle_response(self, response):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in _handle_response(self, response)\n119 if self.deserializer is not None:\n120 # It's the deserializer's responsibility to close the stream\n--&gt; 121 return self.deserializer(response_body, response[\"ContentType\"])\n122 data = response_body.read()\n123 response_body.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in call(self, stream, content_type)\n578 \"\"\"\n579 try:\n--&gt; 580 return json.load(codecs.getreader(\"utf-8\")(stream))\n581 finally:\n582 stream.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n297 cls=cls, object_hook=object_hook,\n298 parse_float=parse_float, parse_int=parse_int,\n--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n300\n301\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 \"\"\"\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\n<\/code><\/pre>\n\n<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.\nI also tried Rest API post to docker deployed model:<\/p>\n\n<pre><code>curl -v -X POST \\ -H 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/Servo:predict\nbut still getting error:\n[![enter image description here][1]][1]\n<\/code><\/pre>\n\n<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.<\/p>\n\n<p>Please help me out. Thanks<\/p>",
        "Challenge_closed_time":1592876274340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592375129670,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering challenges with deploying a tensorflow model with elastic inference on a notebook instance, resulting in errors such as \"jsondecodeerror: expecting value: line 1 column 1 (char 0)\" and \"curl -v -x post \\ -h 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/servo",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62422682",
        "Challenge_link_count":9,
        "Challenge_participation_count":1,
        "Challenge_readability":19.9,
        "Challenge_reading_time":261.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":164,
        "Challenge_solved_time":139.2068527778,
        "Challenge_title":"sagemaker notebook instance Elastic Inference tensorflow model local deployment",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":1504,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310699693432,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2889.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>Solved it. The error I was getting is due to roles\/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.2,
        "Solution_reading_time":4.99,
        "Solution_score_count":-1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2159.1657988889,
        "Challenge_answer_count":2,
        "Challenge_body":"Is there a link that shows how much GPU memory is available on the following GPU instances on AWS?\n\n1. g4-series instances (NVidia T4)\n2. g5-series instances (NVidia A10)\n3. p3d-series instances (NVidia V100)\n4. p4d-series instances (NVidia A100)\n\nUpdate: the information is available for the [p3d series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/) and [g5 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g5\/), though not for the [g4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g4\/) or the [p4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p4\/) instances. Is it possible to retrieve the information for the latter two instances anywhere (without having to launch the instances)?",
        "Challenge_closed_time":1676386194668,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643029354176,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for information on the amount of GPU memory available on AWS instances, specifically the g4, g5, p3d, and p4d series. The user has found information for the p3d and g5 series, but not for the g4 and p4d series. The user is asking if there is a way to retrieve this information without having to launch the instances.",
        "Challenge_last_edit_time":1668613197792,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUvPdBv2rwTEiYKKHDPLUTWA\/how-much-gpu-memory-are-available-on-the-g4-g5-p3d-and-p4d-series-instances",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":9.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":9265.7890255556,
        "Challenge_title":"How much GPU memory are available on the g4, g5, p3d, and p4d series instances?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2195.0,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This link has a table that compares instances' GPU memory\nhttps:\/\/docs.amazonaws.cn\/en_us\/AmazonECS\/latest\/developerguide\/ecs-gpu.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1676386194668,
        "Solution_link_count":1.0,
        "Solution_readability":24.1,
        "Solution_reading_time":1.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1584379010350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":18.4716680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having an issue with AWS when I try to create a device fleet with sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker', region_name=AWS_REGION)\nsagemaker_client.create_device_fleet(\n    DeviceFleetName=device_fleet_name,\n    RoleArn=iot_role_arn,\n    OutputConfig={\n        'S3OutputLocation': s3_device_fleet_output\n    }\n)\n\n<\/code><\/pre>\n<p>It raises the following exception:<\/p>\n<blockquote>\n<p>ClientError: An error occurred (ValidationException) when calling the CreateDeviceFleet operation: The account id &lt;my-account-id&gt; does not have ownership on bucket: &lt;bucket-name&gt;<\/p>\n<\/blockquote>\n<p>I dont get it because I created the bucket so I should be the owner. I have not found how to check or change bucket ownership.<\/p>\n<p>I tried changing the bucket policy as follows but it didn't help.<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;id&gt;:user\/&lt;user&gt;&quot;\n            },\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n\n<\/code><\/pre>\n<p>I also tried with sagemaker's GUI, it fails for the same reason (ValidationException, the account id &lt;my-account-id&gt; does not have ownership on bucket : &lt;bucket-name&gt;).<\/p>",
        "Challenge_closed_time":1641479927648,
        "Challenge_comment_count":2,
        "Challenge_created_time":1641413429643,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to create a device fleet with Sagemaker on AWS. The CreateDeviceFleet operation fails with a ValidationException error message stating that the account ID does not have ownership on the bucket. The user has created the bucket and is unsure how to check or change bucket ownership. The user has also tried changing the bucket policy and using Sagemaker's GUI, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70599052",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":21.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":18.4716680556,
        "Challenge_title":"AWS CreateDeviceFleet operation fail because \"the account id does not have ownership on bucket\"",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584379010350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":36.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>This bucket policy made it work :<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;Statement1&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Principal&quot;: {\n                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role\/&lt;iot-role&gt;&quot;\n            },\n            &quot;Action&quot;: &quot;*&quot;,\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;&quot;,\n                &quot;arn:aws:s3:::&lt;bucket-name&gt;\/*&quot;\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n<p>I still don't fully get it, because the role had full access on s3 buckets so i don't know why editing the bucket's policy changed something, but it works.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":29.5,
        "Solution_reading_time":9.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1564118772683,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":4730.0,
        "Answerer_view_count":167.0,
        "Challenge_adjusted_solved_time":156.0215711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I wrote a simple Keras code, in which I use CNN for fashion mnist dataset. Everything works great. I implemented my own class and classification is OK.<\/p>\n<p>However, I wanted to use Optuna, as OptKeras (Optuna wrapper for Keras), you can see an example here: <a href=\"https:\/\/medium.com\/@Minyus86\/optkeras-112bcc34ec73\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@Minyus86\/optkeras-112bcc34ec73<\/a>.<\/p>\n<p>However, something is wrong with my code. When I try to use optKeras inside my own class. Here's the code: (ordinary <code>run<\/code> method works, but <code>optuna_run<\/code> gives an error: <code>AttributeError: type object 'FrozenTrial' has no attribute '_field_types'<\/code>.<\/p>\n<pre><code>! pip install optkeras\n# -*- coding: utf-8 -*- \n#!\/usr\/bin\/env python3\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN \nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\n\nimport optuna\nfrom optkeras.optkeras import OptKeras\n\nimport sys\nimport math\nimport numpy\nimport scipy.io as sio   \nimport matplotlib.pyplot as plt\n\nclass OptunaTest():\n\n  def __init__(self):\n    self.fashion_mnist = keras.datasets.fashion_mnist\n    (self.train_images, self.train_labels), (self.test_images, self.test_labels) = self.fashion_mnist.load_data()\n    self.class_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n    self.train_images = self.train_images \/ 255.0\n    self.test_images = self.test_images \/ 255.0\n    self.model = None \n    self.study_name = 'FashionMnist' + '_Simple'\n    self.ok = OptKeras(study_name=self.study_name)\n\n  def run(self):\n    self.model = keras.Sequential()\n    self.model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    self.model.add(keras.layers.Dense(128, activation='relu'))\n    self.model.add(keras.layers.Dense(10))\n    self.model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    self.model.fit(self.train_images, self.train_labels, epochs=5)\n    test_loss, test_acc = self.model.evaluate(self.test_images, self.test_labels, verbose=0)\n    predictions = self.model.predict(self.test_images)\n\n    INDEX = 10\n    print(&quot;\\nPREDICTION: &quot; + str(predictions[INDEX]))\n    print(&quot;\\nMAX PREDICTION VAL: &quot; + str(numpy.argmax(predictions[INDEX])))\n    print(&quot;\\nLABEL: &quot; + str(self.test_labels[INDEX]))\n\n  def optuna_run(self, trial):\n    K.clear_session() \n    \n    self.model = keras.Sequential()\n    self.model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    self.model.add(keras.layers.Dense(units = trial.suggest_categorical('units', [32, 64, 128]), activation = trial.suggest_categorical('activation', ['relu', 'linear'])))\n    self.model.add(keras.layers.Dense(units = trial.suggest_categorical('units', [32, 64, 128]), activation = trial.suggest_categorical('activation', ['relu', 'linear'])))\n\n    self.model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    self.model.fit(self.train_images, self.train_labels, epochs=5, callbacks = self.ok.callbacks(trial), verbose = self.ok.keras_verbose)\n    test_loss, test_acc = self.model.evaluate(self.test_images, self.test_labels, verbose=0)\n    predictions = self.model.predict(self.test_images)\n    print(ok.trial_best_value)\n    \n    INDEX = 10\n    print(&quot;\\nPREDICTION: &quot; + str(predictions[INDEX]))\n    print(&quot;\\nMAX PREDICTION VAL: &quot; + str(numpy.argmax(predictions[INDEX])))\n    print(&quot;\\nLABEL: &quot; + str(self.test_labels[INDEX]))\n\n\nif __name__ == &quot;__main__&quot;:\n  ot = OptunaTest()\n  ot.run()\n\n  ot.ok.optimize(ot.optuna_run,  timeout = 60)\n<\/code><\/pre>\n<p>A code can also be found here: <a href=\"https:\/\/colab.research.google.com\/drive\/1uibWa80BdjatA5Kcw27eMUsS7SmwxaDk?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1uibWa80BdjatA5Kcw27eMUsS7SmwxaDk?usp=sharing<\/a>.<\/p>\n<p>The full error message:<\/p>\n<pre><code>[W 2020-06-30 11:09:26,959] Setting status of trial#0 as TrialState.FAIL because of the following error: AttributeError(&quot;type object 'FrozenTrial' has no attribute '_field_types'&quot;,)\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 230, in synch_with_optuna\n    self.best_trial = self.study.best_trial\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optuna\/study.py&quot;, line 97, in best_trial\n    return copy.deepcopy(self._storage.get_best_trial(self._study_id))\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optuna\/storages\/in_memory.py&quot;, line 293, in get_best_trial\n    raise ValueError(&quot;No trials are completed yet.&quot;)\nValueError: No trials are completed yet.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optuna\/study.py&quot;, line 734, in _run_trial\n    result = func(trial)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 130, in fun_tf\n    return fun(trial)\n  File &quot;&lt;ipython-input-11-45495c9f2ae9&gt;&quot;, line 65, in optima_run\n    self.model.fit(self.train_images, self.train_labels, epochs=10, callbacks = self.ok.callbacks(trial), verbose = self.ok.keras_verbose)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 172, in callbacks\n    self.synch_with_optuna()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 232, in synch_with_optuna\n    self.best_trial = get_trial_default()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py&quot;, line 367, in get_trial_default\n    num_fields = optuna.structs.FrozenTrial._field_types.__len__()\nAttributeError: type object 'FrozenTrial' has no attribute '_field_types'\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py in synch_with_optuna(self)\n    229         try:\n--&gt; 230             self.best_trial = self.study.best_trial\n    231         except:\n\n12 frames\n\nValueError: No trials are completed yet.\n\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/optkeras\/optkeras.py in get_trial_default()\n    365 \n    366 def get_trial_default():\n--&gt; 367     num_fields = optuna.structs.FrozenTrial._field_types.__len__()\n    368     assert num_fields in (10, 11, 12)\n    369     if num_fields == 12: # possible future version\n\nAttributeError: type object 'FrozenTrial' has no attribute '_field_types'\n<\/code><\/pre>",
        "Challenge_closed_time":1593917427550,
        "Challenge_comment_count":1,
        "Challenge_created_time":1593516495007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use OptKeras, an Optuna wrapper for Keras, inside their own class to optimize their CNN model for the fashion mnist dataset. However, when they try to use the optuna_run method, they encounter an AttributeError with the message \"type object 'FrozenTrial' has no attribute '_field_types'\". The error occurs when the code tries to synchronize with Optuna and get the best trial.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62656411",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":17.9,
        "Challenge_reading_time":93.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":75,
        "Challenge_solved_time":111.3701508334,
        "Challenge_title":"OptKeras (Keras Optuna Wrapper) - use optkeras inside my own class, AttributeError: type object 'FrozenTrial' has no attribute '_field_types'",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":888.0,
        "Challenge_word_count":541,
        "Platform":"Stack Overflow",
        "Poster_created_time":1344355535128,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3580.0,
        "Poster_view_count":962.0,
        "Solution_body":"<p>It seems that optkeras (version I got was 0.0.7) being not quite up-to-date with optuna library is the reason for the issue. I was able to make it work with optuna 1.5.0 by doing the following changes:<\/p>\n<p>First, you'll need to monkey-patch <code>get_default_trial<\/code> like this before running your code:<\/p>\n<pre><code>import optkeras\noptkeras.optkeras.get_trial_default = lambda: optuna.trial.FrozenTrial(\n            None, None, None, None, None, None, None, None, None, None, None)\n<\/code><\/pre>\n<p>After doing so I'm getting an error with <code>Callback<\/code> saying:<\/p>\n<pre><code>AttributeError: 'OptKeras' object has no attribute '_implements_train_batch_hooks'\n<\/code><\/pre>\n<p>To solve this you'll have to manually edit optkeras.py, but not too much - just add <code>tensorflow.<\/code> to first two lines imports, i.e. make them:<\/p>\n<pre><code>import tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n<\/code><\/pre>\n<p>instead of:<\/p>\n<pre><code>import keras.backend as K\nfrom keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n<\/code><\/pre>\n<p>If you can't change the code after installation it might be a bit of a problem - I would probably just recommend to copy full code of optkeras library (it's just one file optkeras.py) and use fixed version of that in your script or something like that. Unfortunately I don't see a nice way of monkey-patching this import issue. That said I think it can be fairly easy to either change that on-fly even from python (i.e. change <code>optkeras.py<\/code> lines from within python before importing optkeras) or copying the optkeras.py (also from withing python script) + replacing the strings on fly, then importing from the new location.<\/p>\n<p>After that is done I just had to:<\/p>\n<ul>\n<li>fix typo in your code (<code>print(ok.trial_best_value)<\/code> should really be <code>print(self.ok.trial_best_value)<\/code>)<\/li>\n<li>add <code>validation_split=0.1<\/code> to <code>self.model.fit<\/code> call (or you may use something else for your tuning - just with existing code example callback won't get <code>val_loss<\/code> value because there is no validation set and optkeras is using <code>val_loss<\/code> by default - see <code>monitor<\/code> argument for <code>OptKeras<\/code> constructor). My guess would be that you probably will either want to create a fixed validation set instead or monitor training loss <code>loss<\/code> instead of <code>val_loss<\/code>.<\/li>\n<li>add <code>return test_loss<\/code> at the end of <code>optuna_run<\/code> method.<\/li>\n<\/ul>\n<p>After all of these changes everything seems be working.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1594078172663,
        "Solution_link_count":0.0,
        "Solution_readability":9.6,
        "Solution_reading_time":33.73,
        "Solution_score_count":3.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":343.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1589738451347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":99.0917625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to provision an AKS cluster that is connected to a vnet and has an internal load balancer on Azure. I am using code from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet?tabs=python\" rel=\"nofollow noreferrer\">here<\/a> that looks like this:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Verify that cluster does not exist already\ntry:\n    aks_target = AksCompute(workspace=ws, name=aks_cluster_name)\n    print(&quot;Found existing aks cluster&quot;)\n\nexcept:\n    print(&quot;Creating new aks cluster&quot;)\n\n    # Subnet to use for AKS\n    subnet_name = &quot;default&quot;\n    # Create AKS configuration\n    prov_config=AksCompute.provisioning_configuration(load_balancer_type=&quot;InternalLoadBalancer&quot;)\n    # Set info for existing virtual network to create the cluster in\n    prov_config.vnet_resourcegroup_name = &quot;myvnetresourcegroup&quot;\n    prov_config.vnet_name = &quot;myvnetname&quot;\n    prov_config.service_cidr = &quot;10.0.0.0\/16&quot;\n    prov_config.dns_service_ip = &quot;10.0.0.10&quot;\n    prov_config.subnet_name = subnet_name\n    prov_config.docker_bridge_cidr = &quot;172.17.0.1\/16&quot;\n\n    # Create compute target\n    aks_target = ComputeTarget.create(workspace = ws, name = &quot;myaks&quot;, provisioning_configuration = prov_config)\n    # Wait for the operation to complete\n    aks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>However, I get the following error<\/p>\n<pre><code>K8s failed to assign an IP for Load Balancer after waiting for an hour.\n<\/code><\/pre>\n<p>Is this because the AKS cluster does not yet have a 'network contributor' role for the vnet resource group? Is the only way to get this to work to first create AKS outside of AMLS, grant the network contributor role to the vnet resource group, then attach the AKS cluster to AMLS and configure the internal load balancer afterwards?<\/p>",
        "Challenge_closed_time":1604359053332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603997735143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to provision an AKS cluster with an internal load balancer on Azure using code from a Microsoft documentation page. However, they are encountering an error where K8s failed to assign an IP for Load Balancer after waiting for an hour. The user suspects that this is because the AKS cluster does not have a 'network contributor' role for the vnet resource group. They are wondering if the only solution is to create AKS outside of AMLS, grant the network contributor role to the vnet resource group, and then attach the AKS cluster to AMLS and configure the internal load balancer afterwards.",
        "Challenge_last_edit_time":1604002322987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64597526",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":25.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":100.3661636111,
        "Challenge_title":"Provision AKS with internal load balancer from AMLS on Azure",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":352.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I was able to get this to work by first creating an AKS resource without an internal load balancer, then separately updating the load balancer following this code:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute.aks import AksUpdateConfiguration\nfrom azureml.core.compute import AksCompute\n\n# ws = workspace object. Creation not shown in this snippet\naks_target = AksCompute(ws,&quot;myaks&quot;)\n\n# Change to the name of the subnet that contains AKS\nsubnet_name = &quot;default&quot;\n# Update AKS configuration to use an internal load balancer\nupdate_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)\naks_target.update(update_config)\n# Wait for the operation to complete\naks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>No network contributor role was required.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":10.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":575.7216666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When using `fds clone` for non-DVC repo it throws the following error:\r\n\r\n`ERROR: you are not inside of a DVC repository (checked up to mount point '\/')`\r\n\r\nCloning a non-DVC repo using FDS can be a common use case, e.g., cloning a DAGsHub repo containing many files, but none of them are tracked by DVC nur the repo contains DVC config files. \r\n\r\nI suggest that after cloning the Git server, FDS will check if the repo contains DVC files. \r\n\r\nif it contains DVC files:\r\n  - echo 'Starting DVC Clone...`\r\n  - FDS will start a wizard to set the user name and password for each remote storage in the local config. (consider checking if they are set in the global config file first?)\r\n  - FDS will pull all the files from the remotes and show a progress bar (might be reasonable to ask if the user wants to pull the files from each remote)\r\n \r\nIt doesn't contain DVC files:\r\n  - FDS will initialize DVC\r\n  \r\n    if the Git server URL is DAGsHub's:\r\n      - FDS will set DAGsHub storage as the remote using the Git URL (replacing`.git` with `.dvc`).\r\n      - FDS will start a wizard to set the remote user name, password, and name.\r\n      \r\n    else:\r\n       - FDS will start a wizard asking do you want to set a DVC remote\r\n       if yes:\r\n           - With the wizard, the user will set the remote URL, name, username, and password.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1630576282000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628503684000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue where the markdown in the dvc install prompt is not being rendered as markdown.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/87",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.28,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":575.7216666667,
        "Challenge_title":"fsd clone for non-DVC repos throws an error",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":232,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":229.9927777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Description**\r\nError when a MLflow registry model is deployed to triton using **mlflow-triton-plugin** with `--falvor=onnx` flag.\r\nThe plugin is trying to create a `config.pbtxt` in the destination folder before creating that model folder itself.\r\nEasy fix is to create that folder beforehand, but could also be handled from the plugin side.\r\n\r\n```\r\n# create a dir if not exists  \r\nif not os.exists(triton_deployment_dir):\r\n  os.mkdir(triton_deployment_dir)\r\n# then write config to that dir\r\nwith open(os.path.join(triton_deployment_dir, \"config.pbtxt\"),\r\n            \"w\") as cfile:\r\n    cfile.write(config)\r\n```\r\n\r\n**Triton Information**\r\nDocker image: `nvcr.io\/nvidia\/tritonserver:21.12-py3`\r\n\r\n**To Reproduce**\r\n\r\n0. Install mlflow-triton-plugin\r\n1. Log and register an ONNX model to MLflow model registry.\r\n2. Run a triton inference server with these flags: `--model-control-mode=explicit --strict-model-config=false`\r\n3. Create a deployment from mlflow:\r\n `mlflow deployments create -t triton --flavor onnx --name <model-name> -m \"models:\/<model-name>\/1\"`\r\n\r\nError is raised:\r\n```\r\nFile \"mlflow_triton\/deployments.py\", line 105, in create_deployment\r\n  File \"mlflow_triton\/deployments.py\", line 332, in _copy_files_to_triton_repo\r\n  File \"mlflow_triton\/deployments.py\", line 326, in _get_copy_paths\r\nFileNotFoundError: [Errno 2] No such file or directory: '<dest-folder>\/<model-name>\/config.pbtxt'\r\n```\r\n",
        "Challenge_closed_time":1649449895000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648621921000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to load a pyfunc model in the beta version of BentoML 1.0, even though the model gets successfully stored in the local model store. The loading of the model is failing with an AttributeError. The user has provided the code used to train and log the model to mlflow and the code used to load the model into BentoML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4130",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.7,
        "Challenge_reading_time":18.1,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":229.9927777778,
        "Challenge_title":"error creating a triton deployment mlflow plugin",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":159,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9285602778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this? \n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker. \n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models? \n\nMany thanks \n\nTim",
        "Challenge_closed_time":1638917636668,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638914293851,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to access a large image dataset stored in a Sagemaker notebook instance from outside of that instance, specifically via Python Sagemaker Estimator training call. The user wants to access the data in a fast manner to train an AI model and is looking for the recommended way to do this. The user also wants to know the best way to store data when using Sagemaker estimators for training AI models.",
        "Challenge_last_edit_time":1668630486323,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":11.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.9285602778,
        "Challenge_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1538.0,
        "Challenge_word_count":170,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the \/home\/ec2-user\/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test\/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1638917636668,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":28.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":387.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1395737095150,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":154.6126952778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using the SageMaker TensorFlow estimator for training, and specifying an output path for my model artifacts with the <code>output_path<\/code> argument, with a value of <code>s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/<\/code>. <\/p>\n\n<p>After model training, a directory named <code>&lt;training_job_name&gt;\/output<\/code> is created in the specified <code>output_path<\/code>. <\/p>\n\n<p>The issue I'm having is, the source code that's used for training is also uploaded to S3 by default, but instead of being placed in <code>s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\/&lt;training_job_name&gt;\/source<\/code>, it's placed in <code>s3:\/\/&lt;bucket&gt;\/&lt;training_job_name&gt;\/source<\/code>. <\/p>\n\n<p>So how can I specify the S3 upload path for the training job's source code in order to make it use the bucket AND prefix name of <code>output_path<\/code>?<\/p>",
        "Challenge_closed_time":1558768756470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558212150767,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the SageMaker TensorFlow estimator where the source code used for training is uploaded to S3 by default, but it is placed in a different path than the specified output path. The user is looking for a way to specify the S3 upload path for the training job's source code to use the bucket and prefix name of the output path.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56202697",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.2,
        "Challenge_reading_time":11.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":154.6126952778,
        "Challenge_title":"SageMaker TensorFlow Estimator source code S3 upload path",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":645.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>Have you tried using the \u201ccode_location\u201d argument: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a> to specify the location for the source code?<\/p>\n\n<p>Below is a snippet code example that use code_location<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ncode-path = \"s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\"\noutput-path = \"s3:\/\/&lt;bucket&gt;\/&lt;prefix&gt;\"\n\nabalone_estimator = TensorFlow(entry_point='abalone.py',\n                           role=role,\n                           framework_version='1.12.0',\n                           training_steps= 100, \n                           image_name=image,\n                           evaluation_steps= 100,\n                           hyperparameters={'learning_rate': 0.001},\n                           train_instance_count=1,\n                           train_instance_type='ml.c4.xlarge',\n                           code_location= code-path,\n                           output_path = output-path,\n                           base_job_name='my-job-name'\n                           )\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.2,
        "Solution_reading_time":11.38,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":0.4279386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Difficulty in understanding<\/h1>\n<p>Q2) How to download a file from S3?<\/p>\n<p><strong>From<\/strong>  <a href=\"https:\/\/medium.com\/akeneo-labs\/machine-learning-workflow-with-sagemaker-b83b293337ff\" rel=\"nofollow noreferrer\">The Machine Learning Workflow with SageMaker<\/a><\/p>\n<p>And also why are we using this piece of code?<\/p>\n<p><code>estimator.fit(train_data_location)<\/code><\/p>",
        "Challenge_closed_time":1568928522076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568926981497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty understanding how to download a file from S3 and why the code \"estimator.fit(train_data_location)\" is being used in the blog post \"The Machine Learning Workflow with SageMaker\".",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58018893",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":6.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4279386111,
        "Challenge_title":"How do S3 file download and estimator.fit() work in this blog post?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":220.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1556451987416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1309.0,
        "Poster_view_count":288.0,
        "Solution_body":"<h2>Downloading a file from S3:<\/h2>\n\n<p>This code block in the Q2 section defines the function that downloads a file from S3. The user instantiates an S3 client, and then passes the S3 URL along to the <code>s3.Bucket.download_file()<\/code> method.<\/p>\n\n<pre><code>def download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<h2>Estimator.fit() explanation:<\/h2>\n\n<p>The <code>estimator.fit(train_data_location)<\/code> line is what initiates the training process with SageMaker. When run, SageMaker will provision the necessary infrastructure, fetch the data from the location the user designated (here, <code>train_data_location<\/code> which is a path to Amazon S3) and distribute it amongst the training cluster, carry out the training process, return the resulting model, and tear down the training infrastructure. <\/p>\n\n<p>You can find the result of this training job in the SageMaker console.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":19.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":166.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":459.5833333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi there.\nI am working with Vertex AI Jupyterlab Notebook.\nThere were a few such warnings\n\nWARN BlockManager: Block rdd_6_0 already exists on this machine; not re-adding it\nWARN BlockManager: Block rdd_817_0 already exists on this machine; not re-adding it\n\non this as the model was getting trained.\nMay I know if we are safe to ignore this?\nWhat do they mean actually?\nThanks in advance.",
        "Challenge_closed_time":1661250000000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659595500000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered warnings while working with Vertex AI Jupyterlab Notebook, specifically \"Block rdd_6_0 already exists on this machine; not re-adding it\" and \"Block rdd_817_0 already exists on this machine; not re-adding it\" while training a model. The user is seeking clarification on whether these warnings can be ignored and what they mean.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/WARN-BlockManager-Block-rdd-6-0-already-exists-on-this-machine\/m-p\/450462#M486",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":459.5833333333,
        "Challenge_title":"WARN BlockManager: Block rdd_6_0 already exists on this machine; not re-adding it",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":135.0,
        "Challenge_word_count":77,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"No, you should not worry since this is just a warning that tells you that those two blocks will not be re added to your notebook.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":23.80566,
        "Challenge_answer_count":1,
        "Challenge_body":"1. We trained custom model with manually annotated data set of documents but the accuracy is low, we want to annotate and train again. When I create a new version will it learn from the current set of inputs and also preserve the old training ? Do I need to give all the data for every incremental training ? \n\n2. Since there is some low accuracy issue, I want to add a2i . How to do it in console UI for batch processing ?\nWhen the people make additional annotation in a2i, will the comprehend learn incrementally ?\nOr do we need to make run the training job again ?",
        "Challenge_closed_time":1677047264656,
        "Challenge_comment_count":0,
        "Challenge_created_time":1676615139900,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a custom model with a manually annotated dataset of documents, but the accuracy is low. They want to re-annotate and train the model again, but are unsure if the new version will learn from the current set of inputs and preserve the old training. They also want to add Amazon Augmented AI (a2i) to improve accuracy and are unsure if the model will learn incrementally from additional annotations or if they need to run the training job again.",
        "Challenge_last_edit_time":1676961564280,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUuwusX1xNQO6J-M-AkCFlig\/comprehend-incremental-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":7.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":120.0346544444,
        "Challenge_title":"Comprehend incremental training",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":53.0,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You will need to provide all of the data for each incremental training. \n\nComprehend is not integrated with A2I at this time, so you would need to re-submit a new training job with all the annotations.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1677047264656,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":2.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":31.7955766667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In colab, whenever we need GPU, we simply click <code>change runtime type<\/code> and change hardware accelarator to <code>GPU<\/code><\/p>\n<p>and cuda becomes available, <code>torch.cuda.is_available()<\/code> is <code>True<\/code><\/p>\n<p>How to do this is AWS sagemaker, i.e. turning on cuda.\nI am new to AWS and trying to train model using pytorch in aws sagemaker, where Pytorch code is first tested in colab environment.<\/p>\n<p>my sagemaker notebook insatnce is <code>ml.t2.medium<\/code><\/p>",
        "Challenge_closed_time":1617464162903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617348836210,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to AWS and is trying to train a Pytorch model in an AWS Sagemaker notebook instance. They are looking for guidance on how to turn on CUDA, similar to how it is done in Colab, where they can simply change the runtime type to GPU. They are currently using a ml.t2.medium instance.",
        "Challenge_last_edit_time":1617349698827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66915920",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":32.0351925,
        "Challenge_title":"Using pytorch cuda in AWS sagemaker notebook instance",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1608.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567880532003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":137.0,
        "Poster_view_count":100.0,
        "Solution_body":"<p>Using AWS Sagemaker you don't need to worry about the GPU, you simply select an instance type with GPU ans Sagemaker will use it. Specifically <code>ml.t2.medium<\/code> doesn't have a GPU but it's anyway not the right way to train a model.\nBasically you have 2 canonical ways to use Sagemaker (look at the documentation and examples please), the first is to use a notebook with a limited computing resource to spin up a training job using a prebuilt image, in that case when you call the estimator you simply specify what <a href=\"https:\/\/aws.amazon.com\/ec2\/instance-types\/\" rel=\"nofollow noreferrer\">instance type<\/a> you want (you'll choose one with GPU, looking at the costs). The second way is to use your own container, push it to ECR and launch a training job from the console, where you specify the instance type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":10.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":134.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.66,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nI have two version for using component. One is directly uploading local code to each pod, this work fine that we could see the models artifact and metric curve. The other one use the same code as the first one, except for using private github to load the code. In this case, the \"dashboards\", \"artifacts\" and \"resources\" pages show only \"NO DATA\", and \"logs\" page shows training output message normally.\n\nTo reproduce\nAdd private code connection\nconnections:\n  - name: my-repo\n    kind: git\n    schema:\n      url: https:\/\/github.com\/xxx\/my-repo\n    secret:\n      name: \"github-secret-my-repo\"\n\nruning job config:\nrun:\n  kind: job\n  init:\n    - connection: my-repo\n\nHow we use log_metric and log_model\n# log metric\ntracking.log_metric(\"val_loss\", val_loss, step=epoch)\ntracking.log_metric(\"val_precision\", precision, step=epoch)\ntracking.log_metric(\"val_recall\", recall, step=epoch)\n\n# log model\nmodel_output_dir = tracking.get_outputs_path(\"models\", is_dir=True)\nckpt_file = os.path.join(model_output_dir, 'checkpoint.pth.tar')\ntorch.save({xxx}, ckpt_file)\ntracking.log_model(name=\"checkpoint\", path=ckpt_file, framework=\"pytorch\")\n\nExpected behavior\n\nShowing metric curve and saving models normally.\n\nEnvironment\n\nminikube: v1.15.1\npolyaxon ce: 1.7.5",
        "Challenge_closed_time":1619508955000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619492179000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with a private Github repo while using log_model and log_metric. The \"dashboards\", \"artifacts\" and \"resources\" pages show only \"NO DATA\", and \"logs\" page shows training output message normally. The user has provided the code and configuration details for reproducing the issue. The expected behavior is to show metric curve and save models normally. The environment used is minikube v1.15.1 and polyaxon ce 1.7.5.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1302",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.6,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.66,
        "Challenge_title":"Private github repo fail for using log_model and log_metric",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is very hard to debug, you need to check if the git repo has some more information like a hard-coded NO_OP.\n\nAlso I would check if you added the local cache folder .polyaxon to your .gitignore and .dockerignore. If this folder was added to you git repo, then indeed the metrics and artifacts will be saved to a different run.\n\nYou can also validate that the run is running with the correct run-uuid:\n\n...\nprint(tracking.TRACKING_RUN.run_uuid)\n...\n\nIf the run_uuid does not correspond to the currently running run, then the cache is bundled somewhere (git repo or docker image).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":97.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1400869861950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, Brazil",
        "Answerer_reputation_count":135.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":18.1100302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pretrained spacy model on a local folder that I can easily read with <code>m = spacy.load(&quot;path\/model\/&quot;)<\/code><\/p>\n<p>But now I have to upload it as a .tar.gz file to use as a Sagemaker model artifact.\nHow can I read this .tar.gz file?<\/p>\n<p>Ideally I want to read the unzipped folder from memory. Without extracting all to disk and then reading it again<\/p>\n<p>My question is almost a duplicate of this one <a href=\"https:\/\/stackoverflow.com\/questions\/49274650\/directly-load-spacy-model-from-packaged-tar-gz-file\">Directly load spacy model from packaged tar.gz file<\/a>. But the answers don't explain how to untar unzip the folder into memory<\/p>",
        "Challenge_closed_time":1652803358076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652738161967,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a pretrained spacy model on a local folder that needs to be uploaded as a .tar.gz file to use as a Sagemaker model artifact. The user is looking for a way to read the unzipped folder from memory without extracting all to disk and then reading it again. The user's question is similar to a previously asked question, but the answers do not explain how to untar unzip the folder into memory.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72266041",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":18.1100302778,
        "Challenge_title":"Loading a spacy .tar.gz model artifact from s3 Sagemaker",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400869861950,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, Brazil",
        "Poster_reputation_count":135.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Turns out Sagemaker already decompress the <code>.tar.gz<\/code> file automatically.\nSo I can just read the folder exactly like before.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":30.3172266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train a ResNet50 model using keras with tensorflow backend. I'm using a sagemaker GPU instance <strong>ml.p3.2xlarge<\/strong> but my training time is extremely long. I am using conda_tensorflow_p36 kernel and I have verified that I have tensorflow-gpu installed.<\/p>\n<p>When inspecting the output of nvidia-smi I see the process is on the GPU, but the utilization is never above <strong>0%<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Tensorflow also recognizes the GPU.\n<a href=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Screenshot of training time.\n<a href=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is sagemaker in fact using the GPU even though the usage is <strong>0%?<\/strong>\nCould the long epoch training time be caused by another issue?<\/p>",
        "Challenge_closed_time":1650008050343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649867231580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing slow training time while using a ResNet50 model with keras and tensorflow backend on an AWS Sagemaker GPU instance. The user has verified that tensorflow-gpu is installed and the GPU is recognized by tensorflow, but the GPU utilization is never above 0%. The user is unsure if the GPU is being used and is also questioning if there could be another issue causing the long epoch training time.",
        "Challenge_last_edit_time":1649898908327,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71860839",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":15.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":39.1163230556,
        "Challenge_title":"Slow ResNet50 training time using AWS Sagemaker GPU instance",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":120.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540782143880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Looks like you've completed 8 steps and it just takes very long. What's your step time?<br \/>\nIt might be due to data loading. Where ia data stored? Try to take data loading out of the picture by caching and feeding a single image to the DNN repeatedly and see if that helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.8,
        "Solution_reading_time":3.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2491.9302777778,
        "Challenge_answer_count":0,
        "Challenge_body":"### Summary\r\n\r\nyou can call mlflow.log_artifact directly and save the profile JSON:\r\n```\r\nsummary = profile.to_summary()\r\nopen(\"local_path\", \"wt\", transport_params=transport_params) as f:\r\n    f.write(message_to_json(summary))\r\nmlflow.log_artifact(\"local_path\", your\/path\")\r\n```\r\n\r\nbut if you pass a format config to mlflow writer specifying 'json' it isn't supported and instead uses the protobuf bin format.\r\n\r\n\r\n",
        "Challenge_closed_time":1655127391000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1646156442000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a NameError when trying to import the numbertracker from whylogs.core.statistics due to the optional MLFlow dependency not being installed. The error occurs the first time the import is attempted but works fine on the second attempt.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/458",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":5.91,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2491.9302777778,
        "Challenge_title":"Support writing out dataset profiles as json format with mlflow",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"How can i retrieve the profile while inside the start_run()? This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.8,
        "Solution_reading_time":1.6,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":10204.2583313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've fitted a Tensorflow Estimator in SageMaker using Script Mode with <code>framework_version='1.12.0'<\/code> and <code>python_version='py3'<\/code>, using a GPU instance. <\/p>\n\n<p>Calling deploy directly on this estimator works if I select deployment instance type as GPU as well. However, if I select a CPU instance type and\/or try to add an accelerator, it fails with an error that docker cannot find a corresponding image to pull. <\/p>\n\n<p>Anybody know how to train a py3 model on a GPU with Script Mode and then deploy to a CPU+EIA instance? <\/p>\n\n<hr>\n\n<p>I've found a partial workaround by taking the intermediate step of creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not seem to support python 3 (again, doesn't find a corresponding container). If I switch to python_version='py2', it will find the container, but fail to pass health checks because all my code is for python 3.<\/p>",
        "Challenge_closed_time":1549915825300,
        "Challenge_comment_count":1,
        "Challenge_created_time":1549048113447,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue while deploying a Tensorflow Estimator in SageMaker using Script Mode with framework_version='1.12.0' and python_version='py3'. The deployment works fine if the deployment instance type is GPU, but fails with an error when selecting a CPU instance type and\/or adding an accelerator. The user is looking for a way to train a py3 model on a GPU with Script Mode and then deploy it to a CPU+EIA instance. The user has found a partial workaround by creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not support python 3.",
        "Challenge_last_edit_time":1549048982467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54485769",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":12.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":241.0310702778,
        "Challenge_title":"SageMaker deploying to EIA from TF Script Mode Python3",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":378.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>Unfortunately there are no TF + Python 3 + EI serving images at this time. If you would like to use TF + EI, you'll need to make sure your code is compatible with Python 2.<\/p>\n\n<p>Edit: after I originally wrote this, support for TF + Python 3 + EI has been released. At the time of this writing, I believe TF 1.12.0, 1.13.1, and 1.14.0 all have Python 3 + EI support. For the full list, see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1585784312460,
        "Solution_link_count":2.0,
        "Solution_readability":9.4,
        "Solution_reading_time":7.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1434117836363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Toronto, ON, Canada",
        "Answerer_reputation_count":78.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":0.2549266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have installed sagemaker using <code>sc.install_pypi_package(&quot;sagemaker==2.5.1&quot;)<\/code>. However, I get the following error when I try to import sagemaker and it is pointing to python2.7.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" alt=\"cannot import name git_utils\" \/><\/a><\/p>\n<p>I checked my EMR master node running pyspark and the version there is pyspark 2.4.5 running python 3.7.6.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hyctk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hyctk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So then I tried to upgrade the python version of my spark context but it says<\/p>\n<blockquote>\n<p>&quot;ValueError: Package already installed for current Spark context!&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So I thought lemme try uninstalling python2.7 from spark context and that does not let me do it, saying<\/p>\n<blockquote>\n<p>&quot;Not uninstalling python at \/usr\/lib64\/python2.7\/lib-dynload, outside\nenvironment \/tmp\/1598628537004-0&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I doing wrong? I believe the sagemaker import is failing due to spark context referring python2.7. How do I fix this?<\/p>",
        "Challenge_closed_time":1598632128583,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598630122480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has installed sagemaker using sc.install_pypi_package() but gets an error when trying to import sagemaker, which is pointing to python2.7. The EMR master node running pyspark has version pyspark 2.4.5 running python 3.7.6. The user tried to upgrade the python version of the spark context but received a ValueError. The user also attempted to uninstall python2.7 from the spark context but received an error message. The user believes the sagemaker import is failing due to the spark context referring to python2.7 and is seeking a solution to fix this issue.",
        "Challenge_last_edit_time":1598631210847,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63637178",
        "Challenge_link_count":8,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":22.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":0.5572508334,
        "Challenge_title":"Spark is running python 3.7.6 but spark context is showing python 2.7. How to fix using the spark context?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":178,
        "Platform":"Stack Overflow",
        "Poster_created_time":1434117836363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":78.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>Referred to this <a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/install-python-libraries-on-a-running-cluster-with-emr-notebooks\/\" rel=\"nofollow noreferrer\">link<\/a> and updated the python version of spark context to python3. This fixes the issue:<\/p>\n<pre><code>%%configure -f\n{ &quot;conf&quot;:{\n          &quot;spark.pyspark.python&quot;: &quot;python3&quot;,\n          &quot;spark.pyspark.virtualenv.enabled&quot;: &quot;true&quot;,\n          &quot;spark.pyspark.virtualenv.type&quot;:&quot;native&quot;,\n          &quot;spark.pyspark.virtualenv.bin.path&quot;:&quot;\/usr\/bin\/virtualenv&quot;\n         }\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.2,
        "Solution_reading_time":8.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.316325,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi, I am training my models via Azure Machine Learning.<\/p>\n<p>On other day, my training is running with GPU support, however today I found my training is running on a CPU.  <br \/>\nI'm not modified training environment, only training script was modified.  <br \/>\nMy computing cluster is NC6v3 - have a GPU.<\/p>\n<p>I investigate a situation, and I found training script is running on PyTorch 1.6.0.  <br \/>\nOn other day, it ran on Pytorch 1.8.1.  <br \/>\nI think my &quot;don't use GPU&quot; problem is caused by the situation that CUDA toolkit version is not suitable for Pytorch version.<\/p>\n<p>Then, I output a installed package to the log.  <br \/>\nThe log says 'Pytorch 1.8.1 was installed, however uses 1.6.0'.  <br \/>\nI confused by this weird circumstances.  <br \/>\nCan someone tell me the solution?<\/p>\n<p>&lt;My code snippet&gt;  <br \/>\n&lt;&lt;conda_dependencies.yaml&gt;&gt;<\/p>\n<p>channels:  <\/p>\n<ul>\n<li> conda-forge  <\/li>\n<li> pytorch  <\/li>\n<li> nvidia  <br \/>\ndependencies:  <\/li>\n<li> python=3.8.10  <\/li>\n<li> mesa-libgl-cos6-x86_64  <\/li>\n<li> cudatoolkit=11.1  <\/li>\n<li> pytorch==1.8.1  <\/li>\n<li> torchvision==0.9.1  <\/li>\n<li> tqdm  <\/li>\n<li> scikit-learn  <\/li>\n<li> matplotlib  <\/li>\n<li> pandas  <\/li>\n<li> pip &lt; 20.3  <\/li>\n<li> pip:  <\/li>\n<li> azureml-defaults  <\/li>\n<li> opencv-python-headless  <\/li>\n<li> pillow==8.2.0<\/li>\n<\/ul>\n<p>&lt;&lt;Environment definition&gt;&gt;  <br \/>\nenvironment_definition_file = experiment_dir \/ 'conda_dependencies.yaml'  <br \/>\nenvironment_name = 'pytorch-1.8.1-gpu'  <br \/>\nbase_image_name = 'mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04'  <br \/>\nenvironment = Environment.from_docker_image(environment_name, base_image_name, conda_specification = environment_definition_file)  <br \/>\ndocker_run_config = DockerConfiguration(use_docker=True)<\/p>\n<p>script_run_config = ScriptRunConfig(  <br \/>\nsource_directory = experiment_dir,  <br \/>\nscript = SCRIPT_FILE_NAME,  <br \/>\narguments = arguments,  <br \/>\ncompute_target = compute_target,  <br \/>\ndocker_runtime_config = docker_run_config,  <br \/>\nenvironment = environment)<\/p>\n<p>&lt;&lt;Output a log in the training script&gt;&gt;  <br \/>\nimport torch  <br \/>\nimport pip<\/p>\n<p>pip.main(['list'])  <br \/>\nprint(f'PyTorch version: {torch.<strong>version<\/strong>}')<\/p>\n<p>&lt;My logs&gt;  <br \/>\nPackage Version<\/p>\n<hr \/>\n<p>adal 1.2.7  <br \/>\napplicationinsights 0.11.10  <br \/>\n(omission)  <br \/>\ntorch 1.8.1  <br \/>\ntorchvision 0.9.0a0  <br \/>\n(omission)<\/p>\n<p>PyTorch version: 1.6.0<\/p>",
        "Challenge_closed_time":1629156299363,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629119160593,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning where their training script is running on a CPU instead of a GPU. Upon investigation, they found that the training script is running on PyTorch 1.6.0 instead of the required version of PyTorch 1.8.1. The user suspects that the issue is caused by the unsuitable CUDA toolkit version for PyTorch version. The user has provided their code snippet and log output for reference and is seeking a solution to the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/515579\/azure-machine-learning-uses-invalid-pytorch-versio",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":32.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":10.316325,
        "Challenge_title":"Azure Machine Learning - Uses invalid Pytorch version when training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":285,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. These are the <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.pytorch?view=azure-ml-py\">supported versions<\/a> for PyTorch. Please refer to this document for creating a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-pytorch#create-a-custom-environment\">custom environment<\/a>. As shown, you'll need to use versions &lt;= 1.6.0. Hope this helps.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.0,
        "Solution_reading_time":6.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":37.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617627646000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The mlflow run status shows \"FINISHED\" instead of \"FAILED\" when the kedro run fails, making it difficult to distinguish between successful and failed runs in the mlflow ui. The potential solution suggested is to replace certain lines of code or retrieve the current run status from mlflow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":121,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1449207605092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Manila, NCR, Philippines",
        "Answerer_reputation_count":185.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":92.0650375,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using the following lines of code to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that I will be running on a serverless manner through Vertex AI\/Pipelines.<\/p>\n<pre><code>op().\nset_cpu_limit(8).\nset_memory_limit(50G).\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-k80').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>and it works for other GPUs as well i.e. Pascal, Tesla, Volta cards.<\/p>\n<p>However, I can't do the same with the latest accelerator type which is the <code>Tesla A100<\/code> as it requires a special machine type, which is as least an <code>a2-highgpu-1g<\/code>.<\/p>\n<p>How do I make sure that this particular component will run on top of <code>a2-highgpu-1g<\/code> when I run it on Vertex?<\/p>\n<p>If i simply follow the method for older GPUs:<\/p>\n<pre><code>op().\nset_cpu_limit(12). # max for A2-highgpu-1g\nset_memory_limit(85G). # max for A2-highgpu-1g\nadd_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\nset_gpu_limit(1)\n<\/code><\/pre>\n<p>It throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*<\/p>\n<p>Same thing happened when I did not specify the cpu and memory limits, in hope that it will automatically select the right machnie type based on the accelerator constraint.<\/p>\n<pre><code>    op().\n    add_node_selector_constraint('cloud.google.com\/gke-accelerator', 'nvidia-tesla-a100').\n    set_gpu_limit(1)\n<\/code><\/pre>\n<p>Error:\n<code>&quot;NVIDIA_TESLA_A100&quot; is not supported for machine type &quot;n1-highmem-2&quot;,<\/code><\/p>",
        "Challenge_closed_time":1632103988632,
        "Challenge_comment_count":2,
        "Challenge_created_time":1631772554497,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to specify the desired machine type and accelerator\/GPU on a Kubeflow Pipeline (KFP) that will be running on a serverless manner through Vertex AI\/Pipelines. The user is facing challenges in running the pipeline on the latest accelerator type, Tesla A100, as it requires a special machine type, which is at least an a2-highgpu-1g. The user tried to follow the method for older GPUs, but it throws an error when run\/deployed since the machine type that is being spawned is the general type i.e. N1-Highmem-*.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69203143",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.8,
        "Challenge_reading_time":21.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":92.0650375,
        "Challenge_title":"Using Tesla A100 GPU with Kubeflow Pipelines on Vertex AI",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":473.0,
        "Challenge_word_count":197,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449207605092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Manila, NCR, Philippines",
        "Poster_reputation_count":185.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Currently, GCP don't support A2 Machine type for normal KF Components. A potential workaround right now is to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-job\" rel=\"nofollow noreferrer\"><strong>GCP custom job component<\/strong><\/a> that you can explicitly specify the machine type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":4.2,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":141.9971625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi Azure ML users! My Regression model, based on this learning Path:  <br \/>\nCreate a Regression Model with Azure Machine Learning designer --&gt; Deploy a predictive service.  <br \/>\nIt predicts a car's price. I've added Edit Meta Data in Designer to clear features of other columns selected. This displays other details about each car, like engine, manual or automatic, title status and general notes. For prediction AzureML only uses two columns, miles and year, to predict price. 6 columns ClearFeatured, 2 features and one label column for price, 9 total columns selected.  <\/p>\n<ol>\n<li> I enter data manually, CSV, click submit realtime inference and AzureML predicts a price, success.  <\/li>\n<li> I click deploy, and then click Test on the deployed endpoint, AzureML success again.  <\/li>\n<li> I click New Notebook and paste in python script to predict a price, sending 9 columns, like this, but keep getting a schema error.<\/li>\n<\/ol>\n<p>&quot;price&quot;: 5500,  <br \/>\n&quot;year&quot;: 2013,  <br \/>\n&quot;car&quot;: &quot;Mini Cooper&quot;,  <br \/>\n&quot;miles&quot;: 74000,  <br \/>\n&quot;model&quot;: &quot;Sport&quot;,  <br \/>\n&quot;engine&quot;: 1,  <br \/>\n&quot;manual&quot;: &quot;manual&quot;,  <br \/>\n&quot;title&quot;: &quot;rebuilt&quot;,  <br \/>\n&quot;notes&quot;: &quot;silver black lines to 5500 started at 6500&quot;,<\/p>\n<p>-- Here is the deploy error --  <br \/>\nThe request failed with status code: 400  <br \/>\nAccess-Control-Allow-Origin: *  <br \/>\nContent-Length: 1271  <br \/>\nContent-Type: application\/json  <br \/>\nDate: Thu, 15 Jul 2021 03:17:53 GMT  <br \/>\nServer: nginx\/1.14.0 (Ubuntu)  <br \/>\nX-Ms-Request-Id: 6d440216-81bc-441f-aeae-c5190c486028  <br \/>\nX-Ms-Run-Function-Failed: False  <br \/>\nConnection: close<\/p>\n<p>{'error': {'code': 400, 'message': 'Input Data Error. Input data are inconsistent with schema.\\nSchema: {\\'columnAttributes\\': [{\\'name\\': \\'price\\', \\'type\\': \\'Numeric\\', \\'isFeature\\': True, \\'elementType\\': {\\'typeName\\': \\'int64\\', \\'isNullable\\': False}}, {\\'name\\': \\'year\\', \\'type\\': \\'Numeric\\', \\'isFeature\\': True, \\'elementType\\': {\\'typeName\\': \\'int64\\', \\'isNullable\\': False}}, {\\'n\\nData: defaultdict(&lt;class \\'list\\'&gt;, {\\'price\\': [5500], \\'year\\': [2013], \\'car\\': [\\'Mini Cooper\\'], \\'miles\\': [74000], \\'model\\': [\\'Sport\\'], \\'engine\\': <a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114777-azure-ml-deploy-has-schema-error.png?platform=QnA\">1<\/a>, \\'manual\\': [\\'manual\\'], \\'title\\': [\\'rebuilt\\'], \\'notes\\': [\\'silver black lines now down to 5800 started at 6500\\']})\\nTraceback (most recent call last):\\n File &quot;\/azureml-envs\/azureml_d04391a4e9e93a56aa2beac2c36d4d02\/lib\/python3.6\/site-packages\/azureml\/designer\/serving\/dagengine\/dag.py&quot;, line 167, in execute\\n input_data = create_dfd_from_dict(raw_input, schema)\\n File &quot;\/azureml-envs\/azureml_d04391a4e9e93a56aa2beac2c36d4d02\/lib\/python3.6\/site-packages\/azureml\/designer\/serving\/dagengine\/converter.py&quot;, line 19, in create_dfd_from_dict\\n raise ValueError(f\\'Input json_data must have the same column names as the meta data. \\'\\nValueError: Input json_data must have the same column names as the meta data. Different columns are: {\\'more1\\'}\\n', 'details': ''}}<\/p>\n<p>:: problem ::  <br \/>\nWhy is the notebook failing? but the Test of the endpoint has succcess? What am I doing wrong with the schema?<\/p>\n<p>Thank you.<\/p>\n<p>-- more details below, if you interested --  <br \/>\nmy incoming data will match the schema of the original training data, 9 columns, so I did not do this step below.<\/p>\n<p>Learning Path says:  <br \/>\n&quot;The inference pipeline assumes that new data will match the schema of the original training data, so the Automobile price data (Raw) dataset from the training pipeline is included. However, this input data includes the price label that the model predicts, which is unintuitive to include in new car data for which a price prediction has not yet been made.&quot;<\/p>\n<p>Learning Path also says:  <br \/>\n&quot;Now that you've changed the schema of the incoming data to exclude the price field, you need to remove any explicit uses of this field in the remaining modules. Select the Select Columns in Dataset module and then in the settings pane, edit the columns to remove the price field.&quot;<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/114777-azure-ml-deploy-has-schema-error.png?platform=QnA\" alt=\"114777-azure-ml-deploy-has-schema-error.png\" \/><\/p>",
        "Challenge_closed_time":1626832365392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626321175607,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a schema error in AzureML regression model when using a notebook python script, but endpoint test success. The user has created a regression model to predict a car's price and has added Edit Meta Data in Designer to clear features of other columns selected. The user is able to enter data manually, CSV, click submit realtime inference and AzureML predicts a price, success. The user is also able to click deploy, and then click Test on the deployed endpoint, AzureML success again. However, when the user clicks New Notebook and pastes in python script to predict a price, sending 9 columns, they keep getting a schema error. The error message indicates that the input data is inconsistent with the schema. The user is seeking help",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/476616\/400-schema-error-in-azureml-regression-model-when",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":58.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":141.9971625,
        "Challenge_title":"400 schema error in AzureML regression model when using a notebook python script, but endpoint test success",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":544,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>it's working now. I needed to add the more1 column. this was a null column in my dataset, the last column in CSV file.     <br \/>\nin my training pipeline this was omitted from Select Columns in Dataset. But in the Creating Inference,mistakenly put it back in, all my columns in the Enter Data Manually asset. 10 columns in Notebook now matches 10 column names for inference pipeline.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/116545-azure-fix-cars-schema1.png?platform=QnA\" alt=\"116545-azure-fix-cars-schema1.png\" \/>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.7,
        "Solution_reading_time":6.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3547222222,
        "Challenge_answer_count":0,
        "Challenge_body":"#### Environment details\r\n\r\n  - OS: Mac M1 Pro\r\n  - Node.js version: v16.16.0\r\n  - npm version: 8.11.0\r\n  - `@google-cloud\/aiplatform` version: ^2.3.0\r\n\r\n#### Steps to reproduce\r\n\r\n  1. I've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js\r\n  2. The process paused and shows `4 DEADLINE_EXCEEDED: Deadline exceeded` in the line: `await predictionServiceClient.predict(request);`\r\n\r\n\r\nThanks!\r\n",
        "Challenge_closed_time":1664935217000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1664933940000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug where to_wandb is not sectioning by train\/test and overrides runs by checks. When running a suite with train\/test checks and duplicate checks in the suite, the expected behavior is to have sections for each dataset and be able to run a suite with a couple of checks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/issues\/453",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":20.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":558.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":42.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3547222222,
        "Challenge_title":"vertex AI endpoint prediction error, 4 DEADLINE_EXCEEDED: Deadline exceeded",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"> \r\n\r\nWhen I upgrade the nodejs to v16.17.1 and add a call_option\r\n`\r\n      const call_options = {\r\n        timeout: 200000 \/\/ millis\r\n      }\r\n`\r\nproblem solved.\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.59,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0788888889,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI'm currently facing a problem regarding the polyaxon tracking module. When running a job, I can see the logged metrics, parameters, artifacts in the lineage tab, although on the Dashboard tab, I can see only the chart's name and no values being tracked.\n\nHave anyone encountered this issue before? Any help will be much appreciated.\n\nI have tried a basic example of MNIST, when training it on CPU it tracks the metrics, but on GPU it doesn't.\n\nAlso, when I'm running a training job (MNIST) on the CPU it tracks the metrics and I can see the charts in the Dashboard, but when I configure the polyaxonfile to use GPU, the logs are indeed saying that the GPU is being used, but the metrics charts are empty.",
        "Challenge_closed_time":1650441842000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650441558000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the Polyaxon tracking module where the artifacts lineage is being tracked but the Dashboard and Artifacts tabs are empty. The user has tried a basic example of MNIST and found that when training on CPU, metrics are tracked but not on GPU. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1498",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0788888889,
        "Challenge_title":"Artifacts lineage is tracked but the Dashboard and Artifacts tabs are empty",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":140,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You are probably deploying Polyaxon with the default artifacts store and you are using multiple nodes.\n\nIf that's the case you will need to upgrade your deployment with one if the artifacts stores that supports multi-node deployment: https:\/\/polyaxon.com\/integrations\/#Artifacts\n\nMore info about the artifacts store default behavior: https:\/\/polyaxon.com\/docs\/setup\/connections\/#artifactsstore\n\nAlso, when I'm running a training job (MNIST) on the CPU it tracks the metrics and I can see the charts in the Dashboard, but when I configure the polyaxonfile to use GPU, the logs are indeed saying that the GPU is being used, but the metrics charts are empty.\n\nYour GPU is on different node that's why it shows the metrics and artifacts when running with CPU, note that even with CPU if it's on a different node nothing will show up, so you will need a PVC \/ blob storage accessible to all nodes.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":10.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":142.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":116.7776319444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I built a model in Rstudio, published in Azure ML Studio a web service using \"AzureML\" R package. When testing the web service on Azure ML Studio, I encountered an error:<\/p>\n\n<pre><code>Error: AzureML returns error code:\nHTTP status code : 400\nAzureML error code : LibraryExecutionError\nModule execution encountered an internal library error.\nThe following error occurred during evaluation of R script: R_tryEval: return error: Error: bad restore file magic number (file may be corrupted) -- no data loaded\n<\/code><\/pre>\n\n<p>Do you have any <strong>insights<\/strong> on how to solve such issue? Am I <strong>missing some important code in the R script<\/strong>?<\/p>\n\n<p>The model I used was a RandomForest to predicted Species on Iris dataset<\/p>\n\n<pre><code># Iris dataset\ndf = iris\nset.seed(100);\n\nindex = createDataPartition(df$Species, p = 0.7, list = FALSE)\nML.train = df[index,]; \nML.test = df[-index,];  rm(index)\n\nlibrary(randomForest)\nmodel = randomForest::randomForest(Species ~., data = ML.train)\n\nmypredict = function(newdata) {\n      require(randomForest)\n      predict(model, newdata, type = \"response\")\n}\n\n# Create workspace\nwsObj = AzureML::workspace(id = \"my Id\", auth = \"my token\")  # I omitted on purpose my Id and my token values\n\n# Publishing\nlibrary(devtools)\nlibrary(AzureML)\napi = AzureML::publishWebService(ws = wsObj,\n                                 fun = mypredict,\n                                 name = \"IrisWebService\",\n                                 inputSchema = ML.test %&gt;% select(-Species) )\n<\/code><\/pre>",
        "Challenge_closed_time":1575268924192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574853611397,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while testing a web service published via Rstudio RRS feed in Azure ML Studio. The error message indicated a LibraryExecutionError with an internal library error. The user is seeking insights on how to solve the issue and wondering if there is any important code missing in the R script. The model used was a RandomForest to predict Species on the Iris dataset.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59069043",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":18.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":115.3646652778,
        "Challenge_title":"LibraryExecutionError - testing a web service published via Rstudio RRS feed",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":183,
        "Platform":"Stack Overflow",
        "Poster_created_time":1574853098927,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dublin, Ireland",
        "Poster_reputation_count":13.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>AzureML for RStudio is now not supported as the package is removed from CRAN repository from <a href=\"https:\/\/cran.r-project.org\/web\/packages\/AzureML\/index.html\" rel=\"nofollow noreferrer\">2019-07-29<\/a>.\nAzure ML studio with this package will not work as the package(AzureML) is removed.<\/p>\n\n<p>Azure Machine Learning SDK for R can be Downloaded from CRAN at <a href=\"https:\/\/cloud.r-project.org\/web\/packages\/azuremlsdk\/index.html\" rel=\"nofollow noreferrer\">https:\/\/cloud.r-project.org\/web\/packages\/azuremlsdk\/index.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1575274010872,
        "Solution_link_count":3.0,
        "Solution_readability":14.3,
        "Solution_reading_time":7.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1477057589223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH, United States",
        "Answerer_reputation_count":547.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":20.3776283333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure webjob that is calling a ML training experiment via HttpRequests, leveraging the code generated in the ML webportal:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = \"azureStorageConnectionString\",\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/Model_2018421.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>However, the request fails with the following message:<\/p>\n\n<blockquote>\n  <p>The blob reference:\n  experiments\/experimentId\/TenantId\/Model_2018421.ilearner\n  has an invalid or missing file extension. Supported file extensions\n  for this output type are: \\\\\".csv, .tsv, .arff\\\\\"<\/p>\n<\/blockquote>\n\n<p>I'm pretty confused about this, since it's written right the documentation all over the place that if I'm expecting a trained model to use \".ilearner\" as the file extension for the model.<\/p>\n\n<p>I've seen <a href=\"https:\/\/stackoverflow.com\/questions\/47920098\/use-azure-data-factory-updating-azure-machine-learning-models\">this question<\/a> asking about the same error leveraging the DataFactory, and also <a href=\"https:\/\/datascience.stackexchange.com\/questions\/27397\/azure-machine-learning-model-retraining-problem\">this question on datascience.stackexchange<\/a>. Neither one had any clues, answers, or other follow up.<\/p>\n\n<p>Any insight on what I'm missing would be greatly appreciated!<\/p>",
        "Challenge_closed_time":1525787996892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525714637430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure ML Experiment Batch Webservice Call, where the request fails with an error message stating that the blob reference has an invalid or missing file extension. The user is confused as the file extension used for the model is \".ilearner\", which is mentioned in the documentation. The user is seeking insights to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50219664",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.0,
        "Challenge_reading_time":24.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":20.3776283333,
        "Challenge_title":"Azure ML Experiment Batch Webservice Call Fails with Invalid Output Extension",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477057589223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH, United States",
        "Poster_reputation_count":547.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>For anyone looking for your \"Don't Overthink It\" moment of the day:<\/p>\n\n<p>I needed to provide TWO output blob file references:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}.csv\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameCsv}.csv\"\n                        }\n                    },\n                    {\n                        \"output2\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameIlearner}.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>There's an old saying in American English about not making assumptions, and I assumed the second output was an optional parameter used in batch operations. Since I'm not actually looking for more than one result from each call, I thought I was safe to remove the second output parameter.<\/p>\n\n<p>TL\/DR: Keep all the parameters the webservice portal's \"Consume\" tab generates, and make sure the first one is a .csv file reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.0,
        "Solution_reading_time":17.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1138888889,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI'm trying to run a training job and make it resume automatically whenever it is preempted or it encounters an issue.\nI'm using for this the \"termination\" and \"maxRetries\" field to restart the job.\nAfter a problem happens, the job is restarted automatically starting from where the problem has happened if I look at the logs. However, nothing is being saved to the artifacts and any call to tracking.log_metric doesn't seem to have an effect. If I look at the logs, the job then continues until it reaches the end. However instead of just ending, it just keeps restarting (from the point where the problem occurred) until all the \"maxRetries\" are used and fails with the warning \"Underlying job has an issue\" at the status page.\nAny idea what could cause such a problem and if there is anything I could do to avoid it?",
        "Challenge_closed_time":1649329911000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649329501000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with auto-resume for deep learning training. The job is restarting automatically after encountering an issue, but nothing is being saved to the artifacts and tracking.log_metric is not working. The job keeps restarting until all the \"maxRetries\" are used and fails with the warning \"Underlying job has an issue\" at the status page. The user is seeking advice on how to avoid this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1474",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.6,
        "Challenge_reading_time":10.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1138888889,
        "Challenge_title":"Auto-resume for deep learning training is not working",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":154,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Polyaxon provides several strategies to restart, restrat with copy mode, and resumes jobs. The auto-resume behavior is enabled by default\n\nNote that resuming a job can only work if your code supports loading the last checkpoint.\n\nHere's a quick debugging logic to check that the resuming process works as expected:\n\nmain.py\ndef main():\n    tracking.init()\n    checkpoint_path = tracking.get_outputs_path(\"checkpoint.json\")\n    checkpoint_path_exists = os.path.exists(checkpoint_path)\n    print(\"[CHECKPOINT] path found: {}\".format(checkpoint_path_exists))\n    if checkpoint_path_exists:\n        with open(checkpoint_path, \"r\") as checkpoint_file:\n            checkpoint = json.loads(checkpoint_file.read())\n            print(\"[CHECKPOINT] last content: {}\".format(checkpoint))\n    else:\n      print(\"[CHECKPOINT] init ...\")\n      checkpoint = {\n        \"last_time\": time.time(),\n        \"last_index\": 0,\n        \"array\": [],\n      }\n    for i in range(checkpoint[\"last_index\"] + 1, 300):\n      print(\"[CHECKPOINT] step {}\".format(i))\n      tracking.log_progress((i + 1)\/300)\n      tracking.log_metric(name=\"index\", value=i, step=i)\n      checkpoint[\"array\"].append(i)\n      checkpoint[\"last_index\"] = i\n      checkpoint[\"last_time\"] = time.time()\n      if i in [10, 50]:\n        print(\"[CHECKPOINT] Saving last content ...\")\n        with open(checkpoint_path, \"w\") as checkpoint_file:\n          checkpoint_file.write(json.dumps(checkpoint))\n        raise ValueError(\"Error was raised at {}\".format(i))\n      time.sleep(1)\npolyaxonfile.yaml\nversion: 1.1\nkind: component\ntermination:\n  maxRetries: 3\nrun:\n  kind: job\n  container:\n    image: polyaxon\/polyaxon-examples:artifacts\n    workingDir: \"{{ globals.run_artifacts_path }}\/uploads\"\n    command: [\"\/bin\/bash\", -c]\n    args: [\"pip install -U polyaxon --no-cache && python3 main.py\"]\nLogged a dummy metric that resumes from last checkpoint and (apart from the warning regression that I mentioned) the job succeeds after after two failures (you can see the first chart where the x-axis is the time that there's gap time)",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":24.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":194.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1589738451347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":115.8372675,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am training a model using AMLS. I have a training pipeline in which step 1 trains a model then saves the output in temporary datastore model_folder using<\/p>\n<pre><code>os.makedirs(output_folder, exist_ok=True)\noutput_path = output_folder + &quot;\/model.pkl&quot;\njoblib.dump(value=model, filename=output_path)\n<\/code><\/pre>\n<p>Step 2 loads the model and registers it. The model folder is defined in the pipeline as<\/p>\n<pre><code>model_folder = PipelineData(&quot;model_folder&quot;, datastore=ws.get_default_datastore())\n<\/code><\/pre>\n<p>However, step 1 fails when it tries to save the model with the following ServiceError:<\/p>\n<p>Failed to upload outputs due to Exception: Microsoft.RelInfra.Common.Exceptions.OperationFailedException: Cannot upload output xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx. ---&gt; Microsoft.WindowsAzure.Storage.StorageException: This request is not authorized to perform this operation using this permission.<\/p>\n<p>How can I solve this? Earlier in my code I had no problem interacting with the default datastore using<\/p>\n<pre><code>default_ds = ws.get_default_datastore()\ndefault_ds.upload_files(...)\n<\/code><\/pre>\n<p>My <code>70_driver_log.txt<\/code> is as follows:<\/p>\n<pre><code>[2020-08-25T04:03:27.315114] Entering context manager injector.\n[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['train_word2vec.py', '--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10'])\nStarting the daemon thread to refresh tokens in background for process with pid = 113\nEntering Run History Context Manager.\nCurrent directory:  \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\nPreparing to call script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\nAfter variable expansion, calling script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\n\nScript type = None\n[nltk_data] Downloading package stopwords to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/stopwords.zip.\n[nltk_data] Downloading package wordnet to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/wordnet.zip.\nOUTPUT FOLDER: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder\nLoading SQL data...\nLoading abbreviation data...\n\/azureml-envs\/azureml_xxxxx\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py:1783: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n  self.obj[item_labels[indexer[info_axis]]] = value\nPre-processing data...\nSuccesfully pre-processed the the text data\nTraining Word2Vec model...\nSaving the model...\nStarting the daemon thread to refresh tokens in background for process with pid = 113\n\n\nThe experiment completed successfully. Finalizing run...\n[2020-08-25T04:03:52.293994] TimeoutHandler __init__\n[2020-08-25T04:03:52.294149] TimeoutHandler __enter__\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n2 items cleaning up...\nCleanup took 0.44109439849853516 seconds\n[2020-08-25T04:03:52.818991] TimeoutHandler __exit__\n2020\/08\/25 04:04:00 logger.go:293: Process Exiting with Code:  0\n<\/code><\/pre>\n<p>My arg parse arguments include<\/p>\n<pre><code>parser.add_argument('--output_folder', type=str, dest='output_folder', default=&quot;output_folder&quot;, help='output folder')\n<\/code><\/pre>",
        "Challenge_closed_time":1598745717623,
        "Challenge_comment_count":6,
        "Challenge_created_time":1598325386190,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with joblib.dump() failing to save a model to a temporary data store in AMLS. The error message indicates that the request is not authorized to perform the operation using the given permission. The user is seeking a solution to this problem.",
        "Challenge_last_edit_time":1598328703460,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63571552",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":16.7,
        "Challenge_reading_time":68.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":116.7587313889,
        "Challenge_title":"joblib.dump() fails when saving model to temporary data store in AMLS",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":571.0,
        "Challenge_word_count":413,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>Fixed this problem by adding my AMLS workspace to a 'storage blob data contributor' role in the AMLS default storage account. It seemly like usually this role is added by default, but it didn't happen in my case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":38.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1345114008840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation_count":4233.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":119.1751666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I uploaded a PlainText file in a JSON format to the new Azure Machine Learning Studio (studio.azureml.net), but I cannot connect the PlainText object with any module. I get all the time the error message \"Cannot connect PlainText to Dataset...\". <\/p>\n\n<p>At the documentation (<a href=\"http:\/\/help.azureml.net\/Content\/html\/e8219c57-e8dd-4989-9559-bbd73ba5bcea.htm\" rel=\"nofollow\">here<\/a>) is written that \"Plain text can be read and then split up into columns with the help of downstream preprocessing modules.\", but I can't find any downstream preprocessing modules.<\/p>",
        "Challenge_closed_time":1418919495800,
        "Challenge_comment_count":0,
        "Challenge_created_time":1418490465200,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to connect a PlainText file in JSON format to any module in Azure Machine Learning Studio and is receiving an error message stating \"Cannot connect PlainText to Dataset\". The user is also unable to find any downstream preprocessing modules as mentioned in the documentation.",
        "Challenge_last_edit_time":1446192454607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27461432",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":8.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":119.1751666667,
        "Challenge_title":"Cannot connect PlainText (JSON) to Dataset at Azure Machine Learning",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2157.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408374893790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>Actually Azure ML can't process JSON data. It will probably be added in a future update, but the easiest way (in my opinion) to consume that data is to convert it into CSV format. This can be done quickly with Power Query. Then you upload the CSV file as a new dataset.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":3.31,
        "Solution_score_count":6.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1403553737940,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":17835.0,
        "Answerer_view_count":2203.0,
        "Challenge_adjusted_solved_time":18.1621727778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can you introduce a real sample for azure ML and show how can it be possible to see the result of the training?<\/p>",
        "Challenge_closed_time":1483472453408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1483472453410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is requesting for a real sample on how to create a training model using AzureML and how to view the training results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41451123",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":2.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"An example to create a training model on real data in AzureML",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":34,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403553737940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":17835.0,
        "Poster_view_count":2203.0,
        "Solution_body":"<p><a href=\"http:\/\/blog.learningtree.com\/how-to-build-a-predictive-model-using-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> is a good sample to create your first model.\nI should notice that I can't load data from url, as there is a forbidden error to load from url, and I don't know why! \nAnyhow, you can import data manually by copy the data from <a href=\"http:\/\/blog.learningtree.com\/wp-content\/uploads\/2015\/01\/breast-cancer-wisconsin.data_.arff_.txt\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Also, you can find the created model which is published here: \n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1<\/a><\/p>\n\n<p>About see the result of the training model, you can right click on the tick (highlighted by a red circle in the following picture) of Evaluation Model. Then, in the opened menu, go to \"Evaluation Result -> Visualization\".\n<a href=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>After that you can see a window like the following (which shows ROC curve and some related result such as accuracy of the training model):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/B39lI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B39lI.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Besides, you can see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-customer-churn-scenario\" rel=\"nofollow noreferrer\">this example<\/a> as an another sample.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1483537837232,
        "Solution_link_count":11.0,
        "Solution_readability":14.8,
        "Solution_reading_time":24.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":170.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1448457543732,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":211.1152302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there possibility to get stream from Spark Streaming or Apache Storm into Azure Machine Learning? In <strong><em>reader<\/em><\/strong> option there is an input to read data from Hive database\n<a href=\"https:\/\/i.stack.imgur.com\/8Em26.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8Em26.png\" alt=\"hive\"><\/a><\/p>\n\n<p>but how to achive real time stream of data from Spark or Storm, for example <strong><em>Real-time fraud detection<\/em><\/strong><\/p>",
        "Challenge_closed_time":1448457555592,
        "Challenge_comment_count":0,
        "Challenge_created_time":1447697540763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking a way to connect Azure Machine Learning with Spark Streaming or Apache Storm to achieve real-time stream of data for tasks such as real-time fraud detection. The reader option in Azure Machine Learning allows input from Hive database, but the user is looking for a way to get a stream of data from Spark or Storm.",
        "Challenge_last_edit_time":1456849966663,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33741912",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":211.1152302778,
        "Challenge_title":"How connect Azure Machine Learning and Spark Streaming or Apache Storm",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":549.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327481639092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Poznan, Poland",
        "Poster_reputation_count":2923.0,
        "Poster_view_count":838.0,
        "Solution_body":"<p>To do real time Fraud detection typically you will create a Model on Azure ML, then publish that model to oWeb service, then on you Spark or Storm system you will call that Web service, in  sequence ( like payment happened on commercial sites for example), then you will get an immediate answer about the actual parameters you had sent in you web service call.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":27.1,
        "Solution_reading_time":4.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359957233372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":2.74399,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When running an ML training job in Amazon SageMaker, the training script is \"deployed\" and given an ML training instance, which takes about 10 minutes to spin up and get the data it needs. <\/p>\n\n<p>I can only get one error message from the training job, then it dies and the instance is killed along with it. <\/p>\n\n<p>After I make a change to the training script to fix it, I need to deploy and run it which takes another 10 minutes or so.<\/p>\n\n<p>How can I accomplish this faster, or keep the training instance running?<\/p>",
        "Challenge_closed_time":1548282026927,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548272148563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in quickly debugging a SageMaker training script as the training instance takes about 10 minutes to spin up and get the data it needs. Additionally, the user can only get one error message from the training job, after which the instance is killed. The user is seeking ways to accomplish this faster or keep the training instance running.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54334462",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.74399,
        "Challenge_title":"How can I quickly debug a SageMaker training script?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1902.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416193017423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gensokyo",
        "Poster_reputation_count":880.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the \"local mode\" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to \"local\". More details about local mode can be found at <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview<\/a> and the blog post: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.7,
        "Solution_reading_time":13.03,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":102.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2330.3280555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am training a resnet model on multi core tpus on kaggle. I get this error:\r\n```\r\nDumping Computation:\r\n2021-10-08 23:57:50.220206: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92108 = s32[] constant(0)\r\n2021-10-08 23:57:50.220217: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92110 = pred[] compare(s32[] %constant.92102, s32[] %constant.92108), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220227: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92109 = f32[] constant(1)\r\n2021-10-08 23:57:50.220238: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92111 = f32[] convert(s32[] %constant.92102)\r\n2021-10-08 23:57:50.220248: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92112 = f32[] divide(f32[] %constant.92109, f32[] %convert.92111)\r\n2021-10-08 23:57:50.220260: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92113 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220271: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92114 = f32[] select(pred[] %compare.92110, f32[] %divide.92112, f32[] %constant.92113)\r\n2021-10-08 23:57:50.220281: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92115 = f32[] multiply(f32[] %reduce.92107, f32[] %select.92114)\r\n2021-10-08 23:57:50.220292: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92116 = f32[] convert(f32[] %multiply.92115)\r\n2021-10-08 23:57:50.220302: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134449 = f32[1]{0} reshape(f32[] %convert.92116)\r\n2021-10-08 23:57:50.220312: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92081 = f32[1]{0} reshape(f32[] %p3148.47101)\r\n2021-10-08 23:57:50.220323: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92082 = f32[1]{0} concatenate(f32[1]{0} %reshape.92081), dimensions={0}\r\n2021-10-08 23:57:50.220333: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92083 = f32[] constant(0)\r\n2021-10-08 23:57:50.220343: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92089 = f32[] reduce(f32[1]{0} %concatenate.92082, f32[] %constant.92083), dimensions={0}, to_apply=%AddComputation.92085\r\n2021-10-08 23:57:50.220353: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92084 = s32[] constant(1)\r\n2021-10-08 23:57:50.220364: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92090 = s32[] constant(0)\r\n2021-10-08 23:57:50.220375: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92092 = pred[] compare(s32[] %constant.92084, s32[] %constant.92090), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220387: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92091 = f32[] constant(1)\r\n2021-10-08 23:57:50.220397: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92093 = f32[] convert(s32[] %constant.92084)\r\n2021-10-08 23:57:50.220408: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92094 = f32[] divide(f32[] %constant.92091, f32[] %convert.92093)\r\n2021-10-08 23:57:50.220418: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92095 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220465: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92096 = f32[] select(pred[] %compare.92092, f32[] %divide.92094, f32[] %constant.92095)\r\n2021-10-08 23:57:50.220482: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92097 = f32[] multiply(f32[] %reduce.92089, f32[] %select.92096)\r\n2021-10-08 23:57:50.220494: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92098 = f32[] convert(f32[] %multiply.92097)\r\n2021-10-08 23:57:50.220504: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134450 = f32[1]{0} reshape(f32[] %convert.92098)\r\n2021-10-08 23:57:50.220515: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92063 = f32[1]{0} reshape(f32[] %p3147.47082)\r\n2021-10-08 23:57:50.220525: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92064 = f32[1]{0} concatenate(f32[1]{0} %reshape.92063), dimensions={0}\r\n2021-10-08 23:57:50.220535: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92065 = f32[] constant(0)\r\n2021-10-08 23:57:50.220545: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92071 = f32[] reduce(f32[1]{0} %concatenate.92064, f32[] %constant.92065), dimensions={0}, to_apply=%AddComputation.92067\r\n2021-10-08 23:57:50.220556: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92066 = s32[] constant(1)\r\n2021-10-08 23:57:50.220566: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92072 = s32[] constant(0)\r\n2021-10-08 23:57:50.220576: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92074 = pred[] compare(s32[] %constant.92066, s32[] %constant.92072), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220587: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92073 = f32[] constant(1)\r\n2021-10-08 23:57:50.220598: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92075 = f32[] convert(s32[] %constant.92066)\r\n2021-10-08 23:57:50.220608: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92076 = f32[] divide(f32[] %constant.92073, f32[] %convert.92075)\r\n2021-10-08 23:57:50.220618: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92077 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220629: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92078 = f32[] select(pred[] %compare.92074, f32[] %divide.92076, f32[] %constant.92077)\r\n2021-10-08 23:57:50.220640: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92079 = f32[] multiply(f32[] %reduce.92071, f32[] %select.92078)\r\n2021-10-08 23:57:50.220650: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92080 = f32[] convert(f32[] %multiply.92079)\r\n2021-10-08 23:57:50.220660: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134451 = f32[1]{0} reshape(f32[] %convert.92080)\r\n2021-10-08 23:57:50.220670: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92045 = f32[1]{0} reshape(f32[] %p3146.47063)\r\n2021-10-08 23:57:50.220680: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92046 = f32[1]{0} concatenate(f32[1]{0} %reshape.92045), dimensions={0}\r\n2021-10-08 23:57:50.220691: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92047 = f32[] constant(0)\r\n2021-10-08 23:57:50.220701: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92053 = f32[] reduce(f32[1]{0} %concatenate.92046, f32[] %constant.92047), dimensions={0}, to_apply=%AddComputation.92049\r\n2021-10-08 23:57:50.220711: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92048 = s32[] constant(1)\r\n2021-10-08 23:57:50.220722: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92054 = s32[] constant(0)\r\n2021-10-08 23:57:50.220733: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92056 = pred[] compare(s32[] %constant.92048, s32[] %constant.92054), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220759: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92055 = f32[] constant(1)\r\n2021-10-08 23:57:50.220770: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92057 = f32[] convert(s32[] %constant.92048)\r\n2021-10-08 23:57:50.220781: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92058 = f32[] divide(f32[] %constant.92055, f32[] %convert.92057)\r\n2021-10-08 23:57:50.220792: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92059 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220803: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92060 = f32[] select(pred[] %compare.92056, f32[] %divide.92058, f32[] %constant.92059)\r\n2021-10-08 23:57:50.220813: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92061 = f32[] multiply(f32[] %reduce.92053, f32[] %select.92060)\r\n2021-10-08 23:57:50.220823: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92062 = f32[] convert(f32[] %multiply.92061)\r\n2021-10-08 23:57:50.220833: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134452 = f32[1]{0} reshape(f32[] %convert.92062)\r\n2021-10-08 23:57:50.220843: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92027 = f32[1]{0} reshape(f32[] %p3145.47044)\r\n2021-10-08 23:57:50.220854: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92028 = f32[1]{0} concatenate(f32[1]{0} %reshape.92027), dimensions={0}\r\n2021-10-08 23:57:50.220865: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92029 = f32[] constant(0)\r\n2021-10-08 23:57:50.220876: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92035 = f32[] reduce(f32[1]{0} %concatenate.92028, f32[] %constant.92029), dimensions={0}, to_apply=%AddComputation.92031\r\n2021-10-08 23:57:50.220888: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92030 = s32[] constant(1)\r\n2021-10-08 23:57:50.220899: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92036 = s32[] constant(0)\r\n2021-10-08 23:57:50.220910: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92038 = pred[] compare(s32[] %constant.92030, s32[] %constant.92036), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220921: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92037 = f32[] constant(1)\r\n2021-10-08 23:57:50.220932: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92039 = f32[] convert(s32[] %constant.92030)\r\n2021-10-08 23:57:50.220942: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92040 = f32[] divide(f32[] %constant.92037, f32[] %convert.92039)\r\n2021-10-08 23:57:50.220953: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92041 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220964: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92042 = f32[] select(pred[] %compare.92038, f32[] %divide.92040, f32[] %constant.92041)\r\n2021-10-08 23:57:50.220975: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92043 = f32[] multiply(f32[] %reduce.92035, f32[] %select.92042)\r\n2021-10-08 23:57:50.220986: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92044 = f32[] convert(f32[] %multiply.92043)\r\n```\r\nThis text goes on and on for several pages.\r\n\r\nThe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nNote that this only happens when using a logger (wandb or comet.ml) and everything works fine when I do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> I have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### To Reproduce\r\n\r\nSee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-comet-ml) with comet.ml.\r\n\r\n### Expected behavior\r\n\r\nTraining should run normally with no issues and logging should work.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1+cpu\r\n\t- pytorch-lightning: 1.4.4\r\n\t- tqdm:              4.62.1\r\n\t- pytorch-xla  1.7\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\r\n### Additional context\r\nNone\r\n\n\ncc @kaushikb11 @rohitgr7 @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire",
        "Challenge_closed_time":1642181493000,
        "Challenge_comment_count":11,
        "Challenge_created_time":1633792312000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"Upgrading from pytorch-lightning 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments, one for each GPU, when running a ddp multi-gpu experiment on a SLURM cluster. Only one of them logs any metrics, the others just sit. The expected behavior is a single comet experiment for a single call to trainer.fit(). The user is unable to provide a reproducible example due to the inability to do multi-gpu ddp in colab and the need for a comet authentication.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9879",
        "Challenge_link_count":3,
        "Challenge_participation_count":11,
        "Challenge_readability":16.4,
        "Challenge_reading_time":156.3,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":236,
        "Challenge_solved_time":2330.3280555556,
        "Challenge_title":"\"dumps computation\" at the start of validation loop when using wandb\/comet.ml logger during multi-core tpu training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":786,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks @rusty-electron for opening the issue.\r\n\r\nIs there any more information before the line \"Dumping Computation:\"?  No error output, just the logs from wandb logger and the progressbars created by `tqdm`. Dear @rusty-electron,\r\n\r\nWe are working with the Wandb Team on a large fix. Hopefully it will work for this use-case too.\r\n\r\nWe will keep you updated.\r\n\r\nBest,\r\nT.C @tchaton Thanks for the info. I shall be looking out for the fix. @tchaton Is there an issue to track the Wandb updates? @borisdayma Any idea ?\r\n It's actually a few different PR's ongoing.\r\nI think we should have something next week that will handle these scenarios. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n @borisdayma Did it end up being an issue on the wandb side? I didn't follow the development lately. If it's still work in progress, could you point us to a PR or issue? Thx in advance <3  We're actually still in the process of updating the way multiprocess is supported.\r\nThere's been good progress, just a few edge cases to handle. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":16.94,
        "Solution_score_count":null,
        "Solution_sentence_count":23.0,
        "Solution_word_count":238.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.1325,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nI am following the instructions on https:\/\/github.com\/awslabs\/gluon-ts\/blob\/acfd7e14c4ef6eaa62fea6d6233a9e336f6366e4\/examples\/GluonTS_SageMaker_SDK_Tutorial.ipynb but at first step when I ran `!pip install --upgrade mxnet==1.6  git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]` I got the following error,\r\n\r\n## Error message or code output\r\n```Obtaining gluonts[dev] from git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]\r\n  Updating .\/src\/gluonts clone\r\n  Running command git fetch -q --tags\r\n  Running command git reset --hard -q fc203f51f01036e854ce6a0da1a43b562074e187\r\n  Installing build dependencies ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel\r\n       cwd: None\r\n  Complete output (14 lines):\r\n  Traceback (most recent call last):\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/__main__.py\", line 16, in <module>\r\n      from pip._internal.cli.main import main as _main  # isort:skip # noqa\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/_internal\/cli\/main.py\", line 5, in <module>\r\n      import locale\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/locale.py\", line 16, in <module>\r\n      import re\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py\", line 142, in <module>\r\n      class RegexFlag(enum.IntFlag):\r\n  AttributeError: module 'enum' has no attribute 'IntFlag'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel Check the logs for full command output.\r\n\r\n```\r\n\r\n\r\n## Environment\r\nNote: Previously, I installed Gluon-TS (0.5.2) using `! pip install --upgrade mxnet==1.6 gluonts` and if I do `! pip list` I can see the package is installed but when I ran `!pip uninstall glounts` it says `WARNING: Skipping glounts as it is not installed.`\r\n\r\n- Operating system: Sagemaker notebook instance with conda_mxnet_p36 kernel.\r\n- Python version: 3.6\r\n- GluonTS version: 0.5.2 is already installed.\r\n- MXNet version:1.6",
        "Challenge_closed_time":1600271697000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1600235220000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered challenges when attempting to run the example script 'benchmark_m4.py' on a gpu-instance \"ml.p2.xlarge\" on aws, resulting in a keyerror.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/1039",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":13.9,
        "Challenge_reading_time":38.77,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":651.0,
        "Challenge_repo_issue_count":2147.0,
        "Challenge_repo_star_count":3215.0,
        "Challenge_repo_watch_count":70.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":10.1325,
        "Challenge_title":"Issue with installing GlounTS on Sagemaker notebook instance from Github",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":254,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"According to [this](https:\/\/github.com\/iterative\/dvc\/issues\/1995), it could be that `enum34` is installed.\r\n\r\nCan you check whether this is also the case here? Thanks @jaheba ! That was the issue and by running `!pip uninstall -y enum34` it is resolved.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":4.9,
        "Solution_reading_time":3.14,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":145.8923630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Newbie data scientist here, I am just starting my way in Azure, is there any I should start NLP? Any trained model or code sample? Thank you for any idea<\/p>",
        "Challenge_closed_time":1667776545220,
        "Challenge_comment_count":1,
        "Challenge_created_time":1667251332713,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is a newbie data scientist who is looking for guidance on how to start with NLP in Azure. They are seeking suggestions for trained models or code samples to help them get started.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1069986\/trained-nlp-model-snippet",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":145.8923630556,
        "Challenge_title":"Trained NLP model snippet",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=12b79ebe-b30f-4203-a714-62377c3d557b\">@jackson schmidt  <\/a>     <\/p>\n<p>Sorry I have not heard from you. I have done some researches around NLP in Azure. This can be done by two ways -    <\/p>\n<ol>\n<li> Azure Machine Learning Python SDK\/ ML CLI extension    <br \/>\nWe don't have any trained model you can use in Azure ML but you do have the SDK supporting you to train your model    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli<\/a>    <\/li>\n<li> NLP Server     <br \/>\nApache Spark is a parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Azure Synapse Analytics, Azure HDInsight, and Azure Databricks offer access to Spark and take advantage of its processing power.    <\/li>\n<\/ol>\n<p>For customized NLP workloads, Spark NLP serves as an efficient framework for processing a large amount of text. This open-source NLP library provides Python, Java, and Scala libraries that offer the full functionality of traditional NLP libraries such as spaCy, NLTK, Stanford CoreNLP, and Open NLP. Spark NLP also offers functionality such as spell checking, sentiment analysis, and document classification. Spark NLP improves on previous efforts by providing state-of-the-art accuracy, speed, and scalability.    <\/p>\n<p><img src=\"https:\/\/learn.microsoft.com\/en-us\/azure\/architecture\/data-guide\/images\/natural-language-processing-functionality.png\" alt=\"natural-language-processing-functionality.png\" \/>    <\/p>\n<p>The NLP Server is available in Azure Marketplace. To explore large-scale custom NLP in Azure, see NLP Server - <a href=\"https:\/\/azuremarketplace.microsoft.com\/en-US\/marketplace\/apps\/johnsnowlabsinc1646051154808.nlp_server?ocid=gtmrewards_whatsnewblog_nlp_server_040622\">https:\/\/azuremarketplace.microsoft.com\/en-US\/marketplace\/apps\/johnsnowlabsinc1646051154808.nlp_server?ocid=gtmrewards_whatsnewblog_nlp_server_040622<\/a>    <\/p>\n<ol start=\"3\">\n<li> Azure Language Service    <br \/>\nThough we don't have trained model in Azure ML, but we do have REST APIs you can use for Text Analytics, Sentiment Analytics and so on functions for NLP, I would suggest you to check on the document, it may help you achieve your bussiness goals.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/<\/a>    <\/li>\n<\/ol>\n<p>I hope those information helps. Please let me know if you have any questions regarding to any of above.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.6,
        "Solution_reading_time":36.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":311.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1365101584443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":7203.0,
        "Answerer_view_count":445.0,
        "Challenge_adjusted_solved_time":0.5689572222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a stepfunction, the definition for this statemachine below (<code>step-function.json<\/code>) is used in terraform (using the syntax in this page:<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html<\/a>)<\/p>\n<p>The first time if I execute this statemachine, it will create a SageMaker batch transform job named <code>example-jobname<\/code>, but I need to exeucute this statemachine everyday, then it will give me error <code>&quot;error&quot;: &quot;SageMaker.ResourceInUseException&quot;, &quot;cause&quot;: &quot;Job name must be unique within an AWS account and region, and a job with this name already exists <\/code>.<\/p>\n<p>The cause is because the job name is hard-coded as <code>example-jobname<\/code> so if the state machine gets executed after the first time, since the job name needs to be unique, the task will fail, just wondering how I can add a string (something like ExecutionId at the end of the job name). Here's what I have tried:<\/p>\n<ol>\n<li><p>I added <code>&quot;executionId.$&quot;: &quot;States.Format('somestring {}', $$.Execution.Id)&quot;<\/code> in the <code>Parameters<\/code> section in the json file, but when I execute the task I got error <code> &quot;error&quot;: &quot;States.Runtime&quot;, &quot;cause&quot;: &quot;An error occurred while executing the state 'SageMaker CreateTransformJob' (entered at the event id #2). The Parameters '{\\&quot;BatchStrategy\\&quot;:\\&quot;SingleRecord\\&quot;,..............\\&quot;executionId\\&quot;:\\&quot;somestring arn:aws:states:us-east-1:xxxxx:execution:xxxxx-state-machine:xxxxxxxx72950\\&quot;}' could not be used to start the Task: [The field \\&quot;executionId\\&quot; is not supported by Step Functions]&quot;}<\/code><\/p>\n<\/li>\n<li><p>I modified the jobname in the json file to  <code>&quot;TransformJobName&quot;: &quot;example-jobname-States.Format('somestring {}', $$.Execution.Id)&quot;,<\/code>, when I execute the statemachine, it gave me error: <code>&quot;error&quot;: &quot;SageMaker.AmazonSageMakerException&quot;, &quot;cause&quot;: &quot;2 validation errors detected: Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}; Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must have length less than or equal to 63<\/code><\/p>\n<\/li>\n<\/ol>\n<p>I really run out of ideas, can someone help please? Many thanks.<\/p>",
        "Challenge_closed_time":1610637215396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610635167150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with creating a unique SageMaker batch transform job name when executing a step function daily. The job name is hard-coded as \"example-jobname\" and causes an error when executed after the first time. The user has tried adding a string to the job name using the execution ID, but it resulted in an error. Modifying the job name in the JSON file also resulted in an error due to a regular expression pattern constraint. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1611071837132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65721061",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":16.2,
        "Challenge_reading_time":36.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.5689572222,
        "Challenge_title":"How to parse stepfunction executionId to SageMaker batch transform job name?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1209.0,
        "Challenge_word_count":288,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>So as per the <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-train-model.html#sample-train-model-code-examples\" rel=\"nofollow noreferrer\">documentation<\/a>, we should be passing the parameters in the following format<\/p>\n<pre><code>        &quot;Parameters&quot;: {\n            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  \n            ....\n        },\n<\/code><\/pre>\n<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:<\/p>\n<p>either<\/p>\n<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;\n<\/code><\/pre>\n<p>full State machine definition:<\/p>\n<pre><code>    {\n        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,\n        &quot;StartAt&quot;: &quot;Generate Random String&quot;,\n        &quot;States&quot;: {\n            &quot;Generate Random String&quot;: {\n                &quot;Type&quot;: &quot;Task&quot;,\n                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,\n                &quot;ResultPath&quot;: &quot;$.executionid&quot;,\n                &quot;Parameters&quot;: {\n                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;\n                },\n                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;\n            },\n        &quot;SageMaker CreateTransformJob&quot;: {\n            &quot;Type&quot;: &quot;Task&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,\n            &quot;Parameters&quot;: {\n            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n            &quot;DataProcessing&quot;: {\n                &quot;InputFilter&quot;: &quot;$&quot;,\n                &quot;JoinSource&quot;: &quot;Input&quot;,\n                &quot;OutputFilter&quot;: &quot;xxx&quot;\n            },\n            &quot;Environment&quot;: {\n                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;\n            },\n            &quot;MaxConcurrentTransforms&quot;: 100,\n            &quot;MaxPayloadInMB&quot;: 1,\n            &quot;ModelName&quot;: &quot;${model_name}&quot;,\n            &quot;TransformInput&quot;: {\n                &quot;DataSource&quot;: {\n                    &quot;S3DataSource&quot;: {\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;\n                    }\n                },\n                &quot;ContentType&quot;: &quot;application\/jsonlines&quot;,\n                &quot;CompressionType&quot;: &quot;Gzip&quot;,\n                &quot;SplitType&quot;: &quot;Line&quot;\n            },\n            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,\n            &quot;TransformOutput&quot;: {\n                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,\n                &quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n                &quot;AssembleWith&quot;: &quot;Line&quot;\n            },    \n            &quot;TransformResources&quot;: {\n                &quot;InstanceType&quot;: &quot;xxx&quot;,\n                &quot;InstanceCount&quot;: 1\n            }\n        },\n            &quot;End&quot;: true\n        }\n        }\n    }\n<\/code><\/pre>\n<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:<\/p>\n<pre><code> def lambda_handler(event, context):\n    return(event.get('executionId').split(':')[-1])\n<\/code><\/pre>\n<p>Or if you dont wanna pass the execution id , it can simply return the random string like<\/p>\n<pre><code> import string\n def lambda_handler(event, context):\n    return(string.ascii_uppercase + string.digits)\n<\/code><\/pre>\n<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.<\/p>",
        "Solution_comment_count":23.0,
        "Solution_last_edit_time":1610641519768,
        "Solution_link_count":1.0,
        "Solution_readability":24.6,
        "Solution_reading_time":43.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":220.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1635310523496,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":1.9224463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have type error when I run for training on sagemaker by using xgboost conatiner.\nPlease advise me to fix the issue.<\/p>\n<pre><code>container = 'southeast-2','783357654285.dkr.ecr.ap-southeast-2.amazonaws.com\/sagemaker- xgboost:latest'`\n\ntrain_input = TrainingInput(s3_data='s3:\/\/{}\/train'.format(bucket, prefix), content_type='csv')\nvalidation_input = TrainingInput(s3_data='s3:\/\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\nsess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(\ncontainer,\nrole, \ninstance_count=1,\ninstance_type='ml.t2.medium',\noutput_path='s3:\/\/{}\/output'.format(bucket, prefix),\nsagemaker_session=sess\n)\n\nxgb.set_hyperparameters(\nmax_depth=5,\neta=0.1,\ngamma=4,\nmin_child_weight=6,\nsubsample=0.8,\nsilent=0,\nobjective=&quot;binary:logistic&quot;,\nnum_round=25,\n)\n\nxgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})\n<\/code><\/pre>\n<hr \/>\n<p>TypeError                                 Traceback (most recent call last)\n in \n21 )\n22\n---&gt; 23 xgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n685                 * <code>TrialComponentDisplayName<\/code> is used for display in Studio.\n686         &quot;&quot;&quot;\n--&gt; 687         self._prepare_for_training(job_name=job_name)\n688\n689         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _prepare_for_training(self, job_name)\n446                 constructor if applicable.\n447         &quot;&quot;&quot;\n--&gt; 448         self._current_job_name = self._get_or_create_name(job_name)\n449\n450         # if output_path was specified we use it otherwise initialize here.<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _get_or_create_name(self, name)\n435             return name\n436\n--&gt; 437         self._ensure_base_job_name()\n438         return name_from_base(self.base_job_name)\n439<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _ensure_base_job_name(self)\n420         # honor supplied base_job_name or generate it\n421         if self.base_job_name is None:\n--&gt; 422             self.base_job_name = base_name_from_image(self.training_image_uri())\n423\n424     def _get_or_create_name(self, name=None):<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/utils.py in base_name_from_image(image)\n95         str: Algorithm name, as extracted from the image name.\n96     &quot;&quot;&quot;\n---&gt; 97     m = re.match(&quot;^(.+\/)?([^:\/]+)(:[^:]+)?$&quot;, image)\n98     algo_name = m.group(2) if m else image\n99     return algo_name<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py in match(pattern, string, flags)\n170     &quot;&quot;&quot;Try to apply the pattern at the start of the string, returning\n171     a match object, or None if no match was found.&quot;&quot;&quot;\n--&gt; 172     return _compile(pattern, flags).match(string)\n173\n174 def fullmatch(pattern, string, flags=0):<\/p>\n<p>TypeError: expected string or bytes-like object<\/p>",
        "Challenge_closed_time":1638769174620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638762253813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a type error while training on sagemaker using xgboost container. The error occurs while running the fit function and is related to the expected string or bytes-like object. The user is seeking advice to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70240640",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":41.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":1.9224463889,
        "Challenge_title":"xgboost sagemaker train failure",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":245,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635310523496,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<pre><code>import sagemaker\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker.session import TrainingInput\nfrom sagemaker import image_uris\nfrom sagemaker.session import Session\n\n# initialize hyperparameters\nhyperparameters = {\n    &quot;max_depth&quot;:&quot;5&quot;,\n    &quot;eta&quot;:&quot;0.1&quot;,\n    &quot;gamma&quot;:&quot;4&quot;,\n    &quot;min_child_weight&quot;:&quot;6&quot;,\n    &quot;subsample&quot;:&quot;0.7&quot;,\n    &quot;objective&quot;:&quot;binary:logistic&quot;,\n    &quot;num_round&quot;:&quot;25&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\noutput_path = 's3:\/\/{}\/{}\/output'.format(bucket, 'rain-xgb-built-in-algo')\n\n\n# this line automatically looks for the XGBoost image URI and builds an \nXGBoost container.\n# specify the repo_version depending on your preference.\nxgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'ap-southeast- \n2', &quot;1.3-1&quot;)\n\n\n# construct a SageMaker estimator that calls the xgboost-container\nestimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n                                      hyperparameters=hyperparameters,\n                                      role=sagemaker.get_execution_role(),\n                                      instance_count=1, \n                                      instance_type='ml.m5.large', \n                                      volume_size=5, # 5 GB \n                                      output_path=output_path)\n\n\n\n # define the data type and paths to the training and validation datasets\n\n train_input = TrainingInput(&quot;s3:\/\/{}\/{}\/&quot;.format(bucket,'train'), \n content_type='csv')\n validation_input = TrainingInput(&quot;s3:\/\/{}\/{}&quot;.format(bucket,'validation'), \n content_type='csv')\n\n\n # execute the XGBoost training job\n estimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I have rewritten as above and could run training.\nthank you !<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":21.5,
        "Solution_reading_time":23.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":38.1544444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Seems that the Neptune_catalyst.ipynb is failing. \r\nPerhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/neptune-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Challenge_closed_time":1625030635000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1624893279000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while running a ray tune job using the Sigopt suggester on a remote cluster. The Sigopt suggester object was found to be unserializable, but the stack trace did not indicate this. The severity of the issue is medium, as it is a significant difficulty but can be worked around.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/neptune-ai\/examples\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.0,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":160.0,
        "Challenge_repo_star_count":28.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":38.1544444444,
        "Challenge_title":"Neptune_catalyst.ipynb fails",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":22,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"on it. #43 fixing here fixed in #43 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":0.5,
        "Solution_reading_time":0.41,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1106.8952777778,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Inconsistency in MLFlowLogger.log_metrics within steps\r\n\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html) for MLFlowLogger states that it has a method log_metrics which signature is as follows:\r\n\r\n`log_metrics(metrics, step=None)`\r\n\r\nwhere **metrics** (Dict[str, float]) \u2013 Dictionary with metric names as keys and measured quantities as values and \r\n**step** (Optional[int]) \u2013 Step number at which the metrics should be recorded.\r\n\r\nWhen within a training\/validation\/test _step method of a LightningModule:\r\n- Setting `self.logger.experiment.log_metrics({\"train_loss\": loss})` results in the fit method raising `AttributeError: 'MlflowClient' object has no attribute 'log_metrics'`\r\n- Setting `self.logger.experiment.log_metric({\"train_loss\": loss})` results in the fit method raising `TypeError: log_metric() missing 2 required positional arguments: 'key' and 'value'`\r\n- Setting `self.logger.experiment.log_metric(\"train_loss\", loss)` results in the fit method raising `TypeError: log_metric() missing 1 required positional argument: 'value'`\r\n\r\nFound the behavior from the last two options by luck because of a typo. The logger would expect `log_metric` despite the documentation saying the method is called `log_metrics`. Even if I use `log_metric` the method expects parameters other than the Dict[str, float] stated in the documentation.\r\n\r\n### To Reproduce\r\n\r\nThis is the minimum code I found that reproduces the bug:\r\n\r\nhttps:\/\/github.com\/mmazuecos\/pytorch_lightning_mlflow_bug\/blob\/main\/pytorch_lightning_mlflow_bug.py\r\n\r\n### Expected behavior\r\n\r\nThe code should work with the `log_metrics` signature from the documentation.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.21.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1.post2\r\n\t- pytorch-lightning: 1.4.5\r\n\t- tqdm:              4.62.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.11\r\n\t- version:           #148-Ubuntu SMP Sat May 8 02:33:43 UTC 2021\r\n",
        "Challenge_closed_time":1635553585000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1631568762000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The MLFlowLogger creates a new run when resuming from an hpc checkpoint, instead of reusing the run ID, which causes runs to be grouped incorrectly on the MLFlow UI. The user suggests that this can be patched using the logger, and is willing to attempt a PR if the owners agree that it's a bug. The issue can be reproduced on a slurm cluster.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9497",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":26.53,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1106.8952777778,
        "Challenge_title":"Inconsistency in MLFlowLogger.log_metrics within steps",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":226,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"`log_metrics` is part of the implementation of `LightningLoggerBase` yet using the experiment property returns the MlFlowClient which can be used to access methods specific to mlflow. So simply removing the experiment property from your calls should solve your problem.\r\n\r\nThe log_metric option of the mlflow client requires different args, see [here](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/mlflow.py#L226) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.5,
        "Solution_reading_time":8.64,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7154625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have subscribed to Standard_NC6 compute instance. has 56 GB RAM but only 10GB is allocated for the GPU. my model and data is huge which need at least 40GB Ram for gpu. how can I allocate more memory for the GPU ? \nI use Azure machine learning environment + notebooks \nalso I use pytorch for building my model <\/p>",
        "Challenge_closed_time":1681158737128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681156161463,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has subscribed to a Standard_NC6 compute instance with 56 GB RAM, but only 10GB is allocated for the GPU. Their model and data require at least 40GB RAM for the GPU, and they are seeking a solution to allocate more memory for the GPU. They are using Azure machine learning environment and notebooks, as well as PyTorch for building their model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1215210\/limited-gpu-ram",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":3.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.7154625,
        "Challenge_title":"limited gpu ram",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello Turki,<\/p>\n<p>The Standard_NC6 only has 12 GiB of RAM (GPU memory) as seen in:<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/bb1d7ed8-9421-421e-a25c-dfb2b026dd24?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/virtual-machines\/nc-series\">https:\/\/learn.microsoft.com\/en-us\/azure\/virtual-machines\/nc-series<\/a><\/p>\n<p>If your model requires 40 GiB of RAM you will have to upgrade to Standard_NC24 for at least 48 GiB of GPU memory (RAM).<\/p>\n<hr \/>\n<p>If this is helpful please accept answer.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":7.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":102.2437183333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there any plan? Any date we can expect?<\/p>",
        "Challenge_closed_time":1662345271896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661977194510,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about whether Azure supports distributed GPU and is seeking information on any plans or expected dates for its implementation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/989398\/is-azure-supporting-distributed-gpu",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":2.6,
        "Challenge_reading_time":1.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":102.2437183333,
        "Challenge_title":"Is Azure supporting distributed GPU?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":13,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=7f2ff54e-2fc4-4d74-b946-fc6ec46d4863\">@nam  <\/a>     <\/p>\n<p>I hope yo are doing well. We have multiple options for Distributed GPU for Azure Machine Learnig for SDK v1 as below -     <br \/>\n<strong>Message Passing Interface (MPI)<\/strong>    <br \/>\nHorovod    <br \/>\nDeepSpeed    <br \/>\nEnvironment variables from Open MPI    <br \/>\n<strong>PyTorch<\/strong>    <br \/>\nProcess group initialization    <br \/>\nLaunch options    <br \/>\nDistributedDataParallel (per-process-launch)    <br \/>\nUsing torch.distributed.launch (per-node-launch)    <br \/>\nPyTorch Lightning    <br \/>\nHugging Face Transformers    <br \/>\n<strong>TensorFlow<\/strong>    <br \/>\nEnvironment variables for TensorFlow (TF_CONFIG)    <br \/>\n<strong>Accelerate GPU training with InfiniBand<\/strong>    <\/p>\n<p>For V2 there should be big change. Please feel free to let us know any problems. Thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":11.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1497960178323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":12088.0,
        "Answerer_view_count":3630.0,
        "Challenge_adjusted_solved_time":67.3963755556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running pyspark from an Azure Machine Learning notebook. I am trying to move a file using the dbutil module.<\/p>\n\n<pre><code>from pyspark.sql import SparkSession\n    spark = SparkSession.builder.getOrCreate()\n    def get_dbutils(spark):\n        try:\n            from pyspark.dbutils import DBUtils\n            dbutils = DBUtils(spark)\n        except ImportError:\n            import IPython\n            dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n        return dbutils\n\n    dbutils = get_dbutils(spark)\n    dbutils.fs.cp(\"file:source\", \"dbfs:destination\")\n<\/code><\/pre>\n\n<p>I got this error: \nModuleNotFoundError: No module named 'pyspark.dbutils'\nIs there a workaround for this? <\/p>\n\n<p>Here is the error in another Azure Machine Learning notebook:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-1-183f003402ff&gt; in get_dbutils(spark)\n      4         try:\n----&gt; 5             from pyspark.dbutils import DBUtils\n      6             dbutils = DBUtils(spark)\n\nModuleNotFoundError: No module named 'pyspark.dbutils'\n\nDuring handling of the above exception, another exception occurred:\n\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-1-183f003402ff&gt; in &lt;module&gt;\n     10         return dbutils\n     11 \n---&gt; 12 dbutils = get_dbutils(spark)\n\n&lt;ipython-input-1-183f003402ff&gt; in get_dbutils(spark)\n      7         except ImportError:\n      8             import IPython\n----&gt; 9             dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n     10         return dbutils\n     11 \n\nKeyError: 'dbutils'\n<\/code><\/pre>",
        "Challenge_closed_time":1588593659052,
        "Challenge_comment_count":5,
        "Challenge_created_time":1588351032100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleNotFoundError while trying to move a file using the dbutil module in pyspark from an Azure Machine Learning notebook. The error message indicates that there is no module named 'pyspark.dbutils'. The user is seeking a workaround for this issue.",
        "Challenge_last_edit_time":1591892764407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61546680",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":12.2,
        "Challenge_reading_time":19.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":67.3963755556,
        "Challenge_title":"ModuleNotFoundError: No module named 'pyspark.dbutils'",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":7629.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330373362200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kennett Square, PA",
        "Poster_reputation_count":445.0,
        "Poster_view_count":104.0,
        "Solution_body":"<p>This is a known issue with Databricks Utilities - DButils.<\/p>\n\n<p>Most of DButils aren't supported for Databricks Connect. The only parts that do work are <strong>fs<\/strong> and <strong>secrets<\/strong>. <\/p>\n\n<p><strong>Reference:<\/strong> <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/dev-tools\/databricks-connect#limitations\" rel=\"nofollow noreferrer\">Databricks Connect - Limitations<\/a> and <a href=\"https:\/\/datathirst.net\/blog\/2019\/3\/7\/databricks-connect-limitations\" rel=\"nofollow noreferrer\">Known issues<\/a>.<\/p>\n\n<p><strong>Note:<\/strong> Currently fs and secrets work (locally). Widgets (!!!), libraries etc do not work. This shouldn\u2019t be a major issue. If you execute on Databricks using the Python Task dbutils will fail with the error:<\/p>\n\n<pre><code>ImportError: No module named 'pyspark.dbutils'\n<\/code><\/pre>\n\n<p>I'm able to execute the query successfully by running as a notebook.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RFVm8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RFVm8.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.0,
        "Solution_reading_time":14.37,
        "Solution_score_count":3.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1396289884603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"The Netherlands",
        "Answerer_reputation_count":480.0,
        "Answerer_view_count":96.0,
        "Challenge_adjusted_solved_time":1.2545936111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Running AWS SageMaker with a custom model, the TrainingJob fails with an <strong>Algorithm Error<\/strong> when using Keras plus a Tensorflow backend in multi-gpu configuration:<\/p>\n<pre><code>from keras.utils import multi_gpu_model\n\nparallel_model = multi_gpu_model(model, gpus=K)\nparallel_model.compile(loss='categorical_crossentropy',\noptimizer='rmsprop')\nparallel_model.fit(x, y, epochs=20, batch_size=256)\n<\/code><\/pre>\n<p>This simple parallel model loading will fail. There is no further error or exception from CloudWatch logging. This configuration works properly on local machine with 2x NVIDIA GTX 1080, same Keras Tensorflow backend.<\/p>\n<p>According to SageMaker documentation and <a href=\"https:\/\/github.com\/awslabs\/keras-apache-mxnet\/wiki\/Multi-GPU-Model-Training-with-Keras-MXNet\" rel=\"nofollow noreferrer\">tutorials<\/a> the <code>multi_gpu_model<\/code> utility will work ok when Keras backend is MXNet, but I did not find any mention when the backend is Tensorflow with the same multi gpu configuration.<\/p>\n<p><strong>[UPDATE]<\/strong><\/p>\n<p>I have updated the code with the suggested answer below, and I'm adding some logging before the TrainingJob hangs<\/p>\n<p>This logging repeats twice<\/p>\n<pre><code>2018-11-27 10:02:49.878414: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\n2018-11-27 10:02:49.878462: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-27 10:02:49.878471: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:988] 0 1 2 3\n2018-11-27 10:02:49.878477: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1001] 0: N Y Y Y\n2018-11-27 10:02:49.878481: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1001] 1: Y N Y Y\n2018-11-27 10:02:49.878486: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1001] 2: Y Y N Y\n2018-11-27 10:02:49.878492: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1001] 3: Y Y Y N\n2018-11-27 10:02:49.879340: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1115] Created TensorFlow device (\/device:GPU:0 with 14874 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n2018-11-27 10:02:49.879486: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1115] Created TensorFlow device (\/device:GPU:1 with 14874 MB memory) -&gt; physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n2018-11-27 10:02:49.879694: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1115] Created TensorFlow device (\/device:GPU:2 with 14874 MB memory) -&gt; physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\n2018-11-27 10:02:49.879872: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1115] Created TensorFlow device (\/device:GPU:3 with 14874 MB memory) -&gt; physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\n<\/code><\/pre>\n<p>Before there is some logging info about each GPU, that repeats 4 times<\/p>\n<pre><code>2018-11-27 10:02:46.447639: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1432] Found device 3 with properties:\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\npciBusID: 0000:00:1e.0\ntotalMemory: 15.78GiB freeMemory: 15.37GiB\n<\/code><\/pre>\n<p>According to the logging all the 4 GPUs are visible and loaded in the Tensorflow Keras backend. After that no application logging follows, the TrainingJob status is <strong>inProgress<\/strong> for a while, after that it becomes <strong>Failed<\/strong> with the same <strong>Algorithm Error<\/strong>.<\/p>\n<p>Looking at CloudWatch logging I can see some metrics at work. Specifically <code>GPU Memory Utilization<\/code>, <code>CPU Utilization<\/code> are ok, while <code>GPU utilization<\/code> is 0%.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/260hL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/260hL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>[UPDATE]<\/strong><\/p>\n<p>Due to a <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/8123#issuecomment-354857044\" rel=\"nofollow noreferrer\">known<\/a> bug on Keras that is about saving a multi gpu model, I'm using this override of the <strong>multi_gpu_model<\/strong> utility in <strong>keras.utils<\/strong><\/p>\n<pre><code>from keras.layers import Lambda, concatenate\nfrom keras import Model\nimport tensorflow as tf\n    \ndef multi_gpu_model(model, gpus):\n    #source: https:\/\/github.com\/keras-team\/keras\/issues\/8123#issuecomment-354857044\n  if isinstance(gpus, (list, tuple)):\n    num_gpus = len(gpus)\n    target_gpu_ids = gpus\n  else:\n    num_gpus = gpus\n    target_gpu_ids = range(num_gpus)\n\n  def get_slice(data, i, parts):\n    shape = tf.shape(data)\n    batch_size = shape[:1]\n    input_shape = shape[1:]\n    step = batch_size \/\/ parts\n    if i == num_gpus - 1:\n      size = batch_size - step * i\n    else:\n      size = step\n    size = tf.concat([size, input_shape], axis=0)\n    stride = tf.concat([step, input_shape * 0], axis=0)\n    start = stride * i\n    return tf.slice(data, start, size)\n\n  all_outputs = []\n  for i in range(len(model.outputs)):\n    all_outputs.append([])\n\n  # Place a copy of the model on each GPU,\n  # each getting a slice of the inputs.\n  for i, gpu_id in enumerate(target_gpu_ids):\n    with tf.device('\/gpu:%d' % gpu_id):\n      with tf.name_scope('replica_%d' % gpu_id):\n        inputs = []\n        # Retrieve a slice of the input.\n        for x in model.inputs:\n          input_shape = tuple(x.get_shape().as_list())[1:]\n          slice_i = Lambda(get_slice,\n                           output_shape=input_shape,\n                           arguments={'i': i,\n                                      'parts': num_gpus})(x)\n          inputs.append(slice_i)\n\n        # Apply model on slice\n        # (creating a model replica on the target device).\n        outputs = model(inputs)\n        if not isinstance(outputs, list):\n          outputs = [outputs]\n\n        # Save the outputs for merging back together later.\n        for o in range(len(outputs)):\n          all_outputs[o].append(outputs[o])\n\n  # Merge outputs on CPU.\n  with tf.device('\/cpu:0'):\n    merged = []\n    for name, outputs in zip(model.output_names, all_outputs):\n      merged.append(concatenate(outputs,\n                                axis=0, name=name))\n    return Model(model.inputs, merged)\n<\/code><\/pre>\n<p>This works ok on local <code>2x NVIDIA GTX 1080 \/ Intel Xeon \/ Ubuntu 16.04<\/code>. It will fails on SageMaker Training Job.<\/p>\n<p>I have posted this issue on AWS Sagemaker forum in<\/p>\n<ul>\n<li><p><a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=879769\" rel=\"nofollow noreferrer\">TrainingJob custom algorithm with Keras backend and multi GPU<\/a><\/p>\n<\/li>\n<li><p><a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=294095&amp;tstart=0\" rel=\"nofollow noreferrer\">SageMaker Fails when using Multi-GPU with\nkeras.utils.multi_gpu_model<\/a><\/p>\n<\/li>\n<\/ul>\n<p><strong>[UPDATE]<\/strong><\/p>\n<p>I have slightly modified the <code>tf.session<\/code> code adding some initializers<\/p>\n<pre><code>with tf.Session() as session:\n    K.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n<\/code><\/pre>\n<p>and now at least I can see that one GPU (I assume device <code>gpu:0<\/code>) is used from the instance metrics. The multi-gpu does not work anyways.<\/p>",
        "Challenge_closed_time":1543270023360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543265506823,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an Algorithm Error when using Keras with a Tensorflow backend in a multi-GPU configuration on AWS SageMaker. The multi_gpu_model utility fails to work properly, even though it works on the user's local machine with the same configuration. The user has tried modifying the code and adding initializers to the tf.session code, but the multi-GPU still does not work. The user has posted the issue on AWS SageMaker forum for assistance.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53488870",
        "Challenge_link_count":7,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":93.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":84,
        "Challenge_solved_time":1.2545936111,
        "Challenge_title":"SageMaker fails when using Multi-GPU with keras.utils.multi_gpu_model",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1295.0,
        "Challenge_word_count":737,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305708350447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bologna, Italy",
        "Poster_reputation_count":14823.0,
        "Poster_view_count":1847.0,
        "Solution_body":"<p>This might not be the best answer for your problem, but this is what I am using for a multi-gpu model with Tensorflow backend. First i initialize using: <\/p>\n\n<pre><code>def setup_multi_gpus():\n    \"\"\"\n    Setup multi GPU usage\n\n    Example usage:\n    model = Sequential()\n    ...\n    multi_model = multi_gpu_model(model, gpus=num_gpu)\n    multi_model.fit()\n\n    About memory usage:\n    https:\/\/stackoverflow.com\/questions\/34199233\/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n    \"\"\"\n    import tensorflow as tf\n    from keras.utils.training_utils import multi_gpu_model\n    from tensorflow.python.client import device_lib\n\n    # IMPORTANT: Tells tf to not occupy a specific amount of memory\n    from keras.backend.tensorflow_backend import set_session  \n    config = tf.ConfigProto()  \n    config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU  \n    sess = tf.Session(config=config)  \n    set_session(sess)  # set this TensorFlow session as the default session for Keras.\n\n\n    # getting the number of GPUs \n    def get_available_gpus():\n       local_device_protos = device_lib.list_local_devices()\n       return [x.name for x in local_device_protos if x.device_type    == 'GPU']\n\n    num_gpu = len(get_available_gpus())\n    print('Amount of GPUs available: %s' % num_gpu)\n\n    return num_gpu\n<\/code><\/pre>\n\n<p>Then i call<\/p>\n\n<pre><code># Setup multi GPU usage\nnum_gpu = setup_multi_gpus()\n<\/code><\/pre>\n\n<p>and create a model.<\/p>\n\n<pre><code>...\n<\/code><\/pre>\n\n<p>After which you're able to make it a multi GPU model.<\/p>\n\n<pre><code>multi_model = multi_gpu_model(model, gpus=num_gpu)\nmulti_model.compile...\nmulti_model.fit...\n<\/code><\/pre>\n\n<p>The only thing here that is different from what you are doing is the way Tensorflow is initializing the GPU's. I can't imagine it being the problem, but it might be worth trying out. <\/p>\n\n<p>Good luck! <\/p>\n\n<p>Edit: I noticed sequence to sequence not being able to work with multi GPU. Is that the type of model you are trying to train?<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":24.55,
        "Solution_score_count":3.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":225.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2127.9833333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\nWe fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nThe new version of azure-cli is not compatible with the old azureml package and throws an error when creating AzureML workspace:\r\n\r\n```\r\nUnable to create the workspace. \r\n Azure Error: InvalidRequestContent\r\nMessage: The request content was invalid and could not be deserialized: 'Could not find member 'template' on object of type 'DeploymentDefinition'. Path 'template', line 1, position 12.'.\r\n```\r\n\r\nThere is an open issue at Azure cli about the similar error: https:\/\/github.com\/Azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### In which platform does it happen?\r\nLinux Ubuntu\r\n(Haven't tested on other platforms)\r\n\r\n### How do we replicate the issue?\r\nInstall reco_pyspark and run operationalization notebook.\r\n\r\n### Expected behavior (i.e. solution)\r\nFix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### Other Comments\r\nI'm working on #1158 and #900.\r\nIf fixing the azure-cli-core version is okay, then I will address this issue together.\r\n",
        "Challenge_closed_time":1603980914000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1596320174000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug related to the contrib package in AzureML. The product team has recommended removing the contrib package from `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and ensuring that all tests pass. The user is questioning whether contrib is being used or planned to be used by the team. The expected solution is for everything to run smoothly. The issue is happening on DSVM and DB platforms.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1171",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2127.9833333333,
        "Challenge_title":"[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Seems we need to fix `azure-mgmt-cosmosdb` version as well... \r\n```\r\nAttributeError: module 'azure.mgmt.cosmosdb' has no attribute 'CosmosDB'\r\n```\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":1.84,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":2756.3966977778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I made an Azure Machine Learning Service Experiment run and logged neural network losses with Jupyter Notebook. Logging worked fine and NN training completed as it should. However, the experiment is stuck in the running status. Shutting down the compute resources does not shut down the Experiment run and I cannot cancel it from the Experiment panel. In addition, the run does not have any log-files.<\/p>\n<p>Has anyone had the same behavior? Run has now lasted for over 24 hours.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KzAoS.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KzAoS.jpg\" alt=\"AMLS Experiment run\" \/><\/a><\/p>",
        "Challenge_closed_time":1595004339510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594972260903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue with an Azure Machine Learning Service Experiment run that is stuck in the \"Running\" status even though the neural network training has completed successfully. The user is unable to cancel the run from the Experiment panel and shutting down the compute resources does not help. Additionally, there are no log-files available for the run. The user is seeking help as the run has been ongoing for over 24 hours.",
        "Challenge_last_edit_time":1594989303640,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62949488",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":8.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":8.9107241667,
        "Challenge_title":"AMLS Experiment run stuck in status \"Running\"",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":887.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1563874553356,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Helsinki, Finland",
        "Poster_reputation_count":881.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p>this totally happens from time to time. it is certainly frustrating especially because the &quot;Cancel&quot; button it grayed out. You can use either the CLI or Python SDK  to cancel the run.<\/p>\n<h2>SDK<\/h2>\n<h3>&gt;= 1.16.0<\/h3>\n<p>As of version <code>1.16.0<\/code> you no longer an <code>Experiment<\/code> object is no longer needed. Instead you can access using the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#get-workspace--run-id-&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\"><code>Run<\/code><\/a> or <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#get-run-run-id-&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\"><code>Workspace<\/code><\/a> objects directly<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Experiment, Run, VERSION\nprint(&quot;SDK version:&quot;, VERSION)\n\nws = Workspace.from_config()\n\nrun = ws.get_run('YOUR_RUN_ID')\nrun = Run().get(ws, 'YOUR_RUN_ID') # also works\nrun.cancel()\n<\/code><\/pre>\n<h3>&lt; 1.16.0<\/h3>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Experiment, Run, VERSION\nprint(&quot;SDK version:&quot;, VERSION)\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = 'YOUR_EXP_NAME')\n\nrun = Run(exp, run_id='YOUR STEP RUN ID')\n\nrun.cancel() # or run.fail()\n<\/code><\/pre>\n<h1>CLI<\/h1>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-azure-machine-learning-cli#install-the-extension\" rel=\"nofollow noreferrer\">More CLI details here<\/a><\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az login\naz ml run cancel --run YOUR_RUN_ID\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1604912331752,
        "Solution_link_count":3.0,
        "Solution_readability":13.1,
        "Solution_reading_time":23.45,
        "Solution_score_count":5.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":139.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":85.8187694445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm currently trying to implement MLFlow Tracking into my training pipeline and would like to log the hyperparameters of my hyperparameter Tuning of each training job.<\/p>\n\n<p>Does anyone know, how to pull the list of hyperparameters that can be seen on the sagemaker training job interface (on the AWS console)? Is there any other smarter way to list how models perform in comparison in Sagemaker (and displayed)?<\/p>\n\n<p>I would assume there must be an easy and Pythonic way to do this (either boto3 or the sagemaker api) to get this data. I wasn't able to find it in Cloudwatch.<\/p>\n\n<p>Many thanks in advance!<\/p>",
        "Challenge_closed_time":1592173650467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591864702897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to implement MLFlow Tracking into their training pipeline and is looking for a way to pull the list of hyperparameters used in each training job in Sagemaker. They are seeking a Pythonic way to get this data through either boto3 or the Sagemaker API, but have not been able to find it in Cloudwatch.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62320331",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":85.8187694445,
        "Challenge_title":"Sagemaker API to list Hyperparameters",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":484.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469720117216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>there is indeed a rather pythonic way in the SageMaker python SDK:<\/p>\n\n<pre><code>tuner = sagemaker.tuner.HyperparameterTuner.attach('&lt; your tuning jobname&gt;')\n\nresults = tuner.analytics().dataframe()  # all your tuning metadata, in pandas!\n<\/code><\/pre>\n\n<p>See full example here <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-tuneranalytics-samples\/blob\/master\/SageMaker-Tuning-Job-Analytics.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":33.1,
        "Solution_reading_time":7.83,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1395230906503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":129.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><code>CSVS3DataSet<\/code>\/<code>HDFS3DataSet<\/code>\/<code>HDFS3DataSet<\/code> use <code>boto3<\/code>, which is known to be not thread-safe <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/resources.html?highlight=multithreading#multithreading-multiprocessing\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/resources.html?highlight=multithreading#multithreading-multiprocessing<\/a><\/p>\n\n<p>Is it OK to use these datasets with the ParallelRunner?<\/p>",
        "Challenge_closed_time":1574069164940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574069164940,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether S3 Kedro datasets, specifically CSVS3DataSet, HDFS3DataSet, and S3DataSet, are thread-safe as they use boto3, which is known to be not thread-safe. The user is asking if it is safe to use these datasets with the ParallelRunner.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58911398",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":34.3,
        "Challenge_reading_time":7.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Are S3 Kedro datasets thread-safe?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":146.0,
        "Challenge_word_count":28,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395230906503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":129.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p><code>Kedro<\/code> uses <code>s3fs<\/code>, which uses <code>boto3<\/code> library to access S3. <code>Boto3<\/code> is not thread-safe indeed, but only if you are trying to reuse the same Session object.<\/p>\n\n<p>All <code>Kedro<\/code> S3 datasets maintain separate instances of <code>S3FileSystem<\/code>, which means separate boto sessions, so it's safe.<\/p>\n\n<p>It's probably not great in terms of performance, and if you work with hundreds of S3 data sets in parallel, or thousands of small S3 datasets sequentially - the pipeline might run quite long and even fail on connection errors, but you are totally safe with a few dozens of them.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":8.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":94.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.2348202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure ML the input data has to be defined as a Dataset (to create a pipeline). In my code I am passing datasets with the following syntax: input_data = Dataset.File.from_files(datapath)  <\/p>\n<p>I would like to change this datapath as an input parameter from Data Factory (for example via PipelineParamater), so I can apply the same Data Factory pipeline for different datasets. However, in Data Factory you can only pass string as a parameter, not a DataPath.  <\/p>\n<p>What is the solution around this?<\/p>",
        "Challenge_closed_time":1626693295843,
        "Challenge_comment_count":3,
        "Challenge_created_time":1626444050490,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in passing data from Azure Data Factory into Azure ML as the input data has to be defined as a Dataset. The user wants to change the datapath as an input parameter from Data Factory, but can only pass a string as a parameter, not a DataPath. The user is seeking a solution to this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/479034\/passing-data-from-azure-data-factory-into-azure-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":69.2348202778,
        "Challenge_title":"Passing data from Azure Data Factory into Azure ML",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=db220306-9cc7-45e6-8626-e4713f3f5baf\">@Ilze Amanda   <\/a> ,    <\/p>\n<p>Thank you for posting your query on Microsoft Q&amp;A Portal and sharing clarifications on ask.    <\/p>\n<blockquote>\n<p>Unfortunately, we cannot create user defined types in Azure data factory at this moment.    <\/p>\n<\/blockquote>\n<p>But, I will encourage you to log your feedback using below link. Product team will actively monitor feedback there and consider them for future releases. Thank you.    <br \/>\n<a href=\"https:\/\/feedback.azure.com\/forums\/270578-data-factory\">https:\/\/feedback.azure.com\/forums\/270578-data-factory<\/a>     <\/p>\n<p>Hope this will help.    <\/p>\n<p>--------------------------------------    <\/p>\n<ul>\n<li> Please <code>accept an answer<\/code> if correct. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>.    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>.     <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.0,
        "Solution_reading_time":15.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":124.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1655690061687,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philippines",
        "Answerer_reputation_count":437.0,
        "Answerer_view_count":218.0,
        "Challenge_adjusted_solved_time":42.9297986111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is driving me bonkers so any help greatly appreciated.<\/p>\n<p>I am using Google's Vertex ML. I have exported a batch prediction to BigQuery.<\/p>\n<p>The schema is I believe a record with repeat fields.<\/p>\n<p>So I think it would like this in JSON:<\/p>\n<pre><code>[{&quot;category&quot;:true,&quot;score&quot;:.9999},{&quot;category&quot;:false,&quot;score&quot;,.05}]\n<\/code><\/pre>\n<p>I can not figure out how to either unnest or narrow a search where a category is true.<\/p>\n<p>I need to have a flat select that has the correct category column and score value<\/p>\n<pre><code>123 | true | .9999\n123 | false | .05\n<\/code><\/pre>\n<p>or a select with a where clause to only get true values<\/p>\n<pre><code>123 | .9999\n<\/code><\/pre>\n<p>The following unnests everything but it creates four rows joining both the true and false to both the scores.<\/p>\n<pre><code>SELECT\n  row_id,\n  classes,\n  scores\nFROM\n  `database`\ncross JOIN\n  UNNEST(exported.classes) AS classes,\n  UNNEST(exported.scores) AS scores\nLIMIT\n  10\n<\/code><\/pre>\n<p>creates rows like:<\/p>\n<pre><code>123 | true | .9999\n123 | false | .9999\n123 | true | .05\n123 | false | .05\n<\/code><\/pre>\n<p>This does select the values I need but it's still a nested field...<\/p>\n<pre><code>select\nrow_id,\nclasses.classes,\nclasses.scores\nfrom (\nSELECT\n  voter_id,\n  ARRAY_CONCAT([predicted_results]) as the_results\nFROM\n  `data`\nLIMIT\n  10\n),\nunnest(the_results) as classes\n<\/code><\/pre>\n<p>creates rows like<\/p>\n<pre><code>123 | [true:.9999,false:.05]\n<\/code><\/pre>",
        "Challenge_closed_time":1659415712443,
        "Challenge_comment_count":2,
        "Challenge_created_time":1658876367830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble querying and flattening results from Vertex ML that were saved to BigQuery. The schema is a record with repeat fields, and the user needs to unnest or narrow a search to only get true values. The current unnest query creates four rows joining both true and false to both scores, and the user needs a flat select with the correct category column and score value.",
        "Challenge_last_edit_time":1659331589368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73130582",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.2,
        "Challenge_reading_time":19.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":149.8179480556,
        "Challenge_title":"How to query \/ flatten from vertex ml results saved to bigquery",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_created_time":1282610737523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":185.0,
        "Poster_view_count":37.0,
        "Solution_body":"<pre><code>select \n primary_key, \n  predicted_supports.classes[SAFE_OFFSET(index)] as class,\n  predicted_supports.scores[SAFE_OFFSET(index)] as score,\nFROM `database`,\nunnest(generate_array(0,array_length(predicted_supports.classes)-1)) as index\n<\/code><\/pre>\n<p>Here is the ouput:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jVpOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jVpOe.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1659486136643,
        "Solution_link_count":2.0,
        "Solution_readability":20.5,
        "Solution_reading_time":6.16,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":28.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":5.2579222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been exploring Azure ML Pipeline. I am referring to <a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/06A%20-%20Creating%20a%20Pipeline.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> for the below code:<\/p>\n<p>Here is a small snippet from a MS Repo:<\/p>\n<pre><code>train_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\nsource_directory = experiment_folder,\nscript_name = &quot;prep_diabetes.py&quot;,\narguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n'--prepped-data', prepped_data_folder],\noutputs=[prepped_data_folder],\ncompute_target = pipeline_cluster,\nrunconfig = pipeline_run_config,\nallow_reuse = True)\n<\/code><\/pre>\n<p>This suggests that while defining a pipeline, we must provide it a compute resource(pipeline_cluster). This obviously makes sense, since specific compute might be required for a specific step.<\/p>\n<p>But do we need to have this compute resource up and running always, so that whenever a pipeline is triggered, it can find the compute resource?<\/p>\n<p>Also, i figured we can probably keep a cluster with Zero minimum nodes, in which cases cluster is resized whenever pipeline is triggered. But i think there is a minimal cost incurrent in probably container registry regularly in such a setup. Is this the recommended way to deploy ML pipelines or some more efficient approach is possible?<\/p>",
        "Challenge_closed_time":1620823042460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620804113940,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring Azure ML Pipeline and has encountered a challenge regarding the need for a pre-existing compute resource to run a scheduled pipeline. The user is unsure if the compute resource needs to be up and running always or if a cluster with zero minimum nodes can be used. The user is also concerned about the cost incurred in such a setup and is seeking recommendations for a more efficient approach.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67498965",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":18.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.2579222222,
        "Challenge_title":"Pre-existing Compute Resource necessary for running a scheduled Azure ML pipeline?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Poster_created_time":1315165259620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":339.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>Yep you're right -- create a <code>ComputeTarget<\/code> with a minimum of zero nodes. The <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-registry\/#pricing\" rel=\"nofollow noreferrer\">container registry costs<\/a> are ~$0.16 USD\/day and, IIRC, that cost is bundled in with Azure Machine learning.<\/p>\n<p>This is what our team does for our published pipelines in production.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.2,
        "Solution_reading_time":5.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1515931819516,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":177.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":92.4561386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm not very familiar with parallelization in Python and I'm getting an error when trying to train a model on multiple training folds in parallel. Here's a simplified version of my code:<\/p>\n<pre><code>def train_test_model(fold):\n    # here I train the model etc...\n    \n    # now I want to save the parameters and metrics\n    with mlflow.start_run():\n        mlflow.log_param(&quot;run_name&quot;, run_name)\n        mlflow.log_param(&quot;modeltype&quot;, modeltype)\n        # and so on...\n\nif __name__==&quot;__main__&quot;:\n    pool = ThreadPool(processes = num_trials)\n    # run folds in parallel\n    pool.map(lambda fold:train_test_model(fold), folds)\n<\/code><\/pre>\n<p>I'm getting the following error:<\/p>\n<pre><code>Exception: Run with UUID 23e9bb6d22674a518e48af9c51252860 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a> says that <code>mlflow.start_run()<\/code> starts a new run and makes it active which is the root of my problem. Every thread starts a MLFlow run for its corresponding fold and makes it active while I need the runs to run in parallel i.e. all be active(?) and save parameters\/metrics of the corresponding fold. How can I solve that issue?<\/p>",
        "Challenge_closed_time":1603877162672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603544320573,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to train a model on multiple training folds in parallel using Python and MLFlow, but is encountering an error where MLFlow is unable to start multiple runs in parallel. The error message suggests that the current run needs to be ended before starting a new one, but the user needs all runs to be active and save parameters\/metrics of the corresponding fold. The user is seeking a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64513552",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":17.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":92.4561386111,
        "Challenge_title":"How to have multiple MLFlow runs in parallel?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2275.0,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515931819516,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":177.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I found a solution, maybe it will be useful for someone else. You can see details with code examples here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3592\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/3592<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.8,
        "Solution_reading_time":3.23,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1493314794172,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":844.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":13.1640722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am training a Machine learning model in google colab, to be more specific I am training a GAN with PyTorch-lightning. The problem occurs is when I get disconnected from my current runtime due to inactivity. When I try to reconnect my Browser(tried on firefox and chrome) becomes first laggy and than freezes, my pc starts to lag so that I am not able to close my browser and it doesn't go away. I am forced to press the power button of my PC in order to restart the PC.\nI have no clue why this happens.\nI tried various batch sizes(also the size 1) but it still happens. It can't be that my dataset is too big either(since i tried it on a dataset with 10images for testing puposes).\nI hope someone can help me.<\/p>\n\n<p>Here is my code (For using the code you will need comet.nl and enter the comet.ml api key):<\/p>\n\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision  \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers\n\nimport numpy as np\nfrom numpy.random import choice\n\nfrom PIL import Image\n\nimport os\nfrom pathlib import Path\nimport shutil\n\nfrom collections import OrderedDict\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# randomly flip some labels\ndef noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n    # determine the number of labels to flip\n    n_select = int(p_flip * y.shape[0])\n    # choose labels to flip\n    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n    # invert the labels in place\n    y[flip_ix] = 1 - y[flip_ix]\n    return y\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=0.1):\n        self.std = std\n        self.mean = mean\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\ndef get_valid_labels(img):\n  return (0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1  # soft labels\n\ndef get_unvalid_labels(img):\n  return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)  # soft labels\n\nclass Generator(nn.Module):\n    def __init__(self, ngf, nc, latent_dim):\n        super(Generator, self).__init__()\n        self.ngf = ngf\n        self.latent_dim = latent_dim\n        self.nc = nc\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n             nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf, nc):\n        super(Discriminator, self).__init__()\n        self.nc = nc\n        self.ndf = ndf\n\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass DCGAN(pl.LightningModule):\n\n    def __init__(self, hparams, logger, checkpoint_folder, experiment_name):\n        super().__init__()\n        self.hparams = hparams\n        self.logger = logger  # only compatible with comet_logger at the moment\n        self.checkpoint_folder = checkpoint_folder\n        self.experiment_name = experiment_name\n\n        # networks\n        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n        # For experience replay\n        self.exp_replay_dis = torch.tensor([])\n\n        # creating checkpoint folder\n        dirpath = Path(self.checkpoint_folder)\n        if not dirpath.exists():\n          os.makedirs(dirpath, 0o755)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        # For adding Instance noise for more visit: https:\/\/www.inference.vc\/instance-noise-a-trick-for-stabilising-gan-training\/\n        std_gaussian = max(0, self.hparams.level_of_noise - ((self.hparams.level_of_noise * 1.5) * (self.current_epoch \/ self.hparams.epochs)))\n        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian) # the noise decays over time\n\n        imgs, _ = batch\n        imgs = AddGaussianNoiseInst(imgs) # Adding instance noise to real images\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n\n            # generate images\n            self.generated_imgs = self(z)\n            self.generated_imgs = AddGaussianNoiseInst(self.generated_imgs) # Adding instance noise to fake images\n\n            # Experience replay\n            # for discriminator\n            perm = torch.randperm(self.generated_imgs.size(0))  # Shuffeling\n            r_idx = perm[:max(1, self.hparams.experience_save_per_batch)]  # Getting the index\n            self.exp_replay_dis = torch.cat((self.exp_replay_dis, self.generated_imgs[r_idx]), 0).detach()  # Add our new example to the replay buffer\n\n            # ground truth result (ie: all fake)\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), get_valid_labels(self.generated_imgs)) # adversarial loss is binary cross-entropy\n\n            tqdm_dict = {'g_loss': g_loss}\n            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': log\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n            # how well can it label as real?\n            real_loss = self.adversarial_loss(self.discriminator(imgs), get_valid_labels(imgs))\n\n            # Experience replay\n            if self.exp_replay_dis.size(0) &gt;= self.hparams.experience_batch_size:\n              fake_loss = self.adversarial_loss(self.discriminator(self.exp_replay_dis.detach()), get_unvalid_labels(self.exp_replay_dis))  # train on already seen images\n\n              self.exp_replay_dis = torch.tensor([]) # Reset experience replay\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"d_exp_loss\": fake_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n            else:\n              fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), get_unvalid_labels(self.generated_imgs))  # how well can it label as fake?\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n        #                                 transforms.ToTensor(),\n        #                                 transforms.Normalize([0.5], [0.5])\n        #                                 ])\n\n        # train_dataset = torchvision.datasets.ImageFolder(\n        #     root=\".\/drive\/My Drive\/datasets\/ghibli_dataset_small_overfit\/\",\n        #     transform=transform\n        # )\n        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True, batch_size=self.hparams.batch_size)\n\n    def on_epoch_end(self):\n        z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n        # match gpu device (or keep as cpu)\n        if self.on_gpu:\n            z = z.cuda(self.last_imgs.device.index)\n\n        # log sampled images\n        sample_imgs = self.generator(z)\n        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n        self.logger.experiment.log_image(grid.permute(1, 2, 0), f'generated_images_epoch{self.current_epoch}', step=self.current_epoch)\n\n        # save model\n        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n          trainer.save_checkpoint(self.checkpoint_folder + \"\/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n          comet_logger.experiment.log_asset_folder(self.checkpoint_folder, step=self.current_epoch)\n\n          # Deleting the folder where we saved the model so that we dont upload a thing twice\n          dirpath = Path(self.checkpoint_folder)\n          if dirpath.exists() and dirpath.is_dir():\n                shutil.rmtree(dirpath)\n\n          # creating checkpoint folder\n          access_rights = 0o755\n          os.makedirs(dirpath, access_rights)\n\nfrom argparse import Namespace\n\nargs = {\n    'batch_size': 48,\n    'lr': 0.0002,\n    'b1': 0.5,\n    'b2': 0.999,\n    'latent_dim': 128, # tested value which worked(in V4_1): 100\n    'nc': 1,\n    'ndf': 32,\n    'ngf': 32,\n    'epochs': 10,\n    'save_model_every_epoch': 5,\n    'image_size': 64,\n    'num_workers': 2,\n    'level_of_noise': 0.15,\n    'experience_save_per_batch': 1, # this value should be very low; tested value which works: 1\n    'experience_batch_size': 50 # this value shouldnt be too high; tested value which works: 50\n}\nhparams = Namespace(**args)\n\n# Parameters\nexperiment_name = \"DCGAN_V4_2_MNIST\"\ndataset_name = \"MNIST\"\ncheckpoint_folder = \"DCGAN\/\"\ntags = [\"DCGAN\", \"MNIST\", \"OVERFIT\", \"64x64\"]\ndirpath = Path(checkpoint_folder)\n\n# init logger\ncomet_logger = loggers.CometLogger(\n    api_key=\"\",\n    rest_api_key=\"\",\n    project_name=\"gan\",\n    experiment_name=experiment_name,\n    #experiment_key=\"f23d00c0fe3448ee884bfbe3fc3923fd\"  # used for resuming trained id can be found in comet.ml\n)\n\n#defining net\nnet = DCGAN(hparams, comet_logger, checkpoint_folder, experiment_name)\n\n#logging\ncomet_logger.experiment.set_model_graph(str(net))\ncomet_logger.experiment.add_tags(tags=tags)\ncomet_logger.experiment.log_dataset_info(dataset_name)\n\ntrainer = pl.Trainer(#resume_from_checkpoint=\"GHIBLI_DCGAN_OVERFIT_64px_epoch_6000.ckpt\",\n                     logger=comet_logger,\n                     max_epochs=args[\"epochs\"]\n                     )\ntrainer.fit(net)\ncomet_logger.experiment.end()\n<\/code><\/pre>",
        "Challenge_closed_time":1587034953183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586987562523,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is training a machine learning model in Google Colab, specifically a GAN with PyTorch-lightning. The issue occurs when the user gets disconnected from the current runtime due to inactivity, and upon trying to reconnect, the browser becomes laggy and freezes, causing the PC to lag and forcing the user to restart it. The user has tried various batch sizes and datasets but the issue persists. The user has provided their code for reference.",
        "Challenge_last_edit_time":1587130797008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61239274",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":149.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":145,
        "Challenge_solved_time":13.1640722222,
        "Challenge_title":"Google Colab freezes my browser and pc when trying to reconnect to a notebook",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1013.0,
        "Challenge_word_count":1119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493314794172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":844.0,
        "Poster_view_count":170.0,
        "Solution_body":"<p>I fixed it with importing this:<\/p>\n\n<pre><code>from IPython.display import clear_output \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":29.4160033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to read a csv file on an s3 bucket (for which the sagemaker notebook has full access to) into a spark dataframe however I am hitting the following issue where <code>sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar<\/code> can't be found. Any tips on how to resolve this is appreciate!<\/p>\n\n<pre><code>bucket = \"mybucket\"\nprefix = \"folder\/file.csv\"\ndf = spark.read.csv(\"s3:\/\/{}\/{}\/\".format(bucket,prefix))\n\nPy4JJavaError: An error occurred while calling o388.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error reading configuration file\nat java.util.ServiceLoader.fail(ServiceLoader.java:232)\nat java.util.ServiceLoader.parse(ServiceLoader.java:309)\nat java.util.ServiceLoader.access$200(ServiceLoader.java:185)\nat java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)\nat java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)\nat java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)\nat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\nat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\nat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\nat scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)\nat scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)\nat scala.collection.AbstractTraversable.filter(Traversable.scala:104)\nat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:614)\nat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\nat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\nat py4j.Gateway.invoke(Gateway.java:282)\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\nat py4j.GatewayConnection.run(GatewayConnection.java:238)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar (No such file or directory)\n    at java.util.zip.ZipFile.open(Native Method)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:219)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:149)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:166)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:103)\n    at sun.net.www.protocol.jar.URLJarFile.&lt;init&gt;(URLJarFile.java:93)\n    at sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)\n    at sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)\n    at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)\n    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:150)\n    at java.net.URL.openStream(URL.java:1045)\n    at java.util.ServiceLoader.parse(ServiceLoader.java:304)\n    ... 26 more\n<\/code><\/pre>",
        "Challenge_closed_time":1532493818792,
        "Challenge_comment_count":2,
        "Challenge_created_time":1532387294417,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to read a CSV file from an S3 bucket into a Spark dataframe in Sagemaker. The error message indicates that the sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar file cannot be found, resulting in a java.util.ServiceConfigurationError. The user is seeking tips on how to resolve this issue.",
        "Challenge_last_edit_time":1532387921180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51488308",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":34.1,
        "Challenge_reading_time":49.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":29.5901041667,
        "Challenge_title":"Failing to read data from s3 to a spark dataframe in Sagemaker",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2283.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444524456120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation_count":973.0,
        "Poster_view_count":82.0,
        "Solution_body":"<p>(Making comment to the original question as answer)<\/p>\n\n<p>It looks like a jupyter kernel issue. I had a similar issue and I used <code>Sparkmagic (pyspark)<\/code> kernel instead of <code>Sparkmagic (pyspark3)<\/code> and it is working fine. Follow instructions on this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">blog<\/a> and see if it helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":142.9037780555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying out the sample notebooks in AWS Sagemaker, currently in the mxnet mnist example which demonstrates bringing your own code. The entry point parameter passed in when instantiating an estimator instance, only mentions the source file (mnist.py) and not a method name or any other point inside the source file.<\/p>\n\n<p>So how does aws sagemaker figure out which method to send the training data to? <\/p>",
        "Challenge_closed_time":1520316895088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1519726485703,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a challenge in AWS Sagemaker while trying to specify the entry point to the code for the mxnet mnist example. The entry point parameter only mentions the source file and not a method name or any other point inside the source file, leaving the user unsure of how AWS Sagemaker will determine which method to send the training data to.",
        "Challenge_last_edit_time":1519802441487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49006174",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":6.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":164.0026069444,
        "Challenge_title":"How is the entry point to the code specified in AWS sagemaker bring your own code?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":3327.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450260166772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1587.0,
        "Poster_view_count":540.0,
        "Solution_body":"<p>Your python script should implement a few methods like train, model_fn, transform_fn, input_fn etc. SagaMaker would call appropriate method when needed. <\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":25.9,
        "Solution_reading_time":5.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1653396543887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kolkata, India",
        "Answerer_reputation_count":1231.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":1.2397177778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can you kindly show me how do we start the Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook?\n<br> This is working fine in Google Colaboratory by the way.\n<br> What is missing here?<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\nimport sys\n\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\n\nfrom sparknlp.base import *\nfrom sparknlp.common import *\nfrom sparknlp.annotator import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" alt=\"Error\" \/><\/a><\/p>\n<p><br><br> <strong>UPDATE_2022-07-21:<\/strong>\n<br> Hi @Sayan. I am still not able to start Spark session on Vertex AI workbench Jupyterlab notebook after running the commands =(\n<a href=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\n# Included else &quot;JAVA_HOME is not set&quot;\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\nspark = sparknlp.start()\n\nprint(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\nprint(&quot;Apache Spark version: {}&quot;.format(spark.version))\n<\/code><\/pre>\n<p>The error:<\/p>\n<pre><code>\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 71: C:\/Program Files\/Java\/jdk-18.0.1.1\/bin\/java: No such file or directory\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 96: CMD: bad array subscript\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_5831\/489505405.py in &lt;module&gt;\n      6 \n      7 import sparknlp\n----&gt; 8 spark = sparknlp.start()\n      9 \n     10 print(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start(gpu, m1, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\n    242         return SparkRealTimeOutput()\n    243     else:\n--&gt; 244         spark_session = start_without_realtime_output()\n    245         return spark_session\n    246 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start_without_realtime_output()\n    152             builder.config(&quot;spark.jsl.settings.storage.cluster_tmp_dir&quot;, cluster_tmp_dir)\n    153 \n--&gt; 154         return builder.getOrCreate()\n    155 \n    156     def start_with_realtime_output():\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    267                         sparkConf.set(key, value)\n    268                     # This SparkContext may be an existing one.\n--&gt; 269                     sc = SparkContext.getOrCreate(sparkConf)\n    270                     # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    271                     # by all sessions.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    481         with SparkContext._lock:\n    482             if SparkContext._active_spark_context is None:\n--&gt; 483                 SparkContext(conf=conf or SparkConf())\n    484             assert SparkContext._active_spark_context is not None\n    485             return SparkContext._active_spark_context\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\n    193             )\n    194 \n--&gt; 195         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    196         try:\n    197             self._do_init(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    415         with SparkContext._lock:\n    416             if not SparkContext._gateway:\n--&gt; 417                 SparkContext._gateway = gateway or launch_gateway(conf)\n    418                 SparkContext._jvm = SparkContext._gateway.jvm\n    419 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf, popen_kwargs)\n    104 \n    105             if not os.path.isfile(conn_info_file):\n--&gt; 106                 raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\n    107 \n    108             with open(conn_info_file, &quot;rb&quot;) as info:\n\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>",
        "Challenge_closed_time":1658384407350,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658299714453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues in starting a Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook. The user has tried installing Spark NLP and PySpark, setting the JAVA_HOME environment variable, and running the necessary commands, but is still unable to start the Spark session. The user has also provided an error message that they received while trying to start the session.",
        "Challenge_last_edit_time":1658406889243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73047089",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":64.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":23.5258047223,
        "Challenge_title":"How to start Spark session on Vertex AI workbench Jupyterlab notebook?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":238.0,
        "Challenge_word_count":413,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651609055808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>One possible reason is that <code>Java<\/code> is not installed. When you create a <strong>Python-3 Vertex AI Workbench<\/strong> you can have either <code>Debian<\/code> or <code>Ubuntu<\/code> as an OS and it does not come with Java pre-installed. You need to install it manually.\nTo install you can use<\/p>\n<pre><code>sudo apt-get update\nsudo apt-get install default-jdk\n<\/code><\/pre>\n<p>You can follow this <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-install-java-with-apt-get-on-ubuntu-16-04\" rel=\"nofollow noreferrer\">tutorial<\/a> to install Open JDK.<\/p>\n<p>All your problems lie with installing JDK and setting its path in the environment. Once you do this properly you don't need to set path in python also.\nYour code should look something like this<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\n#no need to set the environment path\n\nimport sparknlp\n#all other imports\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><strong>EDIT:<\/strong>\nI have tried your code and had the same error.All I did was Open the terminal inside JupyterLab of the workbench and installed java there.<\/p>\n<p>Opened the JupyterLab from Workbench\n<a href=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Notebook instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Opening the terminal from <em><strong><code>File-&gt;New-&gt;Terminal<\/code><\/strong><\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>From here I downloaded and installed the Java.<\/p>\n<p>You can check whether it has been installed and added to your path by running <code>java --version<\/code> it will return the current version.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1658411352227,
        "Solution_link_count":7.0,
        "Solution_readability":9.7,
        "Solution_reading_time":28.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":240.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":1.2039644444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run below sample code in my notebook, Running on python 3.6 kernel.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml<\/a>\nDownload the MNIST dataset<\/p>\n\n<p>The following code failed with the attribute error, on line of the following code from azureml.opendatasets import MNIST <\/p>\n\n<pre><code>from azureml.core import Dataset\nfrom azureml.opendatasets import MNIST\n\n<\/code><\/pre>",
        "Challenge_closed_time":1581481802847,
        "Challenge_comment_count":3,
        "Challenge_created_time":1581475311887,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an attribute error while trying to run a sample code for training on Azure machine learning. The error is occurring on the line of code that imports MNIST dataset from azureml.opendatasets.",
        "Challenge_last_edit_time":1581477772576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60180314",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.8030444445,
        "Challenge_title":"Azure machine learning failing on sample for training",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":555.0,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Prerequisites:\nThe tutorial and accompanying utils.py file is also available on <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/tutorials\" rel=\"nofollow noreferrer\">GitHub<\/a> if you wish to use it on your own <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment#local\" rel=\"nofollow noreferrer\">local environment<\/a>. Run pip install azureml-sdk[notebooks] azureml-opendatasets matplotlib to install dependencies for this tutorial.<\/p>\n\n<p>If you are using older version then upgrade to the latest Azure ML SDK Version 1.0.85.<\/p>\n\n<p>!pip install --upgrade azureml-sdk<\/p>\n\n<pre><code># check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)\n<\/code><\/pre>\n\n<p>Also <\/p>\n\n<p>!pip install --upgrade azureml-opendataset <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1581482106848,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":10.78,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1432829415467,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":501.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":674.1436052778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment in AzureML which has a R module at its core. Additionally, I have some .RData files stored in Azure blob storage. The blob container is set as private (no anonymous access).<\/p>\n\n<p>Now, I am trying to make a https call from inside the R script to the azure blob storage container in order to download some files. I am using the <code>httr<\/code> package's <code>GET()<\/code> function and properly set up the url, authentication etc...The code works in R on my local machine but the same code gives me the following error when called from inside the R module in the experiment<\/p>\n\n<pre><code>error:1411809D:SSL routines:SSL_CHECK_SERVERHELLO_TLSEXT:tls invalid ecpointformat list\n<\/code><\/pre>\n\n<p>Apparently this is an error from the underlying OpenSSL library (which got fixed a while ago). Some suggested workarounds I found <a href=\"https:\/\/stackoverflow.com\/questions\/20046176\/rcurl-errors-when-fetching-ssl-endpoint\">here<\/a> were to set <code>sslversion = 3<\/code> and <code>ssl_verifypeer = 1<\/code>, or turn off verification <code>ssl_verifypeer = 0<\/code>. Both of these approaches returned the same error.<\/p>\n\n<p>I am guessing that this has something to do with the internal Azure certificate \/ validation...? Or maybe I am missing or overseeing something?<\/p>\n\n<p>Any help or ideas would be greatly appreciated. Thanks in advance.<\/p>\n\n<p>Regards<\/p>",
        "Challenge_closed_time":1450358402136,
        "Challenge_comment_count":1,
        "Challenge_created_time":1447931485157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when trying to make an https call from inside an R module in AzureML to download files from a private Azure blob storage container. The error is related to SSL routines and is caused by an issue with the underlying OpenSSL library. The user has tried suggested workarounds but none have worked. The user suspects that the issue may be related to the internal Azure certificate\/validation.",
        "Challenge_last_edit_time":1495540337367,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33802274",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":18.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":674.1436052778,
        "Challenge_title":"Error:1411809D:SSL routines - When trying to make https call from inside R module in AzureML",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432829415467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":501.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>After a while, an answer came back from the support team, so I am going to post the relevant part as an answer here for anyone who lands here with the same problem. <\/p>\n\n<p>\"This is a known issue. The container (a sandbox technology known as \"drawbridge\" running on top of Azure PaaS VM) executing the Execute R module doesn't support outbound HTTPS traffic. Please try to switch to HTTP and that should work.\"<\/p>\n\n<p>As well as that a solution is on the way :<\/p>\n\n<p>\"We are actively looking at how to fix this bug. \"<\/p>\n\n<p>Here is the original <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/5866e16c-a145-481e-8764-f7c7823742b0\/https-call-from-inside-r-module-possible-?forum=MachineLearning\" rel=\"nofollow\">link<\/a> as a reference.\nhth<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":9.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":107.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.8341666667,
        "Challenge_answer_count":1,
        "Challenge_body":"I'd like to set up Amazon SageMaker XGBoost to train datasets on multiple machines. Is that possible? If so, how?",
        "Challenge_closed_time":1583654787000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583496984000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of parallel training across multiple machines using Amazon SageMaker XGBoost and is seeking guidance on how to set it up.",
        "Challenge_last_edit_time":1668057386500,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOKZq2V_RQaaFzQkapcWpsA\/does-amazon-sagemaker-xgboost-support-parallel-training-across-multiple-machines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":43.8341666667,
        "Challenge_title":"Does Amazon SageMaker XGBoost support parallel training across multiple machines?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":119.0,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see [Docker registry paths and example code](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html) in the Amazon SageMaker developer guide. ",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925571827,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":3.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1599390178756,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":628.2919663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm working in sage maker studio, and I have a single instance running one computationally intensive task:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IntzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IntzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It appears that the kernel running my task is maxed out, but the actual instance is only using a small amount of its resources. Is there some sort of throttling occurring? Can I configure this so that more of the instance is utilized?<\/p>",
        "Challenge_closed_time":1629552761387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627529779217,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sage Maker Studio where the kernel running a computationally intensive task is maxed out, but the instance is only using a small amount of its resources. The user is seeking help to configure the instance to utilize more of its resources and avoid throttling.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68569742",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":561.9394916667,
        "Challenge_title":"Sage Maker Studio CPU Usage",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":946.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>Your ml.c5.xlarge instance comes with 4 vCPU. However, Python only uses a single CPU by default. (Source: <a href=\"https:\/\/stackoverflow.com\/questions\/64121703\/can-i-apply-multithreading-for-computationally-intensive-task-in-python\">Can I apply multithreading for computationally intensive task in python?<\/a>)<\/p>\n<p>As a result, the overall CPU utilization of your ml.c5.xlarge instance is low. To utilize all the vCPUs, you can try multiprocessing.<\/p>\n<p>The examples below are performed using a 2 vCPU + 4 GiB instance.<\/p>\n<p>In the first picture, multiprocessing is not set up. The instance CPU utilization peaks at around 50%.<\/p>\n<p>single processing:<br \/>\n<img src=\"https:\/\/i.stack.imgur.com\/lcn8K.png\" alt=\"single processing\" \/><\/p>\n<p>In the second picture, I created 50 processes to be run simultaneously. The instance CPU utilization rises to 100% immediately.<\/p>\n<p>multiprocessing:<br \/>\n<img src=\"https:\/\/i.stack.imgur.com\/tk65n.png\" alt=\"multiprocessing\" \/><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1629791630296,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":12.79,
        "Solution_score_count":3.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1621409485092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3609.0,
        "Answerer_view_count":2438.0,
        "Challenge_adjusted_solved_time":55.3460088889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run this 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio<\/p>\n<p><a href=\"https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing<\/a><\/p>\n<p>When I get to this step:<\/p>\n<pre><code>import gradio as gr\nimport tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n<\/code><\/pre>\n<p>I get this error:<\/p>\n<pre><code>ContextualVersionConflict: (Flask 1.0.3 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('Flask&gt;=1.1.1'), {'gradio'})\n<\/code><\/pre>\n<p>I tried to install the Flask 1.1.1 version but I get more errors. Any idea what I should do to get past this step in Azure ML Studio?<\/p>\n<pre><code>!pip install \u2013force-reinstall Flask==1.1.1\n\/\/ More errors\n<\/code><\/pre>",
        "Challenge_closed_time":1630303052768,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630103458310,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run a 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio. However, when they try to import gradio, tensorflow, and transformers, they encounter a ContextualVersionConflict error related to Flask. They tried to install Flask 1.1.1 version but it resulted in more errors. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1630103807136,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68959934",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":55.442905,
        "Challenge_title":"Contextual version conflict error, Microsoft Azure Machine Learning Studio",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1630103231523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The issue is because <code>gradio<\/code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.<\/p>\n<p>To uninstall the existing package:\n<code>!pip uninstall Flask -y<\/code><\/p>\n<p>To install latest package:\n<code>!pip install Flask&gt;=1.1.1<\/code><\/p>\n<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.<\/strong><\/p>\n<p>Finally, import gradio.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":7.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1634692867416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philippines",
        "Answerer_reputation_count":3105.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":13.8649916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am just curious for more information on what an intermediate step actually is and how to use pruning if you're using a different ml library that isn't in the tutorial section eg) XGB, Pytorch etc.<\/p>\n<p>For example:<\/p>\n<pre><code>X, y = load_iris(return_X_y=True)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)\nclasses = np.unique(y)\nn_train_iter = 100\n\ndef objective(trial):\n    global num_pruned\n    alpha = trial.suggest_float(&quot;alpha&quot;, 0.0, 1.0)\n    clf = SGDClassifier(alpha=alpha)\n    for step in range(n_train_iter):\n        clf.partial_fit(X_train, y_train, classes=classes)\n\n        intermediate_value = clf.score(X_valid, y_valid)\n        trial.report(intermediate_value, step)\n\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n\n    return clf.score(X_valid, y_valid)\n\n\nstudy = optuna.create_study(\n    direction=&quot;maximize&quot;,\n    pruner=optuna.pruners.HyperbandPruner(\n        min_resource=1, max_resource=n_train_iter, reduction_factor=3\n    ),\n)\nstudy.optimize(objective, n_trials=30)\n<\/code><\/pre>\n<p>What is the point of the <code>for step in range()<\/code> section? Doesn't doing this just make the optimisation take more time and won't you yield the same result for every step in the loop?<\/p>\n<p>I'm really trying to figure out the need for <code>for step in range()<\/code> and is it required every time you wish to use pruning?<\/p>",
        "Challenge_closed_time":1637119128723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637069214753,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on intermediate values and pruning in Optuna. They are specifically curious about the purpose of the \"for step in range()\" section in the code and whether it is necessary for using pruning. They also want to know how to use pruning with ML libraries other than those in the tutorial section, such as XGB and Pytorch.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69990009",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":17.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":13.8649916667,
        "Challenge_title":"Understanding Intermediate Values and Pruning in Optuna",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":841.0,
        "Challenge_word_count":154,
        "Platform":"Stack Overflow",
        "Poster_created_time":1482279816096,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":138.0,
        "Poster_view_count":28.0,
        "Solution_body":"<p>The basic model creation can be done by passing a complete training datasets once. But there are models that can still be improved (an increase in accuracy) by re-training again on the same training datasets.<\/p>\n<p>To see to it that we are not wasting resources here, we would check the accuracy after every step using the validation datasets via <code>intermediate_score<\/code> if accuracy improves, if not we prune the whole trial skipping other steps. Then we go for next trial asking another value of alpha - the hyperparameter that we are trying to determine to have the greatest accuracy on the validation datasets.<\/p>\n<p>For other libraries, it is just a matter of asking ourselves what do we want with our model, accuracy for sure is a good criteria to measure the model's competency. There can be others.<\/p>\n<p>Example optuna pruning, I want the model to continue re-training but only at my specific conditions. If intermediate value cannot defeat my best_accuracy and if steps are already more than half of my max iteration then prune this trial.<\/p>\n<pre><code>best_accuracy = 0.0\n\n\ndef objective(trial):\n    global best_accuracy\n\n    alpha = trial.suggest_float(&quot;alpha&quot;, 0.0, 1.0)\n    clf = SGDClassifier(alpha=alpha)\n\n    for step in range(n_train_iter):\n        clf.partial_fit(X_train, y_train, classes=classes)\n\n        if step &gt; n_train_iter\/\/2:\n            intermediate_value = clf.score(X_valid, y_valid)\n\n            if intermediate_value &lt; best_accuracy:\n                raise optuna.TrialPruned()\n\n    best_accuracy = clf.score(X_valid, y_valid)\n\n    return best_accuracy\n<\/code><\/pre>\n<p>Optuna has specialized pruners at <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":22.36,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":221.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":210.9636111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen we use the basic mlflow logging via `with mlflow.start_run(): ...` context manager, we get a better supplementary info about the run (git commit sha, user, filename) rendered in the Tracking UI ([system tags](https:\/\/mlflow.org\/docs\/latest\/tracking.html#system-tags))\r\n\r\nBut when we use `MLFlowLogger` as a logger in pytorch_lightning, this info is not logged. As a user, I'd like to have a mirrored functionality out-of-the-box.\r\n\r\nI inspected the `start_run()` method of mlflow and deduced that the only thing is left while creating the run via MLflowClient is to add `resolve_tags` from the `context` package:\r\n```python\r\n# pytorch_lightning\/loggers\/mlflow.py\r\nfrom mlflow.tracking.context.registry import resolve_tags\r\n...\r\n    def experiment(self) -> MLflowClient:\r\n        if self._run_id is None:\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n```\r\n\r\nI think it's a better idea to add those tags internally (meaning not to expect users doing that manually) as first - it's as seamless as in the default API, secondly - it's the pytorch_lightning that manages the mlflow's run anyways.\r\n\r\n**PR is following ...**",
        "Challenge_closed_time":1617875123000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617115654000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug where using log_gpu_memory with MLFLow logger causes an error due to an unsupported metric name. The expected behavior is for log_gpu_memory to log gpu memory correctly when using an MLFlow logger. The issue was reproduced in a Colab environment with specific versions of CUDA, packages, and system.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6745",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.5,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":210.9636111111,
        "Challenge_title":"mlflow run context is not logged when using MLFlowLogger",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1327234712912,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":53015.0,
        "Answerer_view_count":3262.0,
        "Challenge_adjusted_solved_time":0.0405816667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Basically I'm receiving an output like this from my azure ws output:<\/p>\n\n<pre><code>{\n    'Results': {\n        'WSOutput': {\n            'type': 'table',\n            'value': {\n                'ColumnNames': ['ID', 'Start', 'Ask', 'Not', 'Passed', 'Suggest'],\n                'ColumnTypes': ['Int32', 'Int32', 'Int32', 'Double', 'Int64', 'Int32'],\n                'Values': [['13256025', '25000', '19000', '0.35', '1', '25000']]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The string, as you can see, has the info to create a datatable object. Now, I can't seem to find an easy way to cast it to an actual datatable POCO. I'm able to manually code a parser with Newtonsoft.Json.Linq but there has to be an easier way. <\/p>\n\n<p>Does anybody know how? I can't seem to find anything on the net.<\/p>",
        "Challenge_closed_time":1519929928907,
        "Challenge_comment_count":7,
        "Challenge_created_time":1519929782813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble converting a datatable string received from Azure ML WS into an actual Datatable C# object. The string contains the necessary information to create a datatable object, but the user is unable to find an easy way to cast it. The user has tried manually coding a parser with Newtonsoft.Json.Linq but is looking for an easier solution.",
        "Challenge_last_edit_time":1519930038023,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49056593",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":6.5,
        "Challenge_reading_time":9.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0405816667,
        "Challenge_title":"Convert a datatable string from Azure ML WS to an actual Datatable C# Object?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1324654920387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Waterloo, ON, Canada",
        "Poster_reputation_count":5211.0,
        "Poster_view_count":449.0,
        "Solution_body":"<p>Yes, there is a open source online gernator on the net (<a href=\"http:\/\/jsonutils.com\/\" rel=\"nofollow noreferrer\">http:\/\/jsonutils.com\/<\/a>). Copy paste your result will give you that:<\/p>\n\n<pre><code> public class Value\n    {\n        public IList&lt;string&gt; ColumnNames { get; set; }\n        public IList&lt;string&gt; ColumnTypes { get; set; }\n        public IList&lt;IList&lt;string&gt;&gt; Values { get; set; }\n    }\n\n    public class WSOutput\n    {\n        public string type { get; set; }\n        public Value value { get; set; }\n    }\n\n    public class Results\n    {\n        public WSOutput WSOutput { get; set; }\n    }\n\n    public class Example\n    {\n        public Results Results { get; set; }\n    }\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":7.82,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to understand what is the optimal way in Kedro to convert Spark dataframe coming out of one node into Pandas required as input for another node without creating a redundant conversion step.<\/p>",
        "Challenge_closed_time":1573500781436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573500781437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on the best way to convert a Spark data frame to Pandas and vice versa in Kedro without having to go through a redundant conversion step.",
        "Challenge_last_edit_time":1573553002123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58807540",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to convert Spark data frame to Pandas and back in Kedro?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":800.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1393579668636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":1450.0,
        "Poster_view_count":162.0,
        "Solution_body":"<p>Kedro currently supports 2 strategies for that:<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/04_data_catalog.html#transcoding-datasets\" rel=\"nofollow noreferrer\">Transcoding<\/a> feature<\/h3>\n\n<p>This requires one to define two <code>DataCatalog<\/code> entries for the same dataset, working with the same file in a common format (Parquet, JSON, CSV, etc.), in your <code>catalog.yml<\/code>:<\/p>\n\n<pre><code>my_dataframe@spark:\n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: data\/02_intermediate\/data.parquet\n\nmy_dataframe@pandas:\n  type: ParquetLocalDataSet\n  filepath: data\/02_intermediate\/data.parquet\n<\/code><\/pre>\n\n<p>And then use them in the pipeline like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func1, \"spark_input\", \"my_dataframe@spark\"),\n    node(my_func2, \"my_dataframe@pandas\", \"output\"),\n])\n<\/code><\/pre>\n\n<p>In this case, <code>kedro<\/code> understands that <code>my_dataframe<\/code> is the same dataset in both cases and resolves the node execution order properly. At the same time, <code>kedro<\/code> would use the <code>SparkDataSet<\/code> implementation for saving and <code>ParquetLocalDataSet<\/code> for loading, so the first node should output <code>pyspark.sql.DataFrame<\/code>, while the second node would receive a <code>pandas.Dataframe<\/code>.<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.pandas_to_spark.html\" rel=\"nofollow noreferrer\">Pandas to Spark<\/a> and <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.spark_to_pandas.html\" rel=\"nofollow noreferrer\">Spark to Pandas<\/a> node decorators<\/h3>\n\n<p><strong>Note:<\/strong> <code>Spark &lt;-&gt; Pandas<\/code> in-memory conversion is <a href=\"https:\/\/stackoverflow.com\/a\/47536675\/3364156\">notorious<\/a> for its memory demands, so this is a viable option only if the dataframe is known to be small.<\/p>\n\n<p>One can decorate the node as per the docs:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from spark import get_spark\nfrom kedro.contrib.decorators import pandas_to_spark\n\n@pandas_to_spark(spark_session)\ndef my_func3(data):\n    data.show() # data is pyspark.sql.DataFrame\n<\/code><\/pre>\n\n<p>Or even the whole pipeline:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func4, \"pandas_input\", \"some_output\"),\n    ...\n]).decorate(pandas_to_spark)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1573501243163,
        "Solution_link_count":4.0,
        "Solution_readability":16.3,
        "Solution_reading_time":31.94,
        "Solution_score_count":3.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":207.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1320746685067,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg",
        "Answerer_reputation_count":7552.0,
        "Answerer_view_count":456.0,
        "Challenge_adjusted_solved_time":87.8971722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Challenge_closed_time":1579206604950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579190415880,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job. While calling the function from a Spark context (in a UDF), the user is encountering a serialization error. The error message suggests that the issue is with failing to deserialize the Predictor.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":67.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":51,
        "Challenge_solved_time":4.4969638889,
        "Challenge_title":"Using Sagemaker predictor in a Spark UDF function",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":322.0,
        "Challenge_word_count":474,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1579506845700,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":35.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":288.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.3539691667,
        "Challenge_answer_count":1,
        "Challenge_body":"I am currently utilizing an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. The data consists of 7157 time series with 152 timesteps in the training set and 52 timesteps in the test set respectively. I estimate the run time for the tuning job on this specific instance type to take about 4-5 days. Looking to find out if DeepAR is engineered to take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. Also would be great for recommendations as to which Accelerated Computing instance would be optimal for this scenario.",
        "Challenge_closed_time":1644892187263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644862112974,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is currently using an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. They are looking for advice on whether DeepAR can take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. They are also seeking recommendations for the optimal Accelerated Computing instance for this scenario.",
        "Challenge_last_edit_time":1668563778934,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUnYV-WoO2R3KY4sNEq-Dshw\/optimal-notebook-instance-type-for-deepar-in-aws-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":8.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.3539691667,
        "Challenge_title":"Optimal notebook instance type for DeepAR in AWS Sagemaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":232.0,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"(As detailed further on the [algorithm details page](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html#deepar-instances)), **yes**, the SageMaker DeepAR algorithm implementation is able to train on GPU-accelerated instances to speed up more challenging jobs. There's also a handy [reference table here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/common-info-all-im-models.html) listing all the SageMaker built-in algorithms and whether they're likely to be accelerated with GPU.\n\n**However**, to be clear, it shouldn't be the *notebook* instance type that affects this... Typically when training models on SageMaker, the notebook would provide your interactive compute environment but you'd run training in *training jobs* - for example using the [SageMaker Python SDK](https:\/\/sagemaker.readthedocs.io\/en\/stable\/) `Estimator` class as shown in the sample notebooks for DeepAR [electricity](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb) and [synthetic](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/deepar_synthetic\/deepar_synthetic.ipynb). The instance type you select for training is independent of the instance type you use for your notebook - for example in the electricity notebook it's set as follows:\n\n```python\nestimator = sagemaker.estimator.Estimator(\n    image_uri=image_name,\n    sagemaker_session=sagemaker_session,\n    role=role,\n    train_instance_count=1,  # <-- Setting training instance count\n    train_instance_type=\"ml.c4.2xlarge\",  # <-- Setting training instance type\n    base_job_name=\"deepar-electricity-demo\",\n    output_path=s3_output_path,\n)\n```\n\nSo normally I wouldn't expect you to need to change your *notebook* instance type to speed up training - just edit the configuration of your training job from within the notebook.\n\nSuggesting a particular type is tricky because [DeepAR hyperparameters](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar_hyperparameters.html) like `context_length`, `embedding_dimension`, and `mini_batch_size` will affect how much GPU capacity is needed for a particular run. Since you're coming from CPU-only baseline, I'd maybe suggest to start small with trying out single-GPU `g4dn.xlarge`, `g5.xlarge` or `p3.2xlarge` instances, perhaps starting with the lowest cost-per-hour? You can keep  an eye on your jobs' GPUUtilization and GPUMemoryUtilization metrics to check whether utilization is low on instances like p3 with \"bigger\" GPUs. Increasing `mini_batch_size` should help fill extra capacity on these and complete your job faster, but it will probably affect model convergence - so may need to tune other parameters like `learning_rate` to try and compensate. So considering all of this, you may find trade-offs between speed and total cost, or speed and accuracy, for good hyperparameter combinations on your dataset. Of course you could also scale up to multi-GPU instance types if you'd like to accelerate further.\n\nIf I understood right you're also using SageMaker Automatic Hyperparameter Tuning to search these parameters, something like [this XGBoost notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/xgboost_random_log\/hpo_xgboost_random_log.ipynb) with the `HyperparameterTuner` class?\n\nIn that case would also mention:\n- Increasing the `max_parallel_jobs` parameter may accelerate the overall run time (by running more of the individual training jobs in parallel) - with a trade-off on how much information is available when each training job in the budget is kicked off.\n- If you're planning to run this training regularly on a dataset which evolves over time, you probably don't need to run HPO each time: Will likely see good results using your previously-optimized hyperparameters, unless something materially changes in the nature of the data and patterns.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1644892187264,
        "Solution_link_count":7.0,
        "Solution_readability":16.5,
        "Solution_reading_time":50.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":448.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1598606826112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":222.6413147222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using sagemaker 2.5.1 and tensorflow 2.3.0\nThe weird part is that the same code worked before, the only change that I could think of is the new release of the two libraries<\/p>",
        "Challenge_closed_time":1599662565023,
        "Challenge_comment_count":1,
        "Challenge_created_time":1598861056290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to save a TensorFlow model in an S3 bucket using SageMaker 2.5.1. The error message 'KeyError: 'callable_inputs'' is being displayed. The user suspects that the issue might be related to the recent release of the two libraries.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63667022",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.3,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":222.6413147222,
        "Challenge_title":"Getting KeyError : 'callable_inputs' when trying to save a TF model in S3 bucket",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":174.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1598606826112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The problem is actually coming from smdebug version 0.9.1\nDowngrading to 0.8.1 solves the issue<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":15.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":182.1773088889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Someone should add \"net#\" as a tag. I'm trying to improve my neural network in Azure Machine Learning Studio by turning it into a convolution neural net using this tutorial:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2\" rel=\"noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2<\/a><\/p>\n\n<p>The differences between mine and the tutorial is I'm doing regression with 35 features and 1 label and they're doing classification with 28x28 features and 10 labels. <\/p>\n\n<p>I start with the basic and 2nd example and get them working with:<\/p>\n\n<pre><code>input Data [35];\n\nhidden H1 [100]\n    from Data all;\n\nhidden H2 [100]\n    from H1 all;\n\noutput Result [1] linear\n    from H2 all;\n<\/code><\/pre>\n\n<p>Now the transformation to convolution I misunderstand. In the tutorial and documentation here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide<\/a> it doesn't mention how the node tuple values are calculated for the hidden layers. The tutorial says:<\/p>\n\n<pre><code>hidden C1 [5, 12, 12]\n  from Picture convolve {\n    InputShape  = [28, 28];\n    KernelShape = [ 5,  5];\n    Stride      = [ 2,  2];\n    MapCount = 5;\n  }\n\nhidden C2 [50, 4, 4]\n   from C1 convolve {\n     InputShape  = [ 5, 12, 12];\n     KernelShape = [ 1,  5,  5];\n     Stride      = [ 1,  2,  2];\n     Sharing     = [ F,  T,  T];\n     MapCount = 10;\n  }\n<\/code><\/pre>\n\n<p>Seems like the [5, 12, 12] and [50,4,4] pop out of no where along with the KernalShape, Stride, and MapCount. How do I know what values are valid for my example? I tried using the same values, but it didn't work and I have a feeling since he has a [28,28] input and I have a [35], I need tuples with 2 integers not 3. <\/p>\n\n<p>I just tried with random values that seem to correlate with the tutorial:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 23]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [200, 6]\n   from C1 convolve {\n     InputShape  = [ 7, 23];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>Right now it seems impossible to debug because the only error code Azure Machine Learning Studio ever gives is:<\/p>\n\n<pre><code>Exception\":{\"ErrorId\":\"LibraryException\",\"ErrorCode\":\"1000\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\",\"Exception\":{\"Library\":\"TLC\",\"ExceptionType\":\"LibraryException\",\"Message\":\"Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\"}}}Error: Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown. Process exited with error code -2\n<\/code><\/pre>\n\n<p>Lastly my setup is <a href=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" alt=\"Azure Machine Learning Setup\"><\/a> <\/p>\n\n<p>Thanks for the help!<\/p>",
        "Challenge_closed_time":1503342256368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1502257048253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to build a convolution neural net in Azure Machine Learning Studio using a tutorial, but is facing challenges in understanding how to transform the basic and second example into convolution. The tutorial does not mention how the node tuple values are calculated for the hidden layers, and the user is unsure of what values are valid for their example. The user has tried using random values that seem to correlate with the tutorial, but is unable to debug due to the error code \"Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\"",
        "Challenge_last_edit_time":1502686418056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45582412",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":41.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":9.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":301.4466986111,
        "Challenge_title":"How to build a Convolution Neural Net in Azure Machine Learning?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1268.0,
        "Challenge_word_count":393,
        "Platform":"Stack Overflow",
        "Poster_created_time":1418505926276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Missouri",
        "Poster_reputation_count":1454.0,
        "Poster_view_count":328.0,
        "Solution_body":"<p>The correct network definition for 35-column length input with given kernels and strides would be following:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 15]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [14, 7, 5]\n   from C1 convolve {\n     InputShape  = [ 7, 15];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>First, the C1 = [7,15]. The first dimension is simply the MapCount. For the second dimension, the kernel shape defines the length of the \"window\" that's used to scan the input columns, and the stride defines how much it moves at each step. So the kernel windows would cover columns 1-7, 3-9, 5-11,...,29-35, yielding the second dimension of 15 when you tally the windows.<\/p>\n\n<p>Next, the C2 = [14,7,5]. The first dimension is again the MapCount. For the second and third dimension, the 1-by-7 kernel \"window\" has to cover the input size of 7-by-15, using steps of 1 and 2 along corresponding dimensions. <\/p>\n\n<p>Note that you could specify C2 hidden layer shape of [98,5] or even [490], if you wanted to flatten the outputs. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":14.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":199.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":1.0551408334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to save Kedro memory dataset in azure as a file and still want to have it in memory as my pipeline will be using this later in the pipeline. Is this possible in Kedro. I tried to look at Transcoding datasets but looks like not possible. Is there any other way to acheive this?<\/p>",
        "Challenge_closed_time":1642520713270,
        "Challenge_comment_count":2,
        "Challenge_created_time":1642516914763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to save a Kedro memory dataset in Azure as a file while still keeping it in memory for later use in their pipeline. They have looked into transcoding datasets but have not found a solution. They are seeking advice on alternative methods to achieve this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70757448",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.1,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.0551408334,
        "Challenge_title":"How to save kedro dataset in azure and still have it in memory",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":231.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>This may be a good opportunity to use <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\">CachedDataSet<\/a> this allows you to wrap any other dataset, but once it's read into memory - make it available to downstream nodes without re-performing the IO operations.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.0,
        "Solution_reading_time":4.16,
        "Solution_score_count":4.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":37.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1488536112480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":349.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":0.1712805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy the existing breast cancer prediction model on Amazon Sagemanker using AWS Lambda and API gateway. I have followed the official documentation from the below url.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/<\/a><\/p>\n\n<p>I am getting a type error at \"predicted_label\".<\/p>\n\n<pre><code> result = json.loads(response['Body'].read().decode())\n print(result)\n pred = int(result['predictions'][0]['predicted_label'])\n predicted_label = 'M' if pred == 1 else 'B'\n\n return predicted_label\n<\/code><\/pre>\n\n<p>please let me know if someone could resolve this issue. Thank you. <\/p>",
        "Challenge_closed_time":1538387613807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538386997197,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to deploy an existing breast cancer prediction model on Amazon Sagemaker using AWS Lambda and API gateway. The user is getting a type error at \"predicted_label\" and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1562245989407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52588354",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":12.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.1712805556,
        "Challenge_title":"How to deploy breast cancer prediction endpoint created by AWS Sagemaker using Lambda and API gateway?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":502.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456483381212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":999.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>By printing the result type by <code>print(type(result))<\/code> you can see its a dictionary. now you can see the key name is \"score\" instead of \"predicted_label\" that you are giving to pred. Hence replace it with<\/p>\n\n<pre><code>pred = int(result['predictions'][0]['score'])\n<\/code><\/pre>\n\n<p>I think this solves your problem.<\/p>\n\n<p>here is my lambda function:<\/p>\n\n<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n   print(\"Received event: \" + json.dumps(event, indent=2))\n\n   data = json.loads(json.dumps(event))\n   payload = data['data']\n   print(payload)\n\n   response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                      ContentType='text\/csv',\n                                      Body=payload)\n   #print(response)\n   print(type(response))\n   for key,value in response.items():\n       print(key,value)\n   result = json.loads(response['Body'].read().decode())\n   print(type(result))\n   print(result['predictions'])\n   pred = int(result['predictions'][0]['score'])\n   print(pred)\n   predicted_label = 'M' if pred == 1 else 'B'\n\n   return predicted_label\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.3,
        "Solution_reading_time":15.19,
        "Solution_score_count":4.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1472967821507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":123.3870294444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Similar to the issue of <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/200\" rel=\"nofollow noreferrer\">The trained model can be deployed on the other platform without dependency of sagemaker or aws service?<\/a>.<\/p>\n\n<p>I have trained a model on AWS SageMaker by using the built-in algorithm <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">Semantic Segmentation<\/a>. This trained model named as <code>model.tar.gz<\/code> is stored on S3. So I want to download this file from S3 and then use it to make inference on my local PC without using AWS SageMaker anymore. Since the built-in algorithm Semantic Segmentation is built using the <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\" rel=\"nofollow noreferrer\">MXNet Gluon framework and the Gluon CV toolkit<\/a>, so I try to refer the documentation of <a href=\"https:\/\/mxnet.apache.org\/\" rel=\"nofollow noreferrer\">mxnet<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\">gluon-cv<\/a> to make inference on local PC.<\/p>\n\n<p>It's easy to download this file from S3, and then I unzip this file to get three files:<\/p>\n\n<ol>\n<li><strong>hyperparams.json<\/strong>: includes the parameters for network architecture, data inputs, and training. Refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/segmentation-hyperparameters.html\" rel=\"nofollow noreferrer\">Semantic Segmentation Hyperparameters<\/a>.<\/li>\n<li><strong>model_algo-1<\/strong><\/li>\n<li><strong>model_best.params<\/strong><\/li>\n<\/ol>\n\n<p>Both <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong> are the trained models, and I think it's the output from <code>net.save_parameters<\/code> (Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/4-train.html\" rel=\"nofollow noreferrer\">Train the neural network<\/a>). I can also load them with the function <code>mxnet.ndarray.load<\/code>.<\/p>\n\n<p>Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/5-predict.html\" rel=\"nofollow noreferrer\">Predict with a pre-trained model<\/a>. I found there are two necessary things:<\/p>\n\n<ol>\n<li>Reconstruct the network for making inference.<\/li>\n<li>Load the trained parameters.<\/li>\n<\/ol>\n\n<p>As for reconstructing the network for making inference, since I have used PSPNet from training, so I can use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network. And I know how to use some services of AWS SageMaker, for example batch transform jobs, to make inference. I want to reproduce it on my local PC. If I use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network, I can't make sure whether the parameters for this network are same those used on AWS SageMaker while making inference. Because I can't see the image <code>501404015308.dkr.ecr.ap-northeast-1.amazonaws.com\/semantic-segmentation:latest<\/code> in detail. <\/p>\n\n<p>As for loading the trained parameters, I can use the <code>load_parameters<\/code>. But as for <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong>, I don't know which one I should use.<\/p>",
        "Challenge_closed_time":1583126137776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582681249783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model on AWS SageMaker using the built-in algorithm Semantic Segmentation and wants to download the model from S3 to make inference on their local PC without using AWS SageMaker. They have downloaded the model files from S3 and are trying to reconstruct the network for making inference using the class gluoncv.model_zoo.PSPNet, but are unsure if the parameters for this network are the same as those used on AWS SageMaker while making inference. They are also unsure which of the two trained model files, model_algo-1 and model_best.params, to use for loading the trained parameters.",
        "Challenge_last_edit_time":1582681944470,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60405600",
        "Challenge_link_count":8,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":42.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":123.5799980556,
        "Challenge_title":"How to make inference on local PC with the model trained on AWS SageMaker by using the built-in algorithm Semantic Segmentation?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":351.0,
        "Challenge_word_count":365,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472967821507,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tokyo, Japan",
        "Poster_reputation_count":61.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>The following code works well for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mxnet as mx\nfrom mxnet import image\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\nimport gluoncv\n\n# use cpu\nctx = mx.cpu(0)\n\n# load test image\nimg = image.imread('.\/img\/IMG_4015.jpg')\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\n\n# reconstruct the PSP network model\nmodel = gluoncv.model_zoo.PSPNet(2)\n\n# load the trained model\nmodel.load_parameters('.\/model\/model_algo-1')\n\n# make inference\noutput = model.predict(img)\npredict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":8.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3588888889,
        "Challenge_answer_count":2,
        "Challenge_body":"A customer has a question about data sources \n\n> \u201cmost of our data is stored in SQL databases, while the SageMaker docs\n> say that I have to put it all in S3. It\u2019s not obvious what the best\n> way to do this is. I can think for example of splitting my analysis\n> code in two; one pre-processing step to go from SQL queries to tabular\n> data, and e.g. store that as Parquet files. For high-dimensional\n> tensor data it\u2019s even less obvious.\u201d\n\nCan someone comment on that?",
        "Challenge_closed_time":1533318766000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533317474000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with using Sagemaker as their data is stored in SQL databases while Sagemaker requires data to be stored in S3. The user is unsure of the best way to handle this and is considering splitting their analysis code into two steps. They are also unsure of how to handle high-dimensional tensor data.",
        "Challenge_last_edit_time":1668289581924,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUh_P30-iXTKmzZv0D4vtLOA\/sagemaker-and-data-on-databases",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3588888889,
        "Challenge_title":"Sagemaker and Data on Databases",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":348.0,
        "Challenge_word_count":89,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"We have an example notebook for interacting from Redshift data from a SageMaker managed notebook, which I believe is suitable for an Exploratory Data Analysis (EDA) use-case: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/working_with_redshift_data\/working_with_redshift_data.ipynb\n\nFor production purposes, the customer should consider separating the job of first extracting data from relational databases to S3 (to build out a data lake), and then using that for downstream processing\/machine learning (including SageMaker, EMR, Athena, Spectrum, etc.). Customers can build extraction pipelines from popular relational databases using AWS Glue, EMR, or their preferred ETL engines like those on the AWS Marketplace.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925552580,
        "Solution_link_count":1.0,
        "Solution_readability":19.8,
        "Solution_reading_time":9.8,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.3833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"In this article, learn how to build an end-to-end data to AI solution on Google Cloud, including a practical example of a real-time fraud detection system and the architecture behind it. You'll also discover how to train, deploy, and monitor machine learning models in production.\n\nThis article is based on a recent Cloud OnBoard session.\u00a0Register here to watch on demand.\u00a0\u00a0\u00a0\n\nIf you have any questions, please leave a comment on the blog (or below) and someone from the Community or Google Cloud team will be happy to help.\n\nRead the blog",
        "Challenge_closed_time":1684833900000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684756920000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The article discusses how to create a data to AI solution on Google Cloud, with a focus on building a real-time fraud detection system. It covers the architecture, training, deployment, and monitoring of machine learning models in production. The reader is encouraged to ask questions in the comments section for assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-build-a-data-to-AI-solution-with-BigQuery-and-Vertex-AI\/m-p\/595708#M1991",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":21.3833333333,
        "Challenge_title":"How to build a data to AI solution with BigQuery and Vertex AI",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":111.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Great read!\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":0.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.3347222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, I need to run XGBoost inferences on 15MM samples (3.9Gb when stored as csv). Since Batch transform does not seem to work on such large batches (max payload 100MB) I split my input file into 646 files, each around 6Mb, stored in S3. I am running the code below:\n\n    transformer = XGB.transformer(\n        instance_count=2, instance_type='ml.c5.9xlarge',\n        output_path='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/xgbtransform\/',\n        max_payload=100)\n\n    transformer.transform(\n        data='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/testchunks\/',\n        split_type='Line')\n\nBut the job fails - Sagemaker tells \"ClientError: Too many objects failed. See logs for more information\" and cloudwatch logs show:\n\n    Bad HTTP status returned from invoke: 415\n    'NoneType' object has no attribute 'lower'\n\nDid I forget something in my batch transform settings?",
        "Challenge_closed_time":1532634125000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532625720000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a 415 error while running XGBoost inferences on 15MM samples using Sagemaker batch transform. The error occurs despite splitting the input file into 646 files, each around 6Mb, stored in S3. The error message suggests that there may be an issue with the batch transform settings.",
        "Challenge_last_edit_time":1668556023886,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUr4Vq95ScROqSguzxNQYDOg\/sagemaker-batch-transform-415-error",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":2.3347222222,
        "Challenge_title":"Sagemaker batch transform 415 error",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":568.0,
        "Challenge_word_count":105,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This indicates that the algorithm thinks it has been passed bad data. Perhaps a problem with your splitting?\n\nI would suggest two things: \n\n1. Try running the algorithm on the original data using the `\"SplitType\": \"Line\"` and `\"BatchStrategy\": \"MultiRecord\"` arguments and see if you have better luck.\n2. Look in the cloudwatch logs for your run and see if  there's any helpful information about what the algorithm didn't like. You can find these in the log group \"\/aws\/sagemaker\/TransformJobs\" in the log stream that begins with your job name.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925589052,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":6.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1462800865783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":316.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":1192.9984197222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We were using kedro version 0.15.8 and we were loading one specific item from the catalog this way:<\/p>\n<pre><code>from kedro.context import load_context\nget_context().catalog.datasets.__dict__[key]\n<\/code><\/pre>\n<p>Now, we are changing to kedro 0.17.0 and trying to load the catalogs datasets the same way(using the framework context):<\/p>\n<pre><code>from kedro.framework.context import load_context\nget_context().catalog.datasets.__dict__[key]\n<\/code><\/pre>\n<p>And now we get the error:<\/p>\n<blockquote>\n<p>kedro.framework.context.context.KedroContextError: Expected an instance of <code>ConfigLoader<\/code>, got <code>NoneType<\/code> instead.<\/p>\n<\/blockquote>\n<p>It's because the hook register_config_loader from the project it's not being used by the hook_manager that calls the function.<\/p>\n<p>The project hooks are the defined the following way:<\/p>\n<pre><code>class ProjectHooks:\n\n    @hook_impl\n\n    def register_pipelines(self) -&gt; Dict[str, Pipeline]:\n\n        &quot;&quot;&quot;Register the project's pipeline.\n\n        Returns:\n\n            A mapping from a pipeline name to a ``Pipeline`` object.\n\n        &quot;&quot;&quot;\n\n        pm = pre_master.create_pipeline()\n\n        return {\n\n            &quot;pre_master&quot;: pm,\n\n            &quot;__default__&quot;: pm\n\n        }\n\n    @hook_impl\n\n    def register_config_loader(self, conf_paths: Iterable[str]) -&gt; ConfigLoader:\n\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n\n    def register_catalog(\n\n        self,\n\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n\n        credentials: Dict[str, Dict[str, Any]],\n\n        load_versions: Dict[str, str],\n\n        save_version: str,\n\n        journal: Journal,\n\n    ) -&gt; DataCatalog:\n\n        return DataCatalog.from_config(\n\n            catalog, credentials, load_versions, save_version, journal\n\n        )\n\nproject_hooks = ProjectHooks()\n<\/code><\/pre>\n<p>And the settings are called the following way:\n&quot;&quot;&quot;Project settings.&quot;&quot;&quot;<\/p>\n<pre><code>from price_based_trading.hooks import ProjectHooks\n\n\nHOOKS = (ProjectHooks(),)\n<\/code><\/pre>\n<p>How can we configure that in a way that the hooks are used calling the method load_context(_working_dir).catalog.datasets ?<\/p>\n<p>I posted the same question in the kedro community: <a href=\"https:\/\/discourse.kedro.community\/t\/how-to-load-a-specific-catalog-item-in-kedro-0-17-0\/310\" rel=\"nofollow noreferrer\">https:\/\/discourse.kedro.community\/t\/how-to-load-a-specific-catalog-item-in-kedro-0-17-0\/310<\/a><\/p>",
        "Challenge_closed_time":1612452830192,
        "Challenge_comment_count":3,
        "Challenge_created_time":1612267210303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to load a specific item from the catalog in Kedro 0.17.0 using the framework context. They were previously using Kedro version 0.15.8 and loading the item using a different method. The error message suggests that the hook register_config_loader from the project is not being used by the hook_manager that calls the function. The user has posted the same question in the Kedro community seeking help to configure the hooks in a way that they can be used to call the method load_context(_working_dir).catalog.datasets.",
        "Challenge_last_edit_time":1612461225272,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66009324",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":15.0,
        "Challenge_reading_time":31.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":51.5610802778,
        "Challenge_title":"How to load a specific catalog dataset instance in kedro 0.17.0?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1298.0,
        "Challenge_word_count":224,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462800865783,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":316.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>It was a silly mistake because I was not creating the Kedro session. To load an item of the catalog it can be done with the following code:<\/p>\n<pre><code>from kedro.framework.session import get_current_session\nfrom kedro.framework.session import KedroSession\n\nKedroSession.create(&quot;name_of_proyect&quot;) as session:\n    key = &quot;item_of_catalog&quot;\n    session = get_current_session()\n    context = session.load_context()\n    kedro_connector = context.catalog.datasets.__dict__[key] \n    \/\/ or kedro_connector = context.catalog._get_datasets(key)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1616756019583,
        "Solution_link_count":0.0,
        "Solution_readability":15.6,
        "Solution_reading_time":7.29,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":51.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":336.4637269444,
        "Challenge_answer_count":8,
        "Challenge_body":"<p>Is it possible to parse additional arguments to the program when running the wandb agent command from the command line?<\/p>\n<p>For example, suppose I have a script <code>train.py<\/code> that takes a <code>--gpu_idx<\/code> argument to specify the GPU index, and I want to run the script with different GPUs using the WandB agent. Can I pass the <code>--gpu_idx<\/code> argument as a key-value pair when running the <code>wandb agent<\/code> command?<\/p>\n<p><code>wandb agent &lt;ID&gt;  --gpu_idx 1<\/code><\/p>\n<p>In the training script, I have something like:<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--gpu_idx', type=int, default=1)\nargs = parser.parse_args()\n\nwandb.init()\nwandb.config.update(args)\n\n# train model with the learning rate\n<\/code><\/pre>",
        "Challenge_closed_time":1679344668276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678133398859,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is asking if it is possible to pass additional arguments to a program when running the wandb agent command from the command line. Specifically, they want to know if they can pass a --gpu_idx argument as a key-value pair when running the wandb agent command to run a script called train.py with different GPUs. They have provided a code snippet of the training script that includes an argparse module to parse the --gpu_idx argument.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/parse-additional-arguments-to-the-program-when-running-the-wandb-agent-command-from-the-command-line\/4010",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":336.4637269444,
        "Challenge_title":"Parse additional arguments to the program when running the wandb agent command from the command line",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":348.0,
        "Challenge_word_count":117,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/liu97\">@liu97<\/a> thanks for the additional context. In that case you won\u2019t be able to do this as you will be using the same <code>yaml<\/code> for all agents. What you could do though in a multi-gpu environment is to specify the GPU as follows:<\/p>\n<pre><code class=\"lang-auto\">CUDA_VISIBLE_DEVICES=0 wandb agent sweep_ID\n<\/code><\/pre>\n<p>Would this work for you? Please also check <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/parallelize-agents#parallelize-on-a-multi-gpu-machine\">this docs page<\/a> for more information.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.7,
        "Solution_reading_time":7.26,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1384795490587,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1012.0,
        "Answerer_view_count":102.0,
        "Challenge_adjusted_solved_time":2.9863186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Having some difficulty executing the following code in AWS SageMaker. It is supposed to just list all of the tables in DynamoDB.<\/p>\n\n<pre><code>import boto3\nresource = boto3.resource('dynamodb', region_name='xxxx')\nresponse  = resource.tables.all()\nfor r in response:\n    print(r.name)\n<\/code><\/pre>\n\n<p>If the SageMaker notebook kernel is set to \"conda_python3\" the code executes fine and the tables are listed out in the notebook as expected (this happens pretty much instantly).<\/p>\n\n<p>However, if I set the kernel to \"Sparkmagic (PySpark)\" the same code infinitely runs and doesn't output the table list at all.<\/p>\n\n<p>Does anyone know why this would happen for the PySpark kernel but not for the conda3 kernel? Ideally I need to run this code as part of a bigger script that relies on PySpark, so would like to get it working with PySpark.<\/p>",
        "Challenge_closed_time":1574430450807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574419108777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in executing a code in AWS SageMaker notebook that is supposed to list all the tables in DynamoDB using boto3 and PySpark. The code executes fine when the kernel is set to \"conda_python3\" but infinitely runs and doesn't output the table list when the kernel is set to \"Sparkmagic (PySpark)\". The user is seeking help to understand why this is happening and how to get it working with PySpark.",
        "Challenge_last_edit_time":1574419700060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58992447",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":11.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3.1505638889,
        "Challenge_title":"AWS SageMaker notebook list tables using boto3 and PySpark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":440.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384795490587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1012.0,
        "Poster_view_count":102.0,
        "Solution_body":"<p>Figured out what the issue was, you need to end an endpoint to tour VPC for DyanmoDB.<\/p>\n\n<p>To do this navigate to:<\/p>\n\n<ol>\n<li>AWS VPC<\/li>\n<li>Endpoints<\/li>\n<li>Create Endpoint<\/li>\n<li>Select the dynamodb service (will be type Gateway)<\/li>\n<li>Select the VPC your Notebook is using<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":3.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1444940402827,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Oakland, CA, USA",
        "Answerer_reputation_count":8474.0,
        "Answerer_view_count":1313.0,
        "Challenge_adjusted_solved_time":10.0860825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running some projects with H2o AutoML using Sagemaker notebook instances, and I would like to know if H2o AutoML can benefit from a GPU Sagemaker instance, if so, how should I configure the notebook? <\/p>",
        "Challenge_closed_time":1567727347088,
        "Challenge_comment_count":4,
        "Challenge_created_time":1567711533230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether H2o AutoML can benefit from a GPU Sagemaker instance and how to configure the notebook if so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57811873",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.3927383333,
        "Challenge_title":"Can H2o AutoML benefit from a GPU instance on Sagemaker platform?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":462.0,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p><a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\" rel=\"nofollow noreferrer\">H2O AutoML<\/a> contains a handful of algorithms and one of them is <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/xgboost.html\" rel=\"nofollow noreferrer\">XGBoost<\/a>, which has been part of H2O AutoML since H2O version 3.22.0.1.  XGBoost is the only GPU-capable algorithm inside of H2O AutoML, however, a lot of the models that are trained in AutoML are XGBoost models, so it still can be useful to utilize a GPU. Keep in mind that you must use H2O 3.22 or above to use this feature.<\/p>\n\n<p>My suggestion is to test it on a GPU-enabled instance and compare the results to a non-GPU instance and see if it's worth the extra cost.  <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1567747843127,
        "Solution_link_count":2.0,
        "Solution_readability":10.2,
        "Solution_reading_time":9.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.6708936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm a novice to Azure and Azure ML, I'm trying to create a batch endpoint and in most of the documentation I found it was mentioned that the input to the batch endpoint would be a file path. In my case I was to connect the endpoint to two blobstorage and get a tuple as input to the batch endpoint. Is it possible? If not is there any other work around as my model takes as input path to two files.<\/p>",
        "Challenge_closed_time":1661915045907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661847830690,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is a novice to Azure and Azure ML and is trying to create a batch endpoint. They want to connect the endpoint to two blob storages and get a tuple as input to the batch endpoint. They are asking if it is possible or if there is any other workaround as their model takes as input path to two files.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/986878\/azureml-in-batch-endpoints-can-we-use-a-tuple(two",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":5.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":18.6708936111,
        "Challenge_title":"AzureML: In Batch Endpoints can we use a tuple(two file paths from different folders) as input?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=d2cc8057-ef7d-498e-944d-421a8e30bd64\">@Samarjeet Singh Patil  <\/a>    <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform! I am sorry you can not use multiple resource as one input for one job, but you can do it in separate jobs.    <\/p>\n<p>An alternative way\/ workaround for you is a FileDataset, you can put your resource into one dataset and then input it.     <\/p>\n<p>FileDataset -     <br \/>\n<em>Represents a collection of file references in datastores or public URLs to use in Azure Machine Learning.    <br \/>\nA FileDataset defines a series of lazily-evaluated, immutable operations to load data from the data source into file streams. Data is not loaded from the source until FileDataset is asked to deliver data.    <br \/>\nA FileDataset is created using the from_files method of the FileDatasetFactory class.    <br \/>\nFor more information, see the article Add &amp; register datasets. To get started working with a file dataset, see <a href=\"https:\/\/aka.ms\/filedataset-samplenotebook.\">https:\/\/aka.ms\/filedataset-samplenotebook.<\/a><\/em>    <\/p>\n<p>Reference - <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.filedataset?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.filedataset?view=azure-ml-py<\/a>    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-batch-endpoints-studio#start-a-batch-scoring-job-with-different-input-options\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-batch-endpoints-studio#start-a-batch-scoring-job-with-different-input-options<\/a>     <\/p>\n<p>I hope it helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.9,
        "Solution_reading_time":23.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":181.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.7666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"I am doing a comparative analysis of predictive analytics software for my Project. I am looking for approximate lines of code for the Google Vertex AI product.",
        "Challenge_closed_time":1638190740000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638115980000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is conducting a comparative analysis of predictive analytics software for their project and is seeking information on the approximate lines of code for Google Vertex AI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-anyone-tell-what-is-the-approximate-SLOC-of-Google-Vertex-AI\/m-p\/176629#M96",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":3.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":20.7666666667,
        "Challenge_title":"Can anyone tell what is the approximate SLOC of Google Vertex AI? For my comparative analysis study.",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,\u00a0\n\nSLOC will be different depending on your specific use case, and you should be in the best position to figure out the approximate SLOC for your scenarios. That being said, you might look into the sample code and notebooks for Vertex AI, the end-to-end machine learning platform on Google Cloud at [1].\n\n[1]\u00a0https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":4.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":9.6420455556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Challenge_closed_time":1565705412680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565670597500,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to use GPU to train a neural network model in Azure Machine Learning Service using P100-NC6s-V2 compute. The training is happening on the CPU and the logs show that it's not able to find CUDA. The user has mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. The warning log shows that it fails to use the GPU and is currently using the CPU. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1565670701316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57471129",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":57.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":9.6708833333,
        "Challenge_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1402.0,
        "Challenge_word_count":396,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408145271463,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.2,
        "Solution_reading_time":11.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1463515446747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":15335.0,
        "Answerer_view_count":1991.0,
        "Challenge_adjusted_solved_time":4.5067230556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>please be easy on me, I am new to ML. I am sure somebody will request to close this as subjective but I cannot find my specific answer and don't know how else to ask. <\/p>\n\n<p>If I have a shop, with three areas of the shop. I have sensors to detect when people come in or out of each area. This happens every 15 seconds. So, in my db, I have a count of the occupancy, per room, every 15 seconds. <\/p>\n\n<p>Using this data, I want to predict the occupancy, per room, in the future but also, if somebody comes in the door, predict most likely room they will go to. <\/p>\n\n<p>Is it possible to predict future occupancy per room and also probability of where people will go when the walk in using a dataset that simply lists the rooms and the occupancy of each room every 15 seconds? Is this a regression model?<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Mike<\/p>",
        "Challenge_closed_time":1544846023056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544840911773,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to machine learning and has a dataset that lists the occupancy of three different rooms in a shop every 15 seconds. They want to predict future occupancy per room and the probability of where people will go when they walk in. They are unsure if this is possible and if it requires a regression model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53789057",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.4198008333,
        "Challenge_title":"Is it possible to get two different types of results from dataset",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":170,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455012473430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Temecula, CA, United States",
        "Poster_reputation_count":1652.0,
        "Poster_view_count":198.0,
        "Solution_body":"<p>Predicting the most likely room, which they would walk in. :<\/p>\n\n<p>This falls under the classification problem. The output falls under a set of categories, in this case it is different rooms.<\/p>\n\n<p>Predicting the Occpancy of each room :\nAs mentioned by @poorna is a regression problem. <\/p>\n\n<p>Two ways you can look at this problem, <\/p>\n\n<ol>\n<li><p>Multi- target regression problem with occupancy of each room as one target and past occupancies of all rooms as input. <\/p><\/li>\n<li><p>Independent forecast problem for each room with past occupancies of corresponding room as input.<\/p><\/li>\n<\/ol>\n\n<p>For learning the basics of machine learning, you can go through this <a href=\"https:\/\/scikit-learn.org\/stable\/tutorial\/basic\/tutorial.html\" rel=\"nofollow noreferrer\">link<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1544857135976,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.1666666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi there,\n\nI'm trying to dynamically append values to a list parameter using the 'Parameter Preset' box when certain conditions are met for a transitional route.\u00a0\n\nThe list parameter is defined as follows:\u00a0\n\nThen, during the route I use the following system function:\n\n\u00a0\n\n$sys.func.APPEND($session.params.negative_products, credit score)\n\n\u00a0\n\n\u2003However, when the condition is met during the conversation, the updated value in $session.params.negative_products is not: [\" \", credit score], but prints out the whole text in the parameter preset box, e.g.,\u00a0$sys.func.APPEND($session.params.negative_products, credit score).\u00a0\n\nWhy is this the case? I thought system functions were able to be used to dynamically change values in parameter presets and I have no idea why it just keeps on printing out the function as a string!\u00a0\n\nAny help would be much appreciated,\n\nVicky",
        "Challenge_closed_time":1674784980000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674629580000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to append values to a list parameter using the 'Parameter Preset' box when certain conditions are met for a transitional route. The system function used to append values is not working as expected and instead of updating the value in $session.params.negative_products, it prints out the whole text in the parameter preset box. The user is seeking help to understand why this is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issue-with-using-system-functions-in-parameter-presets\/m-p\/513851#M1135",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.3,
        "Challenge_reading_time":11.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":43.1666666667,
        "Challenge_title":"Issue with using system functions in parameter presets",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":147.0,
        "Challenge_word_count":131,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey!\n\nWhen referencing the parameter in my fulfilment I use:\u00a0\n\n$session.params.negative_products\n\n\u00a0I did figure a workaround yesterday. I realised when I define $session.params.negative_products within the 'Entry Fulfilment', 'Parameter Presets' section of the page the appending works:\u00a0\n\n1. Define empty list parameter in presets of the Entry Fulfilment, i.e., before I want to append to the list:\u00a0\n\n\u20032. During a conditional route, append to the empty list using system functions in the parameter presets with the following command:\u00a0\n\n$sys.func.APPEND($session.params.negative_products, \"credit score\")\n\n3. This then prints out the correct output when referencing $session.params.negative_products:\u00a0\n\n\" , credit score\"\n\nI think my original issue was that when trying to define $session.params.negative_products as a 'isList' parameter, it wasn't actually generating a list per se. So, when I was trying to call it in $sys.func.APPEND(), the function wasn't reading the $session.params.negative_products as a list, and as a result, just printed out the system command.\u00a0\n\nHope this helps other users who get faced with a similar issue!\n\nVicky\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":14.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":164.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":27.9829091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to perform hyperparameter search using AzureML. My models are small (around 1GB) thus I would like to run multiple models on the same GPU\/node to save costs but I do not know how to achieve this.<\/p>\n<p>The way I currently submit jobs is the following (resulting in one training run per GPU\/node):<\/p>\n<pre><code>experiment = Experiment(workspace, experiment_name)\nconfig = ScriptRunConfig(source_directory=&quot;.\/src&quot;,\n                         script=&quot;train.py&quot;,\n                         compute_target=&quot;gpu_cluster&quot;,\n                         environment=&quot;env_name&quot;,\n                         arguments=[&quot;--args args&quot;])\nrun = experiment.submit(config)\n<\/code><\/pre>\n<p><code>ScriptRunConfig<\/code> can be provided with a <code>distributed_job_config<\/code>. I tried to use <code>MpiConfiguration<\/code> there but if this is done the run fails due to an MPI error that reads as if the cluster is configured to only allow one run per node:<\/p>\n<blockquote>\n<pre><code>Open RTE detected a bad parameter in hostfile: [...]\nThe max_slots parameter is less than the slots parameter:\nslots = 3\nmax_slots = 1\n[...] ORTE_ERROR_LOG: Bad Parameter in file util\/hostfile\/hostfile.c at line 407\n<\/code><\/pre>\n<\/blockquote>\n<p>Using <code>HyperDriveConfig<\/code> also defaults to submitting one run to one GPU and additionally providing a <code>MpiConfiguration<\/code> leads to the same error as shown above.<\/p>\n<p>I guess I could always rewrite my train script to train multiple models in parallel, s.t. each <code>run<\/code> wraps multiple trainings. I would like to avoid this option though, because then logging and checkpoint writes become increasingly messy and it would require a large refactor of the train pipeline. Also this functionality seems so basic that I hope there is a way to do this gracefully. Any ideas?<\/p>",
        "Challenge_closed_time":1635511999763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635412142523,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to perform hyperparameter search using AzureML and run multiple models on the same GPU\/node to save costs. However, the current method of submitting jobs results in one training run per GPU\/node. The user tried using MpiConfiguration in ScriptRunConfig and HyperDriveConfig, but it leads to an MPI error that suggests the cluster is configured to allow only one run per node. The user is looking for a way to run multiple models on the same GPU\/node without rewriting the train script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69751254",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":23.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":27.7381222222,
        "Challenge_title":"Submitting multiple runs to the same node on AzureML",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":364.0,
        "Challenge_word_count":241,
        "Platform":"Stack Overflow",
        "Poster_created_time":1396607378876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":107.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Use Run.create_children method which will start child runs that are \u201clocal\u201d to the parent run, and don\u2019t need authentication.<\/p>\n<p>For AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run  a hyperparameter tuning run.\nSo there would be 1 execution per node.<\/p>\n<p>single service deployed but you can load multiple model versions in the init then the score function, depending on the request\u2019s param, uses particular model version to score.\nor with the new ML Endpoints (Preview).\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints\" rel=\"nofollow noreferrer\">What are endpoints (preview) - Azure Machine Learning | Microsoft Docs<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1635512880996,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":9.0,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":30.2268313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know that loading a .csv file into sagemaker notebook from S3 bucket is pretty straightforward but I want to load a model.tar.gz file stored in S3 bucket. I tried to do the following<\/p>\n\n<pre><code>import botocore \nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.predictor import csv_serializer\nimport boto3\n\nsm_client = boto3.client(service_name='sagemaker')\nruntime_sm_client = boto3.client(service_name='sagemaker-runtime')\n\ns3 = boto3.resource('s3')\ns3_client = boto3.client('s3')\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\nREGION      = boto3.Session().region_name\nBUCKET      = 'sagemaker.prismade.net'\ndata_key    = 'DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\nloc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\nprint(loc)\nwith tarfile.open(loc) as tar:\n    tar.extractall(path='.')\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>--------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-215-bfdddac71b95&gt; in &lt;module&gt;()\n     20 loc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\n     21 print(loc)\n---&gt; 22 with tarfile.open(loc) as tar:\n     23     tar.extractall(path='.')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in open(cls, name, mode, fileobj, bufsize, **kwargs)\n   1567                     saved_pos = fileobj.tell()\n   1568                 try:\n-&gt; 1569                     return func(name, \"r\", fileobj, **kwargs)\n   1570                 except (ReadError, CompressionError):\n   1571                     if fileobj is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in gzopen(cls, name, mode, fileobj, compresslevel, **kwargs)\n   1632 \n   1633         try:\n-&gt; 1634             fileobj = gzip.GzipFile(name, mode + \"b\", compresslevel, fileobj)\n   1635         except OSError:\n   1636             if fileobj is not None and mode == 'r':\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/gzip.py in __init__(self, filename, mode, compresslevel, fileobj, mtime)\n    161             mode += 'b'\n    162         if fileobj is None:\n--&gt; 163             fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n    164         if filename is None:\n    165             filename = getattr(fileobj, 'name', '')\n\nFileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker.prismade.net\/DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\n<\/code><\/pre>\n\n<p>What is the mistake here and how can I accomplish this?<\/p>",
        "Challenge_closed_time":1581004771980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580895955387,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to load a model.tar.gz file stored in an S3 bucket into a SageMaker notebook. They have tried to use the tarfile module to extract the file but are encountering a FileNotFoundError. The user is seeking assistance in identifying the mistake and finding a solution to load the model file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60072981",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":31.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":30.2268313889,
        "Challenge_title":"How to open a model tarfile stored in S3 bucket in sagemaker notebook?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2984.0,
        "Challenge_word_count":234,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550756471932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Not every python library that is designed to work with a file system (tarfile.open, in this example) knows how to read an object from S3 as a file. <\/p>\n\n<p>The simple way to solve it is to first copy the object into the local file system as a file.<\/p>\n\n<pre><code>import boto3\n\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":4.7,
        "Solution_score_count":7.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1566789105747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":156.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":24.4308544445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Where can I look for the <strong>performance metrics<\/strong> generated by <strong>Amazon SageMaker Debugger\/Profiler<\/strong>?<\/p>",
        "Challenge_closed_time":1663457554492,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663381119547,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for the location of performance metrics generated by Amazon SageMaker Debugger\/Profiler.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73751712",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":21.2319291667,
        "Challenge_title":"Where can I find the Performance Metrics generated by SageMaker Debugger\/Profiler?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":23.0,
        "Challenge_word_count":23,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>1.<code>from sagemaker.debugger import ProfilerConfig<\/code><\/p>\n<p><code>profiler_config = ProfilerConfig( framework_profile_params=FrameworkProfile(start_step=1, num_steps=2) )<\/code><\/p>\n<p>2.\n<code>from sagemaker.debugger import TensorBoardOutputConfig<\/code><\/p>\n<p><code>tensorboard_output_config = TensorBoardOutputConfig(s3_output_path= &lt;&lt; add your bucket name an folder &gt;&gt; )<\/code><\/p>\n<ol start=\"3\">\n<li><p>In your estimator - specify :  <code>profiler_config= profiler_config<\/code> and <code>tensorboard_output_config=tensorboard_output_config<\/code><\/p>\n<\/li>\n<li><p>Train your model<\/p>\n<\/li>\n<li><p>Go to the s3 bucket specified  for your training job name that is assigned in Sagemaker . You should see a report under <strong>rule-output<\/strong> &gt; <strong>ProfilerReport<\/strong> *** &gt; <strong>profiler-output\/<\/strong> &gt; <strong>profiler-report.html<\/strong><\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1663469070623,
        "Solution_link_count":0.0,
        "Solution_readability":23.2,
        "Solution_reading_time":12.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5056.4061419444,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across **multiple .py files** for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing [the docs](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-compiler-enable.html#training-compiler-enable-pysdk), I added the `distributed_training_launcher.py` launcher script to my `source_dir` bundle, and passed in the true training script via a `training_script` hyperparameter.\n\n...But when the job tries to start, I get:\n\n```\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n```\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (`p3.2xlarge`) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within [TrainingArguments](https:\/\/huggingface.co\/transformers\/v3.0.2\/main_classes\/trainer.html#transformers.TrainingArguments) itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\n**EDIT: Turns out the below error can be solved by explicitly setting `n_gpus` as mentioned on the [troubleshooting doc](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-compiler-troubleshooting.html#training-compiler-troubleshooting-missing-xla-config) - but that takes me back to the error message above**\n\n```\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration\n```",
        "Challenge_closed_time":1657872107440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639669045329,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use SageMaker Training Compiler with a PyTorch program split across multiple .py files for maintainability. The job needs to run on multiple GPUs. The user added the `distributed_training_launcher.py` launcher script to the `source_dir` bundle, and passed in the true training script via a `training_script` hyperparameter. However, when the job tries to start, the user encountered an error message. The user also tried running in single-GPU mode and saw another error message.",
        "Challenge_last_edit_time":1667926687612,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":49.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":5056.4061419444,
        "Challenge_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":326,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a `_mp_fn` (which can just execute the same code as gets run `if __name__ == \"__main__\"`) and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online [here](https:\/\/github.com\/aws-samples\/amazon-textract-transformer-pipeline\/pull\/14\/commits\/45fa386faa3eee527395251449e6a58e3fb5f13c). For others would also suggest referring to the [official SMTC example notebooks & scripts](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/training-compiler-examples-and-blogs.html)!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1657872107440,
        "Solution_link_count":2.0,
        "Solution_readability":17.9,
        "Solution_reading_time":9.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":18.5281647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to register a data set as a Python step with the Azure Machine Learning Studio designer. Here is my code:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>I get an error saying that &quot;create_new_version&quot; in the ds.register line was an unexpected keyword argument. However, this keyword appears in the documentation and I need it to keep track of new versions of the file.<\/p>\n<p>If I remove the argument, I get a different error: &quot;Local data source path not supported for this operation&quot;, so it still does not work. Any help is appreciated. Thanks!<\/p>",
        "Challenge_closed_time":1628119362627,
        "Challenge_comment_count":3,
        "Challenge_created_time":1628113771587,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to register a data set as a Python step with the Azure Machine Learning Studio designer. The error message states that \"create_new_version\" is an unexpected keyword argument, even though it appears in the documentation. Removing the argument results in a different error message. The user needs help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68658385",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":13.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.5530666667,
        "Challenge_title":"Azure Machine Learning Studio designer - \"create new version\" unexpected when registering a data set",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":565.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1371499229816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1111.0,
        "Poster_view_count":191.0,
        "Solution_body":"<h2>update<\/h2>\n<p>sharing OP's solution here for easier discovery<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom azureml.core import Workspace, Run, Dataset\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    run = Run.get_context()\n    ws = run. experiment.workspace\n    datastore = ws.get_default_datastore()\n    ds = Dataset.Tabular.register_pandas_dataframe(\n        dataframe1, datastore, 'data_set_name',\n        description = 'data set description.')\n    return dataframe1,\n<\/code><\/pre>\n<h2>original answer<\/h2>\n<p>Sorry you're struggling. You're very close!<\/p>\n<p>A few things may be the culprit here.<\/p>\n<ol>\n<li>It looks like you're using the <code>Dataset<\/code> class, which has been deprecated. I recommend trying <code>Dataset.Tabular.register_pandas_dataframe()<\/code> (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.dataset_factory.tabulardatasetfactory?view=azure-ml-py#register-pandas-dataframe-dataframe--target--name--description-none--tags-none--show-progress-true-\" rel=\"nofollow noreferrer\">docs link<\/a>) instead of <code>Dataset.from_pandas_dataframe()<\/code>. (<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/dataset-api-change-notice.md\" rel=\"nofollow noreferrer\">more about the Dataset API deprecation<\/a>)<\/li>\n<li>More conjectire here, but another thing is there might be some limitations to using dataset registration within an &quot;Execute Python Script&quot; (EPS) module due to:\n<ol>\n<li>the workspace object might not have the right permissions<\/li>\n<li>you might not be able to use the <code>register_pandas_dataframe<\/code> method inside the EPS module, but might have better luck with save the dataframe first to parquet, then calling <code>Dataset.Tabular.from_parquet_files<\/code><\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<p>Hopefully something works here!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1628180472980,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":25.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":165.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":161.6828269445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment running without problems when I run single modules as selected parts.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/aVsSC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aVsSC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The situation is pretty different when I run the entire experiment. In that case it fails, but I cannot know why.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/N2h11.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/N2h11.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The experiment returns an error and obviously it doesn't let me deploy the web service, while:<\/p>\n\n<p>1) I cannot know on <strong>which module<\/strong> my error is.<\/p>\n\n<p>2) I don't have an overall description of the error.<\/p>\n\n<p>3) It could be related to the error here but I cannot know because I don't have any feedback about that. I know that it could be a <strong>bug<\/strong> Azure is trying to solve but this is not reported anywhere.<\/p>\n\n<p>I really need to know if that's a bug and if I can do something about that.<\/p>",
        "Challenge_closed_time":1469689033830,
        "Challenge_comment_count":4,
        "Challenge_created_time":1469106975653,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error in their experiment when running the entire experiment, but they cannot determine which module the error is coming from or what the overall description of the error is. They suspect it may be a bug that Azure is trying to solve, but there is no feedback or information available to confirm this. The user is seeking assistance in identifying the issue and determining if there is a solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38505336",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.1,
        "Challenge_reading_time":14.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":161.6828269445,
        "Challenge_title":"The \"perfect error\": untraceable, unnamed, from neverland",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":75.0,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>This issue has been resolved. Please let me know if this happens again<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":1.1,
        "Solution_reading_time":0.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.9563108333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>\u63b2\u984c\u306e\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u73fe\u5728Machine Learning\u3092\u4f7f\u7528\u3057\u3066\u6a5f\u68b0\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002  <br \/>\n\u305d\u3053\u3067\u8cea\u554f\u306b\u306a\u308b\u306e\u3067\u3059\u304c\u3001\u30c7\u30b6\u30a4\u30ca\u30fc\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u5b66\u7fd2\u7d50\u679c\u3092CSV\u3067\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001  <br \/>\nExport Data\u30e2\u30c7\u30eb\u3067CSV\u5f62\u5f0f\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u3066\u3082CSV\u3067\u306f\u306a\u3044\u5f62\u5f0f\u3067\u5171\u6709\u305b\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u304c\u3001\u539f\u56e0\u304c\u308f\u304b\u3089\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002  <br \/>\n\u3054\u6559\u793a\u306e\u307b\u3069\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002<\/p>",
        "Challenge_closed_time":1631268910236,
        "Challenge_comment_count":1,
        "Challenge_created_time":1631251067517,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is currently using Machine Learning and attempting to export the learning results in CSV format using the designer function. However, even though the Export Data model is set to CSV format, the shared file is not in CSV format and the user is unsure of the cause of the issue. The user is seeking guidance on how to resolve this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/546760\/machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":3.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":4.9563108333,
        "Challenge_title":"Machine Learning\u306b\u3064\u3044\u3066\u306e\u8cea\u554f",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":10,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8f940edc-4c98-48e4-8a54-287e99830334\">@\u6817\u7530\u771f\u5b5d  <\/a> Are you referring to the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/export-data\">export data module<\/a> of the designer from ml.azure.com?    <br \/>\nI think I understand the issue, Are you seeing that the .csv format of file is not listed on the blob storage?    <\/p>\n<p>Since the input is a dataframe directory to export module the output format selected should still be the format you selected, in this case CSV. The file name extension only might be missing. You can still open the csv file in excel and it will recognize the delimiters and headers so you can convert it into excel files.     <\/p>\n<p>You can also avoid this by providing the .csv extension in the path itself in export settings and file will be exported as a csv file directly.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/131128-image.png?platform=QnA\" alt=\"131128-image.png\" \/>    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":12.7,
        "Solution_reading_time":26.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":226.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.1764397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How is the output of Fisher Linear Discriminant Analysis experiment interpreted now that the column labels in the output are replaced with Col1, Col2, Col3.......etc? How can the model be used to predict clusters of other input data as deployed web service requires even the dependent valuable(the same same ones we wish to predict)?<\/p>",
        "Challenge_closed_time":1621895240423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621855005240,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in interpreting the output of Fisher Linear Discriminant Analysis experiment as the column labels are replaced with Col1, Col2, Col3, etc. They are also unsure how to use the model to predict clusters of other input data as the deployed web service requires the same dependent variables that they wish to predict.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/407053\/fisher-linear-discriminant-analysis-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":4.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":11.1764397222,
        "Challenge_title":"Fisher Linear Discriminant Analysis Azure",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Are you referring to the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/latent-dirichlet-allocation#lda-transformed-dataset\">categories<\/a> generated from LDA module? If so, then that's expected. LDA is an unsupervised technique, it groups words into categories\/topics and it's up to the analyst to interpret it by observing the results and transforming the output dataset accordingly. Here's are some <a href=\"https:\/\/gallery.azure.ai\/browse?s=lda\">examples<\/a> of LDA approach in Azure AI Gallery. Hope this helps.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":7.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.045,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi, simple Q. I want to launch a tensorboard with the tensorboard profiler pip installed from the GUI. At the moment I am using this:\n\nversion: 1.1\nkind: operation\nhubRef: tensorboard:multi-run\njoins:\n- query: \"uuid: XX\"\n  params:\n    uuids: {value: \"globals.uuid\"}\n\nHowever I want to runPatch it so that it installs:\n\npip install -U tensorboard-plugin-profile\n\nIs there a way to easily do this?",
        "Challenge_closed_time":1651747435000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651747273000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to launch a tensorboard with the tensorboard profiler pip installed from the GUI. They are currently using a multi-run downstream operation and want to patch it to install \"pip install -U tensorboard-plugin-profile\". They are seeking an easy way to do this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1502",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":5.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.045,
        "Challenge_title":"How to patch a multi-run downstream operation, for example tensorboard:multi-run",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can add the following runPatch:\n\n...\npatchStrategy: replace\nrunPatch:\n  container:\n    command: [\"bash\", \"-c\"]\n    args:\n      - \"pip install -U tensorboard-plugin-profile && tensorboard --logdir={{globals.artifacts_path}} --port={{globals.ports[0]}} --path_prefix={{globals.base_url}} --host=0.0.0.0\"\n\nFor the specific case of tensorboard, we will add a new input plugins of type List[str] to the tensorboard component versions , so instead of patching the component, users can pass a parameter:\n\nparams:\n  plugins: { value: [tensorboard-plugin-profile, tensorboard-plugin-custom, ...] }",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.0,
        "Solution_reading_time":7.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":167.2076163889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In this <a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/07B%20-%20Creating%20a%20Batch%20Inferencing%20Service.ipynb\">example<\/a>, all data files for the parallel run step are stored in <strong>one<\/strong> folder.    <\/p>\n<p>I also want to create a parallel run step. The task for each of the several <strong>folders<\/strong>, in which the multiple data files are stored, is exactly identical.     <\/p>\n<p>The folders:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/182769-image.png?platform=QnA\" alt=\"182769-image.png\" \/>    <\/p>\n<p>The content of each folder:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/182833-image.png?platform=QnA\" alt=\"182833-image.png\" \/>    <\/p>\n<p>How should I define the <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.parallelrunstep?view=azure-ml-py\">ParallelRunStep<\/a>-class so that the identical task for each folder (here 'a', 'b', 'c', 'd' and 'e') is executed in parallel?    <br \/>\nTwo folders should run simultaneously in parallel.    <\/p>\n<p>Moreover, I would like to ask how to get <strong>only<\/strong> the stored folder names or folder paths from a given directory path of a blob storage container.    <\/p>",
        "Challenge_closed_time":1647858343236,
        "Challenge_comment_count":1,
        "Challenge_created_time":1647256395817,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to create a parallel run step for several folders, each containing multiple data files, with an identical task to be executed in parallel. They are seeking guidance on how to define the ParallelRunStep-class to achieve this and how to retrieve only the folder names or paths from a given directory path of a blob storage container.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/771015\/list-of-folder-names-as-input-for-parallelrunstep",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":17.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":167.2076163889,
        "Challenge_title":"list of folder names as input for ParallelRunStep-class",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@@AlexanderPakakis-0994 Thanks, An Azure ML dataset is just metadata pointing to a path or collection of paths in an Azure storage account. You should first &quot;merge&quot; those datasets into a collection of adjacent folders (e.g. root\/dataset1\/, root\/dataset2\/, ...) and then run PRS against root\/**.<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1477057589223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH, United States",
        "Answerer_reputation_count":547.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":0.0,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are currently working on making an Azure MachineLearning Studio experiment operational.<\/p>\n\n<p>Our most recent iteration has a webjob that accepts a queue message, gets some data to train the model, and consumes the ML Experiment webservice to put a trained model in a blob location.<\/p>\n\n<p>A second webjob accepts a queue message, pulls the data to be used in the predictive experiment, gets the location path of the trained .ilearner model, and then consumes THAT ML Experiment webservice.<\/p>\n\n<p>The data used to make the predictions is passed in as an input parameter, and the storage account name, key, and .ilearner path are all passed in as global parameters--Dictionary objects defined according to what the data scientist provided.<\/p>\n\n<p>Everything <em>appears<\/em> to work correctly--except in some cases, the predictive experiment fails, and the error message makes it clear the wrong .ilearner file is being used.<\/p>\n\n<p>When a non-existent blob path is passed to the experiment webservice, the error message reflects there is no such blob, so it's clear the webservice is at least validating the .ilearner's existence. <\/p>\n\n<p>The data scientist can run it locally, but has to change the name of the .ilearner file when he exports it locally through PowerShell. Ensuring each trained model has a unique file name did not resolve this issue.<\/p>\n\n<p>All files, when I view them in the Azure Storage Explorer, appear to be getting updated as expected based on last-modified dates. It's almost like there's a cached version of the .ilearner somewhere that isn't being overridden properly.<\/p>",
        "Challenge_closed_time":1535139575903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535139575903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure MachineLearning WebService where the wrong .ilearner file is being used in some cases, despite passing the correct file path as a global parameter. The experiment webservice is validating the .ilearner's existence, but the error message indicates the wrong file is being used. The user has tried ensuring each trained model has a unique file name, but the issue persists. It seems like there may be a cached version of the .ilearner file that isn't being overridden properly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52010761",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":20.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.0,
        "Challenge_title":"Azure MachineLearning WebService Not Using Passed .ilearner Model",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":260,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477057589223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH, United States",
        "Poster_reputation_count":547.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>After ruling out all possibility of passing in the wrong file, our data scientist took a closer look at the experiment itself. He discovered that it was defaulting to one hardcoded .ilearner path he had been using in development.<\/p>\n\n<p>At one point in time, he had created webservice parameters to override this value (hence why I had them defined in my webservice call), but they had been removed during one of the redesigns of the experiment with anyone noticing, because the webservice will apparently accept superfluous arguments.<\/p>\n\n<p><strong>The webservice was accepting my global parameters<\/strong>, and apparently even validating them. But since they weren't wired to anything inside <strong>the experiment the passed .ilearner file info was never applied to anything<\/strong>--the hardcoded .ilearner was being applied no matter what.<\/p>\n\n<p>We were all very surprised there was no exception thrown about passing in parameters to the webservice that weren't actually defined. Had <em>that<\/em> happened, we would have gotten to the bottom of it much more quickly.<\/p>\n\n<p>tl\/dr: The experiment wasn't properly configured to accept an .ilearner file path (or Account Name, or Account Key) as a parameter, and the webservice was happily accepting and ignoring the parameter arguments without raising any alarm since it had the hardcoded value to run with.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":17.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":208.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1506516283190,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Torino, TO, Italia",
        "Answerer_reputation_count":875.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":433.2236119444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello Stackoverflowers,<\/p>\n\n<p>I'm using azureml and I'm wondering if it is possible to log a confusion matrix of the xgboost model I'm training, together with the other metrics I'm already logging. Here's a sample of the code I'm using:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nimport json\n\nwith open('.\/azureml.config', 'r') as f:\n    config = json.load(f)\n\nsvc_pr = ServicePrincipalAuthentication(\n   tenant_id=config['tenant_id'],\n   service_principal_id=config['svc_pr_id'],\n   service_principal_password=config['svc_pr_password'])\n\n\nws = Workspace(workspace_name=config['workspace_name'],\n                        subscription_id=config['subscription_id'],\n                        resource_group=config['resource_group'],\n                        auth=svc_pr)\n\ny_pred = model.predict(dtest)\n\nacc = metrics.accuracy_score(y_test, (y_pred&gt;.5).astype(int))\nrun.log(\"accuracy\",  acc)\nf1 = metrics.f1_score(y_test, (y_pred&gt;.5).astype(int), average='binary')\nrun.log(\"f1 score\",  f1)\n\n\ncmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\nrun.log_confusion_matrix('Confusion matrix', cmtx)\n<\/code><\/pre>\n\n<p>The above code raises this kind of error:<\/p>\n\n<pre><code>TypeError: Object of type ndarray is not JSON serializable\n<\/code><\/pre>\n\n<p>I already tried to transform the matrix in a simpler one, but another error occurred as before I logged a \"manual\" version of it (<code>cmtx = [[30000, 50],[40, 2000]]<\/code>).<\/p>\n\n<pre><code>run.log_confusion_matrix('Confusion matrix', [list([int(y) for y in x]) for x in cmtx])\n\nAzureMLException: AzureMLException:\n    Message: UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-    c5103b205379\/Confusion matrix already exists.\n    InnerException None\n    ErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/Confusion matrix already exists.\"\n    }\n}\n<\/code><\/pre>\n\n<p>This makes me think that I'm not properly handling the command <code>run.log_confusion_matrix()<\/code>. So, again, which is the best way I can log a confusion matrix to my azureml experiments?<\/p>",
        "Challenge_closed_time":1593519812260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591960207257,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to log a confusion matrix of an xgboost model they are training on the AzureML platform using Python. They are encountering errors when trying to log the matrix using the `run.log_confusion_matrix()` command, and have tried transforming the matrix to a simpler version but still encounter errors. They are seeking advice on the best way to log a confusion matrix to their AzureML experiments.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62343056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":29.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":433.2236119444,
        "Challenge_title":"How to log a confusion matrix to azureml platform using python",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1418.0,
        "Challenge_word_count":216,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506516283190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Torino, TO, Italia",
        "Poster_reputation_count":875.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.<\/p>\n<p>You can find the proper function in this link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----<\/a>.<\/p>\n<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:<\/p>\n<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\ncmtx = {\n\n&quot;schema_type&quot;: &quot;confusion_matrix&quot;,\n&quot;parameters&quot;: params,\n &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],\n          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}\n}\nrun.log_confusion_matrix('Confusion matrix - error rate', cmtx)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.4,
        "Solution_reading_time":17.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":126.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1444758849803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11962.0,
        "Answerer_view_count":960.0,
        "Challenge_adjusted_solved_time":3.0605805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook? <\/p>\n\n<p>Like you would do a \"select * from tablename limit 10\" in an RDS using SQL, similarly is there a way to get a sense of the graph data through Jupyter Notebook?<\/p>",
        "Challenge_closed_time":1584902412783,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584891394693,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook, similar to how one can view data in an RDS using SQL.",
        "Challenge_last_edit_time":1637708419888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60801292",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.0605805556,
        "Challenge_title":"View Neptune Graph Schema using Jupyter notebook",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":831.0,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584891066787,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:<\/p>\n\n<pre><code>g.V().groupCount().by(label)\ng.E().groupCount().by(label)\n<\/code><\/pre>\n\n<p>If you have a very large graph try putting something like <code>limit(100000)<\/code> before the <code>groupCount<\/code> step.<\/p>\n\n<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()<\/code> terminal step to the queries as in:<\/p>\n\n<pre><code>g.V().groupCount().by(label).next()\ng.E().groupCount().by(label).next()\n<\/code><\/pre>\n\n<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called \"person\".<\/p>\n\n<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()\n<\/code><\/pre>\n\n<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.81,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":162.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1454844135036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"T\u00fcrkiye",
        "Answerer_reputation_count":462.0,
        "Answerer_view_count":83.0,
        "Challenge_adjusted_solved_time":151.3788311111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying out <strong>Amazon Sagemaker<\/strong>, I haven't figured out how we can have Continuous training.\n<br>\nFor example if i have a CSV file in s3 and I want to train each time the CSV file is updated.<\/p>\n\n<p>I know we can go again to the notebook and re-run the whole notebook to make this happen.\n<br>\nBut i am looking for an automated way, with some python scripts or using a lambda function with s3 events etc<\/p>",
        "Challenge_closed_time":1530199631932,
        "Challenge_comment_count":3,
        "Challenge_created_time":1529654311630,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in implementing continuous training in Amazon Sagemaker. They are looking for an automated way to train the model each time a CSV file in S3 is updated, instead of manually re-running the notebook. They are considering using Python scripts or a Lambda function with S3 events.",
        "Challenge_last_edit_time":1529654668140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50983316",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.3,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":151.4778616667,
        "Challenge_title":"Continuous Training in Sagemaker",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1191.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1440734188430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1491.0,
        "Poster_view_count":112.0,
        "Solution_body":"<p>You can use boto3 sdk for python to start training on lambda then you need to trigger the lambda when csv is update.<\/p>\n\n<blockquote>\n  <p><a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/sagemaker.html<\/a><\/p>\n<\/blockquote>\n\n<p>Example python code<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n<\/blockquote>\n\n<p>Addition: You dont need to use lambda you just start\/cronjob the python script any kind of instance which has python and aws sdk in it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.0,
        "Solution_reading_time":10.3,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":63.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1335447186710,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Egypt",
        "Answerer_reputation_count":1972.0,
        "Answerer_view_count":547.0,
        "Challenge_adjusted_solved_time":1.6057813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a simple requirement, I need to run sagemaker prediction inside a spark job<\/p>\n<p>am trying to run the below<\/p>\n<pre><code>ENDPOINT_NAME = &quot;MY-ENDPOINT_NAME&quot;\nfrom sagemaker_pyspark import SageMakerModel\nfrom sagemaker_pyspark import EndpointCreationPolicy\nfrom sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer\nfrom sagemaker_pyspark.transformation.deserializers import ProtobufResponseRowDeserializer\n\nattachedModel = SageMakerModel(\n    existingEndpointName=ENDPOINT_NAME,\n    endpointCreationPolicy=EndpointCreationPolicy.DO_NOT_CREATE,\n    endpointInstanceType=None,  # Required\n    endpointInitialInstanceCount=None,  # Required\n    requestRowSerializer=ProtobufRequestRowSerializer(\n        featuresColumnName=&quot;featureCol&quot;\n    ),  # Optional: already default value\n    responseRowDeserializer= ProtobufResponseRowDeserializer(schema=ouput_schema),\n)\n\ntransformedData2 = attachedModel.transform(df)\ntransformedData2.show()\n<\/code><\/pre>\n<p>I get the following error <code>TypeError: 'JavaPackage' object is not callable<\/code><\/p>",
        "Challenge_closed_time":1662672827940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662667047127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run sagemaker prediction inside a spark job using the provided code, but is encountering a TypeError: 'JavaPackage' object is not callable.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73654460",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":29.4,
        "Challenge_reading_time":14.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.6057813889,
        "Challenge_title":"how to use sagemaker inside pyspark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":18.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1335447186710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Egypt",
        "Poster_reputation_count":1972.0,
        "Poster_view_count":547.0,
        "Solution_body":"<p>this was solved by ...<\/p>\n<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\nconf = SparkConf() \\\n    .set(&quot;spark.driver.extraClassPath&quot;, classpath)\nsc = SparkContext(conf=conf)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.5,
        "Solution_reading_time":3.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2124.5019444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**To Reproduce**\r\nI've deleted some of the unwanted notebooks from studio lab's files and now I am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Challenge_closed_time":1655626980000,
        "Challenge_comment_count":9,
        "Challenge_created_time":1647978773000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to open their project on Amazon Sagemaker, as the 'open project' button keeps loading indefinitely. The user has tried various solutions such as restarting the project, browser, laptop, clearing cache, and changing the environment from GPU to CPU, but nothing has worked. The user has attached a screenshot and requested assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.35,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2124.5019444444,
        "Challenge_title":"Unable to open database file, Unexpected error while saving file: d2l-pytorch-sagemaker-studio-lab\/dash\/Untitled.ipynb unable to open database file",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey there, I\u2019m not one of the devs sorry\r\n\r\nBut i wanna ask if you can start up a GPU runtime? Kindly Try and let me know thanks @lorazabora  while launching the GPU instance I am getting this as there is no runtime environment available available right now \r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159628994-38b1c339-ac43-4f6a-8ac7-56f32a8174f9.png)\r\n So then indeed everyone is experiencing the same issue\r\n\r\nIt\u2019s been over a week now and not yet fixed, CPU runtimes aren\u2019t enough for my workloads neither that they even make much sense since there\u2019s already many free cloud CPU options out there\r\n\r\nI hope they see and fix this soon @someshfengde Thank you for reporting the problem. Would you please tell us how did you delete the notebooks? Because only delete the specific files does not affect the behavior of Jupyter Lab. We need to know the procedure to reproduce your problem.\r\n\r\nIf you need the Studio Lab as soon as possible, recreate the account is one of the option.\r\n Yes I tried to delete all notebooks from directory maybe because of that\n\nHow can I recreate account?\n\n\nOn Thu, Mar 24, 2022, 13:48 Takahiro Kubo ***@***.***> wrote:\n\n> @someshfengde <https:\/\/github.com\/someshfengde> Thank you for reporting\n> the problem. Would you please tell us how did you delete the notebooks?\n> Because only delete the specific files does not affect the behavior of\n> Jupyter Lab. We need to know the procedure to reproduce your problem.\n>\n> If you need the Studio Lab as soon as possible, recreate the account is\n> one of the option.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94#issuecomment-1077354727>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AKBFX5JUOPOEUHKEZGXZF53VBQQN3ANCNFSM5RL44VJA>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @someshfengde You can delete the account from here.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/160840123-5f628318-f158-4e40-abba-29524ceb4248.png)\r\n Dear @someshfengde , to delete the account was worked for you? If you still have problem, please let us know. If you already solved the problem, please let us know by closing the this issue. Hi @icoxfog417  I resolved the issue by deleting and opening the account again .\r\n Thanks for your response :smile:  closing issue now :)  You are welcome! We are very glad if Studio Lab supports your data science learning.\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":7.6,
        "Solution_reading_time":30.5,
        "Solution_score_count":null,
        "Solution_sentence_count":25.0,
        "Solution_word_count":349.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1460494806016,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":153.4413752778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to make a recommendation model using Recommendations API on Azure MS Cognitive Services. I can't understand three API's parameters below for \"Create\/Trigger a build.\" What do these parameters mean?<\/p>\n\n<p><a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0\" rel=\"nofollow\">https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0<\/a><\/p>\n\n<blockquote>\n  <p>EnableModelingInsights<br> Allows you to compute metrics on the\n  recommendation model. <br> Valid Values: True\/False<\/p>\n  \n  <p>AllowColdItemPlacement<br> Indicates if the recommendation should also\n  push cold items via feature similarity. <br> Valid Values: True\/False<\/p>\n  \n  <p>ReasoningFeatureList<br> Comma-separated list of feature names to be\n  used for reasoning sentences (e.g. recommendation explanations).<br>\n  Valid Values: Feature names, up to 512 chars<\/p>\n<\/blockquote>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1460495281968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459942893017,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble understanding three parameters for \"Create\/Trigger a build\" in the Azure Recommendations API, specifically EnableModelingInsights, AllowColdItemPlacement, and ReasoningFeatureList. They are seeking clarification on what these parameters mean.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36450108",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":153.4413752778,
        "Challenge_title":"Azure Recommendations API's Parameter",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":279.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459941581603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>That page is missing references to content mentioned at other locations.  See this page for a more complete guide...<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/<\/a><\/p>\n\n<p>It describes Cold Items in the Rank Build section in the document as...<\/p>\n\n<p>Features can enhance the recommendation model, but to do so requires the use of meaningful features. For this purpose a new build was introduced - a rank build. This build will rank the usefulness of features. A meaningful feature is a feature with a rank score of 2 and up. After understanding which of the features are meaningful, trigger a recommendation build with the list (or sublist) of meaningful features. It is possible to use these feature for the enhancement of both warm items and cold items. In order to use them for warm items, the UseFeatureInModel build parameter should be set up. In order to use features for cold items, the AllowColdItemPlacement build parameter should be enabled. Note: It is not possible to enable AllowColdItemPlacement without enabling UseFeatureInModel.<\/p>\n\n<p>It also describes the ReasoningFeatureList in the Recommendation Reasoning section as...<\/p>\n\n<p>Recommendation reasoning is another aspect of feature usage. Indeed, the Azure Machine Learning Recommendations engine can use features to provide recommendation explanations (a.k.a. reasoning), leading to more confidence in the recommended item from the recommendation consumer. To enable reasoning, the AllowFeatureCorrelation and ReasoningFeatureList parameters should be setup prior to requesting a recommendation build.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":22.87,
        "Solution_score_count":3.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":227.0,
        "Tool":"Azure Machine Learning"
    }
]