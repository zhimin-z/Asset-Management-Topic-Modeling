[
    {
        "Answerer_created_time":1621409485092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3609.0,
        "Answerer_view_count":2438.0,
        "Challenge_adjusted_solved_time":55.3460088889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run this 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio<\/p>\n<p><a href=\"https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing<\/a><\/p>\n<p>When I get to this step:<\/p>\n<pre><code>import gradio as gr\nimport tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n<\/code><\/pre>\n<p>I get this error:<\/p>\n<pre><code>ContextualVersionConflict: (Flask 1.0.3 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('Flask&gt;=1.1.1'), {'gradio'})\n<\/code><\/pre>\n<p>I tried to install the Flask 1.1.1 version but I get more errors. Any idea what I should do to get past this step in Azure ML Studio?<\/p>\n<pre><code>!pip install \u2013force-reinstall Flask==1.1.1\n\/\/ More errors\n<\/code><\/pre>",
        "Challenge_closed_time":1630303052768,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630103458310,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run a 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio. However, when they try to import gradio, tensorflow, and transformers, they encounter a ContextualVersionConflict error related to Flask. They tried to install Flask 1.1.1 version but it resulted in more errors. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1630103807136,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68959934",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":55.442905,
        "Challenge_title":"Contextual version conflict error, Microsoft Azure Machine Learning Studio",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":237.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1630103231523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The issue is because <code>gradio<\/code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.<\/p>\n<p>To uninstall the existing package:\n<code>!pip uninstall Flask -y<\/code><\/p>\n<p>To install latest package:\n<code>!pip install Flask&gt;=1.1.1<\/code><\/p>\n<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.<\/strong><\/p>\n<p>Finally, import gradio.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":7.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554425457572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":63.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":294.0039433334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Before the AWS Sagemaker batch transform I need to do some transform. is it possible to have an custom script and associate as entry point to BatchTransformer?<\/p>",
        "Challenge_closed_time":1646173336163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645114921967,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to use a custom script as an entry point for AWS Sagemaker Batch Transform before performing any transformation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71161777",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.3,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":294.0039433334,
        "Challenge_title":"Sagemaker Batch Transform entry point",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":230.0,
        "Challenge_word_count":31,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554425457572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":63.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The inference code and requirement.txt should be stored as part of model.gz while training.  They will be used in the batch transform!!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.1,
        "Solution_reading_time":1.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":30.3172266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train a ResNet50 model using keras with tensorflow backend. I'm using a sagemaker GPU instance <strong>ml.p3.2xlarge<\/strong> but my training time is extremely long. I am using conda_tensorflow_p36 kernel and I have verified that I have tensorflow-gpu installed.<\/p>\n<p>When inspecting the output of nvidia-smi I see the process is on the GPU, but the utilization is never above <strong>0%<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Tensorflow also recognizes the GPU.\n<a href=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Screenshot of training time.\n<a href=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is sagemaker in fact using the GPU even though the usage is <strong>0%?<\/strong>\nCould the long epoch training time be caused by another issue?<\/p>",
        "Challenge_closed_time":1650008050343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649867231580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing slow training time while using a ResNet50 model with keras and tensorflow backend on an AWS Sagemaker GPU instance. The user has verified that tensorflow-gpu is installed and the GPU is recognized by tensorflow, but the GPU utilization is never above 0%. The user is unsure if the GPU is being used and is also questioning if there could be another issue causing the long epoch training time.",
        "Challenge_last_edit_time":1649898908327,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71860839",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":15.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":39.1163230556,
        "Challenge_title":"Slow ResNet50 training time using AWS Sagemaker GPU instance",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":120.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540782143880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Looks like you've completed 8 steps and it just takes very long. What's your step time?<br \/>\nIt might be due to data loading. Where ia data stored? Try to take data loading out of the picture by caching and feeding a single image to the DNN repeatedly and see if that helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.8,
        "Solution_reading_time":3.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":10.2829988889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm moving my first steps in <code>amazon sagemaker<\/code>. I'm using script mode to train a classification algorithm. Training is fine, however I'm not able to do incremental training. I want to train again the same model with new data. Here what I did. This is my script:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker import get_execution_role\n\nbucket = 'sagemaker-blablabla'\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\ntf_estimator = TensorFlow(entry_point='main.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          framework_version='1.12', \n                          py_version='py3',\n                          output_path=s3_output_location)\n\ninputs = {'train': train_data, 'test': validation_data}\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>The entry point is my custom keras code, which I adapted to receive arguments from the script.\nNow the training is successfully completed and I have in my s3 bucket the model.tar.gz. I want to train again, but it's not clear to me how to do it.. I tried this<\/p>\n\n<pre><code>trained_model = 's3:\/\/sagemaker-blablabla\/sagemaker-tensorflow-scriptmode-2019-11-27-12-01-42-300\/output\/model.tar.gz'\n\ntf_estimator = sagemaker.estimator.Estimator(image_name='blablabla-west-1.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12-gpu-py3', \n                                              role=get_execution_role(),\n                                              train_instance_count=1, \n                                              train_instance_type='ml.p2.xlarge',\n                                              output_path=s3_output_location,\n                                              model_uri = trained_model)\n\ninputs = {'train': train_data, 'test': validation_data}\n\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>Doesn't work. Firstly, I don't know how to retrieve the training image name (for this I looked for it in the <code>aws<\/code> console, but I guess there should be a smarter solution), second this code throws an exception about the entry point but it is my understanding that I shouldn't need it when I do incremental learning with a ready image.\nI'm surely missing something important, any help? Thank you!<\/p>",
        "Challenge_closed_time":1574981388936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574941643827,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with incremental training on custom code in Amazon SageMaker. They have successfully trained a classification algorithm using script mode but are unable to train the same model with new data. The user has tried to use the trained model and a new estimator but is encountering an exception about the entry point. They are seeking guidance on how to retrieve the training image name and how to perform incremental learning with a ready image.",
        "Challenge_last_edit_time":1574944370140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59088199",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":27.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":11.0403080556,
        "Challenge_title":"incremental training on custom code in amazon sagemaker",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":621.0,
        "Challenge_word_count":224,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>Incremental training is a native feature for the built-in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/now-easily-perform-incremental-learning-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Image Classifier and Object Detector<\/a>. For custom code, it is the developer responsibility to write the incremental training logic and to verify its validity. Here is a possible path:<\/p>\n\n<ol>\n<li>use one of the data channels passed in the <code>fit<\/code> to load a model state (artifact to fine-tune)<\/li>\n<li>in your code, check if the model state channel is filled\nwith artifacts. If it is, instantiate a model from that state\nand continue training. This is framework specific and you may to take\nnecessary precautions to avoid forgetting previous learnings.<\/li>\n<\/ol>\n\n<p>Some frameworks provide better support for incremental learning that others. For example some sklearn models provide an <a href=\"https:\/\/scikit-learn.org\/0.15\/modules\/scaling_strategies.html#incremental-learning\" rel=\"nofollow noreferrer\">incremental_fit<\/a> method. For DL frameworks it is technically very easy to continue training from a checkpoint, but if new data is very different from previously-seen data this may lead your model to forget previous learnings.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.1,
        "Solution_reading_time":16.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":81.0187747222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pipeline defined in Azure Machine Learning. It was launched every day with Azure Data Factory with Machine Learning Execute Pipeline activity. This solution worked without any issues for a few weeks, but since 12\/09\/2021 all pipeline runs have failed with error: User starting the run is not an owner or assigned user to the Compute Instance.  <br \/>\nI did not change anything in ADF or AML.   <\/p>\n<p>Should I assign compute to ADF? How to do this?<\/p>",
        "Challenge_closed_time":1639690537336,
        "Challenge_comment_count":1,
        "Challenge_created_time":1639398869747,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's pipeline in Azure Machine Learning was previously launched daily with Azure Data Factory, but suddenly stopped working with an error message stating that the user is not an owner or assigned user to the Compute Instance. The user did not make any changes to either ADF or AML and is seeking advice on whether they should assign compute to ADF and how to do so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/661588\/executing-pipeline-in-aml-from-adf-suddenly-stoppe",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":81.0187747222,
        "Challenge_title":"Executing pipeline in AML from ADF suddenly stopped working",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I ran into this same issue in a slightly different context. I didn't manage to figure out the root cause but managed to resolve it in practice by standing up a Compute Cluster instead of a Compute Instance (see <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python<\/a>)<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.2,
        "Solution_reading_time":5.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":61.8899861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to implement a self-service solution in Azure so users can run a Jupyter or PySpark notebook on-Demand\/automatically with the dataset they found a search in the Azure Data Catalog.  I visualize, once the user finds the data in a search, there will be a link that will take him\/her to a Notebook and the dataset can be used for analysis.  Any suggestion would be very much appreciated!<\/p>",
        "Challenge_closed_time":1619432530247,
        "Challenge_comment_count":1,
        "Challenge_created_time":1619209726297,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to create a self-service solution in Azure where users can run a Jupyter or PySpark notebook on-demand with a dataset they found in a search in the Azure Data Catalog. The user is looking for suggestions on how to implement this solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/369952\/azure-on-demand-ml-cluster-from-a-search-in-the-da",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":61.8899861111,
        "Challenge_title":"Azure On-Demand ML cluster from a search in the data catalog",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=072ff3f6-8045-41ef-849b-87cd092ffd6e\">@Jairo Melo  <\/a> Thanks for the question.Azure Purview can find, understand, and consume data sources. Please follow the Azure Purview documentation: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/purview\/\">https:\/\/learn.microsoft.com\/en-us\/azure\/purview\/<\/a>    <\/p>\n<p>and We have  Azure Open Datasets where you can download a Notebook for AML, Databricks or Synapse that explores the data: <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/\">Azure Open Datasets Catalog<\/a> | Microsoft Azure. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/open-datasets\/overview-what-are-open-datasets\">What are open datasets? Curated public datasets - Azure Open Datasets | Microsoft Learn.<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.6,
        "Solution_reading_time":10.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":132.9492333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi all,    <\/p>\n<p>I am following the steps on this tutorial:    <br \/>\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\">tutorial-score-model-predict-spark-pool<\/a>    <br \/>\nI tried to used a model created with AutoML and another from designer and I am getting this error: <em>RuntimeError: Load model failed<\/em>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/155981-capture.png?platform=QnA\" alt=\"155981-capture.png\" \/>    <\/p>\n<p>I am using the model according to this: <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html?childToView=637754#comment-637754\">https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html?childToView=637754#comment-637754<\/a>     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/156021-2.png?platform=QnA\" alt=\"156021-2.png\" \/>    <\/p>\n<p>Thank you for your help.    <\/p>",
        "Challenge_closed_time":1639460160303,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638981543063,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a \"RuntimeError: Load model failed\" error while following the steps of a tutorial to score machine learning models with PREDICT in serverless Apache Spark pools using models created with AutoML and Designer. The user has provided screenshots and links to the tutorial and documentation they are following.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/656548\/runtimeerror-load-model-failed-score-machine-learn",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":20.1,
        "Challenge_reading_time":17.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":132.9492333334,
        "Challenge_title":"RuntimeError: Load model failed - Score machine learning models with PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=4bb27b25-616e-491c-b986-136b5bf96f77\">@Anaid  <\/a>,    <\/p>\n<p>Before running this script, update it with the URI for ADLS Gen2 data file along with model output return data type and ADLS\/AML URI for the model file.    <\/p>\n<pre><code>#Set model URI  \n       #Set AML URI, if trained model is registered in AML  \n          AML_MODEL_URI = &quot;&lt;aml model uri&gt;&quot; #In URI &quot;:x&quot; signifies model version in AML. You can   choose which model version you want to run. If &quot;:x&quot; is not provided then by default   latest version will be picked.  \n  \n       #Set ADLS URI, if trained model is uploaded in ADLS  \n          ADLS_MODEL_URI = &quot;abfss:\/\/&lt;filesystemname&gt;@&lt;account name&gt;.dfs.core.windows.net\/&lt;model   mlflow folder path&gt;&quot;  \n<\/code><\/pre>\n<p><strong>Model URI from AML Workspace:<\/strong>    <\/p>\n<pre><code>DATA_FILE = &quot;abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv&quot;  \nAML_MODEL_URI_SKLEARN = &quot;aml:\/\/mlflow_sklearn:1&quot; #Here &quot;:1&quot; signifies model version in AML. We can choose which version we want to run. If &quot;:1&quot; is not provided then by default latest version will be picked  \nRETURN_TYPES = &quot;INT&quot;  \nRUNTIME = &quot;mlflow&quot;  \n<\/code><\/pre>\n<p><strong>Model URI uploaded to ADLS Gen2:<\/strong>    <\/p>\n<pre><code>DATA_FILE = &quot;abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv&quot;  \nAML_MODEL_URI_SKLEARN = &quot;abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/linear_regression\/linear_regression&quot; #Here &quot;:1&quot; signifies model version in AML. We can choose which version we want to run. If &quot;:1&quot; is not provided then by default latest version will be picked  \nRETURN_TYPES = &quot;INT&quot;  \nRUNTIME = &quot;mlflow&quot;  \n<\/code><\/pre>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.8,
        "Solution_reading_time":38.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":295.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.0166666667,
        "Challenge_answer_count":3,
        "Challenge_body":"I am wondering if training jobs on vertex AI run in parallel, based on my tests it seems they do but wondering if anyone can confirm this is true as the number of concurrent jobs grows past say 1000.\n\n\u00a0\n\nThanks!",
        "Challenge_closed_time":1668513420000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668498960000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is questioning whether training jobs on Vertex AI run in parallel, as their tests suggest they do. They are seeking confirmation if this is true, particularly when the number of concurrent jobs exceeds 1000.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Do-Training-Jobs-Run-in-Parallel-VERTEX-AI\/m-p\/489639#M786",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":3.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.0166666667,
        "Challenge_title":"Do Training Jobs Run in Parallel? (VERTEX AI)",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes training jobs run in parallel but the concurrency is subject to quota. See Vertex AI quota document.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.5,
        "Solution_reading_time":1.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1596548853443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":40.8375211111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Information:<\/strong>\nI am loading an existing trained model.tar.gz from an S3 bucket, and want to perform a batch transform with a .csv containing the input data. The data.csv is structured in such a way that reading it into a pandas DataFrame gives me rows of complete prediction inputs.<\/p>\nNotes:\n<ul>\n<li>This is done on Amazon Sagemaker using the Python SDK<\/li>\n<li>BATCH_TRANSFORM_INPUT is the path to data.csv.<\/li>\n<li>I'm able to load the contents inside model.tar.gz and use them for inference on my local machine using tensorflow, and the logs show <code>2020-08-04 13:35:01.123557: I tensorflow_serving\/core\/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}<\/code>so the model seems to have been trained and saved properly.<\/li>\n<li>The data.csv is in the exact same format as the training data, which means one row per &quot;prediction&quot; where all columns in that row represents the different features.<\/li>\n<li>Changing the argument strategy to 'MultiRecord' gives the same error<\/li>\n<li>[path in s3] is a substitute for the real path as i don't want to reveal any bucket information.<\/li>\n<li>TensorFlow ModelServer: 2.0.0+dev.sha.ab786af<\/li>\n<li>TensorFlow Library: 2.0.2<\/li>\n<\/ul>\n<p>Where 1-5 are features, the file data.csv looks like:<\/p>\n<pre><code>+------+-------------------------+---------+----------+---------+----------+----------+\n| UNIT | TS                      | 1       | 2        | 3       | 4        | 5        |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:01:00.000 | 1.81766 | 0.178043 | 1.33607 | 25.42162 | 12.85445 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:02:00.000 | 1.81673 | 0.178168 | 1.30159 | 25.48204 | 12.87305 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:03:00.000 | 1.8155  | 0.176242 | 1.38399 | 25.35309 | 12.47222 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:04:00.000 | 1.81530 | 0.176398 | 1.39781 | 25.18216 | 12.16837 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:05:00.000 | 1.81505 | 0.151682 | 1.38451 | 25.22351 | 12.41623 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n<\/code><\/pre>\n<p>inference.py currently looks like:<\/p>\n<pre><code>def input_handler(data, context):\n    import pandas as pd\n    if context.request_content_type == 'text\/csv':\n        payload = pd.read_csv(data)\n        instance = [{&quot;dataset&quot;: payload}]\n        return json.dumps({&quot;instances&quot;: instance})\n    else:\n        _return_error(416, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or 'Unknown'))\n<\/code><\/pre>\n<h3>The problem:<\/h3>\n<p>When the following code runs in my jupyter Notebook:<\/p>\n<pre><code>sagemaker_model = Model(model_data = '[path in s3]\/savedmodel\/model.tar.gz'),  \n                        sagemaker_session=sagemaker_session,\n                        role = role,\n                        framework_version='2.0',\n                        entry_point = os.path.join('training', 'inference.py')\n                        )\n\ntf_serving_transformer = sagemaker_model.transformer(instance_count=1,\n                                                     instance_type='ml.p2.xlarge',\n                                                     max_payload=1,\n                                                     output_path=BATCH_TRANSFORM_OUTPUT_DIR,\n                                                     strategy='SingleRecord')\n\n\ntf_serving_transformer.transform(data=BATCH_TRANSFORM_INPUT, data_type='S3Prefix', content_type='text\/csv')\ntf_serving_transformer.wait()\n<\/code><\/pre>\n<p>The model seems to get loaded, but I end up with the following error:\n<code>2020-08-04T09:54:27.415:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=1, BatchStrategy=SINGLE_RECORD 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: ClientError: 400 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv:  2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: Message: 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: { &quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: \\&quot;\\&quot; Type: String is not of expected type: float&quot; } <\/code><\/p>\n<p>Error more clearly:<\/p>\n<p><strong>ClientError: 400\nMessage: {&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float&quot;}<\/strong><\/p>\n<p>If i understand this error correctly, something is wrong with the way my data is structured, so that sagemaker fails to deliver the input data to the TFS model. I suppose there is some &quot;input handling&quot; missing in my inference.py. Maybe the csv data has to somehow be translated into a compatible JSON, for TFS to use it? What exactly has to be done in input_handler() ?<\/p>\n<p>I appreciate all help, and am sorry for this confusing case. If there is any additional information needed, please ask and I'll gladly provide what I can.<\/p>",
        "Challenge_closed_time":1596696668183,
        "Challenge_comment_count":3,
        "Challenge_created_time":1596549653107,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to perform a batch transform with a .csv file containing input data on Amazon Sagemaker using the Python SDK. The data.csv is structured in such a way that reading it into a pandas DataFrame gives rows of complete prediction inputs. The user is encountering an error where the input data is not being delivered to the TensorFlow Serving (TFS) model. The error message suggests that there is something wrong with the way the data is structured, and the user suspects that there is some \"input handling\" missing in the inference.py file. The user is seeking help to understand what needs to be done in the input_handler() function to resolve the issue.",
        "Challenge_last_edit_time":1596696699176,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63248562",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.6,
        "Challenge_reading_time":65.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":40.8375211111,
        "Challenge_title":"How to handle a .csv input for use in Tensorflow Serving batch transform?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":774.0,
        "Challenge_word_count":499,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596548853443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p><strong>Solution:<\/strong> The problem was solved by saving the dataframe as .csv using the arguments header=False, index=False. This makes the saved csv not include the dataframe indexing labels. TFS accepted a clean .csv with only float values (without labels). I assume the error message <em>Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float<\/em> refers to the first cell in the csv, which if the csv was exported with labels is just an empty cell. When it got an empty string instead of a float value it got confused.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":6.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":365.0732141667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Looking at the following source code taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a> (SDK v2):<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n\n# initialize hyperparameters\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;verbosity&quot;:&quot;1&quot;,\n        &quot;objective&quot;:&quot;reg:linear&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\nprefix = 'DEMO-xgboost-as-a-framework'\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'abalone-xgb-framework')\n\n# construct a SageMaker XGBoost estimator\n# specify the entry_point to your xgboost training script\nestimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n                    framework_version='1.2-2',\n                    hyperparameters=hyperparameters,\n                    role=sagemaker.get_execution_role(),\n                    instance_count=1,\n                    instance_type='ml.m5.2xlarge',\n                    output_path=output_path)\n\n# define the data type and paths to the training and validation datasets\ncontent_type = &quot;libsvm&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\nvalidation_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'validation'), content_type=content_type)\n\n# execute the XGBoost training job\nestimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I wonder where the your_xgboost_abalone_script.py file has to be placed please? So far I used XGBoost as a built-in algorithm from my local machine with similar code (i.e. I span up a training job remotely). Thanks!<\/p>\n<p>PS:<\/p>\n<p>Looking at <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">this<\/a>, and source_dir, I wonder if one can upload Python files to S3. In this case, I take it is has to be tar.gz? Thanks!<\/p>",
        "Challenge_closed_time":1654714910812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653478466700,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use XGBoost as a framework in Sagemaker and is unsure where to place the \"your_xgboost_abalone_script.py\" file. They are also wondering if Python files can be uploaded to S3 and if they need to be in tar.gz format.",
        "Challenge_last_edit_time":1653482394636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72376872",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.1,
        "Challenge_reading_time":30.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":343.4566977778,
        "Challenge_title":"entry_point file using XGBoost as a framework in sagemaker",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p><code>your_xgboost_abalone_script.py<\/code> can be created locally. The path you provide is relative to where the code is running.<\/p>\n<p>I.e. <code>your_xgboost_abalone_script.py<\/code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).<\/p>\n<p>For example if you have <code>your_xgboost_abalone_script.py<\/code> in the same directory as the source code:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 source_code.py\n\u2514\u2500\u2500 your_xgboost_abalone_script.py\n<\/code><\/pre>\n<p>Then you can point to this file exactly how the documentation depicts:<\/p>\n<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n.\n.\n.\n)\n<\/code><\/pre>\n<p>The SDK will take <code>your_xgboost_abalone_script.py<\/code> repackage it into a model tar ball and upload it to S3 on your behalf.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1654796658207,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":10.69,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":188.1755805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have big data that I need to pass it to already trained Machine Learning model on Azure and has been deployed as online endpoint, I realize that batch endpoints supports adding a reference to blob file as input, my question is: how to do the same for online enpdoints ?    <\/p>\n<p>So far all examples I see are passing the payload as json (Even in the test tab of the online enpoints) but i don't know how to simply pass a blob storage file's uri as the payload.<\/p>",
        "Challenge_closed_time":1656051612080,
        "Challenge_comment_count":1,
        "Challenge_created_time":1655374179990,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to pass a large amount of data to a pre-trained machine learning model on Azure through an online endpoint. While batch endpoints support adding a reference to a blob file as input, the user is unsure how to do the same for online endpoints. The examples seen so far only pass the payload as JSON, and the user is looking for a way to pass a blob storage file's URI as the payload.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/891865\/how-to-use-blob-storage-file-as-input-to-azure-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":6.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":188.1755805556,
        "Challenge_title":"How to use blob storage file as input to azure ml endpoint",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=a66ab675-b0e9-4f63-b57d-101663c4aaa0\">@Mostafa Mansour  <\/a> Thanks for the question. I have checked internally with the product team, Currently Online endpoints are for Realtime synchronous requests.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.5,
        "Solution_reading_time":8.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1658238093960,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":129.3161055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Does anyone know a repo that shows what a simple HelloWorld java or scala code would look like to build the jar that could be executed using the AWS SageMaker <strong>SparkJarProcessing<\/strong> class?<\/p>\n<p>Readthedocs (<a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/spark_distributed_data_processing\/sagemaker-spark-processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/spark_distributed_data_processing\/sagemaker-spark-processing.html<\/a>) mentions:<\/p>\n<p>&quot;In the next example, you\u2019ll take a Spark application jar (located in .\/code\/spark-test-app.jar)...&quot;<\/p>\n<p>My question is how does the source code look like for this jar (spark-test-app.jar)?<\/p>\n<p>I tried building a simple Java project jar<\/p>\n<p>src&gt;com.test&gt;HW.java:<\/p>\n<pre><code>\npublic class HW {\n    public static void main(String[] args) {\n        System.out.printf(&quot;hello world!&quot;);\n    }\n}\n<\/code><\/pre>\n<p>and running it inside SageMaker Notebook conda_python3 kernel using<\/p>\n<pre><code>from sagemaker.spark.processing import SparkJarProcessor\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nprint(role)\n\nspark_processor = SparkJarProcessor(\n    base_job_name=&quot;sm-spark-java&quot;,\n    framework_version=&quot;3.1&quot;,\n    role=role,\n    instance_count=2,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    max_runtime_in_seconds=1200,\n)\n\nspark_processor.run(\n    submit_app=&quot;.\/SparkJarProcessing-1.0-SNAPSHOT.jar&quot;,\n    submit_class=&quot;com.test.HW&quot;,\n    arguments=[&quot;--input&quot;, &quot;abc&quot;],\n    logs=True,\n)\n<\/code><\/pre>\n<p>But end up getting an error:\nCould not execute HW class.<\/p>\n<p>Any sample source code for <strong>spark-test-app.jar<\/strong> would be highly appreciated!<\/p>",
        "Challenge_closed_time":1658238283580,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657772745600,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking help in building a simple spark-test-app.jar to test AWS SageMaker SparkJarProcessing. They have tried building a simple Java project jar but encountered an error while running it inside SageMaker Notebook conda_python3 kernel. The user is looking for sample source code for spark-test-app.jar.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72975189",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":25.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":129.3161055556,
        "Challenge_title":"How to build a simple spark-test-app.jar to test AWS SageMaker SparkJarProcessing",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1600969850140,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>To answer your question, the source code of that class looks like:<\/p>\n<pre><code>package com.amazonaws.sagemaker.spark.test;\n\nimport java.lang.invoke.SerializedLambda;\nimport org.apache.commons.cli.CommandLineParser;\nimport org.apache.commons.cli.ParseException;\nimport org.apache.commons.cli.HelpFormatter;\nimport org.apache.commons.cli.Option;\nimport org.apache.commons.cli.BasicParser;\nimport org.apache.commons.cli.Options;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.commons.cli.CommandLine;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.commons.lang3.StringUtils;\nimport java.util.List;\nimport org.apache.spark.sql.SparkSession;\n\npublic class HelloJavaSparkApp\n{\n    public static void main(final String[] args) {\n        System.out.println(&quot;Hello World, this is Java-Spark!&quot;);\n        final CommandLine parsedArgs = parseArgs(args);\n        final String inputPath = parsedArgs.getOptionValue(&quot;input&quot;);\n        final String outputPath = parsedArgs.getOptionValue(&quot;output&quot;);\n        final SparkSession spark = SparkSession.builder().appName(&quot;Hello Spark App&quot;).getOrCreate();\n        System.out.println(&quot;Got a Spark session with version: &quot; + spark.version());\n        System.out.println(&quot;Reading input from: &quot; + inputPath);\n        final Dataset salesDF = spark.read().json(inputPath);\n        salesDF.printSchema();\n        salesDF.createOrReplaceTempView(&quot;sales&quot;);\n        final Dataset topDF = spark.sql(&quot;SELECT date, sale FROM sales WHERE sale &gt; 750 SORT BY sale DESC&quot;);\n        topDF.show();\n        final Dataset avgDF = salesDF.groupBy(&quot;date&quot;, new String[0]).avg(new String[0]).orderBy(&quot;date&quot;, new String[0]);\n        System.out.println(&quot;Collected average sales: &quot; + StringUtils.join((Object[])new List[] { avgDF.collectAsList() }));\n        spark.sqlContext().udf().register(&quot;double&quot;, n -&gt; n + n, DataTypes.LongType);\n        final Dataset saleDoubleDF = salesDF.selectExpr(new String[] { &quot;date&quot;, &quot;sale&quot;, &quot;double(sale) as sale_double&quot; }).orderBy(&quot;date&quot;, new String[] { &quot;sale&quot; });\n        saleDoubleDF.show();\n        System.out.println(&quot;Writing output to: &quot; + outputPath);\n        saleDoubleDF.coalesce(1).write().json(outputPath);\n        spark.stop();\n    }\n    \n    private static CommandLine parseArgs(final String[] args) {\n        final Options options = new Options();\n        final CommandLineParser parser = (CommandLineParser)new BasicParser();\n        final Option input = new Option(&quot;i&quot;, &quot;input&quot;, true, &quot;input path&quot;);\n        input.setRequired(true);\n        options.addOption(input);\n        final Option output = new Option(&quot;o&quot;, &quot;output&quot;, true, &quot;output path&quot;);\n        output.setRequired(true);\n        options.addOption(output);\n        try {\n            return parser.parse(options, args);\n        }\n        catch (ParseException e) {\n            new HelpFormatter().printHelp(&quot;HelloScalaSparkApp --input \/opt\/ml\/input\/foo --output \/opt\/ml\/output\/bar&quot;, options);\n            throw new RuntimeException((Throwable)e);\n        }\n    }\n}\n<\/code><\/pre>\n<p>At the same time, I have created a simple example that shows how to run an hello world app <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-misc-examples\/blob\/main\/spark-jar-example.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. Please note that I have run that example on Amazon SageMaker Studio Notebooks, using the Data Science 1.0 kernel.<\/p>\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.6,
        "Solution_reading_time":44.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":253.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":45.9877777778,
        "Challenge_answer_count":0,
        "Challenge_body":"When running this code https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/commit\/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30 from a colab notebook Wandb views the entire thing as one training session and continue gradient steps indefinitely. Training session should be forced to end when that model stops training not when the meta training loop finishes. Should only be 28 training steps not 80.\r\n<img width=\"1094\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/3865062\/71710653-65567f80-2dcb-11ea-8558-0f3280c4ab7b.png\">\r\n",
        "Challenge_closed_time":1578199794000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1578034238000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while uploading a golden test dataset to Wandb. They changed the structure of the existing dataset to train\/validation but renamed the file from \"training\" to \"train\", which did not apply correctly in Wisdomify. As a result, the data did not load, and the user needs to change the file name to \"train.tsv\" in the data loading section.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/35",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":9.5,
        "Challenge_reading_time":7.42,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":209.0,
        "Challenge_repo_issue_count":605.0,
        "Challenge_repo_star_count":1230.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":45.9877777778,
        "Challenge_title":"Wandb bug when running train long",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Currently working on this should be fixed. Just need to test it So it seems to not be doing steps anymore on the same model run chart, but the runs are still not terminating until the loop ends. Don't really know if this is a problem or not. Going to close this for now Wandb supports up to 50 concurrent runs. So as long as aren't training on more than 50 rivers at a time this shouldn't be an issue. If it becomes one will revert to the subprocess thing but don't want otherwise as with that it doesn't log debugging. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":6.16,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":101.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1639972619152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":958.0,
        "Answerer_view_count":995.0,
        "Challenge_adjusted_solved_time":19.0550916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running custom training jobs using Google cloud Vertex AI. But when I enter a custom training job page, the GPU utilization display is not shown, instead, there is a message saying &quot;you don't have access to this data.&quot;\n<a href=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" alt=\"enter image description here\" \/><\/a>\nI would appreciate help finding the right IAM role which will allow me to view the GPU utilization.\nThanks!<\/p>",
        "Challenge_closed_time":1660101916403,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660033318073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while running custom training jobs using Google cloud Vertex AI as the GPU utilization display is not shown and a message saying \"you don't have access to this data\" is displayed. The user is seeking help to find the right IAM role to view the GPU utilization.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73288631",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":19.0550916667,
        "Challenge_title":"Allowing users to view GPU utilization in GCP Vertex AI training jobs",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1379514677176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":597.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/cloud.google.com\/monitoring\/access-control#mon_roles_desc\" rel=\"nofollow noreferrer\"><code>monitoring.viewer<\/code><\/a> IAM role to display both CPU and GPU utilization in GCP Vertex AI training jobs on top of <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\"><code>aiplatform.viewer<\/code><\/a> IAM role.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.3,
        "Solution_reading_time":7.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":41.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.4512016667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello Microsoft Q&amp;A Team,    <\/p>\n<p>I get the error     <\/p>\n<blockquote>\n<p>AssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available    <\/p>\n<\/blockquote>\n<p> when executing the following command:    <\/p>\n<blockquote>\n<p>pipeline_job = ml_client.jobs.create_or_update(    <br \/>\n    pipeline_job, experiment_name=&quot;data_preparation&quot;    <br \/>\n)    <br \/>\npipeline_job    <\/p>\n<\/blockquote>\n<p>Yesterday the command worked without an error. I did not make any changes. So I have no idea, what the problem is.    <\/p>\n<p>Thanks for helping me out.    <\/p>\n<p>Cheers    <\/p>\n<p>Lukas    <\/p>",
        "Challenge_closed_time":1667605270036,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667553245710,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an AssetException error with code \"Can't connect to HTTPS URL because the SSL module is not available\" while executing a command using ml_client.jobs. The error occurred suddenly without any changes made by the user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1075753\/aml-assetexception-error-with-code-cant-connect-to",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":9.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":14.4512016667,
        "Challenge_title":"AML - AssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available.",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":96,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=733bec54-8d30-4052-8297-64b100f6e3d4\">@Lukas  <\/a> Thanks for your question. Can you please add more details about the document\/sample that you are trying.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":2.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1251372839052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":48616.0,
        "Answerer_view_count":3348.0,
        "Challenge_adjusted_solved_time":226.94532,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed my code in Azure Machine Learning and run the batch request in R with different operating systems, such as Unix and W10. For some reason, the host outputs are properly formatted only in R of W10 but I am unable to get properly formatted output in Unix systems. Only way I can get properly formatted outputs in all systems is through the Azure GUI and manually download the file. In W10, I have the luxury to get the properly formatted file directly with my Rscript\/Rstudio thing. In R, I have used <code>system(&quot;defaults write org.R-project.R force.LANG en_US.UTF-8&quot;)<\/code> as hinted <a href=\"https:\/\/stackoverflow.com\/questions\/41873359\/r-encoding-failing-with-umlauts-such-as-%C3%A4-and-%C3%B6\">here<\/a> to explicitly specify the encoding but this does not have any effect on the batch request R script that is executed in Azure servers run by Microsoft.<\/p>\n<p>What is happening is that <code>UTF-8 characters bytes are returned as Latin-1 characters bytes<\/code>, for example<\/p>\n<blockquote>\n<ol>\n<li><p><code>\u00f6<\/code> as <code>\u00c3 \u00b6<\/code><\/p>\n<\/li>\n<li><p><code>\u00e4<\/code> as <code>\u00c3 \u00a4<\/code><\/p>\n<\/li>\n<li><p><code>\u00c4<\/code> as <code>\u00c3 \u00a5<\/code><\/p>\n<\/li>\n<\/ol>\n<\/blockquote>\n<p>as can be demonstrated and tested with this tool <a href=\"http:\/\/www.ltg.ed.ac.uk\/%7Erichard\/utf-8.cgi?input=%C3%84&amp;mode=char\" rel=\"nofollow noreferrer\">here<\/a> about Latin-1 characters. So what are best ways to deal with this encoding issue, can it be addressed somehow inside Azure ML? Where can you do bug reports? Does there exist some tool to convert Latin-1 to UTF-8 in R?<\/p>\n<p><strong>How can you get properly formatted UTF-8 files with umlauts with R batch requests in Azure ML (not in Latin-1 characters)?<\/strong><\/p>",
        "Challenge_closed_time":1486366682512,
        "Challenge_comment_count":1,
        "Challenge_created_time":1485549679360,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning batch request where the UTF-8 characters are returned as Latin-1 characters bytes, resulting in mal-formatted umlauts when executed in Unix systems. The user has tried to explicitly specify the encoding but it does not have any effect on the batch request R script that is executed in Azure servers run by Microsoft. The user is seeking advice on how to deal with this encoding issue and if it can be addressed inside Azure ML.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41902672",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":23.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":226.94532,
        "Challenge_title":"Azure Machine Learning Batch request returning mal-formatted umlauts with Unixes",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":248,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>The Batch request R command has a <code>saveBlobToFile<\/code> function. The problem is in the <code>saveBlobToFile<\/code> function that uses wrong encoding with <code>getUrl<\/code>.  <code>getUrl<\/code> function needs to specify the encodings explicitly. Do the following changes<\/p>\n\n<pre><code>blobContent = getURL(blobUrl, .encoding=\"UTF-8\")\n<\/code><\/pre>\n\n<p>where without <code>.encoding<\/code>, the output is <code>ISO8859-1('latin1')<\/code> or something inherited from your system.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1486367568768,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":6.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508520702036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":21.2055166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>when running an AML pipeline on AML compute, I get this kind of error : <\/p>\n\n<p>I can try rebooting the cluster, but that may not fix the problem (if storage gets accumulated no the nodes, that should be cleaned.<\/p>\n\n<pre><code>Session ID: 933fc468-7a22-425d-aa1b-94eba5784faa\n{\"error\":{\"code\":\"ServiceError\",\"message\":\"Job preparation failed: [Errno 28] No space left on device\",\"detailsUri\":null,\"target\":null,\"details\":[],\"innerError\":null,\"debugInfo\":{\"type\":\"OSError\",\"message\":\"[Errno 28] No space left on device\",\"stackTrace\":\" File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 126, in &lt;module&gt;\\n invoke()\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 97, in invoke\\n extract_project(project_dir, options.project_zip, options.snapshots)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 60, in extract_project\\n project_fetcher.fetch_project_snapshot(snapshot[\\\"Id\\\"], snapshot[\\\"PathStack\\\"])\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 72, in fetch_project_snapshot\\n _download_tree(sas_tree, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 98, in _download_tree\\n fh.write(response.read())\\n\",\"innerException\":null,\"data\":null,\"errorResponse\":null}},\"correlation\":null,\"environment\":null,\"location\":null,\"time\":\"0001-01-01T00:00:00+00:00\"}\n<\/code><\/pre>\n\n<p>I would expect the job to run as it should. And in fact, I've checked on the node and the node do have lots of available harddrive space :<\/p>\n\n<pre><code>root@4f57957ac829466a86bad4d4dc51fadd000001:~# df -kh                                                                                               Filesystem      Size  Used Avail Use% Mounted on\nudev             28G     0   28G   0% \/dev\ntmpfs           5.6G  9.0M  5.5G   1% \/run\n\/dev\/sda1       125G  2.8G  122G   3% \/\ntmpfs            28G     0   28G   0% \/dev\/shm\ntmpfs           5.0M     0  5.0M   0% \/run\/lock\ntmpfs            28G     0   28G   0% \/sys\/fs\/cgroup\n\/dev\/sdb1       335G  6.7G  311G   3% \/mnt\ntmpfs           5.6G     0  5.6G   0% \/run\/user\/1002\n<\/code><\/pre>\n\n<p>Suggestions on what I should check?<\/p>",
        "Challenge_closed_time":1568309206067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568232866207,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error message stating \"No space left on device\" when running an AML pipeline on AML compute. The user has checked the node and found that there is enough available hard drive space. The user is seeking suggestions on what to check next.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57896195",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":15.5,
        "Challenge_reading_time":42.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":21.2055166667,
        "Challenge_title":"Out of disk space",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":738.0,
        "Challenge_word_count":210,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538275960603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Montreal, QC, Canada",
        "Poster_reputation_count":381.0,
        "Poster_view_count":50.0,
        "Solution_body":"<p>Seems like you've run into Azure file share constraints. You can use the following sample code to change your runs to use blob storage which can scale to large number of jobs running in parallel:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.0,
        "Solution_reading_time":6.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":21.2124047222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I keep getting this error in sagemaker when iterating through pytorch dataloader batch cycles:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self._process_data(data), w_id)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1299, in _process_data\n    data.reraise()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/_utils.py&quot;, line 429, in reraise\n    raise self.exc_type(msg)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 84, in __init__\n    super(HTTPClientError, self).__init__(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 40, in __init__\n    msg = self.fmt.format(**kwargs)\nKeyError: 'error'\n\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-1-81655136a841&gt; in &lt;module&gt;\n     58                             py_version='py3')\n     59 \n---&gt; 60 pytorch_estimator.fit({'train': Runtime.dataset_path}, job_name=Runtime.job_name)\n     61 \n     62 #print(pytorch_estimator.latest_job_tensorboard_artifacts_path())\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    955         self.jobs.append(self.latest_training_job)\n    956         if wait:\n--&gt; 957             self.latest_training_job.wait(logs=logs)\n    958 \n    959     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1954         # If logs are requested, call logs_for_jobs.\n   1955         if logs != &quot;None&quot;:\n-&gt; 1956             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1957         else:\n   1958             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3751 \n   3752         if wait:\n-&gt; 3753             self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n   3754             if dot:\n   3755                 print()\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3304                 ),\n   3305                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3306                 actual_status=status,\n   3307             )\n   3308 \n\nUnexpectedStatusException: Error for Training job 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/opt\/conda\/bin\/python3.6 main.py --runtime_var dataset_name=U12239-2022-05-09-14-39-18,job_name=2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training,model_name=pix2pix&quot;\n\n  0%|          | 0\/248 [00:00&lt;?, ?it\/s]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\nTraceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self\n<\/code><\/pre>\n<p>Here is the code which results in the error:<\/p>\n<pre><code>def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce, g_scaler, d_scaler,runtime_log_folder,runtime_log_file_name):\n\n    total_output=''\n    \n    loop = tqdm(loader, leave=True)\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\n    print(&quot;Loop&quot;)\n    print(loop)\n    print(&quot;Length loop&quot;)\n    print(len(loop))\n    for idx, (x, y) in enumerate(loop): #&lt;--error happens here\n        print(&quot;Loop index&quot;)\n        print(idx)\n        print(&quot;Loop item&quot;)\n        print(x,y)\n        x = x.to(device)\n        y = y.to(device)\n        \n        # train discriminator\n        with torch.cuda.amp.autocast():\n            y_fake = gen(x)\n\n            D_real = disc(x, y)\n            D_fake = disc(x, y_fake.detach())\n            # use detach so as to avoid breaking computational graph when do optimizer.step on discriminator\n            # can use detach, or when do loss.backward put loss.backward(retain_graph = True)\n\n            D_real_loss = bce(D_real, torch.ones_like(D_real))\n            D_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            D_loss = (D_real_loss + D_fake_loss) \/ 2\n            \n            # log tensorboard\n            \n        disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n        \n        # train generator\n        with torch.cuda.amp.autocast():\n            \n            D_fake = disc(x, y_fake)\n\n            # compute fake loss\n            # trick discriminator to believe these are real, hence send in torch.oneslikedfake\n            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            # compute L1 loss\n            L1 = l1(y_fake, y) * args.l1_lambda\n\n            G_loss = G_fake_loss + L1\n            \n            # log tensorboard\n           \n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n        \n        # print epoch, generator loss, discriminator loss\n        print(f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]')\n        output = f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]\\n'\n        total_output+=output\n\n\n\n    runtime_log = get_json_file_from_s3(runtime_log_folder, runtime_log_file_name)\n    runtime_log += total_output\n    upload_json_file_to_s3(runtime_log_folder,runtime_log_file_name,json.dumps(runtime_log))\n\n\n\ndef __getitem__(self, index):\n    print(&quot;Index &quot;,index)\n    pair_key = self.list_files[index]\n    print(&quot;Pair key &quot;,pair_key)\n    pair = Boto.s3_client.list_objects(Bucket=Boto.bucket_name, Prefix=pair_key, Delimiter='\/')\n\n    input_image_key = pair.get('Contents')[1].get('Key')\n    input_image_path = f's3:\/\/{Boto.bucket_name}\/{input_image_key}'\n    print(&quot;Input image path &quot;,input_image_path)\n    input_image_s3_source = get_file_from_filepath(input_image_path)\n    input_image = np.array(Image.open(input_image_s3_source))\n\n    target_image_key = pair.get('Contents')[0].get('Key')\n    target_image_path = f's3:\/\/{Boto.bucket_name}\/{target_image_key}'\n    print(&quot;Target image path &quot;,target_image_path)\n    target_image_s3_source = get_file_from_filepath(target_image_path)\n    target_image = np.array(Image.open(target_image_s3_source))\n\n    augmentations = config.both_transform(image=input_image, image0=target_image)\n\n    # get input image and target image by doing augmentations of images\n    input_image, target_image = augmentations['image'], augmentations['image0']\n\n    input_image = config.transform_only_input(image=input_image)['image']\n    target_image = config.transform_only_mask(image=target_image)['image']\n    \n    print(&quot;Input image size &quot;,input_image.size())\n    print(&quot;Target image size &quot;,target_image.size())\n    \n    return input_image, target_image\n\n<\/code><\/pre>\n<p>I did multiple runs and here are the traces of the failure points<\/p>\n<pre><code>i) 2022-06-03-05-00-04-pix2pix-U12239-2022-05-09-14-39-18-training\nNo index shown\n[Epoch 0\/100 (b: 0)]\n\nii) 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niii) 2022-06-03-05-44-46-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niv) 2022-06-03-06-08-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 1\/100 (b: 0)]\n\nv) 2022-06-15-02-49-20-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P423712\/Pair_71\/\n[Epoch 0\/100 (b: 0)\n\nvi) 2022-06-15-02-59-43-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P425642\/Pair_27\/\n[Epoch 0\/100 (b: 247)]\n\nvii) 2022-06-15-04-49-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P415414\/Pair_124\/\nNo specific epoch \n<\/code><\/pre>\n<p>My batch size is 248, so as you can see it seems to fail either at the start of the batch (0) or at the end (247). Also there are some common Indexes in the get item which seems to cause it to fail, namely Index 64 and Index 160. However there doesn't seem to be a common data point in the dataset that causes it to fail, as can be seen from the pair key all 3 data points in the datasets are different.<\/p>\n<p>Does anyone have any idea why this error happens please?<\/p>",
        "Challenge_closed_time":1655379897027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655303532370,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error in Sagemaker while iterating through PyTorch dataloader batch cycles. The error occurs at either the start or end of the batch and there are some common indexes that seem to cause the failure. However, there is no common data point in the dataset that causes the error. The user is seeking help to understand why this error is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72633246",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":124.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":74,
        "Challenge_solved_time":21.2124047222,
        "Challenge_title":"Error in pytorch data loader batch cycles",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":745,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643118380396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Try to run the same training script outside of a SageMaker training job and see what happens.<br \/>\nIf the error doesn't happen on a standalone script, try to run it as a <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"nofollow noreferrer\">Local SageMaker training job<\/a>, so you can reproduce it in seconds instead of minutes, and potentially use a debugger to figure out what is the problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":6.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":36.5390777778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi, here is the details of my issue.  <br \/>\nI want to execute a distributed training run with the Tensorflow framework and Horovod.  <br \/>\nTo do this, I've configured a environment called &quot;tf_env&quot; as follow :<\/p>\n<pre><code># Create the environment : the dependencies are in the .yml file\ntf_env = Environment.from_conda_specification(name=&quot;tensorflow_environment&quot;, file_path=&quot;experiments\/package-list.yml&quot;)\n\n# Register the environment\ntf_env.register(workspace=ws)\n\n# Specify a GPU base image\ntf_env.docker.enabled = True\ntf_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n<\/code><\/pre>\n<p>Where my &quot;package-list.yml&quot; contains all the dependencies my &quot;train_script.py&quot; requires.  <br \/>\nI've defined my ScriptConfigRun as follow :<\/p>\n<pre><code>arguments = [\n    (... other arguments ...)\n    &quot;--ds&quot;,  images_ds.as_mount()\n]\n\nsrc = ScriptRunConfig(\n    source_directory=&quot;experiments&quot;,\n    script='train_script.py',\n    arguments=arguments,\n    compute_target=compute_target,\n    environment=tf_env,\n    distributed_job_config=MpiConfiguration(node_count=2)\n)\n<\/code><\/pre>\n<p>Then, when I want to submit the run :<\/p>\n<pre><code>run = best_model_experiment.submit(config=src)\n<\/code><\/pre>\n<p>... it raises this error I don't understand :<\/p>\n<pre><code>ExperimentExecutionException: ExperimentExecutionException:\n    Message: {\n    &quot;error_details&quot;: {\n        &quot;componentName&quot;: &quot;execution&quot;,\n        &quot;correlation&quot;: {\n            &quot;operation&quot;: &quot;***&quot;,\n            &quot;request&quot;: &quot;***&quot;\n        },\n        &quot;environment&quot;: &quot;westeurope&quot;,\n        &quot;error&quot;: {\n            &quot;code&quot;: &quot;UserError&quot;,\n            &quot;message&quot;: &quot;Error when parsing request; unable to deserialize request body&quot;\n        },\n        &quot;location&quot;: &quot;westeurope&quot;,\n        &quot;time&quot;: &quot;***&quot;\n    },\n    &quot;status_code&quot;: 400,\n    &quot;url&quot;: &quot;https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment***&quot;\n}\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;{\\n    \\&quot;error_details\\&quot;: {\\n        \\&quot;componentName\\&quot;: \\&quot;execution\\&quot;,\\n        \\&quot;correlation\\&quot;: {\\n            \\&quot;operation\\&quot;: \\&quot;***\\&quot;,\\n            \\&quot;request\\&quot;: \\&quot;***\\&quot;\\n        },\\n        \\&quot;environment\\&quot;: \\&quot;westeurope\\&quot;,\\n        \\&quot;error\\&quot;: {\\n            \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n            \\&quot;message\\&quot;: \\&quot;Error when parsing request; unable to deserialize request body\\&quot;\\n        },\\n        \\&quot;location\\&quot;: \\&quot;westeurope\\&quot;,\\n        \\&quot;time\\&quot;: \\&quot;***\\&quot;\\n    },\\n    \\&quot;status_code\\&quot;: 400,\\n    \\&quot;url\\&quot;: \\&quot;https:\/\/westeurope.experiments.azureml.net\/execution\/v1.0\/subscriptions\/***\/resourceGroups\/***\/providers\/Microsoft.MachineLearningServices\/workspaces\/***\/experiments\/experiment\/snapshotrun?runId=experiment_***\\&quot;\\n}&quot;\n    }\n}\n<\/code><\/pre>\n<p>Could you please help me decrypt this error ?  <br \/>\nThank you.<\/p>",
        "Challenge_closed_time":1620024521707,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619892981027,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue while submitting a distributed training run with Tensorflow and Horovod on Azure Machine Learning. They have configured an environment and defined a ScriptConfigRun, but when they try to submit the run, they receive an ExperimentExecutionException with an error message that they do not understand. They are seeking help to decrypt the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/379458\/azure-machine-learning-experimentexecutionexceptio",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":24.5,
        "Challenge_reading_time":44.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":36.5390777778,
        "Challenge_title":"Azure Machine Learning ExperimentExecutionException while submitting a distributed training run !",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":216,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Issue solved ! I've given a list in arguments to argparse so it could'nt deserialized the object.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":1.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1320746685067,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg",
        "Answerer_reputation_count":7552.0,
        "Answerer_view_count":456.0,
        "Challenge_adjusted_solved_time":87.8971722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job.\nI am running a (Databricks) notebook which has the following cell:<\/p>\n\n<pre><code>def call_predict():\n        batch_size = 1\n        data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n        tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n        prediction = predictor.predict(tensor_proto)\n        print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n<\/code><\/pre>\n\n<p>If I just call call_predict() it works fine:<\/p>\n\n<pre><code>call_predict()\n<\/code><\/pre>\n\n<p>and I get the output:<\/p>\n\n<pre><code>Process time: 65.261396\nOut[61]: {'model_spec': {'name': u'generic_model',\n  'signature_name': u'serving_default',\n  'version': {'value': 1578909324L}},\n 'outputs': {u'ages': {'dtype': 1,\n   'float_val': [5.680944442749023],\n   'tensor_shape': {'dim': [{'size': 1L}]}}}}\n<\/code><\/pre>\n\n<p>but when I try to call from a Spark context (in a UDF) I get a serialization error.\nThe code I'm trying to run is:<\/p>\n\n<pre><code>dataRange = range(1, 10001)\nrangeRDD = sc.parallelize(dataRange, 8)\nnew_data = rangeRDD.map(lambda x : call_predict())\nnew_data.count()\n<\/code><\/pre>\n\n<p>and the error I get is:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n&lt;command-2282434&gt; in &lt;module&gt;()\n      2 rangeRDD = sc.parallelize(dataRange, 8)\n      3 new_data = rangeRDD.map(lambda x : call_predict())\n----&gt; 4 new_data.count()\n      5 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in count(self)\n   1094         3\n   1095         \"\"\"\n-&gt; 1096         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n   1097 \n   1098     def stats(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in sum(self)\n   1085         6.0\n   1086         \"\"\"\n-&gt; 1087         return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n   1088 \n   1089     def count(self):\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in fold(self, zeroValue, op)\n    956         # zeroValue provided to each partition is unique from the one provided\n    957         # to the final reduce call\n--&gt; 958         vals = self.mapPartitions(func).collect()\n    959         return reduce(op, vals, zeroValue)\n    960 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in collect(self)\n    829         # Default path used in OSS Spark \/ for non-credential passthrough clusters:\n    830         with SCCallSiteSync(self.context) as css:\n--&gt; 831             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n    832         return list(_load_from_socket(sock_info, self._jrdd_deserializer))\n    833 \n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _jrdd(self)\n   2573 \n   2574         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n-&gt; 2575                                       self._jrdd_deserializer, profiler)\n   2576         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n   2577                                              self.preservesPartitioning, self.is_barrier)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _wrap_function(sc, func, deserializer, serializer, profiler)\n   2475     assert serializer, \"serializer should not be empty\"\n   2476     command = (func, profiler, deserializer, serializer)\n-&gt; 2477     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n   2478     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n   2479                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n\/databricks\/spark\/python\/pyspark\/rdd.pyc in _prepare_for_python_RDD(sc, command)\n   2461     # the serialized command will be compressed by broadcast\n   2462     ser = CloudPickleSerializer()\n-&gt; 2463     pickled_command = ser.dumps(command)\n   2464     if len(pickled_command) &gt; sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M\n   2465         # The broadcast will have same life cycle as created PythonRDD\n\n\/databricks\/spark\/python\/pyspark\/serializers.pyc in dumps(self, obj)\n    709                 msg = \"Could not serialize object: %s: %s\" % (e.__class__.__name__, emsg)\n    710             cloudpickle.print_exec(sys.stderr)\n--&gt; 711             raise pickle.PicklingError(msg)\n    712 \n    713 \n\nPicklingError: Could not serialize object: TypeError: can't pickle _ssl._SSLSocket objects\n<\/code><\/pre>\n\n<p>Not sure what is this serialization error - does is complain about failing to deserialize the Predictor<\/p>\n\n<p>My notebook has a cell which was called prior to the above cells with the following imports:<\/p>\n\n<pre><code>import sagemaker\nimport boto3\nfrom sagemaker.tensorflow.model import TensorFlowPredictor\nimport tensorflow as tf\nimport numpy as np\nimport time\n<\/code><\/pre>\n\n<p>The Predictor was created with the following code:<\/p>\n\n<pre><code>sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY,\n                                aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\nsagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY,\n                                        aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\nboto_session = boto3.Session(region_name='us-east-1')\nsagemaker_session = sagemaker.Session(boto_session, sagemaker_client=sagemaker_client, sagemaker_runtime_client=sagemaker_runtime_client)\n\npredictor = TensorFlowPredictor('endpoint-poc', sagemaker_session)\n<\/code><\/pre>",
        "Challenge_closed_time":1579206604950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579190415880,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to run inference on a Tensorflow model deployed on SageMaker from a Python Spark job. While calling the function from a Spark context (in a UDF), the user is encountering a serialization error. The error message suggests that the issue is with failing to deserialize the Predictor.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59773503",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.0,
        "Challenge_reading_time":67.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":51,
        "Challenge_solved_time":4.4969638889,
        "Challenge_title":"Using Sagemaker predictor in a Spark UDF function",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":322.0,
        "Challenge_word_count":474,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>The udf function will be executed by multiple spark tasks in parallel. Those tasks run in completely isolated python processes and they are scheduled to physically different machines. Hence each data, those functions reference, must be on the same node. This is the case for everything created within the udf.<\/p>\n\n<p>Whenever you reference any object outside of the udf from the function, this data structure needs to be serialised (pickled) to each executor. Some object state, like open connections to a socket, cannot be pickled.<\/p>\n\n<p>You need to make sure, that connections are lazily opened each executor. It must happen only on the first function call on that executor. The <a href=\"https:\/\/spark.apache.org\/docs\/latest\/streaming-programming-guide.html#design-patterns-for-using-foreachrdd\" rel=\"nofollow noreferrer\">connection pooling topic<\/a> is covered in the docs, however only in the spark streaming guide (though it also applies for normal batch jobs).<\/p>\n\n<p>Normally one can use the Singleton Pattern for this. But in python people use the Borgh pattern.<\/p>\n\n<pre><code>class Env:\n    _shared_state = {\n        \"sagemaker_client\": None\n        \"sagemaker_runtime_client\": None\n        \"boto_session\": None\n        \"sagemaker_session\": None\n        \"predictor\": None\n    }\n    def __init__(self):\n        self.__dict__ = self._shared_state\n        if not self.predictor:\n            self.sagemaker_client = boto3.client('sagemaker', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n            self.sagemaker_runtime_client = boto3.client('sagemaker-runtime', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, region_name='us-east-1')\n\n            self.boto_session = boto3.Session(region_name='us-east-1')\n            self.sagemaker_session = sagemaker.Session(self.boto_session, sagemaker_client=self.sagemaker_client, sagemaker_runtime_client=self.sagemaker_runtime_client)\n\n            self.predictor = TensorFlowPredictor('endpoint-poc', self.sagemaker_session)\n\n\n#....\ndef call_predict():\n   env = Env()\n   batch_size = 1\n   data = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]]\n   tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[batch_size, len(data[0])], dtype=tf.float32)      \n   prediction = env.predictor.predict(tensor_proto)\n\n   print(\"Process time: {}\".format((time.clock() - start)))\n        return prediction\n\nnew_data = rangeRDD.map(lambda x : call_predict())\n<\/code><\/pre>\n\n<p>The Env class is defined on the master node. Its <code>_shared_state<\/code> has empty entries. When then Env object is instantiated first time, it shares the state with all further instances of Env on any subsequent call to the udf. On each separate parallel running process this will happen exactly one time. This way the sessions are shared and do not need to pickled. <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1579506845700,
        "Solution_link_count":1.0,
        "Solution_readability":12.0,
        "Solution_reading_time":35.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":288.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to understand what is the optimal way in Kedro to convert Spark dataframe coming out of one node into Pandas required as input for another node without creating a redundant conversion step.<\/p>",
        "Challenge_closed_time":1573500781436,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573500781437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on the best way to convert a Spark data frame to Pandas and vice versa in Kedro without having to go through a redundant conversion step.",
        "Challenge_last_edit_time":1573553002123,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58807540",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"How to convert Spark data frame to Pandas and back in Kedro?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":800.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1393579668636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":1450.0,
        "Poster_view_count":162.0,
        "Solution_body":"<p>Kedro currently supports 2 strategies for that:<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/04_data_catalog.html#transcoding-datasets\" rel=\"nofollow noreferrer\">Transcoding<\/a> feature<\/h3>\n\n<p>This requires one to define two <code>DataCatalog<\/code> entries for the same dataset, working with the same file in a common format (Parquet, JSON, CSV, etc.), in your <code>catalog.yml<\/code>:<\/p>\n\n<pre><code>my_dataframe@spark:\n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: data\/02_intermediate\/data.parquet\n\nmy_dataframe@pandas:\n  type: ParquetLocalDataSet\n  filepath: data\/02_intermediate\/data.parquet\n<\/code><\/pre>\n\n<p>And then use them in the pipeline like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func1, \"spark_input\", \"my_dataframe@spark\"),\n    node(my_func2, \"my_dataframe@pandas\", \"output\"),\n])\n<\/code><\/pre>\n\n<p>In this case, <code>kedro<\/code> understands that <code>my_dataframe<\/code> is the same dataset in both cases and resolves the node execution order properly. At the same time, <code>kedro<\/code> would use the <code>SparkDataSet<\/code> implementation for saving and <code>ParquetLocalDataSet<\/code> for loading, so the first node should output <code>pyspark.sql.DataFrame<\/code>, while the second node would receive a <code>pandas.Dataframe<\/code>.<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.pandas_to_spark.html\" rel=\"nofollow noreferrer\">Pandas to Spark<\/a> and <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.spark_to_pandas.html\" rel=\"nofollow noreferrer\">Spark to Pandas<\/a> node decorators<\/h3>\n\n<p><strong>Note:<\/strong> <code>Spark &lt;-&gt; Pandas<\/code> in-memory conversion is <a href=\"https:\/\/stackoverflow.com\/a\/47536675\/3364156\">notorious<\/a> for its memory demands, so this is a viable option only if the dataframe is known to be small.<\/p>\n\n<p>One can decorate the node as per the docs:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from spark import get_spark\nfrom kedro.contrib.decorators import pandas_to_spark\n\n@pandas_to_spark(spark_session)\ndef my_func3(data):\n    data.show() # data is pyspark.sql.DataFrame\n<\/code><\/pre>\n\n<p>Or even the whole pipeline:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func4, \"pandas_input\", \"some_output\"),\n    ...\n]).decorate(pandas_to_spark)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1573501243163,
        "Solution_link_count":4.0,
        "Solution_readability":16.3,
        "Solution_reading_time":31.94,
        "Solution_score_count":3.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":207.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617627646000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a TypeError when using MlflowArtifactDataSet with MlflowModelSaverDataSet, resulting in an unsupported operand type(s) for \/: 'str' and 'str'. The expected result was to save the model locally and in MLflow run at the same time. The bug occurred in kedro 0.16.6 and kedro-mlflow 0.4.0 on Python 3.7.7 and MacOS Catalina. The bug also happened with the last version on develop.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":121,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.0607519444,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>We are running long data preparation run (30+ hours) to pre-build source files for training.  However, part of the dataset was not ready and was excluding from the current run (which is 20+ hours into the run).  I would like to process the remaining data and ADD it to this current artifact.<br>\nI note that whenever I run this code is creates a new version of the artifact.<br>\nHow can I append new data to an existing artifact?<\/p>\n<p>Second question: Can I add new data in-parallel with the original job.  That is, can two different processes add data to the same artifact at the same time?<\/p>",
        "Challenge_closed_time":1664566326300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664425707593,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in adding new data to an existing artifact while running a long data preparation process. They are seeking guidance on how to append new data to the current artifact and whether it is possible to add new data in parallel with the original job.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/continuing-an-artifact\/3198",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":7.4,
        "Challenge_reading_time":7.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":39.0607519444,
        "Challenge_title":"Continuing an artifact",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":795.0,
        "Challenge_word_count":108,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a><\/p>\n<p>After speaking with the the team, you have options via our wandb artifact upsert calls to append to a non-finalized artifact as output of a run, see <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/run#upsert_artifact\">here<\/a>.However, we highly recommend you utilize S3 URI reference instead as it would be the more straightforward approach. Add all data to the S3 bucket <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/track-external-files#amazon-s3-gcs-references\">then setup reference URI<\/a> to generate the new artifact with all your processed data. If you run into any issues, please let me know.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.0,
        "Solution_reading_time":8.59,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":82.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.8202777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nCurrently the example DAG for sagemaker just uses access key and secret key. We need to use a temporary  access token\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1666960895000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666950742000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a bug where the XCom return value of `SageMakerTransformOperatorAsync` and `SageMakerTrainingOperatorAsync` does not produce the expected output. The issue seems to be that some keys do not match the non-async operator output. The user has tried to reproduce the issue by running a DAG with traditional operators and then running the same DAG with async operators, and comparing the outputs. The expected behavior is for the XCom keys and values to match whatever the traditional non-async version of the operators output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/738",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.6,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2.8202777778,
        "Challenge_title":"Sagemaker example DAG to use aws session token",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":120,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1569423384323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":8089.9413483334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Im working on sentence classification using in-build  blazing text algorithm, while invoking endpoint inside lambda function it throughs the content type mismatching error. <\/p>\n\n<p>-- For blazing text it support only application\/jsonlines or application\/json but while invoking , it throughs the error like , it accepts only byte or bytearray<\/p>\n\n<pre><code>input format . application\/json\nevent={\n  \"features\": [\n    \"sensor_subtype Thermostats Thermal Switches product_features Hermetically sealed n Tight tolerances n Tight differentials n Logic level contacts n applications Computers n Medical electronics n Power supplies n Industrial controls n Test equipment n Infotech n description Technical Specifications technical_specs CloseTolerance 2 8 C 5 F DielectricStrength MIL STD 202 Method 301 1250 Vac 60 Hz Terminal to Case ContactResistance MIL STD\"\n  ]\n}\n<\/code><\/pre>\n\n<p>and also i tried application\/jsonlines<\/p>\n\n<p>My code looks like this>>>>>>>>>>>>>>>>>>>>>>>><\/p>\n\n<pre><code>def transform_data(data):\n    try:\n        features = data.copy()\n\n        return features\n\n    except Exception as err:\n        print('Error when transforming: {0},{1}'.format(data,err))\n        raise Exception('Error when transforming: {0},{1}'.format(data,err))\n\n\ndef lambda_handler(event, context):\n    try:    \n        print(\"Received event: \" + json.dumps(event, indent=2))\n\n        request = json.loads(json.dumps(event))\n\n        transformed_data = str(transform_data(request['features'])) #for instance in request['features'])\n        print(ENDPOINT_NAME, \"-------&gt;&gt;&gt;&gt;\")\n        payload=transformed_data\n        result = client.invoke_endpoint(EndpointName=ENDPOINT_NAME, \n                              Body=(payload.encode('utf-8')),\n                              ContentType='application\/json')\n        return result\n<\/code><\/pre>\n\n<pre><code>  \"statusCode\": 400,\n  \"isBase64Encoded\": false,\n  \"body\": \"Call Failed An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (406) from model with message \\\"Invalid payload format\\\".\n_______________LOGS__________________________________\n\uf141\n11:35:22\n[08\/18\/2019 11:35:22 ERROR 140074862942016] Customer Error: Unable to decode payload: Incorrect data format. (caused by ValueError)\n\uf141\n11:35:22\nCaused by: No JSON object could be decoded\n\uf141\n11:35:22\nTraceback (most recent call last): File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/blazingtext\/serve.py\", line 317, in invocations data = json.loads(payload.decode(\"utf-8\")) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/__init__.py\", line 339, in loads return _default_decoder.decode(s) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/decoder.py\", line 364, in decode obj, end = self.\n\uf141\n11:35:22\nValueError: No JSON object could be decoded\n<\/code><\/pre>\n\n<p>I need to predict the sentence in realtime using invoke_endpoint option but it shows invalid payload format <\/p>\n\n<p>I tried with byte format and apllication\/jsonlines format.<\/p>",
        "Challenge_closed_time":1595256539003,
        "Challenge_comment_count":1,
        "Challenge_created_time":1566128809927,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while invoking the endpoint of the in-built Blazing text algorithm inside a lambda function for sentence classification. The error is due to a content type mismatch, as the algorithm only supports application\/jsonlines or application\/json, but the endpoint only accepts byte or bytearray. The user has tried using both application\/json and application\/jsonlines formats, but the error persists. The user needs to predict sentences in real-time using the invoke_endpoint option, but the invalid payload format error is preventing this.",
        "Challenge_last_edit_time":1566133069936,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57544237",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":15.8,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":8091.0358544445,
        "Challenge_title":"Inside lambda function - Blazing text algorithm invoke endpoint doesn't support the input content type",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":493.0,
        "Challenge_word_count":305,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541220234400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":13.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I encountered the same problem when trying to predict on text classification with a BlazingText container. What worked for me was simply changing the key in the payload while keeping the ContentType as application\/json:<\/p>\n<pre><code>sentence = &quot;I'm selling my PS4, practically brand new&quot;\n\npayload = {&quot;instances&quot;: [sentence]}\n\nresponse = client.invoke_endpoint(\n        EndpointName=&quot;text_classification&quot;,\n        Body=json.dumps(payload),\n        ContentType='application\/json'\n        \n    )\n<\/code><\/pre>\n<p>After playing around a little with the payload it seems that blazing text models only accept payloads as a dictionary with &quot;instances&quot; as its key and a list containing your data you want to predict on as its value.<\/p>\n<p>To get to your predictions simply :<\/p>\n<pre><code>print(&quot;ResponseMetadata:&quot;, response[&quot;ResponseMetadata&quot;])\nprint()\nprint(&quot;Body:&quot;, response['Body'].read())\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595256858790,
        "Solution_link_count":0.0,
        "Solution_readability":16.0,
        "Solution_reading_time":12.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":103.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":44.7150719444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When submitting an aml compute job, I want to access the nodes where the compute happens for debugging purposes. The portal gives me the IP address, the port and the nodeID, but no password seems to exist within the portal. <\/p>\n\n<p>How am I supposed to connect to the machine?<\/p>\n\n<p>I am running on NC6 machines for a single node training. I have already tried to run the command given through the portal, but the node is (hopefully so) protected by a password.<\/p>",
        "Challenge_closed_time":1567599301196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567438326937,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing the nodes where the compute happens for debugging purposes while submitting an aml compute job. The portal provides the IP address, port, and nodeID, but there is no password available. The user is running on NC6 machines for single node training and has already tried running the command given through the portal, but the node is password-protected.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57759556",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":6.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":44.7150719444,
        "Challenge_title":"How to access node from azure machine learning compute?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":754.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488451123727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paris",
        "Poster_reputation_count":23.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>In order to be able to log in to the nodes of an AML Compute cluster, you have to provide a username and password and ssh key (ssh key is optional), <strong>when you create the cluster<\/strong>. You can only do that at creation time.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ckh6j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ckh6j.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.6,
        "Solution_reading_time":5.2,
        "Solution_score_count":4.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1439975952067,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":84.5929313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to connect to redshift using sagemaker notebook instances. I want to run Unload commands to unload data from redshift to s3 using IAM role and schedule the sagemaker notebook.\nI want to know how I can import db credentials in sagemaker without hardcoding.<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1618908321396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618603786843,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to connect to Redshift using Sagemaker notebook instances and run Unload commands to unload data from Redshift to S3 using IAM role. They are looking for a way to import db credentials in Sagemaker without hardcoding.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67131617",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":84.5929313889,
        "Challenge_title":"Connect to redshift using sagemaker notebook instances",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1504.0,
        "Challenge_word_count":53,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468599558956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":107.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/secretsmanager\/latest\/userguide\/intro.html\" rel=\"nofollow noreferrer\">AWS Secrets Manager<\/a> to store and access credentials. Your Sagemaker execution role should have permission to read from Secrets Manager (AFAIK AWS managed-role does have it). This is the same mechanism that's used by Sagemaker notebooks to get access to github repo, for example<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":5.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1297232105368,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":27164.0,
        "Answerer_view_count":3016.0,
        "Challenge_adjusted_solved_time":0.6067841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I have this code to write out a json file to be connected to via endpoint. The file is pretty standard in that it has the location of how to connect to the endpoint as well as some data.<\/p>\n<pre><code>%%writefile default-pred.json\n{   PROJECT_ID:&quot;msds434-gcp&quot;,\n    REGION:&quot;us-central1&quot;,\n    ENDPOINT_ID:&quot;2857701089334001664&quot;,\n    INPUT_DATA_FILE:&quot;INPUT-JSON&quot;,\n    &quot;instances&quot;: [\n    {&quot;age&quot;: 39,\n    &quot;bill_amt_1&quot;: 47174,\n    &quot;bill_amt_2&quot;: 47974,\n    &quot;bill_amt_3&quot;: 48630,\n    &quot;bill_amt_4&quot;: 50803,\n    &quot;bill_amt_5&quot;: 30789,\n    &quot;bill_amt_6&quot;: 15874,\n    &quot;education_level&quot;: &quot;1&quot;,\n    &quot;limit_balance&quot;: 50000,\n    &quot;marital_status&quot;: &quot;2&quot;,\n    &quot;pay_0&quot;: 0,\n    &quot;pay_2&quot;:0,\n    &quot;pay_3&quot;: 0,\n    &quot;pay_4&quot;: 0,\n    &quot;pay_5&quot;: &quot;0&quot;,\n    &quot;pay_6&quot;: &quot;0&quot;,\n    &quot;pay_amt_1&quot;: 1800,\n    &quot;pay_amt_2&quot;: 2000,\n    &quot;pay_amt_3&quot;: 3000,\n    &quot;pay_amt_4&quot;: 2000,\n    &quot;pay_amt_5&quot;: 2000,\n    &quot;pay_amt_6&quot;: 2000,\n    &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n    }\n<\/code><\/pre>\n<p>Then I have this trying to connect to the file and then taking the information to connect to the end point in question. I know the information is right as it's the exact code from GCP.<\/p>\n<pre><code>!curl \\\n-X POST \\\n-H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n-H &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n-d &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>So from the information I have I would expect it to parse the information I have and connect to the endpoint, but obviously I have my file wrong somehow. Any idea what it is?<\/p>\n<pre><code>{\n  &quot;error&quot;: {\n    &quot;code&quot;: 400,\n    &quot;message&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.\\nInvalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;,\n    &quot;status&quot;: &quot;INVALID_ARGUMENT&quot;,\n    &quot;details&quot;: [\n      {\n        &quot;@type&quot;: &quot;type.googleapis.com\/google.rpc.BadRequest&quot;,\n        &quot;fieldViolations&quot;: [\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;PROJECT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;REGION\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;ENDPOINT_ID\\&quot;: Cannot find field.&quot;\n          },\n          {\n            &quot;description&quot;: &quot;Invalid JSON payload received. Unknown name \\&quot;INPUT_DATA_FILE\\&quot;: Cannot find field.&quot;\n          }\n        ]\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>What am I missing here?<\/p>",
        "Challenge_closed_time":1653863593763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653862009460,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to write a JSON file to be connected to via endpoint, but encounters an error message stating that the JSON payload received is invalid and cannot find the specified fields. The user is seeking assistance in identifying the issue with the file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72427594",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":40.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":0.4400841667,
        "Challenge_title":"Questions on json and GCP",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":281,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632597456472,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>The data file should only include the data.<\/p>\n<p>You've included <code>PROJECT_ID<\/code>, <code>REGION<\/code>, <code>ENDPOINT<\/code> and should not.<\/p>\n<p>These need to be set in the (bash) environment <strong>before<\/strong> you issue the <code>curl<\/code> command:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>PROJECT_ID=&quot;msds434-gcp&quot;\nREGION=&quot;us-central1&quot;\nENDPOINT_ID=&quot;2857701089334001664&quot;\n\ncurl \\\n--request POST \\\n--header &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \\\n--header &quot;Content-Type: application\/json&quot; \\\nhttps:\/\/us-central1-prediction-aiplatform.googleapis.com\/v1alpha1\/projects\/$PROJECT_ID\/locations\/$REGION\/endpoints\/$ENDPOINT_ID:predict \\\n--data &quot;@default-pred.json&quot;\n<\/code><\/pre>\n<p>The file <code>default-pred.json<\/code> should <strike>probably (I can never find this service's methods in <a href=\"https:\/\/developers.google.com\/apis-explorer\" rel=\"nofollow noreferrer\">APIs Explorer<\/a>!<\/strike>) just be:<\/p>\n<pre><code>{\n  instances&quot;: [\n    { &quot;age&quot;: 39,\n      &quot;bill_amt_1&quot;: 47174,\n      &quot;bill_amt_2&quot;: 47974,\n      &quot;bill_amt_3&quot;: 48630,\n      &quot;bill_amt_4&quot;: 50803,\n      &quot;bill_amt_5&quot;: 30789,\n      &quot;bill_amt_6&quot;: 15874,\n      &quot;education_level&quot;: &quot;1&quot;,\n      &quot;limit_balance&quot;: 50000,\n      &quot;marital_status&quot;: &quot;2&quot;,\n      &quot;pay_0&quot;: 0,\n      &quot;pay_2&quot;:0,\n      &quot;pay_3&quot;: 0,\n      &quot;pay_4&quot;: 0,\n      &quot;pay_5&quot;: &quot;0&quot;,\n      &quot;pay_6&quot;: &quot;0&quot;,\n      &quot;pay_amt_1&quot;: 1800,\n      &quot;pay_amt_2&quot;: 2000,\n      &quot;pay_amt_3&quot;: 3000,\n      &quot;pay_amt_4&quot;: 2000,\n      &quot;pay_amt_5&quot;: 2000,\n      &quot;pay_amt_6&quot;: 2000,\n      &quot;sex&quot;: &quot;1&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>See the documentation for the <code>aiplatform<\/code> <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/reference\/rest\/v1\/projects\/predict\" rel=\"nofollow noreferrer\"><code>predict<\/code><\/a> method as this explains this.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653864193883,
        "Solution_link_count":3.0,
        "Solution_readability":19.8,
        "Solution_reading_time":27.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":135.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1555543581790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Santa Clara, CA, USA",
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":11.7356358333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using SageMaker Notebook in AWS Glue for ETL development. <\/p>\n\n<p>On importing the SparkContext library I am getting the below error. I have tried to restart the kernel but did not heled. Can some one explain me the point \"a\". <\/p>\n\n<blockquote>\n  <p>The code failed because of a fatal error: Error sending http request\n  and maximum retry encountered..*<\/p>\n  \n  <p>Some things to try: <\/p>\n  \n  <p>a. Make sure Spark has enough available resources for Jupyter to\n  create a Spark context. <\/p>\n  \n  <p>b. Contact your Jupyter administrator to make sure the Spark magics\n  library is configured correctly. <\/p>\n  \n  <p>c. Restart the kernel.<\/p>\n<\/blockquote>\n\n<p>Following points to be noted:<\/p>\n\n<ol>\n<li><p>I am creating the sagemaker notebook from AWS Console > AWS Glue > Dev Endpoint > Notebooks.<\/p><\/li>\n<li><p>VPC, Subnet and Security group of the dev endpoint created is same as the RDS to which connection is\nsupposed to be made. While creating dev endpoint, in the networking\npage I choose an existing connection from the list of connections\navailable in the drop down so that VPC, subnet and security group\nare automatically chosen.<\/p><\/li>\n<li>I had increased the DPU from 5 to 10\nbut still getting this error. <\/li>\n<li>Not able reach the step where I can\ncreate connection to RDS because getting error while calling the\nlibrary. <\/li>\n<li>If I skip the networking info while creating the dev end\npoint I am successfully able to call all the relevant libraries\n(screenshot attached). (which is not suggested when connecting to\nRDS as it would not work).<\/li>\n<\/ol>\n\n<p>So, this error (\"The code failed because...\") is coming only when providing a connection.<\/p>\n\n<p>Would be helpful if some one could help out in resolving this issue. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JmonV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JmonV.jpg\" alt=\"When adding connections, getting this error\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ewRKh.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ewRKh.jpg\" alt=\"Without adding connections I am not getting error\"><\/a><\/p>",
        "Challenge_closed_time":1578942905056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578898025257,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while importing the SparkContext library in SageMaker Notebook in AWS Glue for ETL development. The error message suggests that there may not be enough available resources for Jupyter to create a Spark context. The user has tried restarting the kernel and increasing the DPU, but the error persists. The error only occurs when providing a connection to RDS. The user is seeking help in resolving the issue.",
        "Challenge_last_edit_time":1578900656767,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59711677",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":27.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":12.4666108333,
        "Challenge_title":"Why am I getting error while importing SparkContext library in sagemaker notebook?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":5137.0,
        "Challenge_word_count":314,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517149734000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":71.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Wondering if you're configuration for Livy endpoint is valid? Livy runs on port 8998. You should check if the port is open in the security group. <\/p>\n\n<p>This might be useful: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/<\/a><\/p>\n\n<p>Also, if that does not help, you should try stopping and restarting the notebook once. That has helped in the past. <\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.3,
        "Solution_reading_time":7.49,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515266756243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tempe, AZ, USA",
        "Answerer_reputation_count":143.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":1.1404319444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a AML compute cluster with the min &amp; max nodes set to 2. When I execute a pipeline, I expect the cluster to run the training on both instances in parallel. But the cluster status reports that only one node is busy and the other is idle.<\/p>\n<p>Here's my code to submit the pipeline, as you can see, I'm resolving the cluster name and passing that to my Step1, thats training a model on Keras.<\/p>\n<pre><code>aml_compute = AmlCompute(ws, &quot;cluster-name&quot;)\nstep1 = PythonScriptStep(name=&quot;train_step&quot;,\n                         script_name=&quot;Train.py&quot;, \n                         arguments=[&quot;--sourceDir&quot;, os.path.realpath(source_directory) ],\n                         compute_target=aml_compute, \n                         source_directory=source_directory,\n                         runconfig=run_config,\n                         allow_reuse=False)\npipeline_run = Experiment(ws, 'MyExperiment').submit(pipeline1, regenerate_outputs=False)\n<\/code><\/pre>",
        "Challenge_closed_time":1594303708128,
        "Challenge_comment_count":2,
        "Challenge_created_time":1594299602573,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has set up an Azure ML compute cluster with two nodes, but when running a pipeline to train a model on Keras, only one node is being utilized while the other remains idle. The user has provided code for submitting the pipeline and resolving the cluster name.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62815426",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":11.3,
        "Challenge_reading_time":11.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1.1404319444,
        "Challenge_title":"Azure ML: How to train a model on multiple instances",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":623.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>Each python script step runs on a single node even if you allocate multiple nodes in your cluster. I'm not sure whether training on different instances is possible off-the-shelf in AML, but there's definitely the possibility to use that single node more effectively (looking into using all your cores, etc.)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":3.89,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":4.1983277778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do I delete projects in my workspace ?when I click to delete a project the delete button is inactive. Tried clearing cache and whatnot but cannot delete the project from studio.azureml.net... how do I do this ? <\/p>",
        "Challenge_closed_time":1526865758888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526851287060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty deleting a project in their Azure ML Studio workspace as the delete button is inactive. They have attempted to clear their cache but have not been successful in deleting the project.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50439489",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":2.0,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4.0199522222,
        "Challenge_title":"Delete Project Azure ML Studio ( Web App)",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":576.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506435894640,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Southeast Asia",
        "Poster_reputation_count":1922.0,
        "Poster_view_count":404.0,
        "Solution_body":"<p>I have reproduced your issue. Try to go to your project -> EDIT ->remove the <strong>ASSETS<\/strong> of your project. Then the delete button will be able.<\/p>\n\n<p>You could follow the screenshot.<\/p>\n\n<ol>\n<li>The <strong>DELETE<\/strong> button is disable.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/E850F.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E850F.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>2.Go to <strong>EDIT<\/strong> and remove the <strong>ASSETS<\/strong>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PbEZC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PbEZC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2kBgO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2kBgO.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>3.Then the <strong>DELETE<\/strong> button will be able<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/08lOW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/08lOW.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1526866401040,
        "Solution_link_count":8.0,
        "Solution_readability":12.4,
        "Solution_reading_time":14.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":4.4489933333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I started a batch prediction job in AutoML (now VertexAI) for a small csv in one of my buckets, using a classification model, then I noticed the csv had an error but was unable to find a way to cancel the job using the web GUI, it just says &quot;running&quot; but I see no &quot;stop&quot; or &quot;cancel&quot; button.<\/p>\n<p>Fortunately, it was done after 20 minutes, but I need to know how to stop a job since I will require predictions for way bigger files and can't risk having to wait until the job ends by itself. It was kind of desperating being able to watch the log throwing error after error and not being able to stop the job. I tried to delete the job but it said it can't be deleted while its running.<\/p>\n<p>I found a related question, but it was not answered, the job just finished itself after a couple of days. I can't risk that.\n<a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a><\/p>\n<p>I will greatly appreciate any help.<\/p>",
        "Challenge_closed_time":1625628028808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625608160277,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user started a batch prediction job in Google Cloud's AutoML (now VertexAI) using a classification model for a small csv in one of their buckets. However, they noticed an error in the csv and were unable to find a way to cancel the job using the web GUI. The user is seeking help to stop a job since they will require predictions for larger files and cannot risk waiting until the job ends by itself. They tried to delete the job, but it cannot be deleted while it's running.",
        "Challenge_last_edit_time":1625612012432,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68277691",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":5.5190363889,
        "Challenge_title":"How do I stop a Google Cloud's AutoML (now VertexAI) batch prediction job using the web GUI?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":619.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1431573886067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":38.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Unfortunately the cancel\/stop feature is not yet available in the Vertex AI UI. As per <a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a>, the OP sent a feedback. You can ask if there was a public issue tracker created for this so you can monitor the progress of the feature request there.<\/p>\n<p>But there is a workaround for this, just send a request <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.batchPredictionJobs\/cancel\" rel=\"nofollow noreferrer\">projects.locations.batchPredictionJobs.cancel<\/a> via REST.<\/p>\n<p>To do this you can send a request via curl. In this example the model and endpoint are located in <code>us-central1<\/code> thus the location defined in the request.<\/p>\n<p>Just supply your <code>project-id<\/code> and the <code>batch-prediction-id<\/code> on the request. To get the <code>batch-prediction-id<\/code> you can get it via UI:<\/p>\n<p>Get <code>batch-prediction-id<\/code> via UI:<\/p>\n<ul>\n<li>Open &quot;Batch Predictions&quot; tab in the Vertex AI UI<\/li>\n<li>Click on the job you want to cancel<\/li>\n<li>Job information will be displayed and the 1st entry will contain the Job ID<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To cancel the job send a cancel request via curl. If requests is successful, the response body is empty.<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/your-project-id\/locations\/us-central1\/batchPredictionJobs\/batch-prediction-job-id:cancel\n<\/code><\/pre>\n<p>Check in Vertex AI UI if the job was canceled.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/atSqt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/atSqt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":13.5,
        "Solution_reading_time":27.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":219.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":5.6856088889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Sagmaker, and I have created a pipeline from the SageMaker notebook, consisting of training and deployment components.\nIn the training script, we can upload the model to s3 via <strong>SM_MODEL_DIR<\/strong>. But now, I want to upload the classification report to s3. I tried this code. But It shows this is not a proper s3 bucket.<\/p>\n<pre><code>df_classification_report = pd.DataFrame(class_report).transpose()\nclassification_report_file_name = os.path.join(args.output_data_dir,\n                                               f&quot;{args.eval_model_name}_classification_report.csv&quot;)\ndf_classification_report.to_csv(classification_report_file_name)\n# instantiate S3 client and upload to s3\n\n# save classification report to s3\ns3 = boto3.resource('s3')\nprint(f&quot;classification_report is being uploaded to s3- {args.model_dir}&quot;)\ns3.meta.client.upload_file(classification_report_file_name, args.model_dir,\n                            f&quot;{args.eval_model_name}_classification_report.csv&quot;)\n<\/code><\/pre>\n<p>And the error<\/p>\n<pre><code>Invalid bucket name &quot;\/opt\/ml\/output\/data&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>Can anybody help? I really appreciate any help you can provide.<\/p>",
        "Challenge_closed_time":1649187284408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649158713513,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to upload a custom file to S3 from the training script in the training component of AWS SageMaker Pipeline. The user is able to upload the model to S3 via SM_MODEL_DIR but is unable to upload the classification report to S3. The error message indicates that the bucket name is not in the correct format. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1649166816216,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71751105",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":20.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.9363597222,
        "Challenge_title":"Upload custom file to s3 from training script in training component of AWS SageMaker Pipeline",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":458.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1383025927423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dhaka, Bangladesh",
        "Poster_reputation_count":647.0,
        "Poster_view_count":105.0,
        "Solution_body":"<p>SageMaker Training Jobs will compress any files located in <code>\/opt\/ml\/model<\/code> which is the value of <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir\" rel=\"nofollow noreferrer\"><code>SM_MODEL_DIR<\/code><\/a> and upload it to S3 automatically. You could look at saving your file to <code>SM_MODEL_DIR<\/code> (Your classification report will thus be uploaded to S3 in the model tar ball).<\/p>\n<p>The <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\"><code>upload_file()<\/code><\/a> function requires you to pass an S3 bucket.\nYou could also look at manually specify an S3 bucket in your code to upload the file to.<\/p>\n<pre><code>s3.meta.client.upload_file(classification_report_file_name, &lt;YourS3Bucket&gt;,\n                            f&quot;{args.eval_model_name}_classification_report.csv&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.6,
        "Solution_reading_time":12.68,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":16.1952341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Challenge_closed_time":1658964743500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658906440657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a model using FbProphet and Apache Spark in Google-Colab. They are wondering if they can upload the Google-Colab notebook to AWS SageMaker\/Lambda for free, without being charged for Apache Spark and only being charged for AWS SageMaker.",
        "Challenge_last_edit_time":1660220920907,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":16.1952341667,
        "Challenge_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1658906023852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":152.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":9.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":118.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":829.3705555556,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nAfter running a model evaluation suite and exprorint to wandb using \"to_wandb\" function, the confusion matrix appears in the w&b page without the values\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n\r\n**Expected behavior**\r\nThe confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown\r\n![1654716717893](https:\/\/user-images.githubusercontent.com\/21197955\/172704682-e1097eaa-5371-48b6-96d7-f0df1006c043.jpeg)\r\n\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS: linux\r\n - Python Version:3.7.1\r\n - Deepchecks Version:0.7.2\r\n\r\n",
        "Challenge_closed_time":1657703576000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1654717842000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the WandB callback is changing the unique ID of the train step, but not updating the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1592",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.52,
        "Challenge_repo_contributor_count":35.0,
        "Challenge_repo_fork_count":159.0,
        "Challenge_repo_issue_count":2171.0,
        "Challenge_repo_star_count":2280.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":829.3705555556,
        "Challenge_title":"[BUG] Weird behavior with \"to_wandb\" and confusion matrix",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":75,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @DL1992,\r\n\r\nFor me the export works fine, can you provide us with some more info about what you did?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/9868530\/173320868-74292589-5da2-4a15-9583-0855f592a602.png)\r\n hmmm, literally just result = suite.run follow by result.to_wandb.\r\nmy wandb version is 0.12.9\r\n what is the wandb and plotly version on the wandb server? This issue is stale and we couldn't reproduce it.\r\n@DL1992 feel free to reach out to us if this problem persists and we will try to help personally. Closing for now",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.8,
        "Solution_reading_time":6.64,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":76.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":101.3011433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If we use multiple instances for training will the built-in algorithm automatically exploit it? For example, what if we used 2 instances for training using built-in XGBoost container and we used the same customer churn example? Will one instance be ignored?<\/p>",
        "Challenge_closed_time":1663199408803,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662834395330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on whether the SageMaker XGBoost built-in algorithm can automatically utilize multiple instances for training. They are specifically asking if using 2 instances for training will result in both instances being utilized or if one instance will be ignored.",
        "Challenge_last_edit_time":1662834724687,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73674278",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":101.3926313889,
        "Challenge_title":"How to use multiple instances with the SageMaker XGBoost built-in algorithm?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":19.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1507661294190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":98.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Yes SageMaker XGBoost supports distributed training. If you set instance count &gt; 1, SageMaker XGBoost will distribute the files from S3 to individual instances and perform distributed training. This, however, requires number of files on S3 &gt;= number of instances. Otherwise, you will be charged for using two training instances without the benefit of using distributed training.<\/p>\n<p>You can find an example here<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.6,
        "Solution_reading_time":10.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.2563888889,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to control a Spark cluster (using SparkR) from a Sagemaker notebook. I followed these instructions closely: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/  and got it to work.\n\nToday when I try to run the SageMaker notebook (using the exact same code as before) I inexplicably get this error:  \n\n    An error was encountered:\n    [1] \"Error in callJMethod(sparkSession, \\\"read\\\"): Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed.\"\n\nDoes anyone know why this is? I terminated the SparkR kernel and am still getting this error.\n",
        "Challenge_closed_time":1591041375000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591029652000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to control a Spark cluster using SparkR from a Sagemaker notebook and followed the instructions provided by AWS. However, when trying to run the notebook again, the user encountered an error message stating that the Spark operations need to be re-executed. The user terminated the SparkR kernel but is still getting the same error.",
        "Challenge_last_edit_time":1667926595812,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqyiKb_XvRhGxm1RxwjXhJQ\/sparkr-not-working",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.2563888889,
        "Challenge_title":"SparkR not working",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":75.0,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You cannot have multiple SparkContexts in one JVM. The issue is resolved as WON'T FIX. You have to stop the spark session which spawned the sparkcontext (which you have already done). \n\n`sparkR.session.stop()`\n\nhttps:\/\/issues.apache.org\/jira\/browse\/SPARK-2243",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925571654,
        "Solution_link_count":1.0,
        "Solution_readability":7.6,
        "Solution_reading_time":3.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":29.4160033333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to read a csv file on an s3 bucket (for which the sagemaker notebook has full access to) into a spark dataframe however I am hitting the following issue where <code>sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar<\/code> can't be found. Any tips on how to resolve this is appreciate!<\/p>\n\n<pre><code>bucket = \"mybucket\"\nprefix = \"folder\/file.csv\"\ndf = spark.read.csv(\"s3:\/\/{}\/{}\/\".format(bucket,prefix))\n\nPy4JJavaError: An error occurred while calling o388.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error reading configuration file\nat java.util.ServiceLoader.fail(ServiceLoader.java:232)\nat java.util.ServiceLoader.parse(ServiceLoader.java:309)\nat java.util.ServiceLoader.access$200(ServiceLoader.java:185)\nat java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)\nat java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)\nat java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)\nat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\nat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\nat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\nat scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)\nat scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)\nat scala.collection.AbstractTraversable.filter(Traversable.scala:104)\nat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:614)\nat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\nat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\nat py4j.Gateway.invoke(Gateway.java:282)\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\nat py4j.GatewayConnection.run(GatewayConnection.java:238)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar (No such file or directory)\n    at java.util.zip.ZipFile.open(Native Method)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:219)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:149)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:166)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:103)\n    at sun.net.www.protocol.jar.URLJarFile.&lt;init&gt;(URLJarFile.java:93)\n    at sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)\n    at sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)\n    at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)\n    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:150)\n    at java.net.URL.openStream(URL.java:1045)\n    at java.util.ServiceLoader.parse(ServiceLoader.java:304)\n    ... 26 more\n<\/code><\/pre>",
        "Challenge_closed_time":1532493818792,
        "Challenge_comment_count":2,
        "Challenge_created_time":1532387294417,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to read a CSV file from an S3 bucket into a Spark dataframe in Sagemaker. The error message indicates that the sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar file cannot be found, resulting in a java.util.ServiceConfigurationError. The user is seeking tips on how to resolve this issue.",
        "Challenge_last_edit_time":1532387921180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51488308",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":34.1,
        "Challenge_reading_time":49.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":29.5901041667,
        "Challenge_title":"Failing to read data from s3 to a spark dataframe in Sagemaker",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2283.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444524456120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation_count":973.0,
        "Poster_view_count":82.0,
        "Solution_body":"<p>(Making comment to the original question as answer)<\/p>\n\n<p>It looks like a jupyter kernel issue. I had a similar issue and I used <code>Sparkmagic (pyspark)<\/code> kernel instead of <code>Sparkmagic (pyspark3)<\/code> and it is working fine. Follow instructions on this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">blog<\/a> and see if it helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":5.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":184.4022222222,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n*Description:*\r\n\r\nAn apt-get error is seen in `sagemaker-local-test` builds as below. This is because `apt-get` process is already running and in active state.\r\n\r\n```\r\nE: Could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: Resource temporarily unavailable)\r\n--\r\n294 | E: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it?\r\n```\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1598551848000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597888000000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to train on multiple GPUs in Sagemaker Notebook Terminal using Autogluon 0.4.0 TextPredictor. The user gets an error in spawning multiprocessing. However, when the user trains on a single GPU within the same instance and setup, it trains without any problem. The user expects to train across all 4 GPUs in the p3.8xl instance with no errors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/517",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":184.4022222222,
        "Challenge_title":"[bug] apt-get failure in sagemaker-local-test builds",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1488536112480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":349.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":0.1712805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy the existing breast cancer prediction model on Amazon Sagemanker using AWS Lambda and API gateway. I have followed the official documentation from the below url.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/<\/a><\/p>\n\n<p>I am getting a type error at \"predicted_label\".<\/p>\n\n<pre><code> result = json.loads(response['Body'].read().decode())\n print(result)\n pred = int(result['predictions'][0]['predicted_label'])\n predicted_label = 'M' if pred == 1 else 'B'\n\n return predicted_label\n<\/code><\/pre>\n\n<p>please let me know if someone could resolve this issue. Thank you. <\/p>",
        "Challenge_closed_time":1538387613807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538386997197,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to deploy an existing breast cancer prediction model on Amazon Sagemaker using AWS Lambda and API gateway. The user is getting a type error at \"predicted_label\" and is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1562245989407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52588354",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":12.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.1712805556,
        "Challenge_title":"How to deploy breast cancer prediction endpoint created by AWS Sagemaker using Lambda and API gateway?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":502.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456483381212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":999.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>By printing the result type by <code>print(type(result))<\/code> you can see its a dictionary. now you can see the key name is \"score\" instead of \"predicted_label\" that you are giving to pred. Hence replace it with<\/p>\n\n<pre><code>pred = int(result['predictions'][0]['score'])\n<\/code><\/pre>\n\n<p>I think this solves your problem.<\/p>\n\n<p>here is my lambda function:<\/p>\n\n<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n   print(\"Received event: \" + json.dumps(event, indent=2))\n\n   data = json.loads(json.dumps(event))\n   payload = data['data']\n   print(payload)\n\n   response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                      ContentType='text\/csv',\n                                      Body=payload)\n   #print(response)\n   print(type(response))\n   for key,value in response.items():\n       print(key,value)\n   result = json.loads(response['Body'].read().decode())\n   print(type(result))\n   print(result['predictions'])\n   pred = int(result['predictions'][0]['score'])\n   print(pred)\n   predicted_label = 'M' if pred == 1 else 'B'\n\n   return predicted_label\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.3,
        "Solution_reading_time":15.19,
        "Solution_score_count":4.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9285602778,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this? \n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker. \n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models? \n\nMany thanks \n\nTim",
        "Challenge_closed_time":1638917636668,
        "Challenge_comment_count":1,
        "Challenge_created_time":1638914293851,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to access a large image dataset stored in a Sagemaker notebook instance from outside of that instance, specifically via Python Sagemaker Estimator training call. The user wants to access the data in a fast manner to train an AI model and is looking for the recommended way to do this. The user also wants to know the best way to store data when using Sagemaker estimators for training AI models.",
        "Challenge_last_edit_time":1668630486323,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":11.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.9285602778,
        "Challenge_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1538.0,
        "Challenge_word_count":170,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the \/home\/ec2-user\/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test\/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1638917636668,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":28.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":387.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.1167675,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hi, I am using Azure Machine Learning studio.  <\/p>\n<p>I trained my deep learning model on computing cluster, and then outputs images onto 'outputs' directory.  <br \/>\nHowever, in Experiment &gt; Run &gt; outputs + logs tab, directories was displayed instead of my images.  <\/p>\n<p>Is it possible to get my images from 'outputs' directory?  <\/p>\n<p>After investigation, I recognized that images which have filename containing square brackets is not shown in the tab.  <br \/>\nFor instance, if I created 'outputs\/test[0].jpg', then 'outputs\/test' directory was shown in the tab.  <\/p>\n<p>[Sample code]  <\/p>\n<pre><code>import cv2\nimport numpy as np\nfrom pathlib import Path\n\n# Check whether output directory exists or not\noutput_dir = Path('.\/outputs\/')\nif not output_dir.exists():\n    output_dir.mkdir()\n\n# Generate test image\nIMAGE_SIZE = (28, 28)\n\ntest_image = np.zeros(IMAGE_SIZE)\n\n# Save image with filename which contained \/ not contained square brackets\n# 'test.jpg' and 'test' directory will be shown in the 'outputs' directory, however 'test[].jpg' and 'test[0].jpg' are disappeared.\ncv2.imwrite(str(output_dir \/ 'test.jpg'), test_image)\ncv2.imwrite(str(output_dir \/ 'test[].jpg'), test_image)\ncv2.imwrite(str(output_dir \/ 'test[0].jpg'), test_image)\n<\/code><\/pre>",
        "Challenge_closed_time":1610646282360,
        "Challenge_comment_count":1,
        "Challenge_created_time":1610588261997,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning studio where images with filenames containing square brackets are not displayed in the outputs folder. Instead, only the directories are shown in the Experiment > Run > outputs + logs tab. The user has provided sample code to demonstrate the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/229808\/azure-machine-learning-studio-images-which-have-fi",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.3,
        "Challenge_reading_time":17.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":16.1167675,
        "Challenge_title":"Azure Machine Learning studio - Images which have filename containing square brackets is not shown in outputs folder.",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":175,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, it probably recognizes the square bracket as wildcard. Try using a double-backticks to escape the brackets, otherwise, I recommend you rename without the brackets and train again.  <\/p>\n<pre><code>cv2.imwrite(str(output_dir \/ 'test``[``].jpg'), test_image)\n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":3.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":50.3174552778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can you please share any code examples for training random forests with GPU on Azure using libraries.  <br \/>\nI want to run on the multiple nodes.<\/p>",
        "Challenge_closed_time":1595231268032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595050125193,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking code examples for training random forests with GPU on Azure using libraries and wants to run it on multiple nodes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/49008\/random-forests-on-azure-gpu-vm-using-the-sdk",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":2.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":50.3174552778,
        "Challenge_title":"Random forests on Azure GPU VM using the SDK",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":34,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@vautoml-0887<\/a> Thanks for the question. You can run LightGBM with boosting=random_forest, Please follow the below documentation:  <br \/>\n<a href=\"https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting\">https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting<\/a><\/p>\n<p>Here is a general tutorial on how to run LightGBM on GPU, You can run it on any Azure GPU VM:  <br \/>\n<a href=\"https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst\">https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst<\/a><\/p>\n<p>If you need to run it on multiple nodes, there is also a distributed spark implementation available at <a href=\"https:\/\/github.com\/Azure\/mmlspark\">https:\/\/github.com\/Azure\/mmlspark<\/a>.<\/p>\n<p>Random Forests for the GPU using PyCUDA: <a href=\"https:\/\/pypi.org\/project\/cudatree\/\">https:\/\/pypi.org\/project\/cudatree\/<\/a>  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.0,
        "Solution_reading_time":12.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":60.6923205556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm preparing for the Azure Machine Learning exam, and here is a question confuses me.<\/p>\n<blockquote>\n<p>You are designing an Azure Machine Learning workflow. You have a\ndataset that contains two million large digital photographs. You plan\nto detect the presence of trees in the photographs. You need to ensure\nthat your model supports the following:<\/p>\n<p>Solution: You create a Machine\nLearning experiment that implements the Multiclass Decision Jungle\nmodule. Does this meet the goal?<\/p>\n<p>Solution: You create a Machine Learning experiment that implements the\nMulticlass Neural Network module. Does this meet the goal?<\/p>\n<\/blockquote>\n<p>The answer for the first question is No while for second is Yes, but I cannot understand why Multiclass Decision Jungle doesn't meet the goal since it is a classifier. Can someone explain to me the reason?<\/p>",
        "Challenge_closed_time":1559895314607,
        "Challenge_comment_count":3,
        "Challenge_created_time":1559676822253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is preparing for the Azure Machine Learning exam and is confused about a question related to image classification. The user needs to detect the presence of trees in a dataset of two million large digital photographs and is unsure why the Multiclass Decision Jungle module does not meet the goal while the Multiclass Neural Network module does. The user is seeking an explanation for this.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56450223",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.2,
        "Challenge_reading_time":11.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":60.6923205556,
        "Challenge_title":"Image Classification in Azure Machine Learning",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":538.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449453613688,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":388.0,
        "Poster_view_count":322.0,
        "Solution_body":"<p>I suppose that this is part of a series of questions that present the same scenario. And there should be definitely some constraints in the scenario. \nMoreover if you have a look on the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/multiclass-neural-network\" rel=\"nofollow noreferrer\">Azure documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>However, recent research has shown that deep neural networks (DNN)\n  with many layers can be very effective in complex tasks such as image\n  or speech recognition. The successive layers are used to model\n  increasing levels of semantic depth.<\/p>\n<\/blockquote>\n\n<p>Thus, Azure recommends using Neural Networks for image classification. Remember, that the goal of the exam is to test your capacity to design data science solution <strong>using Azure<\/strong> so better to use their official documentation as a reference.<\/p>\n\n<p>And comparing to the other solutions:<\/p>\n\n<ol>\n<li>You create an Azure notebook that supports the Microsoft Cognitive\nToolkit.<\/li>\n<li>You create a Machine Learning experiment that implements\nthe Multiclass Decision Jungle module.<\/li>\n<li>You create an endpoint to the\nComputer vision API. <\/li>\n<li>You create a Machine Learning experiment that\nimplements the Multiclass Neural Network module.<\/li>\n<li>You create an Azure\nnotebook that supports the Microsoft Cognitive Toolkit.<\/li>\n<\/ol>\n\n<p>There are only 2 Azure ML Studio modules, and as the question is about constructing a <strong>workflow<\/strong> I guess we can only choose between them. (CNTK is actually the best solution as it allows constructing a deep neural network with ReLU whereas AML Studio doesn't, and API call is not about data science at all). <\/p>\n\n<p>Finally, I do agree with the other contributors that the question is absurd. Hope this helps.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":22.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":255.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.9002777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nAfter Sagemaker workspace stopped automatically, workspace env status is not updated.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Set sagemaker workspace config's AutoStopIdleTimeInMinutes as 10 minutes\r\n2. Create sagemaker workspace and wait for more than 10 minutes,\r\n3. Check sagemaker notebook instances to confirm the instance status is Stopped\r\n4. Check Service Workbench workspace status, it is still \"AVAILABLE\"\r\n\r\n**Expected behavior**\r\n1. Above step 4, workspace status should be \"STOPPED\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1657702977000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657451336000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is requesting to add three dependencies to the Sagemaker section, which are urllib3, PyYAML, and ipython, to train a Deepracer model locally with GPU support.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/44",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":69.9002777778,
        "Challenge_title":"[Bug] Sagemaker template, after auto stoped, workspace env status is not updated",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":116,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Fixed, refer [commit](https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/commit\/2339efe78d0f4705ef0cd6d2b1c5f06a810e6730) ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":52.8,
        "Solution_reading_time":1.81,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1436432728608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Colleferro, Italy",
        "Answerer_reputation_count":809.0,
        "Answerer_view_count":361.0,
        "Challenge_adjusted_solved_time":193.9353691667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a code in Azure ML which uses the function <code>ggrepel<\/code>. That function requires the version 2.0.0 of the package <code>ggplot2<\/code>. When I try to use it I obtain the error:<\/p>\n\n<pre><code>Error 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\npackage 'ggplot2' 1.0.0 was found, but &gt;= 2.0.0 is required by 'ggrepel'\n<\/code><\/pre>\n\n<p>So, what I did was: <\/p>\n\n<ol>\n<li>updated the R package <code>ggplot2<\/code> of my local version (is there a command to use to check the version of a package?);<\/li>\n<li>taken the folder related to <code>ggplot2<\/code>, and put it in the zip file I pass to Azure. So the x.zip wil contain generic functions, then ggrepel.zip and ggplot2.zip.<\/li>\n<\/ol>\n\n<p>At the end I have written:<\/p>\n\n<pre><code>install.packages(\"src\/ggplot2.zip\",lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggrepel.zip\",lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(ggrepel, lib.loc=\".\", verbose=TRUE)\nlibrary(ggplot2, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>It seems working for ggrepel, but not for ggplot, because I obtain the same issue shown at the beginning. It's like the system does not see the updated package, but the default ggplot2 of Azure ML.<\/p>",
        "Challenge_closed_time":1466607112096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1465908944767,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while using the function 'ggrepel' in Azure ML, which requires version 2.0.0 of the package 'ggplot2'. The user tried updating the package and including it in the zip file passed to Azure, but the system still uses the default version of ggplot2 and shows the same error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37812686",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":16.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":193.9353691667,
        "Challenge_title":"Package usage in AzureML: ggplot2 and ggrepel",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":142.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436432728608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation_count":809.0,
        "Poster_view_count":361.0,
        "Solution_body":"<p>At the end I have solved adding an additional package. The problem is in the fact that you have to check the log of the error and not only the error output (that does not insert all you need). At the end I have solved in this way:<\/p>\n\n<pre><code>install.packages(\"src\/scales_0.4.0.zip\" ,lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggplot2_2.1.0.zip\",lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggrepel.zip\"      ,lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(scales,  lib.loc=\".\", verbose=TRUE)\nlibrary(ggplot2, lib.loc=\".\", verbose=TRUE)\nlibrary(ggrepel, lib.loc=\".\", verbose=TRUE)\n...\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":8.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1523264005616,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Yokohama, Kanagawa, Japan",
        "Answerer_reputation_count":1635.0,
        "Answerer_view_count":143.0,
        "Challenge_adjusted_solved_time":20.8984680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a sagemaker model that is trained on a specific dataset, and training job is created. Now i have a new dataset that the model has to be trained on, how do I retrain the model on new data from the already existing model ? Can we have the model checkpoints saved ?<\/p>",
        "Challenge_closed_time":1584462704472,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584387469987,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a Sagemaker model that was trained on a specific dataset and a training job was created. They now have a new dataset that they want to use to retrain the model. The user is seeking guidance on how to retrain the model on new data from the existing model and whether it is possible to save model checkpoints.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60712225",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":4.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":20.8984680556,
        "Challenge_title":"How to retrain a sagemaker model on new data, after a training job is created",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":660.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1500501171967,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, NY, United States",
        "Poster_reputation_count":33.0,
        "Poster_view_count":8.0,
        "Solution_body":"<blockquote>\n  <p>Only three built-in algorithms currently support incremental training: Object Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/incremental-training.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":33.6,
        "Solution_reading_time":5.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1542402402780,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":186.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":56.0515805556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy an ALS model trained using PySpark on Azure ML service. I am providing a score.py file that loads the trained model using ALSModel.load() function. Following is the code of my score.py file.<\/p>\n<pre><code>import os\nfrom azureml.core.model import Model\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, StringType\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\nsqlContext = SQLContext(sc)\nspark = sqlContext.sparkSession\n\ninput_schema = StructType([StructField(&quot;UserId&quot;, StringType())])\nreader = spark.read\nreader.schema(input_schema)\n\n\ndef init():\n    global model\n    # note here &quot;iris.model&quot; is the name of the model registered under the workspace\n    # this call should return the path to the model.pkl file on the local disk.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), &quot;recommendation-model&quot;)\n    # Load the model file back into a LogisticRegression model\n    model = ALSModel.load(model_path)\n    \n\ndef run(data):\n    try:\n        input_df = reader.json(sc.parallelize([data]))\n        input_df = indexer.transform(input_df)\n        \n        res = model.recommendForUserSubset(input_df[['UserId_index']], 10)\n\n        # you can return any datatype as long as it is JSON-serializable\n        return result.collect()[0]['recommendations']\n    except Exception as e:\n        traceback.print_exc()\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>Following is the error I get when I deploy it as LocalWebService using Model.deploy function in Azure ML service<\/p>\n<pre><code>Generating Docker build context.\nPackage creation Succeeded\nLogging into Docker registry viennaglobal.azurecr.io\nLogging into Docker registry viennaglobal.azurecr.io\nBuilding Docker image from Dockerfile...\nStep 1\/5 : FROM viennaglobal.azurecr.io\/azureml\/azureml_43542b56c5ec3e8d0f68e1556558411f\n ---&gt; 5b3bb174ca5f\nStep 2\/5 : COPY azureml-app \/var\/azureml-app\n ---&gt; 8e540c0746f7\nStep 3\/5 : RUN mkdir -p '\/var\/azureml-app' &amp;&amp; echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjNkN2M1ZjM4LTI1ODEtNGUxNi05NTdhLWEzOTU1OGI1ZjBiMyIsInJlc291cmNlR3JvdXBOYW1lIjoiZGV2LW9tbmljeC10ZnMtYWkiLCJhY2NvdW50TmFtZSI6ImRldi10ZnMtYWktd29ya3NwYWNlIiwid29ya3NwYWNlSWQiOiI1NjkzNGMzNC1iZmYzLTQ3OWUtODRkMy01OGI4YTc3ZTI4ZjEifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode &gt; \/var\/azureml-app\/model_config_map.json\n ---&gt; Running in 502ad8edf91e\n ---&gt; a1bc5e0283d0\nStep 4\/5 : RUN mv '\/var\/azureml-app\/tmpvxhomyin.py' \/var\/azureml-app\/main.py\n ---&gt; Running in eb4ec1a0b702\n ---&gt; 6a3296fe6420\nStep 5\/5 : CMD [&quot;runsvdir&quot;,&quot;\/var\/runit&quot;]\n ---&gt; Running in 834fd746afef\n ---&gt; 5b9f8be538c0\nSuccessfully built 5b9f8be538c0\nSuccessfully tagged recommend-service:latest\nContainer (name:musing_borg, id:0f3163692f5119685eee5ed59c8e00aa96cd472f765e7db67653f1a6ce852e83) cannot be killed.\nContainer has been successfully cleaned up.\nImage sha256:0f146f4752878bbbc0e876f4477cc2877ff12a366fca18c986f9a9c2949d028b successfully removed.\nStarting Docker container...\nDocker container running.\nChecking container health...\nERROR - Error: Container has crashed. Did your init method fail?\n\n\nContainer Logs:\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:00,312735664+00:00 - rsyslog\/run \n2020-07-30T11:57:00,312768364+00:00 - gunicorn\/run \n2020-07-30T11:57:00,313017966+00:00 - iot-server\/run \nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\n2020-07-30T11:57:00,313969073+00:00 - nginx\/run \n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:00,597835804+00:00 - iot-server\/finish 1 0\n2020-07-30T11:57:00,598826211+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 19.9.0\nListening at: http:\/\/127.0.0.1:31311 (10)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 41\nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\nbash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by bash)\nIvy Default Cache set to: \/root\/.ivy2\/cache\nThe jars for the packages stored in: \/root\/.ivy2\/jars\n:: loading settings :: url = jar:file:\/home\/mmlspark\/lib\/spark\/jars\/ivy-2.4.0.jar!\/org\/apache\/ivy\/core\/settings\/ivysettings.xml\ncom.microsoft.ml.spark#mmlspark_2.11 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-e07358bb-d354-4f41-aa4c-f0aa73bb0156;1.0\n    confs: [default]\n    found com.microsoft.ml.spark#mmlspark_2.11;0.15 in spark-list\n    found io.spray#spray-json_2.11;1.3.2 in central\n    found com.microsoft.cntk#cntk;2.4 in central\n    found org.openpnp#opencv;3.2.0-1 in central\n    found com.jcraft#jsch;0.1.54 in central\n    found org.apache.httpcomponents#httpclient;4.5.6 in central\n    found org.apache.httpcomponents#httpcore;4.4.10 in central\n    found commons-logging#commons-logging;1.2 in central\n    found commons-codec#commons-codec;1.10 in central\n    found com.microsoft.ml.lightgbm#lightgbmlib;2.1.250 in central\n:: resolution report :: resolve 318ms :: artifacts dl 11ms\n    :: modules in use:\n    com.jcraft#jsch;0.1.54 from central in [default]\n    com.microsoft.cntk#cntk;2.4 from central in [default]\n    com.microsoft.ml.lightgbm#lightgbmlib;2.1.250 from central in [default]\n    com.microsoft.ml.spark#mmlspark_2.11;0.15 from spark-list in [default]\n    commons-codec#commons-codec;1.10 from central in [default]\n    commons-logging#commons-logging;1.2 from central in [default]\n    io.spray#spray-json_2.11;1.3.2 from central in [default]\n    org.apache.httpcomponents#httpclient;4.5.6 from central in [default]\n    org.apache.httpcomponents#httpcore;4.4.10 from central in [default]\n    org.openpnp#opencv;3.2.0-1 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n    ---------------------------------------------------------------------\n\n:: problems summary ::\n:::: ERRORS\n    unknown resolver repo-1\n\n    unknown resolver repo-1\n\n\n:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n:: retrieving :: org.apache.spark#spark-submit-parent-e07358bb-d354-4f41-aa4c-f0aa73bb0156\n    confs: [default]\n    0 artifacts copied, 10 already retrieved (0kB\/7ms)\n2020-07-30 11:57:02 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nInitialized PySpark session.\nInitializing logger\n2020-07-30 11:57:09,464 | root | INFO | Starting up app insights client\nStarting up app insights client\n2020-07-30 11:57:09,464 | root | INFO | Starting up request id generator\nStarting up request id generator\n2020-07-30 11:57:09,464 | root | INFO | Starting up app insight hooks\nStarting up app insight hooks\n2020-07-30 11:57:09,464 | root | INFO | Invoking user's init function\nInvoking user's init function\n2020-07-30 11:57:19,652 | root | ERROR | User's init function failed\nUser's init function failed\n2020-07-30 11:57:19,656 | root | ERROR | Encountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 163, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/main.py&quot;, line 44, in init\n    model = ALSModel.load(model_path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 362, in load\n    return cls.read().load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 300, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/java_gateway.py&quot;, line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/sql\/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/protocol.py&quot;, line 328, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o64.load.\n: java.util.NoSuchElementException: Param blockSize does not exist.\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n    at org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:591)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:589)\n    at scala.collection.immutable.List.foreach(List.scala:392)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.setParams(ReadWrite.scala:589)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.getAndSetParams(ReadWrite.scala:572)\n    at org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:533)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n\n\nEncountered Exception Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 163, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/main.py&quot;, line 44, in init\n    model = ALSModel.load(model_path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 362, in load\n    return cls.read().load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/ml\/util.py&quot;, line 300, in load\n    java_obj = self._jread.load(path)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/java_gateway.py&quot;, line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/pyspark\/sql\/utils.py&quot;, line 63, in deco\n    return f(*a, **kw)\n  File &quot;\/home\/mmlspark\/lib\/spark\/python\/lib\/py4j-0.10.7-src.zip\/py4j\/protocol.py&quot;, line 328, in get_return_value\n    format(target_id, &quot;.&quot;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o64.load.\n: java.util.NoSuchElementException: Param blockSize does not exist.\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at org.apache.spark.ml.param.Params$$anonfun$getParam$2.apply(params.scala:729)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.ml.param.Params$class.getParam(params.scala:728)\n    at org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:42)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:591)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata$$anonfun$setParams$1.apply(ReadWrite.scala:589)\n    at scala.collection.immutable.List.foreach(List.scala:392)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.setParams(ReadWrite.scala:589)\n    at org.apache.spark.ml.util.DefaultParamsReader$Metadata.getAndSetParams(ReadWrite.scala:572)\n    at org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:533)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:238)\n    at java.lang.Thread.run(Thread.java:748)\n\n\nWorker exiting (pid: 41)\nShutting down: Master\nReason: Worker failed to boot.\n\/bin\/bash: \/azureml-envs\/azureml_7fbe163ce1d4208cd897650a64b7a54d\/lib\/libtinfo.so.5: no version information available (required by \/bin\/bash)\n2020-07-30T11:57:19,833136837+00:00 - gunicorn\/finish 3 0\n2020-07-30T11:57:19,834216245+00:00 - Exit code 3 is not normal. Killing image.\n\n---------------------------------------------------------------------------\nWebserviceException                       Traceback (most recent call last)\n&lt;ipython-input-43-d0992ae9d1c9&gt; in &lt;module&gt;\n      6 local_service = Model.deploy(workspace, &quot;recommend-service&quot;, [register_model], inference_config, deployment_config)\n      7 \n----&gt; 8 local_service.wait_for_deployment()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/local.py in decorated(self, *args, **kwargs)\n     69                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n     70                                           logger=module_logger)\n---&gt; 71             return func(self, *args, **kwargs)\n     72         return decorated\n     73     return decorator\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/local.py in wait_for_deployment(self, show_output)\n    601                                    self._container,\n    602                                    health_url=self._internal_base_url,\n--&gt; 603                                    cleanup_if_failed=False)\n    604 \n    605             self.state = LocalWebservice.STATE_RUNNING\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_model_management\/_util.py in container_health_check(docker_port, container, health_url, cleanup_if_failed)\n    745             # The container has started and crashed.\n    746             _raise_for_container_failure(container, cleanup_if_failed,\n--&gt; 747                                          'Error: Container has crashed. Did your init method fail?')\n    748 \n    749         # The container hasn't crashed, so try to ping the health endpoint.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_model_management\/_util.py in _raise_for_container_failure(container, cleanup, message)\n   1258         cleanup_container(container)\n   1259 \n-&gt; 1260     raise WebserviceException(message, logger=module_logger)\n   1261 \n   1262 \n\nWebserviceException: WebserviceException:\n    Message: Error: Container has crashed. Did your init method fail?\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Error: Container has crashed. Did your init method fail?&quot;\n    }\n}\n<\/code><\/pre>\n<p>However, the ALSModel.load() works fine when executed in a Jupyter notebook.<\/p>",
        "Challenge_closed_time":1596478571823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596276786133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering an error when attempting to deploy an als model trained using pyspark on a service, with the error being a java.util.nosuchelementexception.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63204081",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.7,
        "Challenge_reading_time":224.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":179,
        "Challenge_solved_time":56.0515805556,
        "Challenge_title":"PySpark ALSModel load fails in deployment over Azure ML service with error java.util.NoSuchElementException: Param blockSize does not exist",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":598.0,
        "Challenge_word_count":1195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1596274712792,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Chandigarh, India",
        "Poster_reputation_count":13.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>A couple of things to check:<\/p>\n<ol>\n<li>Is your model registered in the workspace? AZUREML_MODEL_DIR only works for registered models. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none--sample-input-dataset-none--sample-output-dataset-none--resource-configuration-none-\" rel=\"nofollow noreferrer\">this link<\/a> for information about registering a model<\/li>\n<li>Are you specifying the same version of pyspark.ml.recommendation in your InferenceConfig as you use locally? This kind of error might be due to a difference in versions<\/li>\n<li>Have you looked at the output of <code>print(service.get_logs())<\/code>? Check out our <a href=\"https:\/\/review.docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?branch=pr-en-us-124666\" rel=\"nofollow noreferrer\">troubleshoot and debugging documentation here<\/a> for other things you can try<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.2,
        "Solution_reading_time":15.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":334.3928986111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am developing an Android application that uses raw accelerometer data and I want to classify the data by using machine learning, i.e. Azure ML service. For example when device moves like a 1 in space, it should generate number 1 in text field specified in application. I decided to use Machine Learning to classify movements but I couldn't decide how to store data and send it to the machine learning service for training. For now, I am creating an SQLite table in application and add the X,Y,Z value of sensor each time sensor data gets changed. After that I am sending data to machine learning service but I have problem. The data only includes one movement for 1. How can I store multiple data for same movement and data for other movements -that will represent different numbers like 2, 3- and send them to the machine learning service?<\/p>",
        "Challenge_closed_time":1473766291168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1472562476733,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is developing an Android application that uses raw accelerometer data and wants to classify the data using machine learning. They are having trouble deciding how to store and send the data to the machine learning service for training, as they need to store multiple data for the same movement and data for other movements that represent different numbers. Currently, they are creating an SQLite table in the application and adding the X,Y,Z value of the sensor each time the sensor data changes.",
        "Challenge_last_edit_time":1483518837827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39228377",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":334.3928986111,
        "Challenge_title":"How to Store Accelerometer Data for Classification by using Machine Learning",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":730.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428256167643,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"\u0130stanbul, T\u00fcrkiye",
        "Poster_reputation_count":576.0,
        "Poster_view_count":68.0,
        "Solution_body":"<p>@MuhammedKadirY\u00fccel, Based on my understanding, I think you want to send raw accelerometer data to Azure and store into some storage service for importing on Machine Learning service.<\/p>\n\n<p>Per my experience, I think the best practice is that create a <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/services\/event-hubs\/\" rel=\"nofollow\">EventHub<\/a> or <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/services\/iot-hub\/\" rel=\"nofollow\">IoTHub<\/a> instance for receiving these raw accelerometer data. <\/p>\n\n<p>Then create a <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-get-started-with-azure-stream-analytics-to-process-data-from-iot-devices\/\" rel=\"nofollow\">Stream Analytics<\/a> to transfer sensor data from EventHub or IoTHub to Azure Blob Storage. <\/p>\n\n<p>Finally, you can import these data of the blob storage on Machine Learning service, please see <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-import-data-from-online-sources\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-import-data-from-online-sources\/<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":22.7,
        "Solution_reading_time":15.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":0.2048830556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I execute code without parallel computation, <code>n_trials<\/code> in the <code>optimize<\/code> function means how many trials the program runs. When executed via parallel computation (following the tutorial <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/10_key_features\/004_distributed.html\" rel=\"nofollow noreferrer\">here<\/a> via launching it again in another console), it does <code>n_trials<\/code> for each process, not for all the sum of processes like I would like.<\/p>\n<p>Is there a way to make sure that the sum of all parallel processes' trials are equal to a fixed number, regardless of how many process I launch?<\/p>",
        "Challenge_closed_time":1629887227052,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629886489473,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with setting the n_trials parameter for multiple processes when using parallelization. When executing code via parallel computation, n_trials is executed for each process instead of the sum of all processes. The user is seeking a way to ensure that the total number of trials is fixed regardless of the number of processes launched.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68920952",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2048830556,
        "Challenge_title":"How to set n_trials for multiple processes when using parallelization?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":330.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529092998780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":788.0,
        "Poster_view_count":22.0,
        "Solution_body":"<p>Yes, <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.MaxTrialsCallback.html#optuna.study.MaxTrialsCallback\" rel=\"nofollow noreferrer\"><code>MaxTrialsCallback<\/code><\/a> is the exact feature for such a situation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":38.9,
        "Solution_reading_time":3.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1396956946647,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tel Aviv, Israel",
        "Answerer_reputation_count":72196.0,
        "Answerer_view_count":7104.0,
        "Challenge_adjusted_solved_time":0.0607519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to create sparse matrix for one hot encoded features from data frame <code>df<\/code>. But I am getting memory issue for code given below. Shape of <code>sparse_onehot<\/code> is  (450138, 1508)<\/p>\n<pre><code>sp_features = ['id', 'video_id', 'genre']\nsparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features)\nimport scipy\nX = scipy.sparse.csr_matrix(sparse_onehot.values)\n<\/code><\/pre>\n<p>I get memory error as shown below.<\/p>\n<pre><code>MemoryError: Unable to allocate 647. MiB for an array with shape (1508, 450138) and data type uint8\n<\/code><\/pre>\n<p>I have tried <code>scipy.sparse.lil_matrix<\/code> and get same error as above.<\/p>\n<p>Is there any efficient way of handling this?\nThanks in advance<\/p>",
        "Challenge_closed_time":1597911539563,
        "Challenge_comment_count":2,
        "Challenge_created_time":1597908911857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing memory issues while creating a sparse matrix for one hot encoded features from a data frame. The shape of the resulting sparse matrix is (450138, 1508), and the user is getting a memory error while using pd.get_dummies() and scipy.sparse.csr_matrix(). The user has also tried using scipy.sparse.lil_matrix() but is still facing the same error. The user is seeking an efficient way to handle this issue.",
        "Challenge_last_edit_time":1597911320856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63500377",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.1,
        "Challenge_reading_time":9.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.7299183334,
        "Challenge_title":"memory issues for sparse one hot encoded features",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487135761367,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":911.0,
        "Poster_view_count":91.0,
        "Solution_body":"<p>Try setting to <code>True<\/code> the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html\" rel=\"nofollow noreferrer\"><code>sparse<\/code> parameter<\/a>:<\/p>\n<blockquote>\n<p>sparsebool, default False\nWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).<\/p>\n<\/blockquote>\n<pre><code>sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = True)\n<\/code><\/pre>\n<p>This will use a much more memory efficient (but somewhat slower) representation than the default one.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.6,
        "Solution_reading_time":7.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1360164540016,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH",
        "Answerer_reputation_count":11190.0,
        "Answerer_view_count":365.0,
        "Challenge_adjusted_solved_time":49.8260258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is a hard situation to describe.<\/p>\n\n<p>I have a python model train script at:<\/p>\n\n<p><code>myproject\/opt\/program\/train<\/code><\/p>\n\n<p>This gets a file at <code>.\/opt\/ml\/input\/data\/external\/train.csv<\/code><\/p>\n\n<p>When I do <code>python3 opt\/program\/train<\/code> the training runs fine locally.<\/p>\n\n<p>Then I containerize the project and copy <code>opt<\/code> to <code>\/opt<\/code> in my Dockerfile.<\/p>\n\n<p>Now when I run <code>docker run &lt;image name&gt; train<\/code> it also trains fine.<\/p>\n\n<p>Then I deploy the image to SageMaker, create an estimator, and call <code>model.fit(my_data)<\/code> I get:<\/p>\n\n<p><code>Exception during training: [Errno 2] File b'.\/opt\/ml\/input\/data\/external\/train.csv' does not exist<\/code><\/p>\n\n<p>It's definitely there, I was able to train by running the container myself.  Also running the container and exploring the file system I can find the file.<\/p>\n\n<p>So I think I have some filesystem misunderstanding.  From the root of the container, all of these seem to have equivalent outputs.<\/p>\n\n<pre><code>root@798ffe7364c6:\/# ls opt\nml  program\nroot@798ffe7364c6:\/# ls \/opt\nml  program\nroot@798ffe7364c6:\/# ls .\/opt\nml  program\n<\/code><\/pre>\n\n<p>I'm trying to come up with a way to have one path that will work locally, in the container, and on AWS.<\/p>",
        "Challenge_closed_time":1574100008956,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573920322537,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sagemaker where the model training script is unable to find a file at the specified path, even though the file exists and the training runs fine locally and in the container. The user suspects a filesystem misunderstanding and is trying to find a way to have one path that will work locally, in the container, and on AWS.",
        "Challenge_last_edit_time":1573920635263,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58892606",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":16.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":49.9128941667,
        "Challenge_title":"Sagemaker can't find paths in container",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2448.0,
        "Challenge_word_count":175,
        "Platform":"Stack Overflow",
        "Poster_created_time":1360164540016,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH",
        "Poster_reputation_count":11190.0,
        "Poster_view_count":365.0,
        "Solution_body":"<p>I was missing the fact that SageMaker looks for your data channels in S3 and copies those to your container at <code>\/opt\/ml\/input\/data<\/code><\/p>\n\n<p>By default it seems to use <code>training<\/code> and <code>validation<\/code> as the channel names.  Therefore, in my example above, it would have never copied data from my <code>external<\/code> folder on S3 to the right <code>external<\/code> folder in my container.  In fact, I discovered it was copying it instead to <code>\/opt\/ml\/input\/data\/training\/external\/train.csv<\/code>.<\/p>\n\n<p>To resolve this, I would have either had to change my folder names, or use <code>InputDataConfig<\/code> to define other channels.  I chose the later and was able to get it working.<\/p>\n\n<p>More info on <code>InputDataConfig<\/code> here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.2,
        "Solution_reading_time":12.56,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":321.5504361111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>A VAE built using PyTorch runs smoothly when I train it directly. However, with wandb sweeps, I encounter the following BrokerPipeError. According to some forum threads, the main cause seems to be the more than one <code>num_workers<\/code> in the DataLoader module when running on Windows OS. However, I have a DGX-station running Ubuntu, and I still get the error.<\/p>\n<pre><code class=\"lang-auto\">wandb: Waiting for W&amp;B process to finish... (failed 1). Press Control-C to abort syncing.\nException in thread ChkStopThr:\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"\/opt\/conda\/lib\/python3.8\/threading.py\", line 870, in run\n    Exception in thread self._target(*self._args, **self._kwargs)NetStatThr\n:\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 276, in check_stop_status\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/threading.py\", line 932, in _bootstrap_inner\n    self._loop_check_status(\n      File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 214, in _loop_check_status\nself.run()\n  File \"\/opt\/conda\/lib\/python3.8\/threading.py\", line 870, in run\n    local_handle = request()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 787, in deliver_stop_status\n    self._target(*self._args, **self._kwargs)\nreturn self._deliver_stop_status(status)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 258, in check_network_status\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 585, in _deliver_stop_status\n    self._loop_check_status(\nreturn self._deliver_record(record)  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 214, in _loop_check_status\n\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 560, in _deliver_record\n    handle = mailbox._deliver_record(record, interface=self)\n    local_handle = request()  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/mailbox.py\", line 455, in _deliver_record\n\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 795, in deliver_network_status\n    interface._publish(record)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 221, in send_record_publish\n    return self._deliver_network_status(status)\n      File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 601, in _deliver_network_status\nself.send_server_request(server_req)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 152, in _send_message\n    return self._deliver_record(record)\n      File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 560, in _deliver_record\nself._sendall_with_error_handle(header + data)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 130, in _sendall_with_error_handle\n    handle = mailbox._deliver_record(record, interface=self)\nsent = self._sock.send(data)  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/mailbox.py\", line 455, in _deliver_record\n\nBrokenPipeError: [Errno 32] Broken pipe\n    interface._publish(record)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nBrokenPipeError: [Errno 32] Broken pipe\n<\/code><\/pre>\n<p>Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Challenge_closed_time":1680807676343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679650094773,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a BrokenPipeError while using wandb sweeps on a VAE built using PyTorch. The error is usually caused by having more than one num_workers in the DataLoader module when running on Windows OS, but the user is running Ubuntu on a DGX-station and still getting the error. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/brokenpipeerror-on-ubuntu-machine\/4117",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":18.3,
        "Challenge_reading_time":59.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":59,
        "Challenge_solved_time":321.5504361111,
        "Challenge_title":"BrokenPipeError on Ubuntu machine",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":295.0,
        "Challenge_word_count":313,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello! Looks like it there is a Connection issue between your machine and the <code>wandb<\/code> server. Is there a load balancer, a VPN, or a proxy that your machine is behind that may be blocking the connection? The reason I ask is because  <code>sent = self._sock.send(data)<\/code>  is the main error in the stack trace which means that the client is struggling to send data to the server.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":4.86,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":66.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6102777778,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nHi,\r\n\r\nI am trying to integrate pycaret with mlflow using your parameter `log_experiment` in `setup()`. When I set it to true, everything is stores as planned in my local MlFlow server, but not the metrics.\r\n\r\nIn the documentation is says the `log_experiment=True` should control everything. So I am not sure if I do something wrong here of if it is a bug from your side.\r\n\r\nWould be glad if you could help!\n\n### Reproducible Example\n\n```python\nfrom pycaret.datasets import get_data\r\nfrom pycaret.regression import *\r\ndf = get_data('bike')\r\nexp = RegressionExperiment()\r\nexp.setup(data=df, log_experiment=True)\r\nmodel = exp.create_model(\"lr\")\r\npred = exp.predict_model(estimator=model)\r\nexp.finalize_model(estimator=model)\n```\n\n\n### Expected Behavior\n\nshould log metrics\n\n### Actual Results\n\n```python-traceback\nNo metrics logged.\n```\n\n\n### Installed Versions\n\n<details>\r\nPyCaret 3.0.0rc3\r\n<\/details>\r\n",
        "Challenge_closed_time":1660653306000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1660651109000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to save plots through MLFLOW. The error message states that \"plot_model() got an unexpected keyword argument 'system'\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2856",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":16.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.6102777778,
        "Challenge_title":"MlFlow not logging metrics",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":161,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Update:\r\n\r\nWhen I write `exp.get_logs()` I can see some metrics there. But some runs still have the status \"RUNNING\", unsure why.\r\n\r\nAlso, all the runs that exist when I start the server using `!mlflow ui` are missing metrics. Edit: found issue, not on you! Sorry :)",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.7,
        "Solution_reading_time":3.16,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":8.6530769444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm training a large-ish model, trying to use for the purpose <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Azure Machine Learning service<\/a> in Azure notebooks.<\/p>\n\n<p>I thus create an <code>Estimator<\/code> to train locally:<\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\n\nestimator = Estimator(source_directory='.\/source_dir',\n                      compute_target='local',\n                      entry_script='train.py')\n<\/code><\/pre>\n\n<p>(my <code>train.py<\/code> should load and train starting from a large word vector file).<\/p>\n\n<p>When running with <\/p>\n\n<pre><code>run = experiment.submit(config=estimator)\n<\/code><\/pre>\n\n<p>I get <\/p>\n\n<blockquote>\n  <p>TrainingException: <\/p>\n  \n  <p>====================================================================<\/p>\n  \n  <p>While attempting to take snapshot of\n  \/data\/home\/username\/notebooks\/source_dir Your total\n  snapshot size exceeds the limit of 300.0 MB. Please see\n  <a href=\"http:\/\/aka.ms\/aml-largefiles\" rel=\"nofollow noreferrer\">http:\/\/aka.ms\/aml-largefiles<\/a> on how to work with large files.<\/p>\n  \n  <p>====================================================================<\/p>\n<\/blockquote>\n\n<p>The link provided in the error is likely <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/26076\" rel=\"nofollow noreferrer\">broken<\/a>. \nContents in my <code>.\/source_dir<\/code> indeed exceed 300 MB.<br>\nHow can I solve this?<\/p>",
        "Challenge_closed_time":1554445793380,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554414642303,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a TrainingException while training a large model with Azure Machine Learning service in Azure notebooks. The error message indicates that the total snapshot size exceeds the limit of 300.0 MB, and the link provided in the error is broken. The user is seeking a solution to overcome this issue.",
        "Challenge_last_edit_time":1554642637940,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55525445",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":20.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":8.6530769444,
        "Challenge_title":"How to overcome TrainingException when training a large model with Azure Machine Learning service?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1127.0,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>You can place the training files outside <code>source_dir<\/code> so that they don't get uploaded as part of submitting the experiment, and then upload them separately to the data store (which is basically using the Azure storage associated with your workspace). All you need to do then is reference the training files from <code>train.py<\/code>. <\/p>\n\n<p>See the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">Train model tutorial<\/a> for an example of how to upload data to the data store and then access it from the training file.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1554449103928,
        "Solution_link_count":1.0,
        "Solution_readability":13.6,
        "Solution_reading_time":7.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1398051491376,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Massachusetts",
        "Answerer_reputation_count":459.0,
        "Answerer_view_count":108.0,
        "Challenge_adjusted_solved_time":0.1220702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have close to 100 processing jobs to which I want to add certain tags. I've found commands that you can use to tag one resource with a list of tags. Is there any way I can do this for multiple jobs? Through CLI or through python+boto?<\/p>",
        "Challenge_closed_time":1657516175783,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657515736330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to add tags to almost 100 processing jobs and is looking for a way to do it for multiple jobs using AWS CLI or python+boto.",
        "Challenge_last_edit_time":1657607287156,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72933908",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":3.4,
        "Challenge_reading_time":3.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1220702778,
        "Challenge_title":"Can I use AWS CLI to add tags to all processing jobs matching a certain regex",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":38.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1498814861883,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":37.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You can <code>ResourceGroupsTaggingAPI<\/code>'s method <code>tag_resources()<\/code>.<br \/>\nThis is used to apply one or more tags to the specified list of resources.<\/p>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.tag_resources\" rel=\"nofollow noreferrer\">Tag Resources using boto3<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.untag_resources\" rel=\"nofollow noreferrer\">UnTag Resources using boto3<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":31.0,
        "Solution_reading_time":9.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393284798160,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Jose, CA",
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":92.0291155556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are using SageMaker Batch Transform job and to fit as many records in a mini-batch as can fit within the <code>MaxPayloadInMB<\/code> limit, we are setting <code>BatchStrategy<\/code> to <code>MultiRecord<\/code> and <code>SplitType<\/code> to <code>Line<\/code>.<\/p>\n<p>Input to the SageMaker batch transform job is:<\/p>\n<pre><code>{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 90, &quot;Experience&quot;: 26, &quot;Income&quot;: 30, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 55, &quot;Experience&quot;: 26, &quot;Income&quot;: 450, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-69e22778-594916685f4ceca66c08bfbc&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:46:32.386Z&quot;}\n<\/code><\/pre>\n<p>This is the SageMaker batch transform job config:<\/p>\n<pre><code>apiVersion: sagemaker.aws.amazon.com\/v1\nkind: BatchTransformJob\nmetadata:\n        generateName: '...-batchtransform'\nspec:\n        batchStrategy: MultiRecord\n        dataProcessing:\n                JoinSource: Input\n                OutputFilter: $\n                inputFilter: $.requestBody\n        modelClientConfig:\n                invocationsMaxRetries: 0\n                invocationsTimeoutInSeconds: 3\n        mName: '..'\n        region: us-west-2\n        transformInput:\n                contentType: application\/json\n                dataSource:\n                        s3DataSource:\n                                s3DataType: S3Prefix\n                                s3Uri: s3:\/\/......\/part-\n                splitType: Line\n        transformOutput:\n                accept: application\/json\n                assembleWith: Line\n                kmsKeyId: '....'\n                s3OutputPath: s3:\/\/....\/batch_output\n        transformResources:\n                instanceCount: ..\n                instanceType: '..'\n<\/code><\/pre>\n<p>The SageMaker batch transform job fails with:<\/p>\n<p>Error in batch transform data-log -<\/p>\n<blockquote>\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n400 Bad Request 2022-01-27T00:55:39.781:[sagemaker\nlogs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:\n<p>Failed to decode JSON object: Extra data: line 2 column 1 (char\n163)<\/p>\n<\/blockquote>\n<p><strong>Observation:<\/strong>\nThis issue occurs when we provide <code>batchStrategy: MultiRecord<\/code> in the manifest along with these data processing configs:<\/p>\n<pre><code>dataProcessing:\n        JoinSource: Input\n        OutputFilter: $\n        inputFilter: $.requestBody\n<\/code><\/pre>\n<p><strong>NOTE:<\/strong> If we put <code>batchStrategy: SingleRecord<\/code> along with the aforementioned data processing configs, it just works fine (job succeeds)!<\/p>\n<p><strong>Question:<\/strong> How can we achieve successful run with <code>batchStrategy: MultiRecord<\/code> along with the aforementioned data processing config?<\/p>\n<p>A successful output with <code>batchStrategy: SingleRecord<\/code> looks like this:<\/p>\n<blockquote>\n<p>{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:90,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:30}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-69e22778-594916685f4ceca66c08bfbc&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:55,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:450}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:46:32.386Z&quot;}\nRegion name \u2013 optional: Relevant resource ARN \u2013 optional:\narn:aws:sagemaker:us-west-2:435945521637:transform-job\/my-pipeline-9v28r-bat-e548fbfb125946528957e0f123456789<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1643938529208,
        "Challenge_comment_count":3,
        "Challenge_created_time":1643344870580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with a SageMaker batch transform job that fails when using 'batchStrategy: MultiRecord' along with data processing. The error message indicates a failure to decode a JSON object due to extra data. The job succeeds when using 'batchStrategy: SingleRecord' with the same data processing configuration. The user is seeking a solution to successfully run the job with 'batchStrategy: MultiRecord' and the data processing configuration.",
        "Challenge_last_edit_time":1643607224392,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70888883",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":22.6,
        "Challenge_reading_time":66.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":164.9051744444,
        "Challenge_title":"Sagemaker batch transform job failure for 'batchStrategy: MultiRecord' along with data processing",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":659.0,
        "Challenge_word_count":278,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316155655123,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cupertino, CA, USA",
        "Poster_reputation_count":8141.0,
        "Poster_view_count":1066.0,
        "Solution_body":"<p>When your input data is in JSON line format and you choose a SingleRecord BatchStrategy, your container will receive a single JSON payload body like below<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>However, if you use MultiRecord, Batch transform will split your JSON line input (which might contain 100 lines for example) into multiple records (say 10 records) all sent at once to your container as shown below:<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n.\n.\n.\n{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>Therefore your container should be able to handle such input for it to work. However, from the error message, I can see it is complaining about invalid JSON format as it reads the second row of the request.<\/p>\n<p>I also noticed that you have supplied <code>ContentType<\/code> and <code>AcceptType<\/code> as <code>application\/json<\/code> but instead should be <code>application\/jsonlines<\/code><\/p>\n<p>Could you please test your container to see if it can handle multiple JSON line records per single invocation.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":13.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1464811778510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":196.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":7.7506297223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server\" rel=\"nofollow noreferrer\">NVIDIA Triton<\/a>\u00a0vs\u00a0<a href=\"https:\/\/pytorch.org\/serve\/\" rel=\"nofollow noreferrer\">TorchServe<\/a>\u00a0for SageMaker inference? When to recommend each?<\/p>\n<p>Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker.<\/p>\n<p>Anyone has a good comparison matrix for both?<\/p>",
        "Challenge_closed_time":1663971240670,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663943338403,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking a comparison between NVIDIA Triton and TorchServe for SageMaker inference, as both are modern, production grade inference servers. They are specifically looking for a comparison matrix to determine when to recommend each.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73829280",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":7.7506297223,
        "Challenge_title":"NVIDIA Triton vs TorchServe for SageMaker Inference",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":12.0,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>Important notes to add here where both serving stacks differ:<\/p>\n<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.\nif you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).\nThere is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code\/handlers instead of just being able to serve a model's forward function.<\/p>\n<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.<\/p>\n<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized\/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":27.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":363.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1424548126510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":665.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":129.3699730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My understanding is Dev Endpoints in AWS Glue can be used to develop code iteratively and then deploy it to a Glue job. I find this specially useful when developing Spark jobs because every time you run a job, it takes several minutes to launch a Hadoop cluster in the background. However, I am seeing a discrepancy when using Python shell in Glue instead of Spark. <code>Import pg<\/code> doesn't work in a Dev Endpoint I created using Sagemaker JupyterLab Python notebook, but works in AWS Glue when I create a job using Python shell. Shouldn't the same libraries exist in the dev endpoint that exist in Glue? What is the point of having a dev endpoint if you cannot reproduce the same code in both places (dev endpoint and the Glue job)?<\/p>",
        "Challenge_closed_time":1552530714763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552064664580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is experiencing a discrepancy between AWS Glue and its Dev Endpoint when using Python shell instead of Spark. The user is unable to import a library in a Dev Endpoint created using Sagemaker JupyterLab Python notebook, but it works in AWS Glue when creating a job using Python shell. The user questions the purpose of having a Dev Endpoint if the same code cannot be reproduced in both places.",
        "Challenge_last_edit_time":1552064982860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55067802",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":129.4583841667,
        "Challenge_title":"Discrepancy between AWS Glue and its Dev Endpoint",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462207957683,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1305.0,
        "Poster_view_count":174.0,
        "Solution_body":"<p>Firstly, Python shell jobs would not launch a Hadooo Cluster in the backend as it does not give you a Spark environment for your jobs.\nSecondly, since PyGreSQL is not written in Pure Python, it will not work with Glue's native environment (Glue Spark Job, Dev endpoint etc)\nThirdly, Python Shell has additional support for certain package built-in.<\/p>\n\n<p>Thus, I don't see a point of using DevEndpoint for Python Shell jobs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":5.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.3833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"In this article, learn how to build an end-to-end data to AI solution on Google Cloud, including a practical example of a real-time fraud detection system and the architecture behind it. You'll also discover how to train, deploy, and monitor machine learning models in production.\n\nThis article is based on a recent Cloud OnBoard session.\u00a0Register here to watch on demand.\u00a0\u00a0\u00a0\n\nIf you have any questions, please leave a comment on the blog (or below) and someone from the Community or Google Cloud team will be happy to help.\n\nRead the blog",
        "Challenge_closed_time":1684833900000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684756920000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The article discusses how to create a data to AI solution on Google Cloud, with a focus on building a real-time fraud detection system. It covers the architecture, training, deployment, and monitoring of machine learning models in production. The reader is encouraged to ask questions in the comments section for assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-build-a-data-to-AI-solution-with-BigQuery-and-Vertex-AI\/m-p\/595708#M1991",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":21.3833333333,
        "Challenge_title":"How to build a data to AI solution with BigQuery and Vertex AI",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":111.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Great read!\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":0.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1400869861950,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, Brazil",
        "Answerer_reputation_count":135.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":18.1100302778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pretrained spacy model on a local folder that I can easily read with <code>m = spacy.load(&quot;path\/model\/&quot;)<\/code><\/p>\n<p>But now I have to upload it as a .tar.gz file to use as a Sagemaker model artifact.\nHow can I read this .tar.gz file?<\/p>\n<p>Ideally I want to read the unzipped folder from memory. Without extracting all to disk and then reading it again<\/p>\n<p>My question is almost a duplicate of this one <a href=\"https:\/\/stackoverflow.com\/questions\/49274650\/directly-load-spacy-model-from-packaged-tar-gz-file\">Directly load spacy model from packaged tar.gz file<\/a>. But the answers don't explain how to untar unzip the folder into memory<\/p>",
        "Challenge_closed_time":1652803358076,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652738161967,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a pretrained spacy model on a local folder that needs to be uploaded as a .tar.gz file to use as a Sagemaker model artifact. The user is looking for a way to read the unzipped folder from memory without extracting all to disk and then reading it again. The user's question is similar to a previously asked question, but the answers do not explain how to untar unzip the folder into memory.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72266041",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":9.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":18.1100302778,
        "Challenge_title":"Loading a spacy .tar.gz model artifact from s3 Sagemaker",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Poster_created_time":1400869861950,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, Brazil",
        "Poster_reputation_count":135.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Turns out Sagemaker already decompress the <code>.tar.gz<\/code> file automatically.\nSo I can just read the folder exactly like before.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1599390178756,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":628.2919663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm working in sage maker studio, and I have a single instance running one computationally intensive task:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/IntzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IntzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It appears that the kernel running my task is maxed out, but the actual instance is only using a small amount of its resources. Is there some sort of throttling occurring? Can I configure this so that more of the instance is utilized?<\/p>",
        "Challenge_closed_time":1629552761387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627529779217,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sage Maker Studio where the kernel running a computationally intensive task is maxed out, but the instance is only using a small amount of its resources. The user is seeking help to configure the instance to utilize more of its resources and avoid throttling.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68569742",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":561.9394916667,
        "Challenge_title":"Sage Maker Studio CPU Usage",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":946.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>Your ml.c5.xlarge instance comes with 4 vCPU. However, Python only uses a single CPU by default. (Source: <a href=\"https:\/\/stackoverflow.com\/questions\/64121703\/can-i-apply-multithreading-for-computationally-intensive-task-in-python\">Can I apply multithreading for computationally intensive task in python?<\/a>)<\/p>\n<p>As a result, the overall CPU utilization of your ml.c5.xlarge instance is low. To utilize all the vCPUs, you can try multiprocessing.<\/p>\n<p>The examples below are performed using a 2 vCPU + 4 GiB instance.<\/p>\n<p>In the first picture, multiprocessing is not set up. The instance CPU utilization peaks at around 50%.<\/p>\n<p>single processing:<br \/>\n<img src=\"https:\/\/i.stack.imgur.com\/lcn8K.png\" alt=\"single processing\" \/><\/p>\n<p>In the second picture, I created 50 processes to be run simultaneously. The instance CPU utilization rises to 100% immediately.<\/p>\n<p>multiprocessing:<br \/>\n<img src=\"https:\/\/i.stack.imgur.com\/tk65n.png\" alt=\"multiprocessing\" \/><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1629791630296,
        "Solution_link_count":3.0,
        "Solution_readability":12.1,
        "Solution_reading_time":12.79,
        "Solution_score_count":3.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":111.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.6317019444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I trained a model with Designer, created a real-time inference pipeline which was succesfully submitted. When deploying to either ACI or AKS it fails and I get the error &quot;ModuleNotFoundError: No module named 'azureml.api'&quot;. I've had no problems deploying this model many times in the past and haven't changed anything. Even if I use one of the sample pipelines (automobiles basic), I get the same error when deploying to real-time. <\/p>",
        "Challenge_closed_time":1632968850120,
        "Challenge_comment_count":2,
        "Challenge_created_time":1632862175993,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues while deploying a model from Designer to ACI or AKS. The error message \"ModuleNotFoundError: No module named 'azureml.api'\" is displayed, even when using sample pipelines. The user has successfully deployed the model in the past and has not made any changes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/569925\/deployment-from-designer-fails-in-every-possible-w",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":29.6317019444,
        "Challenge_title":"Deployment from Designer fails in every possible way",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>It's an known issue caused by unexpected module version upgrade. It's been resolved by applying hotfix to all regions. For users, please rerun training pipeline by check on &quot;Regenerate Output&quot;, and run corresponding inference pipeline and try deployment again.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.5,
        "Solution_reading_time":3.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359884693920,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Challenge_adjusted_solved_time":1.5477430556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Im trying to set comet (<a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a>) to track my Tensorflow experiment, after I create an Experiment and log the data set i dont get the accuracy in my report.<\/p>\n\n<p>my code:<\/p>\n\n<pre><code>mnist = get_data()\ntrain_step, cross_entropy, accuracy, x, y, y_ = build_model_graph(hyper_params)\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\nexperiment.log_multiple_params(hyper_params)\nexperiment.log_dataset_hash(mnist)\n<\/code><\/pre>\n\n<p>in the example account : <a href=\"https:\/\/www.comet.ml\/view\/Jon-Snow\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml\/view\/Jon-Snow<\/a> I see that accuracy is reported<\/p>",
        "Challenge_closed_time":1506100257932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506094686057,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to configure comet.ml to track their Tensorflow experiment, but after creating an experiment and logging the data set, they are not getting accuracy in their report. They have shared their code and noticed that accuracy is reported in an example account on comet.ml.",
        "Challenge_last_edit_time":1514341154200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46368389",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":9.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1.5477430556,
        "Challenge_title":"How to configure comet (comet.ml) to log Tensorflow?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":338.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506066897167,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>you can report accuracy using this method:<\/p>\n\n<ul>\n<li><code>experiment.log_accuracy(train_accuracy)<\/code><\/li>\n<\/ul>\n\n<p>take a look at the full Tensorflow example in our guide:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1513514205487,
        "Solution_link_count":2.0,
        "Solution_readability":21.0,
        "Solution_reading_time":5.41,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.8703702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am aware that azureml will drop support for python 2.7, but I have got some old codes and have to finish training the models. Since I will not use the codes afterwards anyway, so I do not want to spend much time to port to python 3.  <\/p>\n<p>As I tried to run the codes in python 2.7 on a compute cluster, I got the error <code>ImportError: cannot import name OutputFileDatasetConfig<\/code> coming from this line:  <br \/>\n<code>from azureml.data import OutputFileDatasetConfig<\/code>   <br \/>\nThe environment, that I have created for python 2.7, has azureml-core v1.1.5. I cannot find any documentation for this version, so I do not know, if it supports <code>OutputFileDatasetConfig<\/code>.  <\/p>\n<p>Can someone tell me, how I can run my codes in python 2.7 on compute clusters? Thanks!   <\/p>",
        "Challenge_closed_time":1637207379763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637171846430,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to run Python 2.7 scripts on a compute cluster but is encountering an error related to the import of OutputFileDatasetConfig from azureml.data. The user has created an environment for Python 2.7 with azureml-core v1.1.5 but is unsure if this version supports OutputFileDatasetConfig. The user is seeking guidance on how to run their codes in Python 2.7 on compute clusters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/631040\/how-to-run-python-2-7-scripts-on-a-computer-cluste",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":10.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":9.8703702778,
        "Challenge_title":"How to run python 2.7 scripts on a computer cluster",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":139,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=0f653a2d-e976-46b6-9242-19e12e493242\">@Lu  <\/a>     <\/p>\n<p>Thanks for reaching out to us. I have not found any official document either.     <\/p>\n<p>In this scenario, I think the quickest way to solve the problem is to raise a support ticket.    <\/p>\n<p>Let me know if you have no support plan, please share the ticket id since I will forward this issue to product team to see what we can do more.    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":10.0,
        "Solution_reading_time":19.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1363320186152,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1352.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":9.9320377778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a minimal example of a neural network with a back-propagation trainer, testing it on the IRIS data set. I started of with 7 hidden nodes and it worked well.<\/p>\n\n<p>I lowered the number of nodes in the hidden layer to 1 (expecting it to fail), but was surprised to see that the accuracy went up.<\/p>\n\n<p>I set up the experiment in azure ml, just to validate that it wasn't my code. Same thing there, 98.3333% accuracy with a single hidden node.<\/p>\n\n<p>Can anyone explain to me what is happening here?<\/p>",
        "Challenge_closed_time":1462119764790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1462108714660,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a neural network with a back-propagation trainer and tested it on the IRIS dataset. They initially used 7 hidden nodes and achieved good accuracy. However, when they reduced the number of nodes to 1, they were surprised to see that the accuracy actually increased. The user is seeking an explanation for this unexpected result.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36967126",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":7.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.0694805556,
        "Challenge_title":"Why do I get good accuracy with IRIS dataset with a single hidden node?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":4488.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426502047332,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":825.0,
        "Poster_view_count":82.0,
        "Solution_body":"<p>First, it has been well established that a variety of classification models yield incredibly good results on Iris (Iris is very predictable); see <a href=\"http:\/\/lab.fs.uni-lj.si\/lasin\/wp\/IMIT_files\/neural\/doc\/seminar8.pdf\" rel=\"noreferrer\">here<\/a>, for example.<\/p>\n\n<p>Secondly, we can observe that there are relatively few features in the Iris dataset. Moreover, if you look at the <a href=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.names\" rel=\"noreferrer\">dataset description<\/a> you can see that two of the features are very highly correlated with the class outcomes.<\/p>\n\n<p>These correlation values are linear, single-feature correlations, which indicates that one can most likely apply a linear model and observe good results. Neural nets are highly nonlinear; they become more and more complex and capture greater and greater nonlinear feature combinations as the number of hidden nodes and hidden layers is increased.<\/p>\n\n<p>Taking these facts into account, that (a) there are few features to begin with and (b) that there are high linear correlations with class, would all point to a less complex, linear function as being the appropriate predictive model-- by using a single hidden node, you are very nearly using a linear model.<\/p>\n\n<p>It can also be noted that, in the absence of any hidden layer (i.e., just input and output nodes), and when the logistic transfer function is used, this is equivalent to logistic regression.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1462144469996,
        "Solution_link_count":2.0,
        "Solution_readability":12.5,
        "Solution_reading_time":18.6,
        "Solution_score_count":6.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":206.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":303.4149847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use  TensorFlow Hub in Azure ML Studio<\/p>\n<p>I am using the kernel Python 3.8 PT and TF<\/p>\n<p>And I installed  a few modules:<\/p>\n<pre><code>!pip install bert-for-tf2\n!pip install sentencepiece\n!pip install &quot;tensorflow&gt;=2.0.0&quot;\n!pip install --upgrade tensorflow-hub\n<\/code><\/pre>\n<p>With pip list, I can see they are installed:<\/p>\n<pre><code>tensorflow                              2.8.0\ntensorflow-estimator                    2.3.0\ntensorflow-gpu                          2.3.0\ntensorflow-hub                          0.12.0\ntensorflow-io-gcs-filesystem            0.24.0\n<\/code><\/pre>\n<p>However when I try to use it as per the documentation (<a href=\"https:\/\/www.tensorflow.org\/hub\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/hub<\/a>)<\/p>\n<p>Then I get the classic:<\/p>\n<pre><code>ModuleNotFoundError: No module named 'tensorflow_hub'\n<\/code><\/pre>",
        "Challenge_closed_time":1650950165912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649857871967,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to use TensorFlow Hub in Azure ML Studio. They have installed the required modules and verified their installation, but when they try to use it as per the documentation, they encounter a \"ModuleNotFoundError: No module named 'tensorflow_hub'\" error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71858668",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":303.4149847222,
        "Challenge_title":"How to use tensorflow hub in Azure ML",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>To resolve this <code>ModuleNotFoundError: No module named 'tensorflow_hub'<\/code>  error, try following ways:<\/p>\n<ul>\n<li>Try installing\/upgrading the latest version of <code>tensorflow<\/code> and <code>tensorflow-hub<\/code> and then import:<\/li>\n<\/ul>\n<pre><code>!pip install --upgrade tensorflow\n\n!pip install --upgrade tensorflow_hub\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n<\/code><\/pre>\n<ul>\n<li>Install the current environment as a new kernel:<\/li>\n<\/ul>\n<pre><code>python3 -m ipykernel install --user --name=testenvironment\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/stackoverflow.com\/questions\/63884339\/modulenotfounderror-no-module-named-tensorflow-hub\">ModuleNotFoundError: No module named 'tensorflow_hub', No module named 'tensorflow_hub'<\/a> and <a href=\"https:\/\/github.com\/tensorflow\/hub\/issues\/767\" rel=\"nofollow noreferrer\">How to use Tensorflow Hub Model?<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":12.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1653396543887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kolkata, India",
        "Answerer_reputation_count":1231.0,
        "Answerer_view_count":290.0,
        "Challenge_adjusted_solved_time":1.2397177778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can you kindly show me how do we start the Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook?\n<br> This is working fine in Google Colaboratory by the way.\n<br> What is missing here?<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\nimport sys\n\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\n\nfrom sparknlp.base import *\nfrom sparknlp.common import *\nfrom sparknlp.annotator import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" alt=\"Error\" \/><\/a><\/p>\n<p><br><br> <strong>UPDATE_2022-07-21:<\/strong>\n<br> Hi @Sayan. I am still not able to start Spark session on Vertex AI workbench Jupyterlab notebook after running the commands =(\n<a href=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\n# Included else &quot;JAVA_HOME is not set&quot;\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\nspark = sparknlp.start()\n\nprint(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\nprint(&quot;Apache Spark version: {}&quot;.format(spark.version))\n<\/code><\/pre>\n<p>The error:<\/p>\n<pre><code>\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 71: C:\/Program Files\/Java\/jdk-18.0.1.1\/bin\/java: No such file or directory\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 96: CMD: bad array subscript\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_5831\/489505405.py in &lt;module&gt;\n      6 \n      7 import sparknlp\n----&gt; 8 spark = sparknlp.start()\n      9 \n     10 print(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start(gpu, m1, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\n    242         return SparkRealTimeOutput()\n    243     else:\n--&gt; 244         spark_session = start_without_realtime_output()\n    245         return spark_session\n    246 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start_without_realtime_output()\n    152             builder.config(&quot;spark.jsl.settings.storage.cluster_tmp_dir&quot;, cluster_tmp_dir)\n    153 \n--&gt; 154         return builder.getOrCreate()\n    155 \n    156     def start_with_realtime_output():\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    267                         sparkConf.set(key, value)\n    268                     # This SparkContext may be an existing one.\n--&gt; 269                     sc = SparkContext.getOrCreate(sparkConf)\n    270                     # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    271                     # by all sessions.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    481         with SparkContext._lock:\n    482             if SparkContext._active_spark_context is None:\n--&gt; 483                 SparkContext(conf=conf or SparkConf())\n    484             assert SparkContext._active_spark_context is not None\n    485             return SparkContext._active_spark_context\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\n    193             )\n    194 \n--&gt; 195         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    196         try:\n    197             self._do_init(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    415         with SparkContext._lock:\n    416             if not SparkContext._gateway:\n--&gt; 417                 SparkContext._gateway = gateway or launch_gateway(conf)\n    418                 SparkContext._jvm = SparkContext._gateway.jvm\n    419 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf, popen_kwargs)\n    104 \n    105             if not os.path.isfile(conn_info_file):\n--&gt; 106                 raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\n    107 \n    108             with open(conn_info_file, &quot;rb&quot;) as info:\n\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>",
        "Challenge_closed_time":1658384407350,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658299714453,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues in starting a Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook. The user has tried installing Spark NLP and PySpark, setting the JAVA_HOME environment variable, and running the necessary commands, but is still unable to start the Spark session. The user has also provided an error message that they received while trying to start the session.",
        "Challenge_last_edit_time":1658406889243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73047089",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":64.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":23.5258047223,
        "Challenge_title":"How to start Spark session on Vertex AI workbench Jupyterlab notebook?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":238.0,
        "Challenge_word_count":413,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651609055808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>One possible reason is that <code>Java<\/code> is not installed. When you create a <strong>Python-3 Vertex AI Workbench<\/strong> you can have either <code>Debian<\/code> or <code>Ubuntu<\/code> as an OS and it does not come with Java pre-installed. You need to install it manually.\nTo install you can use<\/p>\n<pre><code>sudo apt-get update\nsudo apt-get install default-jdk\n<\/code><\/pre>\n<p>You can follow this <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-install-java-with-apt-get-on-ubuntu-16-04\" rel=\"nofollow noreferrer\">tutorial<\/a> to install Open JDK.<\/p>\n<p>All your problems lie with installing JDK and setting its path in the environment. Once you do this properly you don't need to set path in python also.\nYour code should look something like this<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\n#no need to set the environment path\n\nimport sparknlp\n#all other imports\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><strong>EDIT:<\/strong>\nI have tried your code and had the same error.All I did was Open the terminal inside JupyterLab of the workbench and installed java there.<\/p>\n<p>Opened the JupyterLab from Workbench\n<a href=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Notebook instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Opening the terminal from <em><strong><code>File-&gt;New-&gt;Terminal<\/code><\/strong><\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>From here I downloaded and installed the Java.<\/p>\n<p>You can check whether it has been installed and added to your path by running <code>java --version<\/code> it will return the current version.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1658411352227,
        "Solution_link_count":7.0,
        "Solution_readability":9.7,
        "Solution_reading_time":28.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":240.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.5002777778,
        "Challenge_answer_count":0,
        "Challenge_body":"I've installed and updated the sdk yet when attempting to import the module for the ExplainationDashboard i keep getting the error that the interpret module does not exist.\r\ni am running build 1.0.72 and following the sample in 'how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb \r\nthe failing line in the sample notebook is:\r\n**from azureml.contrib.interpret.visualize import ExplanationDashboard**\r\n**\"ModuleNotFoundError: No module named 'azureml.contrib.interpret' \"**\r\nthis is the update command i ran:\r\npip install --upgrade azureml-sdk[explain,automl,contrib] \r\n(the install ran fine - no errors)\r\njim",
        "Challenge_closed_time":1573060395000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1572788594000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while using Azureml Automl, specifically in the azureml.PipelineStep automl step. The error message is \"Error: Null\" and the user is unsure how to interpret it. The error occurs consistently when the dataset has more than 1200 features, but works fine with fewer features. The user is questioning if there is a limitation with the number of features allowed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/639",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.78,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":75.5002777778,
        "Challenge_title":"azureml.contrib.interpret - ModuleNotFoundError - build 1.0.72",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"azureml-contrib-interpret is not a part of azureml-sdk contrib extras. Please install it separately",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":1.28,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4389.5575297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Challenge_closed_time":1638396070903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638395443060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is confused about SageMaker pipelines and has several questions about them. They are unsure if it is a stand-alone service, how it integrates with codepipeline, and how the Python SDK differs from CDK and CloudFormation. They are also having trouble finding examples besides the Python SDK usage and are seeking clarification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.1744008334,
        "Challenge_title":"What are SageMaker pipelines actually?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":716.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578250359256,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam",
        "Poster_reputation_count":197.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1654197850167,
        "Solution_link_count":8.0,
        "Solution_readability":18.8,
        "Solution_reading_time":68.6,
        "Solution_score_count":2.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":402.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1459917054448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Frankfurt, Germany",
        "Answerer_reputation_count":9168.0,
        "Answerer_view_count":675.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a time series predictor with Keras and  Dockerized the model with with Flash and Gunicorn as per AWS docs. I am loading the serialized model with this code.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n    return cls.model\n<\/code><\/pre>\n\n<p>Then I used the predict method to produce the results , the dockerized container is working perfectly in the local environment , but when I try to host the model in sagemaker it produces this error.<\/p>\n\n<pre><code>ValueError: Tensor Tensor(\"dense_1\/BiasAdd:0\", shape=(?, 1), dtype=float32) is not an element of this graph.\n<\/code><\/pre>\n\n<p>So how can I resolve this issue ?<\/p>",
        "Challenge_closed_time":1541480239328,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541480239330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a ValueError while hosting a time series predictor model in Sagemaker with Gunicorn and Flask and Keras. The error message states that the Tensor is not an element of this graph. The model works fine in the local environment but produces an error when hosted in Sagemaker. The user is seeking a solution to resolve this issue.",
        "Challenge_last_edit_time":1541480860787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53165953",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"ValueError: Tensor is not an element of this graph, when hosting a model in Sagemaker with Gunicorn and Flask and Keras",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":223.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459917054448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Frankfurt, Germany",
        "Poster_reputation_count":9168.0,
        "Poster_view_count":675.0,
        "Solution_body":"<p>The issue was resolved by calling _make_predict_function() method in the model load phase.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n        cls.model._make_predict_function()\n    return cls.model\n<\/code><\/pre>\n\n<p>Bug Reference : <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/6462\" rel=\"nofollow noreferrer\">https:\/\/github.com\/keras-team\/keras\/issues\/6462<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.3,
        "Solution_reading_time":5.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":431.3538888889,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nWhen connecting to Sagemaker workspaces, there is an intermittent issue where a blank browser launches instead of Sagemaker.  The issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nSTEP 1: Login as an Admin \r\nSTEP 2: Create a workspace (SageMaker)\r\nSTEP 3: Start the Workspace \r\nSTEP 5: Click \"Connect\"\r\nSTEP 6: A new blank web browser tab opens \r\nSTEP 7: Click \"Connect\" again, another blank web browser tab opens\r\n\r\nUser receives a \"Something Went Wrong\" general error in SWB at Step 6\r\n\r\nIn the client logs for the browser, there is also this error noted:\r\n              \"name\": \"x-cache\",\r\n              \"value\": \"Error from cloudfront\"\r\n   \r\n\r\n**Expected behavior**\r\nSagemaker workspace launch in browser\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed 3.0.0\r\n\r\n**Additional context**\r\nUser has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. \r\n\r\nThe issue is experienced approximately once a week. Sometimes clearing cache solves the issue. Other times going to incognito, and it does not solve the issue.\r\n\r\n",
        "Challenge_closed_time":1624287519000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1622734645000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where calling `stop_training_job` from the SageMaker client against an existing job that is not \"InProgress\" causes the client to hang. This issue only seems to happen within the sm-executor. The DEBUG output from the sm-executor shows that the request was rejected because the training job is in status Stopped.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/509",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":16.66,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":431.3538888889,
        "Challenge_title":"[Bug] Blank Page on Sagemaker Workspace Connect",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":210,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The relevant code snippet for this issue is [here](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/7988c933d5f4d6c9878249a7521d64d891707824\/addons\/addon-base-raas-ui\/packages\/base-raas-ui\/src\/parts\/environments-sc\/parts\/ScEnvironmentHttpConnections.js#L60).\r\n\r\nI was unable to reproduce this issue (I'm using Chrome 90.0.4430.212) , but let's try to track this issue down.\r\n\r\nAre we unable to get the Sagemaker URL or is the browser failing to refer the user to the Sagemaker url?\r\n\r\nTo check if the browser is getting the Sagemaker URL correctly:\r\n1. Go to the Sagemaker workspace and click the connection button to open the \"HTTP Connections\" card \r\n2. Open the network tab in Chrome Inspect tool\r\n3. Clear all of the network activity\r\n4. Click on the `url` row on the sidebar\r\n5. In the new tab that opens up, select the sub tab for `response`.\r\n6. Check if a `url` is provided and try navigating to that `url` in the browser.\r\n\r\nIf the URL is valid and you can can navigate to the Sagemaker instance that way, then the issue is with the browser failing to redirect the user.\r\n\r\nhttps:\/\/user-images.githubusercontent.com\/3661906\/120824482-84a88d80-c526-11eb-883b-13d9b4cfa7f4.mov\r\nHere's a video of the process.\r\n\r\nPlease follow the instructions above to help us debug what is the cause of the issue. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.4,
        "Solution_reading_time":16.2,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":183.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1460437080990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":386.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":1.4118238889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How do you rename an Azure ML Experiment?\u00a0I cannot see any property field you can set. All I can find is Save\u00a0As something else, then delete the\u00a0original\u00a0experiment. <\/p>\n\n<p>When I\u00a0Save for the first time, it doesn't ask me for a name, it just saves it with a standard date. <\/p>\n\n<p>Am I missing something simple and obvious? <\/p>",
        "Challenge_closed_time":1485656495463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1485651412897,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble renaming an Azure ML experiment as there is no visible property field to set a new name. The only option found is to save the experiment as something else and delete the original. The user is seeking a simpler solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41916570",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.3,
        "Challenge_reading_time":4.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.4118238889,
        "Challenge_title":"Rename an Azure ML experiment",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":753.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1407201889907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA",
        "Poster_reputation_count":2674.0,
        "Poster_view_count":355.0,
        "Solution_body":"<p>put the cursor on the name text when an experiment is open, and edit away.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":0.98,
        "Solution_score_count":3.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":19.3457913889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello everyone,    <\/p>\n<p>I am tring to deploy R script as a web service using Azure Machine Learning. I created pipeline as below.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/143603-001.png?platform=QnA\" alt=\"143603-001.png\" \/>    <\/p>\n<p>I can deploy the model and endpoint from [Deploy] button but I cannot control some properties: i.e. resource name, dns name.    <\/p>\n<p>It seems that the <code>az ml model deploy<\/code> command can be used to deploy the endpoint.     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#using-the-azure-cli\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance#using-the-azure-cli<\/a>    <\/p>\n<p>I have no information for <code>inferenceconfig.json<\/code>. How to write <code>score.py<\/code> to execute R script? Is it any example?    <\/p>",
        "Challenge_closed_time":1635283098256,
        "Challenge_comment_count":2,
        "Challenge_created_time":1635213453407,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to deploy an R script as a web service using Azure Machine Learning and has created a pipeline. They are able to deploy the model and endpoint from the \"Deploy\" button but cannot control some properties such as resource name and DNS name. The user is looking for information on how to use the \"az ml model deploy\" command to deploy the endpoint and how to write \"score.py\" to execute the R script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/603664\/how-to-deploy-r-script-web-service-via-azure-cli",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":12.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.3457913889,
        "Challenge_title":"How to deploy R script web service via Azure CLI",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, the following document describes how to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli#define-an-inference-configuration\">define an inference configuration<\/a>.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.5,
        "Solution_reading_time":4.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.4538166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to find the best way to run my machine learning models on GPUs for inference as an http request. Do Azure functions support GPUs? if not, what are other options I can look into?  <\/p>\n<p>note: I also want to use packaged models, not necessarily ones of my own creation (such as <a href=\"https:\/\/github.com\/JaidedAI\/EasyOCR\">easyOCR<\/a> for python)  <\/p>",
        "Challenge_closed_time":1630921359623,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630916125883,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for the best way to deploy their machine learning model using GPUs as a web-based API. They are unsure if Azure functions support GPUs and are seeking alternative options. Additionally, they want to use pre-packaged models like easyOCR for Python.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/541074\/what-is-the-best-way-to-deploy-my-machine-learning",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":5.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.4538166667,
        "Challenge_title":"What is the best way to deploy my machine learning model using GPUs, specifically as a web based API?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,    <\/p>\n<p>If you need GPU support on ML inference the only supported option is the Azure Kubernetes Service as stated in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus\">this documentation<\/a>    <\/p>\n<p>For guidance on deploying an ML model to AKS, please refer to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">this documenation on deploying to AKS<\/a>    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":6.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":46.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":145.8923630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Newbie data scientist here, I am just starting my way in Azure, is there any I should start NLP? Any trained model or code sample? Thank you for any idea<\/p>",
        "Challenge_closed_time":1667776545220,
        "Challenge_comment_count":1,
        "Challenge_created_time":1667251332713,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is a newbie data scientist who is looking for guidance on how to start with NLP in Azure. They are seeking suggestions for trained models or code samples to help them get started.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1069986\/trained-nlp-model-snippet",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":145.8923630556,
        "Challenge_title":"Trained NLP model snippet",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=12b79ebe-b30f-4203-a714-62377c3d557b\">@jackson schmidt  <\/a>     <\/p>\n<p>Sorry I have not heard from you. I have done some researches around NLP in Azure. This can be done by two ways -    <\/p>\n<ol>\n<li> Azure Machine Learning Python SDK\/ ML CLI extension    <br \/>\nWe don't have any trained model you can use in Azure ML but you do have the SDK supporting you to train your model    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli<\/a>    <\/li>\n<li> NLP Server     <br \/>\nApache Spark is a parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Azure Synapse Analytics, Azure HDInsight, and Azure Databricks offer access to Spark and take advantage of its processing power.    <\/li>\n<\/ol>\n<p>For customized NLP workloads, Spark NLP serves as an efficient framework for processing a large amount of text. This open-source NLP library provides Python, Java, and Scala libraries that offer the full functionality of traditional NLP libraries such as spaCy, NLTK, Stanford CoreNLP, and Open NLP. Spark NLP also offers functionality such as spell checking, sentiment analysis, and document classification. Spark NLP improves on previous efforts by providing state-of-the-art accuracy, speed, and scalability.    <\/p>\n<p><img src=\"https:\/\/learn.microsoft.com\/en-us\/azure\/architecture\/data-guide\/images\/natural-language-processing-functionality.png\" alt=\"natural-language-processing-functionality.png\" \/>    <\/p>\n<p>The NLP Server is available in Azure Marketplace. To explore large-scale custom NLP in Azure, see NLP Server - <a href=\"https:\/\/azuremarketplace.microsoft.com\/en-US\/marketplace\/apps\/johnsnowlabsinc1646051154808.nlp_server?ocid=gtmrewards_whatsnewblog_nlp_server_040622\">https:\/\/azuremarketplace.microsoft.com\/en-US\/marketplace\/apps\/johnsnowlabsinc1646051154808.nlp_server?ocid=gtmrewards_whatsnewblog_nlp_server_040622<\/a>    <\/p>\n<ol start=\"3\">\n<li> Azure Language Service    <br \/>\nThough we don't have trained model in Azure ML, but we do have REST APIs you can use for Text Analytics, Sentiment Analytics and so on functions for NLP, I would suggest you to check on the document, it may help you achieve your bussiness goals.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/<\/a>    <\/li>\n<\/ol>\n<p>I hope those information helps. Please let me know if you have any questions regarding to any of above.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.6,
        "Solution_reading_time":36.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":311.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1253167241888,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney, Australia",
        "Answerer_reputation_count":213670.0,
        "Answerer_view_count":23384.0,
        "Challenge_adjusted_solved_time":9.8921491666,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently trying to do distributed GPU training with 2 instances of ml.p3.8xlarge and after 4 attempts I have not been able to start a training job with the spot instances since AWS did not have any available instances in my region.<\/p>\n<p>How do I increase the number of regions I'm willing to choose from in SageMaker? At the moment I'm only using:<\/p>\n<p><code>sess.boto_region_name = us-east-1<\/code> (sagemaker session region)<\/p>\n<p>But I'm assuming if I allow SageMaker to choose from other regions, I will be able to start a training job with spot instances.<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1628913109040,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628872633507,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in starting a training job with spot instances in Amazon SageMaker due to the unavailability of instances in their region. They are seeking guidance on how to increase the number of regions they can choose from in SageMaker to resolve the issue.",
        "Challenge_last_edit_time":1628877497303,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68775733",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":8.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":11.2432036111,
        "Challenge_title":"How to choose from a list of Regions for GPU instances in Amazon SageMaker?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":120.0,
        "Challenge_word_count":106,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468889301236,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":819.0,
        "Poster_view_count":87.0,
        "Solution_body":"<p>Most AWS services are regional-based, meaning they run in a given region and do not spread <em>beyond<\/em> one region.<\/p>\n<p>If you wish to run SageMaker in multiple regions, you would need to launch it <em>separately<\/em> in each region. So, you would only be 'choosing' <em>one<\/em> region when requesting SageMaker to perform some work.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":4.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1539125320752,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":61.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":5233.5912852778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to understand how parameters servers (PS's) work for distributed training in Tensorflow on Amazon SageMaker. <\/p>\n\n<p>To make things more concrete, I am able to run the example from AWS using PS's: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/tf-distributed-training.ipynb<\/a><\/p>\n\n<p>Here is the code block that initializes the estimator for Tensorflow:<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ngit_config = {'repo': 'https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode', 'branch': 'master'}\n\nps_instance_type = 'ml.p3.2xlarge'\nps_instance_count = 2\n\nmodel_dir = \"\/opt\/ml\/model\"\n\ndistributions = {'parameter_server': {\n                    'enabled': True}\n                }\nhyperparameters = {'epochs': 60, 'batch-size' : 256}\n\nestimator_ps = TensorFlow(\n                       git_config=git_config,\n                       source_dir='tf-distribution-options\/code',\n                       entry_point='train_ps.py', \n                       base_job_name='ps-cifar10-tf',\n                       role=role,\n                       framework_version='1.13',\n                       py_version='py3',\n                       hyperparameters=hyperparameters,\n                       train_instance_count=ps_instance_count, \n                       train_instance_type=ps_instance_type,\n                       model_dir=model_dir,\n                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n                       distributions=distributions)\n<\/code><\/pre>\n\n<p>Going through the documentation for Tensorflow, it seems that a device scope can be used for assigning a variable to a particular worker. However, I never see this done when running training jobs on SageMaker. In the example from AWS, the model is defined by:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-distribution-options\/code\/model_def.py<\/a><\/p>\n\n<p>Here is a snippet:<\/p>\n\n<pre><code>def get_model(learning_rate, weight_decay, optimizer, momentum, size, mpi=False, hvd=False):\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(HEIGHT, WIDTH, DEPTH)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3)))\n\n    ...\n\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation('softmax'))\n\n    if mpi:\n        size = hvd.size()\n\n    if optimizer.lower() == 'sgd':\n        ...\n\n    if mpi:\n        opt = hvd.DistributedOptimizer(opt)\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model\n<\/code><\/pre>\n\n<p>Here, there are no references to distribution strategies (except with MPI, but that flag is set to False for PS's). Somehow, Tensorflow or the SageMaker container is able to decide where the variables for each layer should be stored. However, I'm not seeing anything in the container code that does anything with the distribution strategy.<\/p>\n\n<p>I am able to run this code and train the model using 1 and 2 instances. When i do so, I see a decrease of almost 50% in the runtime, suggesting that a distributed training is occurring.<\/p>\n\n<p>My questions are:<\/p>\n\n<ol>\n<li>How does Tensorflow decide the distribution of variables on the PS's? In the example code, there is no explicit reference to devices. Somehow the distribution is done automatically.<\/li>\n<li>Is it possible to see which parameters have been assigned to each PS? Or to see what the communication between PS's looks like? If so, how?<\/li>\n<\/ol>",
        "Challenge_closed_time":1600204151310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581363222683,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to understand how parameter servers work for distributed training in Tensorflow on Amazon SageMaker. They are able to run an example from AWS using parameter servers, but they are unsure how Tensorflow decides the distribution of variables on the parameter servers. They are also wondering if it is possible to see which parameters have been assigned to each parameter server and what the communication between parameter servers looks like.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60157184",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":16.1,
        "Challenge_reading_time":47.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":5233.5912852778,
        "Challenge_title":"Tensorflow Parameter Servers on SageMaker",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":307.0,
        "Challenge_word_count":350,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416337478872,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":13.0,
        "Solution_body":"<blockquote>\n<p>My questions are:<\/p>\n<p>How does Tensorflow decide the distribution of variables on the PS's?\nIn the example code, there is no explicit reference to devices.\nSomehow the distribution is done automatically.<\/p>\n<\/blockquote>\n<p>The TensorFlow image provided by SageMaker has the code to setup TF_CONFIG and launching parameter server for multi work training. See the code [here][1] The setup is for each node in the cluster there is a PS and a worker thread configured.<\/p>\n<p>It's not using any DistributionStrategy so the default strategy is used. See [here][2].<\/p>\n<p>If you would like to use a different DistributionStrategy or different TF_CONFIG you will need to disable <code>parameter_server<\/code> option when launching the SageMaker training job and set everything up in your training script.<\/p>\n<blockquote>\n<p>Is it possible to see which parameters have been assigned to each PS?\nOr to see what the communication between PS's looks like? If so, how?<\/p>\n<\/blockquote>\n<p>You should be able to get some information from the output log which can be found in CloudWatch. The link is available on the Training Job console page.\n[1]: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-training-toolkit\/blob\/master\/src\/sagemaker_tensorflow_container\/training.py#L37<\/a>\n[2]: <a href=\"https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/distributed_training#default_strategy<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.6,
        "Solution_reading_time":21.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.3284980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using .NII file format which represents, the neuroimaging dataset. I need to use Auto ML to label the dataset of images that are nearly 2GB in size per patient. The main issue is with using Auto ML to label the dataset of images with.NII file extension and classify whether the patient is having dementia or not.<\/p>\n<p><strong>Requirement:<\/strong> Forget about the problem domain of implementation like dementia. I would like to know about the procedure of using Auto ML for Computer vision applications through ML studio to use.NII file format dataset images.<\/p>\n<p>Any help would be thankful.<\/p>",
        "Challenge_closed_time":1652091549923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652090367330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in using AutoML to label and classify MRI images in .NII file format for dementia diagnosis. The main issue is with implementing AutoML for computer vision applications through ML studio to use the .NII file format dataset images. The user is seeking guidance on the procedure for using AutoML in this context.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72170169",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":8.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3284980556,
        "Challenge_title":"Implementing Computer Vison on AutoML to classify dementia using MRI images in .NII file format",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>The requirement of using .nii or other file formats in Azure auto ML is a challenging task. Unfortunately, Auto ML image input format will be using in only JSON format. Kindly check the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-automl-images-schema\" rel=\"nofollow noreferrer\">document<\/a><\/p>\n<p>Answering regarding requirement of .nii format of dataset, there are different file format convertors available like &quot;<em><strong><a href=\"http:\/\/medicalimageconverter.com\/?msclkid=403b597ecf8111ecac65b3422c7b95b5\" rel=\"nofollow noreferrer\">Medical Image Convertor<\/a><\/strong><\/em>&quot;. This software is commercial and can be used for 10days for free. Convert .nii file formats into JPG and proceed with the general documentation provided in the top of the answer.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":10.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2833333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello GCP community, I have the following question, I am training in Vertex using a custom container, I am porting pipelines that were in Kubeflow to vertex and using this to train:\n\n\n\nfrom google.cloud import aiplatform\n\njob = aiplatform.CustomContainerTrainingJob(display_name=\"training-job\", container_uri=container_uri)\n# define training code arguments\ntraining_args = [\"--num-epochs\", \"2\", ]\nmodel = job.run(\nreplica_count=1,\nmachine_type=\"n1-standard-8\",\naccelerator_type=\"NVIDIA_TESLA_V100\",\naccelerator_count=1,\nargs=training_args,\nsync=False,\n)\n\nIt looks ok, but here is my question is there anyway in which I can do the training but in a SPOT machine to try to reduce my training costs.\n\nThanks!",
        "Challenge_closed_time":1674130140000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674129120000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is training in Vertex using a custom container and wants to know if it is possible to use spot machines to reduce training costs. They are currently using a standard machine type and NVIDIA_TESLA_V100 accelerator.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Usage-of-spot-machines-while-training-in-Vertex-AI\/m-p\/511862#M1094",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.1,
        "Challenge_reading_time":9.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2833333333,
        "Challenge_title":"Usage of spot machines while training in Vertex AI",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":370.0,
        "Challenge_word_count":93,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi David\n\nNo unfortunately there is no support for spot \/ preemptible instances with Vertex AI.\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1497960178323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":12088.0,
        "Answerer_view_count":3630.0,
        "Challenge_adjusted_solved_time":67.3963755556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running pyspark from an Azure Machine Learning notebook. I am trying to move a file using the dbutil module.<\/p>\n\n<pre><code>from pyspark.sql import SparkSession\n    spark = SparkSession.builder.getOrCreate()\n    def get_dbutils(spark):\n        try:\n            from pyspark.dbutils import DBUtils\n            dbutils = DBUtils(spark)\n        except ImportError:\n            import IPython\n            dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n        return dbutils\n\n    dbutils = get_dbutils(spark)\n    dbutils.fs.cp(\"file:source\", \"dbfs:destination\")\n<\/code><\/pre>\n\n<p>I got this error: \nModuleNotFoundError: No module named 'pyspark.dbutils'\nIs there a workaround for this? <\/p>\n\n<p>Here is the error in another Azure Machine Learning notebook:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-1-183f003402ff&gt; in get_dbutils(spark)\n      4         try:\n----&gt; 5             from pyspark.dbutils import DBUtils\n      6             dbutils = DBUtils(spark)\n\nModuleNotFoundError: No module named 'pyspark.dbutils'\n\nDuring handling of the above exception, another exception occurred:\n\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-1-183f003402ff&gt; in &lt;module&gt;\n     10         return dbutils\n     11 \n---&gt; 12 dbutils = get_dbutils(spark)\n\n&lt;ipython-input-1-183f003402ff&gt; in get_dbutils(spark)\n      7         except ImportError:\n      8             import IPython\n----&gt; 9             dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n     10         return dbutils\n     11 \n\nKeyError: 'dbutils'\n<\/code><\/pre>",
        "Challenge_closed_time":1588593659052,
        "Challenge_comment_count":5,
        "Challenge_created_time":1588351032100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleNotFoundError while trying to move a file using the dbutil module in pyspark from an Azure Machine Learning notebook. The error message indicates that there is no module named 'pyspark.dbutils'. The user is seeking a workaround for this issue.",
        "Challenge_last_edit_time":1591892764407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61546680",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":12.2,
        "Challenge_reading_time":19.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":67.3963755556,
        "Challenge_title":"ModuleNotFoundError: No module named 'pyspark.dbutils'",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":7629.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330373362200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kennett Square, PA",
        "Poster_reputation_count":445.0,
        "Poster_view_count":104.0,
        "Solution_body":"<p>This is a known issue with Databricks Utilities - DButils.<\/p>\n\n<p>Most of DButils aren't supported for Databricks Connect. The only parts that do work are <strong>fs<\/strong> and <strong>secrets<\/strong>. <\/p>\n\n<p><strong>Reference:<\/strong> <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/dev-tools\/databricks-connect#limitations\" rel=\"nofollow noreferrer\">Databricks Connect - Limitations<\/a> and <a href=\"https:\/\/datathirst.net\/blog\/2019\/3\/7\/databricks-connect-limitations\" rel=\"nofollow noreferrer\">Known issues<\/a>.<\/p>\n\n<p><strong>Note:<\/strong> Currently fs and secrets work (locally). Widgets (!!!), libraries etc do not work. This shouldn\u2019t be a major issue. If you execute on Databricks using the Python Task dbutils will fail with the error:<\/p>\n\n<pre><code>ImportError: No module named 'pyspark.dbutils'\n<\/code><\/pre>\n\n<p>I'm able to execute the query successfully by running as a notebook.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RFVm8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RFVm8.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.0,
        "Solution_reading_time":14.37,
        "Solution_score_count":3.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.1764397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How is the output of Fisher Linear Discriminant Analysis experiment interpreted now that the column labels in the output are replaced with Col1, Col2, Col3.......etc? How can the model be used to predict clusters of other input data as deployed web service requires even the dependent valuable(the same same ones we wish to predict)?<\/p>",
        "Challenge_closed_time":1621895240423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621855005240,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in interpreting the output of Fisher Linear Discriminant Analysis experiment as the column labels are replaced with Col1, Col2, Col3, etc. They are also unsure how to use the model to predict clusters of other input data as the deployed web service requires the same dependent variables that they wish to predict.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/407053\/fisher-linear-discriminant-analysis-azure",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":4.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":11.1764397222,
        "Challenge_title":"Fisher Linear Discriminant Analysis Azure",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Are you referring to the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/latent-dirichlet-allocation#lda-transformed-dataset\">categories<\/a> generated from LDA module? If so, then that's expected. LDA is an unsupervised technique, it groups words into categories\/topics and it's up to the analyst to interpret it by observing the results and transforming the output dataset accordingly. Here's are some <a href=\"https:\/\/gallery.azure.ai\/browse?s=lda\">examples<\/a> of LDA approach in Azure AI Gallery. Hope this helps.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":7.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1335447186710,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Egypt",
        "Answerer_reputation_count":1972.0,
        "Answerer_view_count":547.0,
        "Challenge_adjusted_solved_time":1.6057813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a simple requirement, I need to run sagemaker prediction inside a spark job<\/p>\n<p>am trying to run the below<\/p>\n<pre><code>ENDPOINT_NAME = &quot;MY-ENDPOINT_NAME&quot;\nfrom sagemaker_pyspark import SageMakerModel\nfrom sagemaker_pyspark import EndpointCreationPolicy\nfrom sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer\nfrom sagemaker_pyspark.transformation.deserializers import ProtobufResponseRowDeserializer\n\nattachedModel = SageMakerModel(\n    existingEndpointName=ENDPOINT_NAME,\n    endpointCreationPolicy=EndpointCreationPolicy.DO_NOT_CREATE,\n    endpointInstanceType=None,  # Required\n    endpointInitialInstanceCount=None,  # Required\n    requestRowSerializer=ProtobufRequestRowSerializer(\n        featuresColumnName=&quot;featureCol&quot;\n    ),  # Optional: already default value\n    responseRowDeserializer= ProtobufResponseRowDeserializer(schema=ouput_schema),\n)\n\ntransformedData2 = attachedModel.transform(df)\ntransformedData2.show()\n<\/code><\/pre>\n<p>I get the following error <code>TypeError: 'JavaPackage' object is not callable<\/code><\/p>",
        "Challenge_closed_time":1662672827940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662667047127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run sagemaker prediction inside a spark job using the provided code, but is encountering a TypeError: 'JavaPackage' object is not callable.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73654460",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":29.4,
        "Challenge_reading_time":14.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.6057813889,
        "Challenge_title":"how to use sagemaker inside pyspark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":18.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1335447186710,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Egypt",
        "Poster_reputation_count":1972.0,
        "Poster_view_count":547.0,
        "Solution_body":"<p>this was solved by ...<\/p>\n<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\nconf = SparkConf() \\\n    .set(&quot;spark.driver.extraClassPath&quot;, classpath)\nsc = SparkContext(conf=conf)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.5,
        "Solution_reading_time":3.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":14.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1522076554448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":4030.6928727778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to retrieve the pickle off my trained model, which I know is in the run file inside my experiments in Databricks.<\/p>\n<p>It seems that the <code>mlflow.pyfunc.load_model<\/code> can only do the <code>predict<\/code> method.<\/p>\n<p>There is an option to directly access the pickle?<\/p>\n<p>I also tried to use the path in the run using the <code>pickle.load(path)<\/code> (example of path: dbfs:\/databricks\/mlflow-tracking\/20526156406\/92f3ec23bf614c9d934dd0195\/artifacts\/model\/model.pkl).<\/p>",
        "Challenge_closed_time":1629748421287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628544411170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to retrieve the pickle file of a trained model from their experiments in Databricks. They have tried using mlflow.pyfunc.load_model but it only allows the predict method. They have also attempted to use the path in the run using pickle.load(path) but it did not work.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68718719",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":334.4472547222,
        "Challenge_title":"How can I retrive the model.pkl in the experiment in Databricks",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":3081.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1522076554448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":96.0,
        "Poster_view_count":31.0,
        "Solution_body":"<p>I recently found the solution which can be done by the following two approaches:<\/p>\n<ol>\n<li>Use the customized predict function at the moment of saving the model (check <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">databricks<\/a> documentation for more details).<\/li>\n<\/ol>\n<p>example give by Databricks<\/p>\n<pre><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n# Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Load the model artefacts as we are downloading the artefact:<\/li>\n<\/ol>\n<pre><code>from mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\ntmp_path = client.download_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path='model\/model.pkl')\n\nf = open(tmp_path,'rb')\n\nmodel = pickle.load(f)\n\nf.close()\n\n \n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;&quot;)\n\nclient.list_artifacts(run_id=&quot;0c7946c81fb64952bc8ccb3c7c66bca3&quot;, path=&quot;model&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643054905512,
        "Solution_link_count":1.0,
        "Solution_readability":16.0,
        "Solution_reading_time":18.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":109.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":5.6708125,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Sagemaker Object2Vec to train on data of size 2GB.<\/p>\n<p>ml.p2.xlarge instance took 12 hours to train the data on 4 epochs going at the speed of 5000 samples\/sec.<\/p>\n<p>Now, I am using a higher level instance ml.p2.16xlarge and it only trains at 400 samples\/sec with this in the logs<\/p>\n<pre><code>[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:739: only 114 out of 240 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .vvvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: v.vvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vv.vvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvv.vvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvv.vvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvv.vvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvv.vv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvv.v.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvvv........\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: ..........vvvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........v.vvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vv.vvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvv.vvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvv.vv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvv.v\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvvv.\n<\/code><\/pre>\n<p>There are about 50 million samples.<\/p>\n<p>What can I do to correct this?<\/p>",
        "Challenge_closed_time":1595937519408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595917104483,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with Sagemaker Object2Vec training, where a higher level instance is only training at 400 samples\/sec compared to the previous instance that trained at 5000 samples\/sec. The logs show a message that only 114 out of 240 GPU pairs are enabled direct access, which may affect performance. The user has 50 million samples and is seeking advice on how to correct this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63128111",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":30.5,
        "Challenge_reading_time":56.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":5.6708125,
        "Challenge_title":"Sagemaker Object2Vec training samples per second",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":59.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504123657672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>2 ideas:<\/p>\n<ol>\n<li>Before increasing the GPU count, grow batch size so that a single\nGPU is as busy as possible<\/li>\n<li>Use P3 instances instead of P2. P3 is more recent, has more memory, more CUDA cores, faster memory bandwidth and NVLink inter-GPU connections. Though it's more\nexpensive by hour, your total training cost may be much smaller if\nproperly tuned<\/li>\n<\/ol>\n<p>Also, if your problem involves sparse updates, meaning if just a small fraction of all tokens appear in a given mini-batch, you can try using <code>token_embedding_storage_type = 'row_sparse'<\/code>, which I think refers to using sparse gradient updates like described here <a href=\"https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.9,
        "Solution_reading_time":12.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1627764232887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":4.7981325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a SageMaker semantic segmentation model, using the built-in sagemaker semantic segmentation algorithm. This deploys ok to a SageMaker endpoint and I can run inference in the cloud  successfully from it.\nI would like to use the model on a edge device (AWS Panorama Appliance) which should just mean compiling the model with SageMaker Neo to the specifications of the target device.<\/p>\n<p>However, regardless of what my target device is (the Neo settings), I cant seem to compile the model with Neo as I get the following error:<\/p>\n<pre><code>ClientError: InputConfiguration: No valid Mxnet model file -symbol.json found\n<\/code><\/pre>\n<p>The model.tar.gz for semantic segmentation models contains hyperparams.json, model_algo-1, model_best.params. According to the docs, model_algo-1 is the serialized mxnet model. Aren't gluon models supported by Neo?<\/p>\n<p>Incidentally I encountered the exact same problem with another SageMaker built-in algorithm, the k-Nearest Neighbour (k-NN). It too seems to be compiled without a -symbol.json.<\/p>\n<p>Is there some scripts I can run to recreated a -symbol.json file or convert the compiled sagemaker model?<\/p>\n<p>After building my model with an Estimator, I got to compile it in SageMaker Neo with code:<\/p>\n<pre><code>optimized_ic = my_estimator.compile_model(\n target_instance_family=&quot;ml_c5&quot;,\n target_platform_os=&quot;LINUX&quot;,\n target_platform_arch=&quot;ARM64&quot;,\n input_shape={&quot;data&quot;: [1,3,512,512]},  \n output_path=s3_optimized_output_location,\n framework=&quot;mxnet&quot;,\n framework_version=&quot;1.8&quot;, \n)\n<\/code><\/pre>\n<p>I would expect this to compile ok, but that is where I get the error saying the model is missing the *-symbol.json file.<\/p>",
        "Challenge_closed_time":1648038217500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647989575803,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to compile a SageMaker semantic segmentation model with SageMaker Neo for use on an edge device. The error message states that there is no valid Mxnet model file -symbol.json found. The user is unsure if gluon models are supported by Neo and is seeking help to recreate a -symbol.json file or convert the compiled sagemaker model. The user encountered the same problem with another built-in algorithm, k-Nearest Neighbour (k-NN).",
        "Challenge_last_edit_time":1648020944223,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71579883",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":23.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":13.5115825,
        "Challenge_title":"Missing -symbol.json error when trying to compile a SageMaker semantic segmentation model (built-in algorithm) with SageMaker Neo",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":236,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586928819952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>For some reason, AWS has decided to not make its built-in algorithms directly compatible with Neo... However, you can re-engineer the network parameters using the model.tar.gz output file and then compile.<\/p>\n<p>Step 1: Extract model from tar file<\/p>\n<pre><code>import tarfile\n#path to local tar file\nmodel = 'ss_model.tar.gz'\n\n#extract tar file \nt = tarfile.open(model, 'r:gz')\nt.extractall()\n<\/code><\/pre>\n<p>This should output two files:\nmodel_algo-1, model_best.params<\/p>\n<ol start=\"2\">\n<li>Load weights into network from gluon model zoo for the architecture that you chose<\/li>\n<\/ol>\n<p>In this case I used DeepLabv3 with resnet50<\/p>\n<pre><code>import gluoncv\nimport mxnet as mx\nfrom gluoncv import model_zoo\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\n\nmodel = model_zoo.DeepLabV3(nclass=2, backbone='resnet50', pretrained_base=False, height=800, width=1280, crop_size=240)\nmodel.load_parameters(&quot;model_algo-1&quot;)\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check the parameters have loaded correctly by making a prediction with new model<\/li>\n<\/ol>\n<p>Use an image that was used for training.<\/p>\n<pre><code>#use cpu\nctx = mx.cpu(0)\n#decode image bytes of loaded file\nimg = image.imdecode(imbytes)\n\n#transform image\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\nprint('tranformed image shape: ', img.shape)\n\n#get prediction\noutput = model.predict(img)\n<\/code><\/pre>\n<ol start=\"4\">\n<li>Hybridise model into output required by Sagemaker Neo<\/li>\n<\/ol>\n<p>Additional check for image shape compatibility<\/p>\n<pre><code>model.hybridize()\nmodel(mx.nd.ones((1,3,800,1280)))\nexport_block('deeplabv3-res50', model, data_shape=(3,800,1280), preprocess=None, layout='CHW')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Recompile model into tar.gz format<\/li>\n<\/ol>\n<p>This contains the params and json file which Neo looks for.<\/p>\n<pre><code>tar = tarfile.open(&quot;comp_model.tar.gz&quot;, &quot;w:gz&quot;)\nfor name in [&quot;deeplabv3-res50-0000.params&quot;, &quot;deeplabv3-res50-symbol.json&quot;]:\n    tar.add(name)\ntar.close()\n<\/code><\/pre>\n<ol start=\"6\">\n<li>Save tar.gz file to s3 and then compile using Neo GUI<\/li>\n<\/ol>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":28.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":231.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4416666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi,\r\nI was trying to follow this documentation: https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/noaa-integrated-surface-data\/ (Go to \"Data access\" tab)to use opendatasets module to access historical weather data. But it gives me the error message `No name 'opendatasets' in module 'azureml'`. \r\nI tried `pip install azureml-sdk[opendatasets]` as well, it shows `WARNING: azureml-sdk 1.0.55 does not provide the extra 'opendatasets'`.\r\nDo you know how to use the opendatasets module in azureml?\r\n\r\nThanks!",
        "Challenge_closed_time":1565217619000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1565216029000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a 502 Bad Gateway error after running a Tesla K80 compute instance on Azure ML and attempting to connect with VSCode on day 2. The error message suggests that the target may not be in a running state.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/518",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4416666667,
        "Challenge_title":"No name 'opendatasets' in module 'azureml' Error",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Find the solution, maybe because `opendatasets` is a preview module, so it is not included in azureml sdk yet. You can download through pip `pip install azureml-opendatasets` in your env. > pip install azureml-opendatasets\r\n\r\nThanks, was looking for the solution, this worked !! However, I had another error \" [WinError 5] Access is denied:\" This was solved by adding --user at the end of your command.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":4.91,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.5558333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Problem:\r\n```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> select image_id, ML_predict(ssd, image) FROM coco limit 1;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Undefined function: '<anonymous>'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 17\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n```\r\n\r\nSteps to reproduce:\r\n\r\n1. Register models into mlflow\r\n2. Start Spark thrift server\r\n3. Use `beeline` to connec to the thrift server:  `beeline -u jdbc:hive2:\/\/localhost:10001\/default`\r\n4. run `SELECT ML_PREDICT(ssd, image) FROM coco`",
        "Challenge_closed_time":1642203169000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642186768000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered inconsistent checks in the linter for subprocess.call and mlflow.log_artifact functions, which caused the template create WFs to fail. The user expected these functions to pass and is looking for a solution to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/492",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":39.2,
        "Challenge_reading_time":31.45,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":4.5558333333,
        "Challenge_title":"MlflowCatalog anonymous function is not registered ",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1393021961043,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":838.0,
        "Answerer_view_count":193.0,
        "Challenge_adjusted_solved_time":4512.6045455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm following this example notebook to learn SageMaker's processing jobs API: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb<\/a><\/p>\n<p>I'm trying to modify their code to avoid using the default S3 bucket, namely: <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code><\/p>\n<p>For their data processing step with the <code>.run<\/code> method:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[ProcessingInput(source=input_data, destination=&quot;\/opt\/ml\/processing\/input&quot;)],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>I was able to modify it to use my own S3 bucket by using the <code>destination<\/code> parameter like this:<\/p>\n<pre><code>sklearn_processor.run( \n    code=output_bucket_uri + &quot;preprocessing.py&quot;, \n    inputs=[ProcessingInput( \n        source=input_bucket_uri + &quot;census-income.csv&quot;, \n        destination=path+&quot;input\/&quot;, \n    )], \n    outputs=[ \n        ProcessingOutput( \n            output_name=&quot;train_data&quot;, \n            source=path+&quot;train\/&quot;, \n            destination=output_bucket_uri + &quot;train\/&quot;, \n        ), \n        ProcessingOutput( \n            output_name=&quot;test_data&quot;, \n            source=path+&quot;test\/&quot;, \n            destination=output_bucket_uri + &quot;test\/&quot;, \n        ), \n    ], \n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;], \n)\n<\/code><\/pre>\n<p>But for the <code>.fit<\/code> method:<\/p>\n<pre><code>sklearn.fit({&quot;train&quot;: preprocessed_training_data})\n<\/code><\/pre>\n<p>I have not been able to find a parameter to pass it so that the output artifacts are saved to a S3 bucket that I specify instead of the default s3 bucket <code>s3:\/\/sagemaker-&lt;region&gt;-&lt;account_id&gt;\/<\/code>.<\/p>",
        "Challenge_closed_time":1648557908696,
        "Challenge_comment_count":0,
        "Challenge_created_time":1632276763933,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to modify the code in an example notebook to use their own S3 bucket instead of the default S3 bucket for SageMaker's processing jobs API. They were able to modify the code for the data processing step, but are unable to find a parameter to specify the S3 bucket for the output artifacts in the fit method.",
        "Challenge_last_edit_time":1632312532332,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69277390",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":26.2,
        "Challenge_reading_time":32.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":4522.5402119444,
        "Challenge_title":"Can I specify S3 bucket for sagemaker.sklearn.estimator's SKLearn?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1329161697310,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4154.0,
        "Poster_view_count":314.0,
        "Solution_body":"<p>For SKLearnProcessor, the ideal way to specify default bucket is by creating a sagemaker session with that bucket, and sending that as sagemaker_session parameter. Example:<\/p>\n<pre><code>from sagemaker.session import Session    \nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role='&lt;arn-role&gt;',\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1,\n                                     sagemaker_session=Session(default_bucket='&lt;s3-bucket-name&gt;'))\n<\/code><\/pre>\n<p>I know this is not your exact question but you have added an alternative to this in your question details. So I am adding it here as a cleaner approach.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":7.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":61.4723369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Challenge_closed_time":1559329920120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559108619707,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"InternalServerError\" error while performing binary classification on time series using Sagemaker. The transform job works fine when the user inputs a few samples, but it returns an error when the user inputs more samples for prediction, which is greater than 100MB. The user tried to use both SingleRecord and MultiRecord parameters when deploying the model, but the result was the same. The user suspects that the jsonlines file isn't being split by Line and is too big to be processed in one batch, which throws the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56353814",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":64.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":61.4723369444,
        "Challenge_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1826.0,
        "Challenge_word_count":483,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559099281007,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.3,
        "Solution_reading_time":23.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":191.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":10204.2583313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've fitted a Tensorflow Estimator in SageMaker using Script Mode with <code>framework_version='1.12.0'<\/code> and <code>python_version='py3'<\/code>, using a GPU instance. <\/p>\n\n<p>Calling deploy directly on this estimator works if I select deployment instance type as GPU as well. However, if I select a CPU instance type and\/or try to add an accelerator, it fails with an error that docker cannot find a corresponding image to pull. <\/p>\n\n<p>Anybody know how to train a py3 model on a GPU with Script Mode and then deploy to a CPU+EIA instance? <\/p>\n\n<hr>\n\n<p>I've found a partial workaround by taking the intermediate step of creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not seem to support python 3 (again, doesn't find a corresponding container). If I switch to python_version='py2', it will find the container, but fail to pass health checks because all my code is for python 3.<\/p>",
        "Challenge_closed_time":1549915825300,
        "Challenge_comment_count":1,
        "Challenge_created_time":1549048113447,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has encountered an issue while deploying a Tensorflow Estimator in SageMaker using Script Mode with framework_version='1.12.0' and python_version='py3'. The deployment works fine if the deployment instance type is GPU, but fails with an error when selecting a CPU instance type and\/or adding an accelerator. The user is looking for a way to train a py3 model on a GPU with Script Mode and then deploy it to a CPU+EIA instance. The user has found a partial workaround by creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not support python 3.",
        "Challenge_last_edit_time":1549048982467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54485769",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":12.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":241.0310702778,
        "Challenge_title":"SageMaker deploying to EIA from TF Script Mode Python3",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":378.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>Unfortunately there are no TF + Python 3 + EI serving images at this time. If you would like to use TF + EI, you'll need to make sure your code is compatible with Python 2.<\/p>\n\n<p>Edit: after I originally wrote this, support for TF + Python 3 + EI has been released. At the time of this writing, I believe TF 1.12.0, 1.13.1, and 1.14.0 all have Python 3 + EI support. For the full list, see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1585784312460,
        "Solution_link_count":2.0,
        "Solution_readability":9.4,
        "Solution_reading_time":7.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.66,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nI have two version for using component. One is directly uploading local code to each pod, this work fine that we could see the models artifact and metric curve. The other one use the same code as the first one, except for using private github to load the code. In this case, the \"dashboards\", \"artifacts\" and \"resources\" pages show only \"NO DATA\", and \"logs\" page shows training output message normally.\n\nTo reproduce\nAdd private code connection\nconnections:\n  - name: my-repo\n    kind: git\n    schema:\n      url: https:\/\/github.com\/xxx\/my-repo\n    secret:\n      name: \"github-secret-my-repo\"\n\nruning job config:\nrun:\n  kind: job\n  init:\n    - connection: my-repo\n\nHow we use log_metric and log_model\n# log metric\ntracking.log_metric(\"val_loss\", val_loss, step=epoch)\ntracking.log_metric(\"val_precision\", precision, step=epoch)\ntracking.log_metric(\"val_recall\", recall, step=epoch)\n\n# log model\nmodel_output_dir = tracking.get_outputs_path(\"models\", is_dir=True)\nckpt_file = os.path.join(model_output_dir, 'checkpoint.pth.tar')\ntorch.save({xxx}, ckpt_file)\ntracking.log_model(name=\"checkpoint\", path=ckpt_file, framework=\"pytorch\")\n\nExpected behavior\n\nShowing metric curve and saving models normally.\n\nEnvironment\n\nminikube: v1.15.1\npolyaxon ce: 1.7.5",
        "Challenge_closed_time":1619508955000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619492179000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with a private Github repo while using log_model and log_metric. The \"dashboards\", \"artifacts\" and \"resources\" pages show only \"NO DATA\", and \"logs\" page shows training output message normally. The user has provided the code and configuration details for reproducing the issue. The expected behavior is to show metric curve and save models normally. The environment used is minikube v1.15.1 and polyaxon ce 1.7.5.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1302",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.6,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.66,
        "Challenge_title":"Private github repo fail for using log_model and log_metric",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is very hard to debug, you need to check if the git repo has some more information like a hard-coded NO_OP.\n\nAlso I would check if you added the local cache folder .polyaxon to your .gitignore and .dockerignore. If this folder was added to you git repo, then indeed the metrics and artifacts will be saved to a different run.\n\nYou can also validate that the run is running with the correct run-uuid:\n\n...\nprint(tracking.TRACKING_RUN.run_uuid)\n...\n\nIf the run_uuid does not correspond to the currently running run, then the cache is bundled somewhere (git repo or docker image).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":97.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.1972222222,
        "Challenge_answer_count":1,
        "Challenge_body":"How does Amazon SageMaker batch transform handle failures? Is there a way to automate failure handling and retries built into the service?",
        "Challenge_closed_time":1593617691000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593595381000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how Amazon SageMaker batch transform handles failures and if there is a way to automate failure handling and retries within the service.",
        "Challenge_last_edit_time":1668082995576,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUE10OtSwDRCiB-0pP6wflYQ\/is-there-a-way-to-automate-failure-handling-and-retries-when-using-amazon-sagemaker-batch-transform",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.1972222222,
        "Challenge_title":"Is there a way to automate failure handling and retries when using Amazon SageMaker batch transform?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":37,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":" You can use the [ModelClientConfig](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_ModelClientConfig.html#sagemaker-Type-ModelClientConfig-InvocationsMaxRetries) API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925593260,
        "Solution_link_count":1.0,
        "Solution_readability":23.6,
        "Solution_reading_time":4.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1429270603900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Jos\u00e9 dos Campos, Sao Jose dos Campos - State of S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":138.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":714.527975,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have following this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/semantic_segmentation_pascalvoc\/semantic_segmentation_pascalvoc.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a>, which is mainly for jupyter notebook, and made some minimal modification for external processing. I've created a project that could prepare my dataset locally, upload it to S3, train, and finally deploy the model predictor to the same bucket. Perfect!<\/p>\n<p>So, after to train and saved it in S3 bucket:<\/p>\n<pre><code> ss_model.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n<p>it failed while deploying as an endpoint. So, I have found tricks to host an endpoint in many ways, but not from a model already saved in S3. Because in order to host, you probably need to get the estimator, which in normal way is something like:<\/p>\n<pre><code> self.estimator = sagemaker.estimator.Estimator(self.training_image,\n                                                role,\n                                                train_instance_count=1,\n                                                train_instance_type='ml.p3.2xlarge',\n                                                train_volume_size=50,\n                                                train_max_run=360000,\n                                                output_path=output,\n                                                base_job_name='ss-training',\n                                                sagemaker_session=sess)\n<\/code><\/pre>\n<p>My question is: is there a way to load an estimator from a model saved in S3 (.tar)? Or, anyway, to create an endpoint without train it again?<\/p>",
        "Challenge_closed_time":1593463280863,
        "Challenge_comment_count":1,
        "Challenge_created_time":1593283354330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has followed a tutorial to create a project using AWS SageMaker that prepares a dataset locally, uploads it to S3, trains a model, and deploys the model predictor to the same bucket. However, while deploying the model as an endpoint, it failed. The user is looking for a way to load an estimator from a model saved in S3 or create an endpoint without training it again.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62614143",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.5,
        "Challenge_reading_time":17.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":49.9795925,
        "Challenge_title":"AWS SageMaker: Create an endpoint using a trained model hosted in S3",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1360.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429270603900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Jos\u00e9 dos Campos, Sao Jose dos Campos - State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":138.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>So, after to run on many pages, just found a clue <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/blazingtext_hosting_pretrained_fasttext\/blazingtext_hosting_pretrained_fasttext.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. And I finally found out how to load the model and create the endpoint:<\/p>\n<pre><code>def create_endpoint(self):\n    sess = sagemaker.Session()\n    training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=&quot;latest&quot;)        \n    role = &quot;YOUR_ROLE_ARN_WITH_SAGEMAKER_EXECUTION&quot;\n    model = &quot;s3:\/\/BUCKET\/PREFIX\/...\/output\/model.tar.gz&quot;\n\n    sm_model = sagemaker.Model(model_data=model, image=training_image, role=role, sagemaker_session=sess)\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n<\/code><\/pre>\n<p><strong>Please, do not forget to disable your endpoint after using. This is really important! Endpoints are charged by &quot;running&quot; not only by the use<\/strong><\/p>\n<p>I hope it also can help you out!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1595855655040,
        "Solution_link_count":1.0,
        "Solution_readability":19.4,
        "Solution_reading_time":14.32,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":215.8874158333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>hi I would like to know how to estimate the price for designer ?<\/p>",
        "Challenge_closed_time":1665358871540,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664581676843,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to estimate the price for a designer using machine learning.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1031348\/machine-learning-pricing",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":215.8874158333,
        "Challenge_title":"machine learning pricing",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":16,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=5c0df626-d144-412c-8dd3-6bbe9f527d31\">@Louis  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform, I am sorry I should have seen your question earlier.     <\/p>\n<p>This price depends on your settings about the compute. I would say you should try the calculator to estimate your price to avoid surprise - <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/calculator\/\">https:\/\/azure.microsoft.com\/en-us\/pricing\/calculator\/<\/a>    <\/p>\n<p>The screenshot of pricing calculator is as below -    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/248822-image.png?platform=QnA\" alt=\"248822-image.png\" \/>    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <br \/>\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":10.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":467.3672177778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am able to submit jobs to Azure ML services using a compute cluster. It works well, and the autoscaling combined with good flexibility for custom environments seems to be exactly what I need. However, so far all these jobs seem to only use one compute node of the cluster. Ideally I would like to use multiple nodes for a computation, but all methods that I see rely on rather deep integration with azure ML services.<\/p>\n\n<p>My modelling case is a bit atypical. From previous experiments I identified a group of architectures (pipelines of preprocessing steps + estimators in Scikit-learn) that worked well. \nHyperparameter tuning for one of these estimators can be performed reasonably fast (couple of minutes) with <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">RandomizedSearchCV<\/a>. So it seems less effective to parallelize this step.<\/p>\n\n<p>Now I want to tune and train this entire list of architectures.\nThis should be very easily to parallelize since all architectures can be trained independently. <\/p>\n\n<p>Ideally I would like something like (in pseudocode)<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tuned = AzurePool.map(tune_model, [model1, model2,...])\n<\/code><\/pre>\n\n<p>However, I could not find any resources on how I could achieve this with an Azure ML Compute cluster.\nAn acceptable alternative would come in the form of a plug-and-play substitute for sklearn's CV-tuning methods, similar to the ones provided in <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV\" rel=\"nofollow noreferrer\">dask<\/a> or <a href=\"https:\/\/databricks.github.io\/spark-sklearn-docs\/#spark_sklearn.GridSearchCV\" rel=\"nofollow noreferrer\">spark<\/a>.<\/p>",
        "Challenge_closed_time":1565990325968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565966913733,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in parallelizing work on an Azure ML Service Compute cluster. They have been able to submit jobs to the cluster, but all jobs seem to use only one compute node. The user wants to train a list of architectures in parallel, but they could not find any resources on how to achieve this with an Azure ML Compute cluster. They are looking for a plug-and-play substitute for sklearn's CV-tuning methods.",
        "Challenge_last_edit_time":1565987551452,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57526707",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.0,
        "Challenge_reading_time":25.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":6.5033986111,
        "Challenge_title":"How to parallelize work on an Azure ML Service Compute cluster?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":818.0,
        "Challenge_word_count":233,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525187747288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgium",
        "Poster_reputation_count":1466.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>There are a number of ways you could tackle this with AzureML. The simplest would be to just launch a number of jobs using the AzureML Python SDK (the underlying example is taken from <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training\/train-hyperparameter-tune-deploy-with-sklearn\/train-hyperparameter-tune-deploy-with-sklearn.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\n\nruns = []\n\nfor kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n    for penalty in [0.5, 1, 1.5]:\n        print ('submitting run for kernel', kernel, 'penalty', penalty)\n        script_params = {\n            '--kernel': kernel,\n            '--penalty': penalty,\n        }\n\n        estimator = SKLearn(source_directory=project_folder, \n                            script_params=script_params,\n                            compute_target=compute_target,\n                            entry_script='train_iris.py',\n                            pip_packages=['joblib==0.13.2'])\n\n        runs.append(experiment.submit(estimator))\n<\/code><\/pre>\n\n<p>The above requires you to factor your training out into a script (or a set of scripts in a folder) along with the python packages required. The above estimator is a convenience wrapper for using Scikit Learn. There are also estimators for Tensorflow, Pytorch, Chainer and a generic one (<code>azureml.train.estimator.Estimator<\/code>) -- they all differ in the Python packages and base docker they use.<\/p>\n\n<p>A second option, if you are actually tuning parameters, is to use the HyperDrive service like so (using the same <code>SKLearn<\/code> Estimator as above):<\/p>\n\n<pre><code>from azureml.train.sklearn import SKLearn\nfrom azureml.train.hyperdrive.runconfig import HyperDriveConfig\nfrom azureml.train.hyperdrive.sampling import RandomParameterSampling\nfrom azureml.train.hyperdrive.run import PrimaryMetricGoal\nfrom azureml.train.hyperdrive.parameter_expressions import choice\n\nestimator = SKLearn(source_directory=project_folder, \n                    script_params=script_params,\n                    compute_target=compute_target,\n                    entry_script='train_iris.py',\n                    pip_packages=['joblib==0.13.2'])\n\nparam_sampling = RandomParameterSampling( {\n    \"--kernel\": choice('linear', 'rbf', 'poly', 'sigmoid'),\n    \"--penalty\": choice(0.5, 1, 1.5)\n    }\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                                         hyperparameter_sampling=param_sampling, \n                                         primary_metric_name='Accuracy',\n                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                                         max_total_runs=12,\n                                         max_concurrent_runs=4)\n\nhyperdrive_run = experiment.submit(hyperdrive_run_config)\n<\/code><\/pre>\n\n<p>Or you could use DASK to schedule the work as you were mentioning. Here is a sample of how to set up DASK on and AzureML Compute Cluster so you can do interactive work on it: <a href=\"https:\/\/github.com\/danielsc\/azureml-and-dask\" rel=\"nofollow noreferrer\">https:\/\/github.com\/danielsc\/azureml-and-dask<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1567670073436,
        "Solution_link_count":3.0,
        "Solution_readability":17.0,
        "Solution_reading_time":36.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":250.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":9.6420455556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Challenge_closed_time":1565705412680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565670597500,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to use GPU to train a neural network model in Azure Machine Learning Service using P100-NC6s-V2 compute. The training is happening on the CPU and the logs show that it's not able to find CUDA. The user has mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. The warning log shows that it fails to use the GPU and is currently using the CPU. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1565670701316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57471129",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":57.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":9.6708833333,
        "Challenge_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1402.0,
        "Challenge_word_count":396,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408145271463,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":65.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.2,
        "Solution_reading_time":11.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.8813888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nUnable to create comet logger when using pytorch lightning cli.\r\n\r\n### To Reproduce\r\nhttps:\/\/colab.research.google.com\/drive\/1cvEyYHceKjunKpcGY39oFrinWnIVydJV?usp=sharing\r\n\r\n### Expected behavior\r\nRun model.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           11.1\r\n* Packages:\r\n\t- numpy:             1.21.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.10.0+cu111\r\n\t- pytorch-lightning: 1.6.0\r\n\t- tqdm:              4.63.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.13\r\n\t- version:           1 SMP Tue Dec 7 09:58:10 PST 2021\r\n\r\n### Additional context\r\n\r\nError message:\r\n```\r\nEpoch 1: 100% 32\/32 [00:00<00:00, 300.70it\/s, loss=-15.4, v_num=ff79]Traceback (most recent call last):\r\n  File \"main.py\", line 48, in <module>\r\n    cli = LightningCLI(BoringModel, LitDataset, save_config_callback=None)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 564, in __init__\r\n    self._run_subcommand(self.subcommand)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 835, in _run_subcommand\r\n    fn(**fn_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 772, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 724, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 812, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1237, in _run\r\n    results = self._run_stage()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1324, in _run_stage\r\n    return self._run_train()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1354, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 269, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 246, in advance\r\n    self.trainer._logger_connector.update_train_step_metrics()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 202, in update_train_step_metrics\r\n    self.log_metrics(self.metrics[\"log\"])\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 130, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 252, in log_metrics\r\n    self.experiment.log_metrics(metrics_without_epoch, step=step, epoch=epoch)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 41, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 39, in get_experiment\r\n    return fn(self)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 223, in experiment\r\n    offline_directory=self.save_dir, project_name=self._project_name, **self._kwargs\r\nTypeError: __init__() got an unexpected keyword argument 'agg_key_funcs'\r\n```\r\nFor some reason, `self._kwargs` there has `{'agg_key_funcs': None, 'agg_default_func': None}`.\n\ncc @awaelchli @edward-io @borda @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy",
        "Challenge_closed_time":1650063297000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1649790124000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered a challenge where the training of a resnet model on multi-core tpus crashed and printed an error message of \"dumping computation\" at the start of the validation loop when using a logger ( or .ml).",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/12734",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":20.9,
        "Challenge_reading_time":56.91,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":75.8813888889,
        "Challenge_title":"Unable to create comet logger when using pytorch lightning cli.",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":286,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Facing the same issue but with W and B. see https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12529 and https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12714 This was fixed and included in the 1.6.1 release. Could you try upgrading lightning? \r\n`pip install --upgrade pytorch-lightning` \ud83e\udd1f  @awaelchli not the same bug, but my colab link to reproduce the bug is now throwing another error.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.5,
        "Solution_reading_time":5.29,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.2,
        "Challenge_answer_count":4,
        "Challenge_body":"Hi, I'm using Google Colab +pro and unfortunately I`m getting several Ram calls and have not been able to move forward or train some models\n\nWhich is the next tool that I should get in order to be able to run the Google Colab models without the Ram calls?\n\nShould I get a Google Compute Engine and try to connect the google colab files to it?\n\nShould I up load the model to vertex AI?\n\nWhat characteristics should I need to take into consideration before I select any of the different tools?",
        "Challenge_closed_time":1652442120000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652171400000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues with RAM calls while using Google Colab +pro and is seeking advice on the next tool to use in order to run models without encountering these issues. They are considering options such as Google Compute Engine and Vertex AI and are seeking guidance on the characteristics to consider before selecting a tool.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Next-Step-from-Google-Colab-Pro\/m-p\/421797#M322",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.5,
        "Challenge_reading_time":6.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":75.2,
        "Challenge_title":"Next Step from Google Colab +Pro",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":387.0,
        "Challenge_word_count":97,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,\n\nI have provided a few links to help you through configuring your Google Colab Model.\n\nThis link below contains all Google Colab related questions on Stack Overflow:\n\nhttps:\/\/stackoverflow.com\/search?q=colab&s=7e8e7982-76a3-4765-8bad-63af4a9415fb\n\nThe following link explains how to double the Ram in Google Colab:\n\nhttps:\/\/towardsdatascience.com\/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-...\n\nThe last link is a HOW-TO guide:\n\nhttps:\/\/neptune.ai\/blog\/how-to-use-google-colab-for-deep-learning-complete-tutorial#:~:text=Open%20a....\n\nRegards\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":14.7,
        "Solution_reading_time":7.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1401922736652,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brisbane",
        "Answerer_reputation_count":748.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":510.2445766667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Challenge_closed_time":1523591814356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521754623920,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in making hyperparameters available to the serving endpoint in SageMaker Tensorflow. While the training instances have access to input parameters using hyperparameters, there is no way to pass in parameters to control data processing in the input function when the endpoint is deployed. The user is seeking advice on the best way to pass parameters to the serving instance.",
        "Challenge_last_edit_time":1521754933880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":13.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":510.3306766667,
        "Challenge_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1426.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1443201378360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":749.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":12.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1635310523496,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":1.9224463889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have type error when I run for training on sagemaker by using xgboost conatiner.\nPlease advise me to fix the issue.<\/p>\n<pre><code>container = 'southeast-2','783357654285.dkr.ecr.ap-southeast-2.amazonaws.com\/sagemaker- xgboost:latest'`\n\ntrain_input = TrainingInput(s3_data='s3:\/\/{}\/train'.format(bucket, prefix), content_type='csv')\nvalidation_input = TrainingInput(s3_data='s3:\/\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\nsess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(\ncontainer,\nrole, \ninstance_count=1,\ninstance_type='ml.t2.medium',\noutput_path='s3:\/\/{}\/output'.format(bucket, prefix),\nsagemaker_session=sess\n)\n\nxgb.set_hyperparameters(\nmax_depth=5,\neta=0.1,\ngamma=4,\nmin_child_weight=6,\nsubsample=0.8,\nsilent=0,\nobjective=&quot;binary:logistic&quot;,\nnum_round=25,\n)\n\nxgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})\n<\/code><\/pre>\n<hr \/>\n<p>TypeError                                 Traceback (most recent call last)\n in \n21 )\n22\n---&gt; 23 xgb.fit({&quot;train&quot;: train_input, &quot;validation&quot;: validation_input})<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n685                 * <code>TrialComponentDisplayName<\/code> is used for display in Studio.\n686         &quot;&quot;&quot;\n--&gt; 687         self._prepare_for_training(job_name=job_name)\n688\n689         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _prepare_for_training(self, job_name)\n446                 constructor if applicable.\n447         &quot;&quot;&quot;\n--&gt; 448         self._current_job_name = self._get_or_create_name(job_name)\n449\n450         # if output_path was specified we use it otherwise initialize here.<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _get_or_create_name(self, name)\n435             return name\n436\n--&gt; 437         self._ensure_base_job_name()\n438         return name_from_base(self.base_job_name)\n439<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in _ensure_base_job_name(self)\n420         # honor supplied base_job_name or generate it\n421         if self.base_job_name is None:\n--&gt; 422             self.base_job_name = base_name_from_image(self.training_image_uri())\n423\n424     def _get_or_create_name(self, name=None):<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/utils.py in base_name_from_image(image)\n95         str: Algorithm name, as extracted from the image name.\n96     &quot;&quot;&quot;\n---&gt; 97     m = re.match(&quot;^(.+\/)?([^:\/]+)(:[^:]+)?$&quot;, image)\n98     algo_name = m.group(2) if m else image\n99     return algo_name<\/p>\n<p>~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py in match(pattern, string, flags)\n170     &quot;&quot;&quot;Try to apply the pattern at the start of the string, returning\n171     a match object, or None if no match was found.&quot;&quot;&quot;\n--&gt; 172     return _compile(pattern, flags).match(string)\n173\n174 def fullmatch(pattern, string, flags=0):<\/p>\n<p>TypeError: expected string or bytes-like object<\/p>",
        "Challenge_closed_time":1638769174620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638762253813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a type error while training on sagemaker using xgboost container. The error occurs while running the fit function and is related to the expected string or bytes-like object. The user is seeking advice to fix the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70240640",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":41.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":1.9224463889,
        "Challenge_title":"xgboost sagemaker train failure",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":245,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635310523496,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<pre><code>import sagemaker\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker.session import TrainingInput\nfrom sagemaker import image_uris\nfrom sagemaker.session import Session\n\n# initialize hyperparameters\nhyperparameters = {\n    &quot;max_depth&quot;:&quot;5&quot;,\n    &quot;eta&quot;:&quot;0.1&quot;,\n    &quot;gamma&quot;:&quot;4&quot;,\n    &quot;min_child_weight&quot;:&quot;6&quot;,\n    &quot;subsample&quot;:&quot;0.7&quot;,\n    &quot;objective&quot;:&quot;binary:logistic&quot;,\n    &quot;num_round&quot;:&quot;25&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\noutput_path = 's3:\/\/{}\/{}\/output'.format(bucket, 'rain-xgb-built-in-algo')\n\n\n# this line automatically looks for the XGBoost image URI and builds an \nXGBoost container.\n# specify the repo_version depending on your preference.\nxgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'ap-southeast- \n2', &quot;1.3-1&quot;)\n\n\n# construct a SageMaker estimator that calls the xgboost-container\nestimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n                                      hyperparameters=hyperparameters,\n                                      role=sagemaker.get_execution_role(),\n                                      instance_count=1, \n                                      instance_type='ml.m5.large', \n                                      volume_size=5, # 5 GB \n                                      output_path=output_path)\n\n\n\n # define the data type and paths to the training and validation datasets\n\n train_input = TrainingInput(&quot;s3:\/\/{}\/{}\/&quot;.format(bucket,'train'), \n content_type='csv')\n validation_input = TrainingInput(&quot;s3:\/\/{}\/{}&quot;.format(bucket,'validation'), \n content_type='csv')\n\n\n # execute the XGBoost training job\n estimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I have rewritten as above and could run training.\nthank you !<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":21.5,
        "Solution_reading_time":23.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":659.0783333333,
        "Challenge_answer_count":0,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No --> Yes\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Open remote connection to Azure Machine Learning Compute Instance\r\n\r\nThis does not seem to cause any issues, but it's annoying to see the error message every time.\r\n\r\nAction: azureAccount.onSubscriptionsChanged\r\nError type: REQUEST_SEND_ERROR\r\nError Message: request to redacted:url failed, reason: getaddrinfo ENOTFOUND redacted:idworkspace.westeurope.api.azureml.ms\r\n\r\n\r\nVersion: 0.8.2\r\nOS: linux\r\nOS Release: 5.4.0-1068-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.66.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nnew t extension.js:2:486489\r\nt.<anonymous> extension.js:2:470040\r\nextension.js:2:2450576\r\nObject.throw extension.js:2:2450681\r\nc extension.js:2:2449471\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":1652117002000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1649744320000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while installing an Azure ML training model piece. They installed grep via chocolatey, which allowed the script to go further, but now they are receiving an error message stating that the dataset with the name 'mnist_opendataset' is not found. The user is seeking help to troubleshoot this error as they need to demo it to a customer next week.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1541",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":659.0783333333,
        "Challenge_title":"Reoccurring error on opening connection to Azure Machine Learning Compute Instance",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@evakkuri thanks for filing this issue. Is this happening every time you open a remote connection? Are you connecting to Compute Instance through the ML Studio? Currently this happens every time I connect. I'm not connecting via ML Studio, instead through VS Code with the Azure Machine Learning extension. @sevillal Can you please follow up here :) ? @evakkuri we have published version v0.10.0 of the extension. Could you please upgrade and retry to check if you issue is still reproducible? Please reopen this issue if that's the case.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":6.61,
        "Solution_score_count":null,
        "Solution_sentence_count":10.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452814040316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":21.0570775,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The Import Data module for Azure Table documention can be found here: <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/mt674699\" rel=\"nofollow\">https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/mt674699<\/a><\/p>\n\n<p>In there it mentions that:<\/p>\n\n<blockquote>\n  <p>The Import Data module does not support filtering as data is being read. The exception is reading from data feeds, which sometimes allow you to specify a filter condition as part of the feed URL.<\/p>\n<\/blockquote>\n\n<p>There is a large amount of data in our table storage and it is not feasible to re-download the entire data set each time we run the experiment. I'm aware that there is the option to cache the data, however there is new data constantly being inserted and we would like to be able to use the new data whenever the experiment is run.<\/p>\n\n<p>Is there an alternative to the Import Data module that we could use to get table storage data with an ODATA query instead?<\/p>",
        "Challenge_closed_time":1474678989116,
        "Challenge_comment_count":0,
        "Challenge_created_time":1474603183637,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in importing data from Azure Table Storage into Azure Machine Learning Studio with an ODATA query. The Import Data module does not support filtering as data is being read, and caching the data is not feasible due to constantly inserting new data. The user is seeking an alternative to the Import Data module that supports ODATA query.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39652384",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":21.0570775,
        "Challenge_title":"How can I import into Azure machine learning studio from azure table storage with ODATA query?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":324.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1450652266692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brisbane, Australia",
        "Poster_reputation_count":802.0,
        "Poster_view_count":152.0,
        "Solution_body":"<p>There is no generic way to incrementally update a dataset. <\/p>\n\n<p>However, depending on what you want to do with the data, there are different options for adding new data:<\/p>\n\n<p>The Add Rows module effectively concatenates two datasets. So you could use the old, cached dataset on the left-hand input and add the new data on the right-hand input. That way you only have to read in the new data.\nHowever, you would have to create some complex logic for figuring out which rows were new and old, and then maintain that outside Azure ML.<\/p>\n\n<p>You could create an OData feed based on table storage, to enable filtering and get the new data that way. Just be aware that right now only public feeds are supported. And you would have to use Join or Add Rows to recombine the old and new data as described above. <\/p>\n\n<p>You might also look into ways of using the <a href=\"https:\/\/blog.maartenballiauw.be\/post\/2012\/10\/08\/what-partitionkey-and-rowkey-are-for-in-windows-azure-table-storage.html\" rel=\"nofollow\">table names<\/a>, partitions, and rowkeys to chunk your data. <\/p>\n\n<p>If you are retraining a model and you want to update your feature statistics, the <a href=\"https:\/\/msdn.microsoft.com\/library\/dn913056.aspx\" rel=\"nofollow\">Learning with Counts<\/a> modules support incremental updates of count-based features. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":16.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":196.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":53.6307336111,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>How do I use the Python API to find and delete runs without a particular tag?<\/p>\n<p>I tried writing this MongoDB query, but it doesn\u2019t appear to work:<\/p>\n<pre><code class=\"lang-auto\">for r in (api.runs(\n    path...\n    filters={\"tags\": {\"$in\": \"keep\"}}\n)):\n<\/code><\/pre>\n<p>This is similar to <a href=\"https:\/\/community.wandb.ai\/t\/using-the-python-api-to-delete-models-with-no-tag-minimal\/1498\">this post<\/a><\/p>",
        "Challenge_closed_time":1675689384206,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675496313565,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help on how to use the Python API to find and delete runs that do not have a specific tag. They have attempted to use a MongoDB query but it did not work. The user also references a similar post on deleting models without a tag.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-the-python-api-to-delete-runs-without-a-particular-tag\/3818",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":11.0,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":53.6307336111,
        "Challenge_title":"Using the python API to delete runs without a particular tag",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/turian\">@turian<\/a>, thanks for your question! You can delete runs without a concrete tag with a code like:<\/p>\n<pre><code class=\"lang-auto\">import wandb\napi = wandb.Api()\nruns = api.runs('entity_name\/project_name')\nfor run in runs:\n  if \"keep\" not in run.tags:\n    run.delete()\n<\/code><\/pre>\n<p>Could you please try it and see if it works?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":4.74,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":48.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":59.9219444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
        "Challenge_closed_time":1597488847000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1597273128000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error when deploying an MLflow registry model to Triton using the mlflow-triton-plugin with the --flavor=onnx flag. The plugin is trying to create a config.pbtxt in the destination folder before creating the model folder itself. The error can be fixed by creating the folder beforehand, but it could also be handled from the plugin side. The error message indicates that the config.pbtxt file is not found in the destination folder.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2939",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.5,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":59.9219444444,
        "Challenge_title":"mlflow checkpoints in the wrong location ",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":82,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@david-waterworth mind try the latest 0.9rc12? It was fixed here: #2502 \r\nThe checkpoints subfolder will go here: `mlflow\\1{guid}\\checkpoints`, is that what you want @david-waterworth ?\r\n Thanks @awaelchli  yes that's what I want - thanks!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":2.95,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1444940402827,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Oakland, CA, USA",
        "Answerer_reputation_count":8474.0,
        "Answerer_view_count":1313.0,
        "Challenge_adjusted_solved_time":10.0860825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running some projects with H2o AutoML using Sagemaker notebook instances, and I would like to know if H2o AutoML can benefit from a GPU Sagemaker instance, if so, how should I configure the notebook? <\/p>",
        "Challenge_closed_time":1567727347088,
        "Challenge_comment_count":4,
        "Challenge_created_time":1567711533230,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether H2o AutoML can benefit from a GPU Sagemaker instance and how to configure the notebook if so.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57811873",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4.3927383333,
        "Challenge_title":"Can H2o AutoML benefit from a GPU instance on Sagemaker platform?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":462.0,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p><a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\" rel=\"nofollow noreferrer\">H2O AutoML<\/a> contains a handful of algorithms and one of them is <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/xgboost.html\" rel=\"nofollow noreferrer\">XGBoost<\/a>, which has been part of H2O AutoML since H2O version 3.22.0.1.  XGBoost is the only GPU-capable algorithm inside of H2O AutoML, however, a lot of the models that are trained in AutoML are XGBoost models, so it still can be useful to utilize a GPU. Keep in mind that you must use H2O 3.22 or above to use this feature.<\/p>\n\n<p>My suggestion is to test it on a GPU-enabled instance and compare the results to a non-GPU instance and see if it's worth the extra cost.  <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1567747843127,
        "Solution_link_count":2.0,
        "Solution_readability":10.2,
        "Solution_reading_time":9.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2159.1657988889,
        "Challenge_answer_count":2,
        "Challenge_body":"Is there a link that shows how much GPU memory is available on the following GPU instances on AWS?\n\n1. g4-series instances (NVidia T4)\n2. g5-series instances (NVidia A10)\n3. p3d-series instances (NVidia V100)\n4. p4d-series instances (NVidia A100)\n\nUpdate: the information is available for the [p3d series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p3\/) and [g5 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g5\/), though not for the [g4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/g4\/) or the [p4 series](https:\/\/aws.amazon.com\/ec2\/instance-types\/p4\/) instances. Is it possible to retrieve the information for the latter two instances anywhere (without having to launch the instances)?",
        "Challenge_closed_time":1676386194668,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643029354176,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for information on the amount of GPU memory available on AWS instances, specifically the g4, g5, p3d, and p4d series. The user has found information for the p3d and g5 series, but not for the g4 and p4d series. The user is asking if there is a way to retrieve this information without having to launch the instances.",
        "Challenge_last_edit_time":1668613197792,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUvPdBv2rwTEiYKKHDPLUTWA\/how-much-gpu-memory-are-available-on-the-g4-g5-p3d-and-p4d-series-instances",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":9.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":9265.7890255556,
        "Challenge_title":"How much GPU memory are available on the g4, g5, p3d, and p4d series instances?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2195.0,
        "Challenge_word_count":95,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This link has a table that compares instances' GPU memory\nhttps:\/\/docs.amazonaws.cn\/en_us\/AmazonECS\/latest\/developerguide\/ecs-gpu.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1676386194668,
        "Solution_link_count":1.0,
        "Solution_readability":24.1,
        "Solution_reading_time":1.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1635045129020,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":53.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":250.6002630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Challenge_closed_time":1636075476147,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635046349497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a simple recommender system using TfidfVectorizer and linear_kernel, but is facing challenges in deploying it through Sagemaker with an endpoint. They are unsure if the output from tf.fit_transform(ds['content']) is considered the model or the output from linear_kernel(tfidf_matrix, tfidf_matrix).",
        "Challenge_last_edit_time":1635173315200,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":21.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":285.8685138889,
        "Challenge_title":"How to Deploy ML Recommender System on AWS",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1635045129020,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":4.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":94.5976361111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any possibility of doing the following operations in Azure ML Studio through REST calls?\n1) Create and upload a new dataset.\n2) Create a new Automated ML run selecting an already created dataset, configuring the experiment name, target column and training cluster and selecting the task type (e.g. Classification\/Regression).\n3) Deploy the run on a container and retrieve the container endpoint URL.<\/p>",
        "Challenge_closed_time":1586668869390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586328317900,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of performing certain operations in Azure ML Studio through REST calls, including creating and uploading a new dataset, creating a new Automated ML run with specific configurations, and deploying the run on a container and retrieving the container endpoint URL.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61094767",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":6.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":94.5976361111,
        "Challenge_title":"Doing the following operations in Azure ML Studio through REST calls",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":91.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Please follow the APIs available at the <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/<\/a> \n for Azure ML studio through REST API calls, but other than dataset-related API.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rTTNz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rTTNz.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.9,
        "Solution_reading_time":5.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1509672524840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":7512.4283525,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When trying to run a ScriptRunConfig, using :<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>src = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', ds.as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>\n\n<p>It doesn't work and breaks with this when I submit the job : <\/p>\n\n<pre><code>... lots of things... and then\nTypeError: Object of type 'DataReference' is not JSON serializable\n<\/code><\/pre>\n\n<p>However if I run it with the Estimator, it works. One of the differences is the fact that with a <code>ScriptRunConfig<\/code> we're using a list for parameters and the other is a dictionary.<\/p>\n\n<p>Thanks for any pointers!<\/p>",
        "Challenge_closed_time":1568945686667,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568929720367,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to run a ScriptRunConfig with datastore reference on AML. The error message \"TypeError: Object of type 'DataReference' is not JSON serializable\" is displayed when the job is submitted. However, the same code works with Estimator, and the user suspects that the difference in parameters (list vs dictionary) might be the cause of the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58019308",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":10.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":4.4350833334,
        "Challenge_title":"ScriptRunConfig with datastore reference on AML",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1541.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1538275960603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Montreal, QC, Canada",
        "Poster_reputation_count":381.0,
        "Poster_view_count":50.0,
        "Solution_body":"<p>Being able to use <code>DataReference<\/code> in <code>ScriptRunConfig<\/code> is a bit more involved than doing just <code>ds.as_mount()<\/code>. You will need to convert it into a string in <code>arguments<\/code> and then update the <code>RunConfiguration<\/code>'s <code>data_references<\/code> section with the <code>DataReferenceConfiguration<\/code> created from <code>ds<\/code>. Please <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\" rel=\"nofollow noreferrer\">see here<\/a> for an example notebook on how to do that.<\/p>\n<p>If you are just reading from the input location and not doing any writes to it, please check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\"><code>Dataset<\/code><\/a>. It allows you to do exactly what you are doing without doing anything extra. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">Here is an example notebook<\/a> that shows this in action.<\/p>\n<p>Below is a short version of the notebook<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\n\n# more imports and code\n\nds = Datastore(workspace, 'mydatastore')\ndataset = Dataset.File.from_files(path=(ds, 'path\/to\/input-data\/within-datastore'))\n\nsrc = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', dataset.as_named_input('input').as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1595974462436,
        "Solution_link_count":3.0,
        "Solution_readability":17.8,
        "Solution_reading_time":23.02,
        "Solution_score_count":4.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.0094444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi, does SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?\nI'm trying [this sample with 1.6][1] and the deployment fails with error\n\n    FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/model\/model.pth'\n\nthe documentation doesn't mention such a constraint\n\n  [1]: https:\/\/github.com\/aws-samples\/amazon-sagemaker-bert-classify-pytorch",
        "Challenge_closed_time":1610121735000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610118101000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a sample with SageMaker PyTorch Hosting 1.6. The deployment fails with an error stating that the model.pth file is not found. The user is questioning if SageMaker PyTorch Hosting 1.6 only works with artifacts named model.pth, as this constraint is not mentioned in the documentation.",
        "Challenge_last_edit_time":1668602387619,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUn-IyC9nySDKKibLL3Lvw3A\/sagemaker-pytorch-hosting-1-6-works-only-with-artifacts-named-model-pth",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.0094444444,
        "Challenge_title":"SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":52,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes - from the thread on this open issue: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/86\n\nIn the issue thread they note that the new Pytorch 1.6 image requires that the model filename is `model.pth`, linking to the relevant code where this default is set: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/9a6869e\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L121\n\nAlso noted in the thread is that users have successfully adapted their code to use torchserve in Pytorch 1.6 by changing it to save their model in a file named `model.pth`. Once renamed, they were still able to use custom inference scripts to load their model by defining a custom `model_fn`: https:\/\/github.com\/data-science-on-aws\/workshop\/blob\/374329adf15bf1810bfc4a9e73501ee5d3b4e0f5\/09_deploy\/wip\/pytorch\/code\/inference.py",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1667925547320,
        "Solution_link_count":3.0,
        "Solution_readability":11.9,
        "Solution_reading_time":10.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1464391892936,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Answerer_reputation_count":2243.0,
        "Answerer_view_count":148.0,
        "Challenge_adjusted_solved_time":48.6672925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Challenge_closed_time":1573653626700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1573479066647,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a credit-fraud dataset on AWS Sagemaker and created an endpoint of the model. They want to share it with a friend who has some credit data and wants to know whether the transaction is fraud or not. The user is unsure of the correct way to share the endpoint and is considering sharing their ARN, but believes it may not be the right way. They are seeking advice on alternative methods.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58802366",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":6.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":48.4889036111,
        "Challenge_title":"Deploying the sagemaker endpoint created as a service",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":226.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568318861627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1573654268900,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":8.56,
        "Solution_score_count":6.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1587281590603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":473.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":511.5126291667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using sagemaker model monitor.<\/p>\n\n<p>When capturing data, it outputs the following json file.<\/p>\n\n<pre><code>{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text\/csv\",\"mode\":\"INPUT\",\"data\":\"MSwwLjUzLDAuNDIsMC4xMzUsMC42NzcsMC4yNTY1LDAuMTQxNSwwLjIx\",\"encoding\":\"BASE64\"},\"endpointOutput\":{\"observedContentType\":\"text\/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"MTEuNjQzNDU1NTA1MzcxMDk0\",\"encoding\":\"BASE64\"}},\"eventMetadata\":{\"eventId\":\"33404924-c0d4-4044-9dc2-1e1f5575cb0a\",\"inferenceTime\":\"2020-06-04T05:45:45Z\"},\"eventVersion\":\"0\"}\n<\/code><\/pre>\n\n<p>I want the encoding to be csv but somehow it outputs base64.<br>\nWhen or where do we change the setting of the encoding?<br>\nIs it during the invoking the endpoint? or set when making endpoint config.<br>\nI looked for some documents but I couldn't find it.<\/p>",
        "Challenge_closed_time":1593091560008,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591250114543,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Sagemaker model monitor where the captured data is outputting in JSON format with BASE64 encoding instead of CSV encoding. The user is seeking guidance on where and how to change the encoding setting, either during endpoint invocation or endpoint configuration, but is unable to find relevant documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62187748",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":11.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":511.5126291667,
        "Challenge_title":"Change datacapture encoding data to csv",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":620.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532422348876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>I just came across this same problem! Seems like you need to specify <code>CaptureContentTypeHeader<\/code> params to tell SageMaker which content type headers to treat as CSV (or JSON), versus the default which is to base64 encode the payload!<\/p>\n<p>So e.g. adding the following to your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\">CreateEndpointConfig<\/a> call or boto3\/sagemaker SDK equivalent should fix it:<\/p>\n<pre><code>{\n   &quot;DataCaptureConfig&quot;: { \n      &quot;CaptureContentTypeHeader&quot;: { \n         &quot;CsvContentTypes&quot;: [ &quot;text\/csv&quot; ]\n      },\n   }\n}\n<\/code><\/pre>\n<p>I guess this is to allow for non-standard Content-Type headers? Providing a layer of config to resolve e.g:<\/p>\n<ul>\n<li><code>application\/x-mycoolmodel<\/code> -&gt; <code>JSON<\/code>, versus<\/li>\n<li><code>application\/x-secretsauce<\/code> -&gt; <code>BASE64<\/code><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":12.37,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1373922677212,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1350.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":24155.5917811111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a simple experiment in Azure ML and trigger it with an http client. In Azure ML workspace, everything works ok when executed. However, the experiment times out and fails when I trigger the experiment using an http client. Setting a timeout value for the http client does not seem to work.<\/p>\n\n<p>Is there any way we can set this timeout value so that the experiment does not fail?<\/p>",
        "Challenge_closed_time":1522603988172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1435643857760,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an experiment in Azure ML and is triggering it with an HTTP client. While the experiment works fine in the Azure ML workspace, it times out and fails when triggered using the HTTP client. The user is looking for a way to set the timeout value for the HTTP client to prevent the experiment from failing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31130629",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24155.5917811111,
        "Challenge_title":"Azure ML web service times out",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1681.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1313488868532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":209.0,
        "Poster_view_count":42.0,
        "Solution_body":"<p>Looks like it isn't possible to set this timeout based on <a href=\"https:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/6472562-configurable-timeout-for-experiments-and-web-servi\" rel=\"nofollow noreferrer\">a feature request that is still marked as \"planned\" as of 4\/1\/2018<\/a>.<\/p>\n\n<p>The recommendation from <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/sqlserver\/en-US\/cb4ee96d-c2ca-4c65-b02f-0ccb26181f7f\/timeout-in-web-service?forum=MachineLearning\" rel=\"nofollow noreferrer\">MSDN forums from 2017<\/a> is to use the Batch Execution Service, which starts the machine learning experiment and then asynchronously asks whether it's done.<\/p>\n\n<p>Here's a code snippet from the Azure ML Web Services Management Sample Code (all comments are from their sample code):<\/p>\n\n<pre><code>        using (HttpClient client = new HttpClient())\n        {\n            var request = new BatchExecutionRequest()\n            {\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt; () {\n                    {\n                        \"output\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = storageConnectionString,\n                            RelativeLocation = string.Format(\"{0}\/outputresults.file_extension\", StorageContainerName) \/*Replace this with the location you would like to use for your output file, and valid file extension (usually .csv for scoring results, or .ilearner for trained models)*\/\n                        }\n                    },\n                },    \n\n                GlobalParameters = new Dictionary&lt;string, string&gt;() {\n                }\n            };\n\n            client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\n            \/\/ WARNING: The 'await' statement below can result in a deadlock\n            \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n            \/\/ One way to address this would be to call ConfigureAwait(false)\n            \/\/ so that the execution does not attempt to resume on the original context.\n            \/\/ For instance, replace code such as:\n            \/\/      result = await DoSomeTask()\n            \/\/ with the following:\n            \/\/      result = await DoSomeTask().ConfigureAwait(false)\n\n            Console.WriteLine(\"Submitting the job...\");\n\n            \/\/ submit the job\n            var response = await client.PostAsJsonAsync(BaseUrl + \"?api-version=2.0\", request);\n\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobId = await response.Content.ReadAsAsync&lt;string&gt;();\n            Console.WriteLine(string.Format(\"Job ID: {0}\", jobId));\n\n            \/\/ start the job\n            Console.WriteLine(\"Starting the job...\");\n            response = await client.PostAsync(BaseUrl + \"\/\" + jobId + \"\/start?api-version=2.0\", null);\n            if (!response.IsSuccessStatusCode)\n            {\n                await WriteFailedResponse(response);\n                return;\n            }\n\n            string jobLocation = BaseUrl + \"\/\" + jobId + \"?api-version=2.0\";\n            Stopwatch watch = Stopwatch.StartNew();\n            bool done = false;\n            while (!done)\n            {\n                Console.WriteLine(\"Checking the job status...\");\n                response = await client.GetAsync(jobLocation);\n                if (!response.IsSuccessStatusCode)\n                {\n                    await WriteFailedResponse(response);\n                    return;\n                }\n\n                BatchScoreStatus status = await response.Content.ReadAsAsync&lt;BatchScoreStatus&gt;();\n                if (watch.ElapsedMilliseconds &gt; TimeOutInMilliseconds)\n                {\n                    done = true;\n                    Console.WriteLine(string.Format(\"Timed out. Deleting job {0} ...\", jobId));\n                    await client.DeleteAsync(jobLocation);\n                }\n                switch (status.StatusCode) {\n                    case BatchScoreStatusCode.NotStarted:\n                        Console.WriteLine(string.Format(\"Job {0} not yet started...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Running:\n                        Console.WriteLine(string.Format(\"Job {0} running...\", jobId));\n                        break;\n                    case BatchScoreStatusCode.Failed:\n                        Console.WriteLine(string.Format(\"Job {0} failed!\", jobId));\n                        Console.WriteLine(string.Format(\"Error details: {0}\", status.Details));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Cancelled:\n                        Console.WriteLine(string.Format(\"Job {0} cancelled!\", jobId));\n                        done = true;\n                        break;\n                    case BatchScoreStatusCode.Finished:\n                        done = true;\n                        Console.WriteLine(string.Format(\"Job {0} finished!\", jobId));\n                        ProcessResults(status);\n                        break;\n                }\n\n                if (!done) {\n                    Thread.Sleep(1000); \/\/ Wait one second\n                }\n            }\n        }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.3,
        "Solution_reading_time":50.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":45.0,
        "Solution_word_count":338.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5533333333,
        "Challenge_answer_count":0,
        "Challenge_body":"The following sample notebook fails \r\n### img-classification-part1-training.ipynb\r\n\r\nwhen running:\r\n\r\n### from azureml.core import Dataset\r\n\r\nfrom azureml.core import Dataset\r\nfrom azureml.opendatasets import MNIST\r\n\r\ndata_folder = os.path.join(os.getcwd(), 'data')\r\nos.makedirs(data_folder, exist_ok=True)\r\n\r\nmnist_file_dataset = MNIST.get_file_dataset()\r\nmnist_file_dataset.download(data_folder, overwrite=True)\r\n\r\nmnist_file_dataset = mnist_file_dataset.register(workspace=ws,\r\n                                                 name='mnist_opendataset',\r\n                                                 description='training and test dataset',\r\n                                                 create_new_version=True)\r\n\r\n\r\n**Here is the error**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-ac2e91b46eec> in <module>\r\n----> 1 from azureml.core import Dataset\r\n      2 from azureml.opendatasets import MNIST\r\n      3 \r\n      4 data_folder = os.path.join(os.getcwd(), 'data')\r\n      5 os.makedirs(data_folder, exist_ok=True)\r\n\r\nImportError: cannot import name 'Dataset'\r\n\r\nreference: yml file:\r\nname: img-classification-part1-training\r\ndependencies:\r\n- pip:\r\n  - azureml-sdk\r\n  - azureml-widgets\r\n  - matplotlib\r\n  - sklearn\r\n  - pandas\r\n  - azureml-opendatasets\r\n\r\nAzure ML SDK Version:  1.0.17\r\nPython 3.6 - AzureML\r\n\r\n@microsoft\r\nPlease kindly investigate.\r\nMany thanks :)",
        "Challenge_closed_time":1581440044000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1581438052000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while using Azure Machine Learning with the XGBoostClassifier model. The error message indicates that the blacklisted and whitelisted models are the same, and suggests removing models from the blacklist or adding models to the whitelist. The user provided the automl_settings and automl_config used, and noted that XGBoostClassifier was installed in the notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/787",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":17.47,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.5533333333,
        "Challenge_title":"Import Error - from azureml.core import Dataset  - ImportError: cannot import name 'Dataset'",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":110,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@andrewkinsella, version `1.0.17` is from [almost a year ago](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2019-02-25). During that time, the `Datasets` class has evolved significantly (for the better). Can you try upgrading the SDK to the newest version and trying again? @MayMSFT  Thank you very much @swanderz \r\nI will try your recommendation.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":5.1,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.1325,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nI am following the instructions on https:\/\/github.com\/awslabs\/gluon-ts\/blob\/acfd7e14c4ef6eaa62fea6d6233a9e336f6366e4\/examples\/GluonTS_SageMaker_SDK_Tutorial.ipynb but at first step when I ran `!pip install --upgrade mxnet==1.6  git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]` I got the following error,\r\n\r\n## Error message or code output\r\n```Obtaining gluonts[dev] from git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]\r\n  Updating .\/src\/gluonts clone\r\n  Running command git fetch -q --tags\r\n  Running command git reset --hard -q fc203f51f01036e854ce6a0da1a43b562074e187\r\n  Installing build dependencies ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel\r\n       cwd: None\r\n  Complete output (14 lines):\r\n  Traceback (most recent call last):\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/__main__.py\", line 16, in <module>\r\n      from pip._internal.cli.main import main as _main  # isort:skip # noqa\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/_internal\/cli\/main.py\", line 5, in <module>\r\n      import locale\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/locale.py\", line 16, in <module>\r\n      import re\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py\", line 142, in <module>\r\n      class RegexFlag(enum.IntFlag):\r\n  AttributeError: module 'enum' has no attribute 'IntFlag'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel Check the logs for full command output.\r\n\r\n```\r\n\r\n\r\n## Environment\r\nNote: Previously, I installed Gluon-TS (0.5.2) using `! pip install --upgrade mxnet==1.6 gluonts` and if I do `! pip list` I can see the package is installed but when I ran `!pip uninstall glounts` it says `WARNING: Skipping glounts as it is not installed.`\r\n\r\n- Operating system: Sagemaker notebook instance with conda_mxnet_p36 kernel.\r\n- Python version: 3.6\r\n- GluonTS version: 0.5.2 is already installed.\r\n- MXNet version:1.6",
        "Challenge_closed_time":1600271697000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1600235220000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered challenges when attempting to run the example script 'benchmark_m4.py' on a gpu-instance \"ml.p2.xlarge\" on aws, resulting in a keyerror.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/1039",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":13.9,
        "Challenge_reading_time":38.77,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":651.0,
        "Challenge_repo_issue_count":2147.0,
        "Challenge_repo_star_count":3215.0,
        "Challenge_repo_watch_count":70.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":10.1325,
        "Challenge_title":"Issue with installing GlounTS on Sagemaker notebook instance from Github",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":254,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"According to [this](https:\/\/github.com\/iterative\/dvc\/issues\/1995), it could be that `enum34` is installed.\r\n\r\nCan you check whether this is also the case here? Thanks @jaheba ! That was the issue and by running `!pip uninstall -y enum34` it is resolved.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":4.9,
        "Solution_reading_time":3.14,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":5.2579222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been exploring Azure ML Pipeline. I am referring to <a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/06A%20-%20Creating%20a%20Pipeline.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> for the below code:<\/p>\n<p>Here is a small snippet from a MS Repo:<\/p>\n<pre><code>train_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\nsource_directory = experiment_folder,\nscript_name = &quot;prep_diabetes.py&quot;,\narguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n'--prepped-data', prepped_data_folder],\noutputs=[prepped_data_folder],\ncompute_target = pipeline_cluster,\nrunconfig = pipeline_run_config,\nallow_reuse = True)\n<\/code><\/pre>\n<p>This suggests that while defining a pipeline, we must provide it a compute resource(pipeline_cluster). This obviously makes sense, since specific compute might be required for a specific step.<\/p>\n<p>But do we need to have this compute resource up and running always, so that whenever a pipeline is triggered, it can find the compute resource?<\/p>\n<p>Also, i figured we can probably keep a cluster with Zero minimum nodes, in which cases cluster is resized whenever pipeline is triggered. But i think there is a minimal cost incurrent in probably container registry regularly in such a setup. Is this the recommended way to deploy ML pipelines or some more efficient approach is possible?<\/p>",
        "Challenge_closed_time":1620823042460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620804113940,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring Azure ML Pipeline and has encountered a challenge regarding the need for a pre-existing compute resource to run a scheduled pipeline. The user is unsure if the compute resource needs to be up and running always or if a cluster with zero minimum nodes can be used. The user is also concerned about the cost incurred in such a setup and is seeking recommendations for a more efficient approach.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67498965",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":18.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.2579222222,
        "Challenge_title":"Pre-existing Compute Resource necessary for running a scheduled Azure ML pipeline?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Poster_created_time":1315165259620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation_count":339.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>Yep you're right -- create a <code>ComputeTarget<\/code> with a minimum of zero nodes. The <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-registry\/#pricing\" rel=\"nofollow noreferrer\">container registry costs<\/a> are ~$0.16 USD\/day and, IIRC, that cost is bundled in with Azure Machine learning.<\/p>\n<p>This is what our team does for our published pipelines in production.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.2,
        "Solution_reading_time":5.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":0.8939655556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to <a href=\"https:\/\/spacy.io\/usage\/training\" rel=\"nofollow noreferrer\">train a spaCy model<\/a> , but turning the code into a Vertex AI Pipeline <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#:%7E:text=Step%201%3A%20Create%20a%20Python%20function%20based%20component\" rel=\"nofollow noreferrer\">Component<\/a>. My current code is:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_name: str, dev_name: str) -&gt; NamedTuple(&quot;output&quot;, [(&quot;model_path&quot;, str)]):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_name : Name of the spaCy &quot;train&quot; set, used for model training.\n    dev_name: Name of the spaCy &quot;dev&quot; set, , used for model training.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE FAILS TO BE COMPILED HERE\n    \n    # NOTE: The remaining code has already been tested and proven to be functional.\n    #       It has been edited since the project is private.\n    \n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    location = &quot;gcs\/secret_model_destination_path\/TestModel&quot;\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, location,\n                    &quot;--paths.train&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(train_name),\n                    &quot;--paths.dev&quot;, &quot;gcs\/secret_bucket\/secret_path\/{}.spacy&quot;.format(dev_name),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n    \n    return (location,)\n<\/code><\/pre>\n<p>The Vertex AI Logs display the following as main cause of the failure:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bRotZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The libraries are successfully installed, and yet I feel like there is some missing library \/ setting (as I know by <a href=\"https:\/\/dev.to\/davidgerva\/spacy-3-on-a-google-cloud-compute-instance-to-train-a-ner-transformer-model-23hf\" rel=\"nofollow noreferrer\">experience<\/a>); however I don't know  how to make it &quot;Python-based Vertex AI Components Compatible&quot;. BTW, the use of GPU is <strong>mandatory<\/strong> in my code.<\/p>\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":1651093924243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650978304193,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to train a spaCy model as a Vertex AI Pipeline component, but the image fails to compile due to a missing library or setting. The user has installed the required libraries and believes that there may be a compatibility issue with Python-based Vertex AI components. The code requires the use of a GPU. The user is seeking suggestions to resolve the issue.",
        "Challenge_last_edit_time":1651090705967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72014493",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":16.7,
        "Challenge_reading_time":37.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":32.1166805556,
        "Challenge_title":"Training spaCy model as a Vertex AI Pipeline \"Component\"",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":234.0,
        "Challenge_word_count":227,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>After some rehearsals, I think I have figured out what my code was missing. Actually, the <code>train<\/code> component definition was correct (with some minor tweaks relative to what was originally posted); however <strong>the pipeline was missing the GPU definition<\/strong>. I will first include a dummy example code, which trains a NER model using spaCy, and orchestrates everything via Vertex AI Pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output, OutputPath, InputPath\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Component definition\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;generate.yaml&quot;\n)\ndef generate_spacy_file(train_path: OutputPath(), dev_path: OutputPath()):\n    &quot;&quot;&quot;\n    Generates a small, dummy 'train.spacy' &amp; 'dev.spacy' file\n    \n    Returns:\n    -------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    &quot;&quot;&quot;\n    import spacy\n    from spacy.training import Example\n    from spacy.tokens import DocBin\n\n    td = [    # Train (dummy) dataset, in 'spacy V2 presentation'\n              (&quot;Walmart is a leading e-commerce company&quot;, {&quot;entities&quot;: [(0, 7, &quot;ORG&quot;)]}),\n              (&quot;I reached Chennai yesterday.&quot;, {&quot;entities&quot;: [(19, 28, &quot;GPE&quot;)]}),\n              (&quot;I recently ordered a book from Amazon&quot;, {&quot;entities&quot;: [(24,32, &quot;ORG&quot;)]}),\n              (&quot;I was driving a BMW&quot;, {&quot;entities&quot;: [(16,19, &quot;PRODUCT&quot;)]}),\n              (&quot;I ordered this from ShopClues&quot;, {&quot;entities&quot;: [(20,29, &quot;ORG&quot;)]}),\n              (&quot;Fridge can be ordered in Amazon &quot;, {&quot;entities&quot;: [(0,6, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a new Washer&quot;, {&quot;entities&quot;: [(16,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a old table&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I bought a fancy dress&quot;, {&quot;entities&quot;: [(18,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a camera&quot;, {&quot;entities&quot;: [(12,18, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a tent for our trip&quot;, {&quot;entities&quot;: [(12,16, &quot;PRODUCT&quot;)]}),\n              (&quot;I rented a screwdriver from our neighbour&quot;, {&quot;entities&quot;: [(12,22, &quot;PRODUCT&quot;)]}),\n              (&quot;I repaired my computer&quot;, {&quot;entities&quot;: [(15,23, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my clock fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n              (&quot;I got my truck fixed&quot;, {&quot;entities&quot;: [(16,21, &quot;PRODUCT&quot;)]}),\n    ]\n    \n    dd = [    # Development (dummy) dataset (CV), in 'spacy V2 presentation'\n              (&quot;Flipkart started it's journey from zero&quot;, {&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Max&quot;, {&quot;entities&quot;: [(24,27, &quot;ORG&quot;)]}),\n              (&quot;Flipkart is recognized as leader in market&quot;,{&quot;entities&quot;: [(0,8, &quot;ORG&quot;)]}),\n              (&quot;I recently ordered from Swiggy&quot;, {&quot;entities&quot;: [(24,29, &quot;ORG&quot;)]})\n    ]\n\n    \n    # Converting Train &amp; Development datasets, from 'spaCy V2' to 'spaCy V3'\n    nlp = spacy.blank(&quot;en&quot;)\n    db_train = DocBin()\n    db_dev = DocBin()\n\n    for text, annotations in td:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_train.add(example.reference)\n        \n    for text, annotations in dd:\n        example = Example.from_dict(nlp.make_doc(text), annotations)\n        db_dev.add(example.reference)\n    \n    db_train.to_disk(train_path + &quot;.spacy&quot;)  # &lt;== Obtaining and storing &quot;train.spacy&quot;\n    db_dev.to_disk(dev_path + &quot;.spacy&quot;)      # &lt;== Obtaining and storing &quot;dev.spacy&quot;\n    \n\n# ----------------------- ORIGINALLY POSTED CODE -----------------------\n\n@component(\n    packages_to_install=[\n        &quot;setuptools&quot;,\n        &quot;wheel&quot;, \n        &quot;spacy[cuda113,transformers,lookups]&quot;,\n    ],\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/base-cu113&quot;,\n    output_component_file=&quot;train.yaml&quot;\n)\ndef train(train_path: InputPath(), dev_path: InputPath(), output_path: OutputPath()):\n    &quot;&quot;&quot;\n    Trains a spacy model\n    \n    Parameters:\n    ----------\n    train_path : Relative location in GCS, for the &quot;train.spacy&quot; file.\n    dev_path: Relative location in GCS, for the &quot;dev.spacy&quot; file.\n    \n    Returns:\n    -------\n    output : Destination path of the saved model.\n    &quot;&quot;&quot;\n    import spacy\n    import subprocess\n    \n    spacy.require_gpu()  # &lt;=== IMAGE NOW MANAGES TO GET BUILT!\n\n    # Presets for training\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;init&quot;, &quot;fill-config&quot;, &quot;gcs\/secret_path_to_config\/base_config.cfg&quot;, &quot;config.cfg&quot;])\n\n    # Training model\n    subprocess.run([&quot;python&quot;, &quot;-m&quot;, &quot;spacy&quot;, &quot;train&quot;, &quot;config.cfg&quot;,\n                    &quot;--output&quot;, output_path,\n                    &quot;--paths.train&quot;, &quot;{}.spacy&quot;.format(train_path),\n                    &quot;--paths.dev&quot;, &quot;{}.spacy&quot;.format(dev_path),\n                    &quot;--gpu-id&quot;, &quot;0&quot;])\n\n# ----------------------------------------------------------------------\n    \n\n# Pipeline definition\n\n@pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;spacy-dummy-pipeline&quot;,\n)\ndef spacy_pipeline():\n    &quot;&quot;&quot;\n    Builds a custom pipeline\n    &quot;&quot;&quot;\n    # Generating dummy &quot;train.spacy&quot; + &quot;dev.spacy&quot;\n    train_dev_sets = generate_spacy_file()\n    # With the output of the previous component, train a spaCy modeL    \n    model = train(\n        train_dev_sets.outputs[&quot;train_path&quot;],\n        train_dev_sets.outputs[&quot;dev_path&quot;]\n    \n    # ------ !!! THIS SECTION DOES THE TRICK !!! ------\n    ).add_node_selector_constraint(\n        label_name=&quot;cloud.google.com\/gke-accelerator&quot;,\n        value=&quot;NVIDIA_TESLA_T4&quot;\n    ).set_gpu_limit(1).set_memory_limit('32G')\n    # -------------------------------------------------\n\n# Pipeline compilation   \n\ncompiler.Compiler().compile(\n    pipeline_func=spacy_pipeline, package_path=&quot;pipeline_spacy_job.json&quot;\n)\n\n\n# Pipeline run\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun = aiplatform.PipelineJob(  # Include your own naming here\n    display_name=&quot;spacy-dummy-pipeline&quot;,\n    template_path=&quot;pipeline_spacy_job.json&quot;,\n    job_id=&quot;ml-pipeline-spacydummy-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={},\n    enable_caching=True,\n)\n\n\n# Pipeline gets submitted\n\nrun.submit()\n<\/code><\/pre>\n<p>Now, the explanation; according to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/machine-types\" rel=\"nofollow noreferrer\">Google<\/a>:<\/p>\n<blockquote>\n<p>By default, the component will run on as a Vertex AI CustomJob using an e2-standard-4 machine, with 4 core CPUs and 16GB memory.<\/p>\n<\/blockquote>\n<p>Therefore, when the <code>train<\/code> component gets compiled, it fails as &quot;<em>it was not seeing any GPU available as resource<\/em>&quot;; in the same link however, all the available settings for both CPU and GPU are mentioned. In my case as you can see, I set <code>train<\/code> component to run under ONE (1) <code>NVIDIA_TESLA_T4<\/code> GPU card, and I also increased my CPU memory, to 32GB. With these modifications, the resulting pipeline looks as follows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0I31.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0I31.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>And as you can see, it gets compiled successfully, as well as trains (and eventually obtains) a functional spaCy model. From here, you can tweak this code, to fit your own needs.<\/p>\n<p>I hope this helps to anyone who might be interested.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.7,
        "Solution_reading_time":104.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":686.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":79.1199802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are defining in Databricks a PythonScriptStep(). When using PythonScriptStep() within our pipeline script we can't find the scoring.py file.<\/p>\n<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>We getting the following error message:<\/p>\n<pre><code>Step [Scoring_Step]: script not found at: \/databricks\/driver\/scoring.py. Make sure to specify an appropriate source_directory on the Step or default_source_directory on the Pipeline.\n<\/code><\/pre>\n<p>For some reason Databricks is searching for the file in '\/databricks\/driver\/' instead of the folder we entered.<\/p>\n<p>There is also the way to use DatabricksStep() instead of PythonScriptStep(), but because of specific reasons we need to use the PythonSriptStep() class.<\/p>\n<p>Could anybody help us with this specific problem?<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Challenge_closed_time":1656324774632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655997421433,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while defining a PythonScriptStep() in Databricks. They are unable to find the scoring.py file and are receiving an error message indicating that Databricks is searching for the file in '\/databricks\/driver\/' instead of the specified folder. The user needs to use PythonScriptStep() instead of DatabricksStep() due to specific reasons. They are seeking assistance to resolve this issue.",
        "Challenge_last_edit_time":1656039942703,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72732616",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":16.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":90.9314441667,
        "Challenge_title":"Can't find scoring.py when using PythonScriptStep() in Databricks",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":123,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544598969960,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":37.0,
        "Poster_view_count":21.0,
        "Solution_body":"<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>Change the above code block with below code block. It will resolve the error<\/p>\n<pre><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>Reference link for the <a href=\"https:\/\/scoring_step%20=%20PythonScriptStep(%20%20%20%20%20name=%22Scoring_Step%22,%20%20%20%20%20source_directory=os.getenv(%22DATABRICKS_NOTEBOOK_PATH%22,%20%22\/Users\/USER_NAME\/source_directory%22),%20%20%20%20%20script_name=%22.\/scoring.py%22,%20%20%20%20%20arguments=%5B%22--input_dataset%22,%20ds_consumption%5D,%20%20%20%20%20compute_target=pipeline_cluster,%20%20%20%20%20runconfig=pipeline_run_config,%20%20%20%20%20allow_reuse=False)\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":32.4,
        "Solution_reading_time":19.66,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.3011111111,
        "Challenge_answer_count":0,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nmlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"URI_HERE\")\r\nt = Trainer(logger=mlflow_logger)\r\nt.logger.experiment_id\r\n```\r\nthrows a `JSONDecodeError` exception.\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 120, in experiment_id\r\n    _ = self.experiment\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment\r\n    return fn(self)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 98, in experiment\r\n    expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 154, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/utils\/rest_utils.py\", line 145, in call_endpoint\r\n    js_dict = json.loads(response.text)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\nEnvironment details\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - PyTorch Lightning Version: 0.9.0rc12\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.7\r\n - CUDA\/cuDNN version: Not relevant\r\n - GPU models and configuration: Not relevant\r\n - Any other relevant information: Not relevant\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1597848054000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1597814570000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while deploying the MLflow UI, specifically an AttributeError related to a missing attribute called 'mlModelFilterPattern'. The user needs to review the configuration parameter being used.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3046",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.5,
        "Challenge_reading_time":47.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":9.3011111111,
        "Challenge_title":"MLFlowLogger throws a JSONDecodeError",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":299,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Hi, thanks for submitting the bug. I don't know what's going on here. I cannot reproduce with your instructions. \r\n\r\nI'm running your sample code \r\n\r\n```python \r\n    mlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"http:\/\/127.0.0.1:5000\")\r\n    trainer = Trainer(logger=mlflow_logger)\r\n    trainer.logger.experiment_id\r\n```\r\nand the tracking uri I got from running \r\n```bash\r\nmlflow ui\r\n```\r\nThe experiment shows up in the UI and I get no errors. I verified this with the latest version of PL and mlflow, python 3.7.\r\n\r\nIs there any other information you can provide on the issue? Thanks for the quick response, @awaelchli! \r\n\r\nI did nothing different this morning and I am able to log metrics\/parameters to mlflow. I will close this now and in case I encounter this again, I will reopen this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.0,
        "Solution_reading_time":10.52,
        "Solution_score_count":null,
        "Solution_sentence_count":13.0,
        "Solution_word_count":124.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":109.7675,
        "Challenge_answer_count":0,
        "Challenge_body":"It seems they both assume that the current working dir is where they can find the `.git` and `.dvc` dirs.\r\nWe should correctly detect those paths, as it affects all our logic to e.g. automatically dvc init on behalf of the user.\r\n\r\nRelevant resources:\r\n1. https:\/\/stackoverflow.com\/a\/957978\r\n2. https:\/\/dvc.org\/doc\/command-reference\/root",
        "Challenge_closed_time":1630227884000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629832721000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the DVC add prompt is always displayed, even when there is no selection to make since the list of files is empty. The prompt should only be displayed if there is something to add.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/92",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.02,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":109.7675,
        "Challenge_title":"DVC and Git services don't correctly detect the repo root directory",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.9563108333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>\u63b2\u984c\u306e\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u73fe\u5728Machine Learning\u3092\u4f7f\u7528\u3057\u3066\u6a5f\u68b0\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002  <br \/>\n\u305d\u3053\u3067\u8cea\u554f\u306b\u306a\u308b\u306e\u3067\u3059\u304c\u3001\u30c7\u30b6\u30a4\u30ca\u30fc\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u5b66\u7fd2\u7d50\u679c\u3092CSV\u3067\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001  <br \/>\nExport Data\u30e2\u30c7\u30eb\u3067CSV\u5f62\u5f0f\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u3066\u3082CSV\u3067\u306f\u306a\u3044\u5f62\u5f0f\u3067\u5171\u6709\u305b\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u304c\u3001\u539f\u56e0\u304c\u308f\u304b\u3089\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002  <br \/>\n\u3054\u6559\u793a\u306e\u307b\u3069\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002<\/p>",
        "Challenge_closed_time":1631268910236,
        "Challenge_comment_count":1,
        "Challenge_created_time":1631251067517,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is currently using Machine Learning and attempting to export the learning results in CSV format using the designer function. However, even though the Export Data model is set to CSV format, the shared file is not in CSV format and the user is unsure of the cause of the issue. The user is seeking guidance on how to resolve this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/546760\/machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.2,
        "Challenge_reading_time":3.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":4.9563108333,
        "Challenge_title":"Machine Learning\u306b\u3064\u3044\u3066\u306e\u8cea\u554f",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":10,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8f940edc-4c98-48e4-8a54-287e99830334\">@\u6817\u7530\u771f\u5b5d  <\/a> Are you referring to the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/export-data\">export data module<\/a> of the designer from ml.azure.com?    <br \/>\nI think I understand the issue, Are you seeing that the .csv format of file is not listed on the blob storage?    <\/p>\n<p>Since the input is a dataframe directory to export module the output format selected should still be the format you selected, in this case CSV. The file name extension only might be missing. You can still open the csv file in excel and it will recognize the delimiters and headers so you can convert it into excel files.     <\/p>\n<p>You can also avoid this by providing the .csv extension in the path itself in export settings and file will be exported as a csv file directly.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/131128-image.png?platform=QnA\" alt=\"131128-image.png\" \/>    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":12.7,
        "Solution_reading_time":26.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":226.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":10.6524075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>While distributed dask can be setup manually on AML compute, the process requires lot of configs to be maintained. Is there any native support.<\/p>",
        "Challenge_closed_time":1666266282800,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666227934133,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in setting up distributed dask on Azure Machine Learning compute due to the need for manual configuration. They are seeking information on whether there is native support available for this process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1055350\/ray-dask-native-support-be-added-to-azure-machine",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":2.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":10.6524075,
        "Challenge_title":"ray+dask native support  be added to Azure Machine Learning",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=dfa9d536-725c-462d-87c8-47fbafb1a2bc\">@D-0887  <\/a> Thanks for the question. you can do is to setup the compute cluster &amp; compute instance in the same vnet and pip install ray-on-aml. This allows both interactive and job use of Ray and Dask right within Azure ML.    <\/p>\n<p>Here is the document Library to turn Azure ML Compute into Ray and Dask cluster.    <br \/>\n<a href=\"https:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784\">https:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784<\/a><\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":9.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":61.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1534481301200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Virginia, US",
        "Answerer_reputation_count":7114.0,
        "Answerer_view_count":1111.0,
        "Challenge_adjusted_solved_time":0.1925833333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a bunch of json files that look like this<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n<\/code><\/pre>\n<p>Which I can read in with<\/p>\n<pre><code>f = open(file_name)\ndata = []\nfor line in f:\n   data.append(json.dumps(line))\n<\/code><\/pre>\n<p>But I have another file with output like this<\/p>\n<pre><code>{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n<\/code><\/pre>\n<p>I.e. the json is formatted over several lines, so I can't simply read the json in line for line. Is there an easy way to parse this? Or do I have to write something that stitches together each json object line by line and the does json.loads?<\/p>",
        "Challenge_closed_time":1635633656723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635629350997,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in reading a file with multiple JSON objects spread out over multiple lines. While the user can read in the JSON files with a simple code, the JSON output in another file is formatted over several lines, making it difficult to parse. The user is seeking an easy way to parse the JSON or a solution to stitch together each JSON object line by line and then do json.loads.",
        "Challenge_last_edit_time":1635655049276,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69782294",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.6,
        "Challenge_reading_time":17.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.196035,
        "Challenge_title":"AWS Sagemaker output how to read file with multiple json objects spread out over multiple lines",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>Hmm,  as far as I know there's unfortunately no way to load a <a href=\"https:\/\/jsonlines.org\/\" rel=\"nofollow noreferrer\">JSONL<\/a> format data using <code>json.loads<\/code>. One option though, is to come up with a helper function that can convert it to a valid JSON string, as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\n\nstring = &quot;&quot;&quot;\n{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n    # replace the first occurrence of '{'\n    s = s.replace('{', '[{', 1)\n\n    # replace the last occurrence of '}\n    s = s.rsplit('}', 1)[0] + '}]'\n\n    # now go in and replace all occurrences of '}' immediately followed\n    # by newline with a '},'\n    s = s.replace('}\\n', '},\\n')\n\n    return s\n\n\nprint(json.loads(json_lines_to_json(string)))\n<\/code><\/pre>\n<p>Prints:<\/p>\n<pre><code>[{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}]\n<\/code><\/pre>\n<p><strong>Note:<\/strong> your first example actually doesn't seem like valid JSON (or at least JSON lines from my understanding). In particular, this part appears to be invalid due to a trailing comma after the last array element:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...}\n<\/code><\/pre>\n<p>To ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...},\n<\/code><\/pre>\n<hr \/>\n<p>There also appears to be a <a href=\"https:\/\/stackoverflow.com\/questions\/50475635\/loading-jsonl-file-as-json-objects\/50475669\">similar question<\/a> where they suggest splitting on newlines and calling <code>json.loads<\/code> on each line; actually it should be (slightly) less performant to call <code>json.loads<\/code> multiple times on each object, rather than once on the list, as I show below.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from timeit import timeit\nimport json\n\n\nstring = &quot;&quot;&quot;\\\n{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], &quot;word&quot;: &quot;blah blah blah&quot;}\\\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n\n    # Strip newlines from end, then replace all occurrences of '}' followed\n    # by a newline, by a '},' followed by a newline.\n    s = s.rstrip('\\n').replace('}\\n', '},\\n')\n\n    # return string value wrapped in brackets (list)\n    return f'[{s}]'\n\n\nn = 10_000\n\nprint('string replace:        ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals()))\nprint('json.loads each line:  ', timeit(r'[json.loads(line) for line in string.split(&quot;\\n&quot;)]', number=n, globals=globals()))\n<\/code><\/pre>\n<p>Result:<\/p>\n<pre><code>string replace:         0.07599360000000001\njson.loads each line:   0.1078384\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1635655742576,
        "Solution_link_count":2.0,
        "Solution_readability":9.2,
        "Solution_reading_time":46.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":352.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":31.5388561111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code class=\"lang-auto\">for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> shows logs only for the first file in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code class=\"lang-auto\">wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code class=\"lang-auto\">def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Challenge_closed_time":1650552651672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650439111790,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while fine-tuning multiple models using a for loop with Huggingface Trainer and Wandb. Although the code is training and saving models for all files in the data directory, Wandb is only showing logs for the first file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-for-huggingface-trainer-saves-only-first-model\/2270",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":17.1,
        "Challenge_reading_time":17.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":31.5388561111,
        "Challenge_title":"Wandb for Huggingface Trainer saves only first model",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":207.0,
        "Challenge_word_count":100,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code class=\"lang-auto\">\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" class=\"inline-onebox\">Launch Experiments with wandb.init - Documentation<\/a><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.4,
        "Solution_reading_time":7.73,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":40.4833333333,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nGetting errors with the new Sagemaker Async Operators that I don't get with the traditional ones. I'm using a personal Access Key, Secret, and Session Token as I did with the non async operators for auth.\r\n\r\n```\r\nbotocore.exceptions.ClientError: An error occurred (UnrecognizedClientException) when calling the DescribeTrainingJob operation: The security token included in the request is invalid.\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nUse the SageMaker async operators with user Access Key, Secret, and Session Token\r\n\r\n**Expected behavior**\r\nExpect it to not have auth\/token errors.\r\n\r\n\r\n**Additional context**\r\nWhen I switch back to the traditional operators in the same dag with the same auth creds it works fine.\r\n\r\n\r\n@kentdanas also had similar issues and her auth was setup a little different.",
        "Challenge_closed_time":1666859588000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1666713848000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing errors while trying to use Sagemaker Debugger with HPO and is getting a \"FileNotFoundError\" related to the debughookconfig.json file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/725",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.75,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":40.4833333333,
        "Challenge_title":"Token error with Sagemaker Async Operators",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":127,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The issue is with the session token is not considered while the secrete and access key is given in the connection proper field, not in the extra config",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":1.82,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1365101584443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Munich, Germany",
        "Answerer_reputation_count":7203.0,
        "Answerer_view_count":445.0,
        "Challenge_adjusted_solved_time":0.5689572222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created a stepfunction, the definition for this statemachine below (<code>step-function.json<\/code>) is used in terraform (using the syntax in this page:<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html<\/a>)<\/p>\n<p>The first time if I execute this statemachine, it will create a SageMaker batch transform job named <code>example-jobname<\/code>, but I need to exeucute this statemachine everyday, then it will give me error <code>&quot;error&quot;: &quot;SageMaker.ResourceInUseException&quot;, &quot;cause&quot;: &quot;Job name must be unique within an AWS account and region, and a job with this name already exists <\/code>.<\/p>\n<p>The cause is because the job name is hard-coded as <code>example-jobname<\/code> so if the state machine gets executed after the first time, since the job name needs to be unique, the task will fail, just wondering how I can add a string (something like ExecutionId at the end of the job name). Here's what I have tried:<\/p>\n<ol>\n<li><p>I added <code>&quot;executionId.$&quot;: &quot;States.Format('somestring {}', $$.Execution.Id)&quot;<\/code> in the <code>Parameters<\/code> section in the json file, but when I execute the task I got error <code> &quot;error&quot;: &quot;States.Runtime&quot;, &quot;cause&quot;: &quot;An error occurred while executing the state 'SageMaker CreateTransformJob' (entered at the event id #2). The Parameters '{\\&quot;BatchStrategy\\&quot;:\\&quot;SingleRecord\\&quot;,..............\\&quot;executionId\\&quot;:\\&quot;somestring arn:aws:states:us-east-1:xxxxx:execution:xxxxx-state-machine:xxxxxxxx72950\\&quot;}' could not be used to start the Task: [The field \\&quot;executionId\\&quot; is not supported by Step Functions]&quot;}<\/code><\/p>\n<\/li>\n<li><p>I modified the jobname in the json file to  <code>&quot;TransformJobName&quot;: &quot;example-jobname-States.Format('somestring {}', $$.Execution.Id)&quot;,<\/code>, when I execute the statemachine, it gave me error: <code>&quot;error&quot;: &quot;SageMaker.AmazonSageMakerException&quot;, &quot;cause&quot;: &quot;2 validation errors detected: Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}; Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must have length less than or equal to 63<\/code><\/p>\n<\/li>\n<\/ol>\n<p>I really run out of ideas, can someone help please? Many thanks.<\/p>",
        "Challenge_closed_time":1610637215396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610635167150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with creating a unique SageMaker batch transform job name when executing a step function daily. The job name is hard-coded as \"example-jobname\" and causes an error when executed after the first time. The user has tried adding a string to the job name using the execution ID, but it resulted in an error. Modifying the job name in the JSON file also resulted in an error due to a regular expression pattern constraint. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1611071837132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65721061",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":16.2,
        "Challenge_reading_time":36.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.5689572222,
        "Challenge_title":"How to parse stepfunction executionId to SageMaker batch transform job name?",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1209.0,
        "Challenge_word_count":288,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>So as per the <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-train-model.html#sample-train-model-code-examples\" rel=\"nofollow noreferrer\">documentation<\/a>, we should be passing the parameters in the following format<\/p>\n<pre><code>        &quot;Parameters&quot;: {\n            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  \n            ....\n        },\n<\/code><\/pre>\n<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:<\/p>\n<p>either<\/p>\n<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;\n<\/code><\/pre>\n<p>full State machine definition:<\/p>\n<pre><code>    {\n        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,\n        &quot;StartAt&quot;: &quot;Generate Random String&quot;,\n        &quot;States&quot;: {\n            &quot;Generate Random String&quot;: {\n                &quot;Type&quot;: &quot;Task&quot;,\n                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,\n                &quot;ResultPath&quot;: &quot;$.executionid&quot;,\n                &quot;Parameters&quot;: {\n                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;\n                },\n                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;\n            },\n        &quot;SageMaker CreateTransformJob&quot;: {\n            &quot;Type&quot;: &quot;Task&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,\n            &quot;Parameters&quot;: {\n            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n            &quot;DataProcessing&quot;: {\n                &quot;InputFilter&quot;: &quot;$&quot;,\n                &quot;JoinSource&quot;: &quot;Input&quot;,\n                &quot;OutputFilter&quot;: &quot;xxx&quot;\n            },\n            &quot;Environment&quot;: {\n                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;\n            },\n            &quot;MaxConcurrentTransforms&quot;: 100,\n            &quot;MaxPayloadInMB&quot;: 1,\n            &quot;ModelName&quot;: &quot;${model_name}&quot;,\n            &quot;TransformInput&quot;: {\n                &quot;DataSource&quot;: {\n                    &quot;S3DataSource&quot;: {\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;\n                    }\n                },\n                &quot;ContentType&quot;: &quot;application\/jsonlines&quot;,\n                &quot;CompressionType&quot;: &quot;Gzip&quot;,\n                &quot;SplitType&quot;: &quot;Line&quot;\n            },\n            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,\n            &quot;TransformOutput&quot;: {\n                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,\n                &quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n                &quot;AssembleWith&quot;: &quot;Line&quot;\n            },    \n            &quot;TransformResources&quot;: {\n                &quot;InstanceType&quot;: &quot;xxx&quot;,\n                &quot;InstanceCount&quot;: 1\n            }\n        },\n            &quot;End&quot;: true\n        }\n        }\n    }\n<\/code><\/pre>\n<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:<\/p>\n<pre><code> def lambda_handler(event, context):\n    return(event.get('executionId').split(':')[-1])\n<\/code><\/pre>\n<p>Or if you dont wanna pass the execution id , it can simply return the random string like<\/p>\n<pre><code> import string\n def lambda_handler(event, context):\n    return(string.ascii_uppercase + string.digits)\n<\/code><\/pre>\n<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.<\/p>",
        "Solution_comment_count":23.0,
        "Solution_last_edit_time":1610641519768,
        "Solution_link_count":1.0,
        "Solution_readability":24.6,
        "Solution_reading_time":43.95,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":220.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":36.5267972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have lots of run files created by running PyTorch estimator\/ ScriptRunStep experiments that are saved in azureml blob storage container. Previously, I'd been viewing these runs in the Experiments tab of the ml.azure.com portal and associating tags to these runs to categorise and load the desired models.<\/p>\n<p>However, a coworker recently deleted my workspace. I created a new one which is connected to the previously-existing blob container, the run files therefore still exist and can be accessed on this new workspace, but they no longer show up in the Experiment viewer on ml.azure.com. Neither can I see the tags I'd associated to the runs.<\/p>\n<p><strong>Is there any way to load these old run files into the Experiment viewer or is it only possible to view runs created inside the current workspace?<\/strong><\/p>\n<p>Sample scriptrunconfig code:<\/p>\n<pre><code>data_ref = DataReference(datastore=ds,\n                         data_reference_name=&quot;&lt;name&gt;&quot;,        \n                         path_on_datastore = &quot;&lt;path&gt;&quot;)\nargs = ['--data_dir',   str(data_ref),     \n        '--num_epochs', 30,     \n        '--lr',         0.01,          \n        '--classifier', 'int_ext' ]  \n\nsrc = ScriptRunConfig(source_directory='.',                       \n                      arguments=args,                      \n                      compute_target = compute_target,                       \n                      environment = env,                       \n                      script='train.py') \nsrc.run_config.data_references = {data_ref.data_reference_name: \n                                  data_ref.to_config()} \n<\/code><\/pre>",
        "Challenge_closed_time":1616813814780,
        "Challenge_comment_count":7,
        "Challenge_created_time":1616682318310,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has run PyTorch estimator\/ScriptRunStep experiments and saved the run files in an AzureML blob storage container. However, after a coworker deleted their workspace, they created a new one that is connected to the same blob container, but the old run files no longer show up in the Experiment viewer on ml.azure.com. The user is seeking a way to load these old run files into the Experiment viewer or if it is only possible to view runs created inside the current workspace.",
        "Challenge_last_edit_time":1617188538763,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66801546",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":12.8,
        "Challenge_reading_time":17.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":36.5267972222,
        "Challenge_title":"Load Azure ML experiment run information from datastore",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":168,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606130833532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Sorry for your loss! First, I'd make absolutely sure that you can't recover the deleted workspace. Definitely worthwhile to open an priority support ticket with Azure.<\/p>\n<p>Another thing you might try is:<\/p>\n<ol>\n<li>create a new workspace (which will create a new storage account for you for the new workspace's logs)<\/li>\n<li>copy your old workspace's data into the new workspace's storage account.<\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":5.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1651331397896,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":43.4867702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to setup a shared cache in minikube in such a way that different services can use that cache to pull and update DVC models and data needed for training Machine Learning models. The structure of the project is to use 1 pod to periodically update the cache with new models and outputs. Then, multiple pods can read the cache to recreate the updated models and data. So I need to be able to update the local cache directory and pull from it using DVC commands, so that all the services have consistent view on the latest models and data created by a service.<\/p>\n<p>More specifically, I have a docker image called <code>inference-service<\/code> that should only <code>dvc pull<\/code> or some how use the info in the shared dvc cache to get the latest model and data locally in <code>models<\/code> and <code>data<\/code> folders (see dockerfile) in minikube. I have another image called <code>test-service<\/code> that\nruns the ML pipeline using <code>dvc repro<\/code> which creates the models and data that DVC needs (dvc.yaml) to track and store in the shared cache. So <code>test-service<\/code> should push created outputs from the ML pipeline into the shared cache so that <code>inference-service<\/code> can pull it and use it instead of running dvc repro by itself. <code>test-service<\/code> should only re-train and write the updated models and data into the shared cache while <code>inference-service<\/code> should only read and recreate the updated\/latest models and data from the shared cache.<\/p>\n<p><em><strong>Problem: the cache does get mounted on the minikube VM, but the inference service does not pull (using <code>dvc pull -f<\/code>) the data and models after the test service is done with <code>dvc repro<\/code> and results the following warnings and failures:<\/strong><\/em><\/p>\n<p><em>relevant kubernetes pod log of inference-service<\/em><\/p>\n<pre><code>WARNING: Output 'data\/processed\/train_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/train_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/validation_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/validation_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/processed\/test_preprocessed.pkl'(stage: 'preprocess') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit preprocess` to associate existing 'data\/processed\/test_preprocessed.pkl' with stage: 'preprocess'.\nWARNING: Output 'data\/interim\/train_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/train_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/validation_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/validation_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'data\/interim\/test_featurized.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nYou can also use `dvc commit featurize` to associate existing 'data\/interim\/test_featurized.pkl' with stage: 'featurize'.\nWARNING: Output 'models\/mlb.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/tfidf_vectorizer.pkl'(stage: 'featurize') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'models\/model.pkl'(stage: 'train') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: Output 'reports\/scores.json'(stage: 'evaluate') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\nWARNING: No file hash info found for '\/root\/models\/model.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/reports\/scores.json'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/train_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/validation_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/processed\/test_preprocessed.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/train_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/validation_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/data\/interim\/test_featurized.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/mlb.pkl'. It won't be created.\nWARNING: No file hash info found for '\/root\/models\/tfidf_vectorizer.pkl'. It won't be created.\n10 files failed\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/root\/models\/model.pkl\n\/root\/reports\/scores.json\n\/root\/data\/processed\/train_preprocessed.pkl\n\/root\/data\/processed\/validation_preprocessed.pkl\n\/root\/data\/processed\/test_preprocessed.pkl\n\/root\/data\/interim\/train_featurized.pkl\n\/root\/data\/interim\/validation_featurized.pkl\n\/root\/data\/interim\/test_featurized.pkl\n\/root\/models\/mlb.pkl\n\/root\/models\/tfidf_vectorizer.pkl\nIs your cache up to date?\n<\/code><\/pre>\n<p><em>relevant kubernetes pod log of test-service<\/em><\/p>\n<pre><code>Stage 'preprocess' is cached - skipping run, checking out outputs\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\nStage 'featurize' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'train' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nStage 'evaluate' is cached - skipping run, checking out outputs\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage.\n<\/code><\/pre>\n<p><strong>Project Tree<\/strong><\/p>\n<pre><code>\u251c\u2500 .dvc\n\u2502  \u251c\u2500 .gitignore\n\u2502  \u251c\u2500 config\n\u2502  \u2514\u2500 tmp\n\u251c\u2500 deployment\n\u2502  \u251c\u2500 docker-compose\n\u2502  \u2502  \u251c\u2500 docker-compose.yml\n\u2502  \u251c\u2500 minikube-dep\n\u2502  \u2502  \u251c\u2500 inference-test-services_dep.yaml\n\u2502  \u251c\u2500 startup_minikube_with_mount.sh.sh\n\u251c\u2500 Dockerfile # for inference service\n\u251c\u2500 dvc-cache # services should push and pull from this cache folder and see this as the DVC repo\n\u251c- dvc.yaml\n\u251c- params.yaml\n\u251c\u2500 src\n\u2502  \u251c\u2500 build_features.py\n|  \u251c\u2500 preprocess_data.py\n|  \u251c\u2500 serve_model.py\n|  \u251c\u2500 startup.sh  \n|  \u251c\u2500 requirements.txt\n\u251c\u2500 test_dep\n\u2502  \u251c\u2500 .dvc # same as .dvc in the root folder\n|  |  \u251c\u2500...\n\u2502  \u251c\u2500 Dockerfile # for test service\n\u2502  \u251c\u2500 dvc.yaml\n|  \u251c\u2500 params.yaml\n\u2502  \u2514\u2500 src\n\u2502     \u251c\u2500 build_features.py # same as root src folder\n|     \u251c\u2500 preprocess_data.py # same as root src folder\n|     \u251c\u2500 serve_model.py # same as root src folder\n|     \u251c\u2500 startup_test.sh  \n|     \u251c\u2500 requirements.txt  # same as root src folder\n<\/code><\/pre>\n<p><strong>dvc.yaml<\/strong><\/p>\n<pre><code>stages:\n  preprocess:\n    cmd: python ${preprocess.script}\n    params:\n      - preprocess\n    deps:\n      - ${preprocess.script}\n      - ${preprocess.input_train}\n      - ${preprocess.input_val}\n      - ${preprocess.input_test}\n    outs:\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n  featurize:\n    cmd: python ${featurize.script}\n    params:\n      - preprocess\n      - featurize\n    deps:\n      - ${featurize.script}\n      - ${preprocess.output_train}\n      - ${preprocess.output_val}\n      - ${preprocess.output_test}\n    outs:\n      - ${featurize.output_train}\n      - ${featurize.output_val}\n      - ${featurize.output_test}\n      - ${featurize.mlb_out}\n      - ${featurize.tfidf_vectorizer_out}\n  train:\n    cmd: python ${train.script}\n    params:\n      - featurize\n      - train\n    deps:\n      - ${train.script}\n      - ${featurize.output_train}\n    outs:\n      - ${train.model_out}\n  evaluate:\n    cmd: python ${evaluate.script}\n    params:\n      - featurize\n      - train\n      - evaluate\n    deps:\n      - ${evaluate.script}\n      - ${train.model_out}\n      - ${featurize.output_val}\n    metrics:\n      - ${evaluate.scores_path}\n<\/code><\/pre>\n<p><strong>params.yaml<\/strong><\/p>\n<pre><code>preprocess:\n  script: src\/preprocess\/preprocess_data.py\n  input_train: data\/raw\/train.tsv\n  input_val: data\/raw\/validation.tsv\n  input_test: data\/raw\/test.tsv\n  output_train: data\/processed\/train_preprocessed.pkl\n  output_val: data\/processed\/validation_preprocessed.pkl\n  output_test: data\/processed\/test_preprocessed.pkl\n\nfeaturize:\n  script: src\/features\/build_features.py\n  output_train: data\/interim\/train_featurized.pkl\n  output_val: data\/interim\/validation_featurized.pkl\n  output_test: data\/interim\/test_featurized.pkl\n  mlb_out: models\/mlb.pkl\n  tfidf_vectorizer_out: models\/tfidf_vectorizer.pkl\n\ntrain:\n  script: src\/models\/train_model.py\n  model_out: models\/model.pkl\n\nevaluate:\n  script: src\/models\/evaluate_model.py\n  scores_path: reports\/scores.json\n  roc_json: reports\/roc_plot.json\n  prc_json: reports\/prc_plot.json\n<\/code><\/pre>",
        "Challenge_closed_time":1654875420463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654718868090,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user needs to set up a shared cache in minikube for different services to use for pulling and updating DVC models and data for machine learning models. The cache is updated periodically by one pod, and multiple pods can read from it. However, the inference service is not pulling the data and models from the cache after the test service is done with dvc repro, resulting in warnings and failures. The user has provided relevant logs and project tree information.",
        "Challenge_last_edit_time":1655157056467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72551630",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":119.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":135,
        "Challenge_solved_time":43.4867702778,
        "Challenge_title":"How to setup a DVC shared cache without git repository between different services in minikube?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":1051,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562403039336,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":298.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>After running <code>dvc repro<\/code> in <code>test-service<\/code>, a new <code>dvc.lock<\/code> will be created, containing the file hashes relative to your pipeline (i.e. the hash for <code>models\/model.pkl<\/code> etc).<\/p>\n<p>If you're running a shared cache, <code>inference-service<\/code> should have access to the updated <code>dvc.lock<\/code>. If that is present, it will be sufficient to run <code>dvc checkout<\/code> to populate the workspace with the files corresponding to the hashes in the shared cache.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":6.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":67.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1424548126510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":665.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":53.1446780556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>To create my Glue scripts, I use development endpoints with Sagemaker notebooks that run the Pyspark (Sparkmagic) kernel.\nThe latest version of Glue (version 1.0) supports Spark 2.4. However, my Sagemaker notebook uses Spark version 2.2.1. \nThe function I want to test only exists as of Spark 2.3. \nIs there a way to solve this mismatch between the dev endpoint and the Glue job? Can I somehow set the Spark version of the notebook?<br>\nI couldn't find anything in the documentation.<\/p>",
        "Challenge_closed_time":1566968757932,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566812956087,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in setting the Spark version for Sagemaker on Glue Dev Endpoint. The latest version of Glue supports Spark 2.4, but the user's Sagemaker notebook uses Spark version 2.2.1, and the function they want to test only exists as of Spark 2.3. The user is seeking a solution to this mismatch between the dev endpoint and the Glue job and is unable to find any relevant information in the documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57655516",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":6.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":43.2782902778,
        "Challenge_title":"Set Spark version for Sagemaker on Glue Dev Endpoint",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":642.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505381376316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":68.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>When you create a SageMaker notebook for the Glue dev endpoint, it launches a SageMaker notebook instance with a specific lifecycle configuration. This LC provides the configurations to create a connection between the SageMaker notebook and the development endpoint. Upon running cells from the PySpark kernel, the code is sent to the Livy server running in the development endpoint via REST APIs. <\/p>\n\n<p>Thus, the PySpark version that you see and on which the SageMaker notebook runs depends on the development endpoint and is not configurable from the SageMaker point of view.<\/p>\n\n<p>Since Glue is a managed service, root access is restricted for the development endpoint. Thus, you cannot update the spark version to a more later version. The feature of using Spark version 2.4 has been newly introduced in Glue and it seems that it has not yet been released for dev endpoint.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1567004276928,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":10.93,
        "Solution_score_count":5.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":144.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1746036111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi guys, I'm new to Azure ML. Following the URL below, I tried to run my python script on local machine. By local, I meant exactly Windows on my local physical machine in my house.  But it seems python script 'transform_titanic.py' was executed on Azure.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#local-compute-target\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#local-compute-target<\/a>    <\/p>\n<p>I executed the script below on my local computer, and expected it runs 'transform_titanic.py' on my local computer.    <\/p>\n<pre><code>   from azureml.core import Environment, Experiment, ScriptRunConfig, Workspace  \n   from dotenv import load_dotenv  \n     \n   load_dotenv()  \n     \n   ws = Workspace(  \n       os.environ['SUBSCRIPTION_ID']  \n       os.environ['RESOURCE_GROUP']  \n       os.environ['WORKSPACE_NAME']  \n   )  \n     \n   exp = Experiment(workspace=ws, name='experiment')  \n     \n   env = Environment('user-managed-env')  \n   env.python.user_managed_dependencies = True  \n     \n   script_run_config = ScriptRunConfig(  \n       source_directory='src\/transform',  \n       script='transform_titanic.py',  \n       arguments=['--input_dataset_name1', 'titanic'],  \n   )  \n     \n   script_run_config.run_config.target = 'local'  \n   script_run_config.run_config.environment = env  \n     \n   run = exp.submit(config=script_run_config)  \n   print(run.get_portal_url())  \n   run.wait_for_completion()  \n<\/code><\/pre>",
        "Challenge_closed_time":1600495830720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600495202147,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is new to Azure ML and tried to run a Python script on their local Windows machine using the \"local\" compute target. However, the script was executed on Azure instead of their local machine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/99901\/what-does-local-mean-in-compute-target",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":19.0,
        "Challenge_reading_time":18.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.1746036111,
        "Challenge_title":"What does \"local\" mean in compute target?",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Sorry, I found it was run on my local computer. Some artifact created in the script was in C:\\Users{username}\\AppData\\Local\\Temp\\azureml_runs\\local_experiment_XXXXXXXXXX  <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.3,
        "Solution_reading_time":2.32,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":102.2437183333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there any plan? Any date we can expect?<\/p>",
        "Challenge_closed_time":1662345271896,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661977194510,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about whether Azure supports distributed GPU and is seeking information on any plans or expected dates for its implementation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/989398\/is-azure-supporting-distributed-gpu",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":2.6,
        "Challenge_reading_time":1.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":102.2437183333,
        "Challenge_title":"Is Azure supporting distributed GPU?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":13,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=7f2ff54e-2fc4-4d74-b946-fc6ec46d4863\">@nam  <\/a>     <\/p>\n<p>I hope yo are doing well. We have multiple options for Distributed GPU for Azure Machine Learnig for SDK v1 as below -     <br \/>\n<strong>Message Passing Interface (MPI)<\/strong>    <br \/>\nHorovod    <br \/>\nDeepSpeed    <br \/>\nEnvironment variables from Open MPI    <br \/>\n<strong>PyTorch<\/strong>    <br \/>\nProcess group initialization    <br \/>\nLaunch options    <br \/>\nDistributedDataParallel (per-process-launch)    <br \/>\nUsing torch.distributed.launch (per-node-launch)    <br \/>\nPyTorch Lightning    <br \/>\nHugging Face Transformers    <br \/>\n<strong>TensorFlow<\/strong>    <br \/>\nEnvironment variables for TensorFlow (TF_CONFIG)    <br \/>\n<strong>Accelerate GPU training with InfiniBand<\/strong>    <\/p>\n<p>For V2 there should be big change. Please feel free to let us know any problems. Thanks.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":11.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1102777778,
        "Challenge_answer_count":1,
        "Challenge_body":"SageMaker Processing can launch [multi-instance jobs][1]. What is the underlying [cluster manager][2]? Yarn? Mesos? Something custom?\n\n\n  [1]: https:\/\/aws.amazon.com\/blogs\/machine-learning\/running-on-demand-serverless-apache-spark-data-processing-jobs-using-amazon-sagemaker-managed-spark-containers-and-the-amazon-sagemaker-sdk\/\n  [2]: https:\/\/spark.apache.org\/docs\/latest\/cluster-overview.html",
        "Challenge_closed_time":1602771143000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602770746000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the cluster manager used in SageMaker Spark Processing for launching multi-instance jobs and wants to know if it is Yarn, Mesos, or a custom solution.",
        "Challenge_last_edit_time":1668615616912,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUShPm0t4vR4S8XBKMiAcA6g\/what-is-the-cluster-manager-in-sagemaker-spark-processing",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":27.3,
        "Challenge_reading_time":6.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1102777778,
        "Challenge_title":"What is the cluster manager in SageMaker Spark Processing?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":188.0,
        "Challenge_word_count":28,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The Spark container uses YARN - for ref the bootstrap script on github: https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/src\/smspark\/bootstrapper.py and the [Dockerfile](https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/spark\/processing\/2.4\/py3\/docker\/Dockerfile.cpu) with hadoop-yarn dependencies",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1609869333654,
        "Solution_link_count":2.0,
        "Solution_readability":25.8,
        "Solution_reading_time":4.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":23.0099397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running local unsupervised learning (predominantly clustering) on a large, single node with GPU.<\/p>\n<p>Does SageMaker support <strong>distributed unsupervised learning<\/strong> using <strong>clustering<\/strong>?<\/p>\n<p>If yes, please provide the relevant example (preferably non-TensorFlow).<\/p>",
        "Challenge_closed_time":1663485400223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663402564440,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if SageMaker supports distributed unsupervised learning using clustering and is seeking an example, preferably non-TensorFlow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73753271",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":23.0099397222,
        "Challenge_title":"Distributed Unsupervised Learning in SageMaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":14.0,
        "Challenge_word_count":36,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>SageMaker Training allow you to bring your own training scripts, and supports various forms of distributed training, like data\/model parallel, and frameworks like PyTorch DDP, Horovod, DeepSpeed, etc.\nAdditionally, if you want to bring your data, but not code, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algorithms-unsupervised.html\" rel=\"nofollow noreferrer\">SageMaker training offers various unsupervised built-in algorithms<\/a>, some of which are parallelizable.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.2,
        "Solution_reading_time":6.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1384795490587,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1012.0,
        "Answerer_view_count":102.0,
        "Challenge_adjusted_solved_time":2.9863186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Having some difficulty executing the following code in AWS SageMaker. It is supposed to just list all of the tables in DynamoDB.<\/p>\n\n<pre><code>import boto3\nresource = boto3.resource('dynamodb', region_name='xxxx')\nresponse  = resource.tables.all()\nfor r in response:\n    print(r.name)\n<\/code><\/pre>\n\n<p>If the SageMaker notebook kernel is set to \"conda_python3\" the code executes fine and the tables are listed out in the notebook as expected (this happens pretty much instantly).<\/p>\n\n<p>However, if I set the kernel to \"Sparkmagic (PySpark)\" the same code infinitely runs and doesn't output the table list at all.<\/p>\n\n<p>Does anyone know why this would happen for the PySpark kernel but not for the conda3 kernel? Ideally I need to run this code as part of a bigger script that relies on PySpark, so would like to get it working with PySpark.<\/p>",
        "Challenge_closed_time":1574430450807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574419108777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in executing a code in AWS SageMaker notebook that is supposed to list all the tables in DynamoDB using boto3 and PySpark. The code executes fine when the kernel is set to \"conda_python3\" but infinitely runs and doesn't output the table list when the kernel is set to \"Sparkmagic (PySpark)\". The user is seeking help to understand why this is happening and how to get it working with PySpark.",
        "Challenge_last_edit_time":1574419700060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58992447",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":11.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":3.1505638889,
        "Challenge_title":"AWS SageMaker notebook list tables using boto3 and PySpark",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":440.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384795490587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1012.0,
        "Poster_view_count":102.0,
        "Solution_body":"<p>Figured out what the issue was, you need to end an endpoint to tour VPC for DyanmoDB.<\/p>\n\n<p>To do this navigate to:<\/p>\n\n<ol>\n<li>AWS VPC<\/li>\n<li>Endpoints<\/li>\n<li>Create Endpoint<\/li>\n<li>Select the dynamodb service (will be type Gateway)<\/li>\n<li>Select the VPC your Notebook is using<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":3.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":3.4251711111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed a machine learning model using Azure ML's clustering. Few of the requests made from the cluster are triggering 404 HTTP error. I followed the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">document<\/a> to do modifications in my swagger.json file. Finally ended up with &quot;list index out of range&quot; error. It seems to be having issue with the global parameter but I am no sure about it. I am using the API from postman with some default headers like mentioned in the body below<\/p>\n<pre><code>{\n    &quot;Inputs&quot;: {\n         &quot;input_1&quot; : &quot;content&quot;\n         &quot;input_2: : &quot;content&quot;\n         ......\n    },\n    &quot;GlobalParameters&quot;: 0\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1651111084096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651094225790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a \"list index out of range\" error while modifying the swagger.json file as per the Azure ML documentation to resolve the 404 HTTP error triggered by some requests made from the cluster. The error seems to be related to the global parameter, but the user is unsure. The user is using the API from Postman with default headers.",
        "Challenge_last_edit_time":1651098753480,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72035391",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":10.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4.6828627778,
        "Challenge_title":"AzureML schema \"list index out of range\" error",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":78.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Change the &quot;GlobalParameter&quot; value to any floating number other than 1.0 or even you can remove it and execute. Sometimes, Global parameter will cause the issue. Check the below documentation.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.0,
        "Solution_reading_time":6.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1577919980176,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation_count":5588.0,
        "Answerer_view_count":398.0,
        "Challenge_adjusted_solved_time":171.7806,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed an ANN tool by using pycharm\/tensorflow on my own computer. I uploaded the h5 and json files to Amazon Sagemaker by creating a Notebook Instance. I was finally able to successfully create an endpoint and make it work. The following code in Notebook Instance -Jupyter works:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\nclient = boto3.client('runtime.sagemaker')\ndata = np.random.randn(1,6).tolist()\nendpoint_name = 'sagemaker-tensorflow-**********'\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>However, the problem occurs when I created a lambda function and call the endpoint from there. The input should be a row of 6 features -that is a 1-by-6 vector. I enter the following input into lambda {&quot;data&quot;:&quot;1,1,1,1,1,1&quot;} and it gives me the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 20, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n<\/code><\/pre>\n<p>I think the problem is that the input needs to be 1-by-6 instead of 6-by-1 and I don't know how to do that.<\/p>",
        "Challenge_closed_time":1599633575463,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599015165303,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has developed an ANN tool using PyCharm\/TensorFlow and uploaded the h5 and json files to Amazon Sagemaker by creating a Notebook Instance. The endpoint works fine when called from the Notebook Instance, but when the user created a Lambda function and called the endpoint from there, an error occurred. The user suspects that the input needs to be 1-by-6 instead of 6-by-1, but is unsure how to fix it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63698011",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":20.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":171.7806,
        "Challenge_title":"AWS Notebook Instance is working but Lambda is not accepting the input",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":187,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369539919747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":311.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>I assume the content type you specified is <code>text\/csv<\/code>, so try out:<\/p>\n<pre><code>{&quot;data&quot;: [&quot;1,1,1,1,1,1&quot;]}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.3,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":2025.5465147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Status:<\/p>\n<ul>\n<li>Custom container is built using the doc - <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/li>\n<li>predict.py is coded to accommodate the custom inference script and its working well<\/li>\n<li>Using the classsagemaker.model.Model() class to pass the trained model.tar.gz and custom container image inorder to deploy the model<\/li>\n<\/ul>\n<p>Challenge:<\/p>\n<ul>\n<li>In the same Model class there is a ENV  parameter through which we can apparently send the environment variables to the custom image<\/li>\n<li>Tried passing a python dict to this , but facing difficulty to read this json dict inide the predict.py script<\/li>\n<\/ul>\n<p>Somebody faced the same difficulty ?<\/p>",
        "Challenge_closed_time":1645489319000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1638197351547,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in passing a python dictionary as an environment variable to a custom inference container in Sagemaker. The predict.py script is unable to read the JSON dictionary passed through the ENV parameter of the sagemaker.model.Model() class. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70156631",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":15.2,
        "Challenge_reading_time":12.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2025.5465147222,
        "Challenge_title":"How to pass additional parameters (as a dict) to sagemeker custom inference container?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604146329127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can pass your environment dict in your Model as:<\/p>\n<pre><code>Model(\n.\n.\nenv= {&quot;my_env&quot;: &quot;my_env_value&quot;}\n.\n.\n)\n<\/code><\/pre>\n<p>SageMaker will pass the enviroments dict to your container and you can access it in your predict.py script for example with:<\/p>\n<pre><code>my_env = os.environ.get('my_env',&quot;env key not set in Model&quot;)\nprint(my_env)\n<\/code><\/pre>\n<p>If your env dict was passed to your Model containing they <code>my_env<\/code> then you will receive the output : <code>my_env_value<\/code>. Else, then you will receive <code>env key not set in Model<\/code><\/p>\n<p>I work for AWS and my opinions are my own.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":8.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":85.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1478616262200,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kolkata, West Bengal, India",
        "Answerer_reputation_count":375.0,
        "Answerer_view_count":69.0,
        "Challenge_adjusted_solved_time":43.9919516666,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to train a PyTorch FLAIR model in AWS Sagemaker.\nWhile doing so getting the following error:<\/p>\n<pre><code>RuntimeError: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 11.17 GiB total capacity; 9.29 GiB already allocated; 7.31 MiB free; 10.80 GiB reserved in total by PyTorch)\n<\/code><\/pre>\n<p>For training I used sagemaker.pytorch.estimator.PyTorch class.<\/p>\n<p>I tried with different variants of instance types from ml.m5, g4dn to p3(even with a 96GB memory one).\nIn the ml.m5 getting the error with CPUmemoryIssue, in g4dn with GPUMemoryIssue and in the P3 getting GPUMemoryIssue mostly because Pytorch is using only one of the GPU of 12GB out of 8*12GB.<\/p>\n<p>Not getting anywhere to complete this training, even in local tried with a CPU machine and got the following error:<\/p>\n<pre><code>RuntimeError: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 67108864 bytes. Buy new RAM!\n<\/code><\/pre>\n<p>The model training script:<\/p>\n<pre><code>    corpus = ClassificationCorpus(data_folder, test_file='..\/data\/exports\/val.csv', train_file='..\/data\/exports\/train.csv')\n                                          \n    print(&quot;finished loading corpus&quot;)\n\n    word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n\n    document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n\n    classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n\n    trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n\n    trainer.train('..\/model_files', max_epochs=12,learning_rate=0.0001, train_with_dev=False, embeddings_storage_mode=&quot;none&quot;)\n<\/code><\/pre>\n<p>P.S.: I was able to train the same architecture with a smaller dataset in my local GPU machine with a 4GB GTX 1650 DDR5 memory and it was really quick.<\/p>",
        "Challenge_closed_time":1597771718656,
        "Challenge_comment_count":4,
        "Challenge_created_time":1597607305160,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a CUDA out of memory error while training a PyTorch FLAIR model in AWS Sagemaker. They have tried different instance types but are still facing memory issues. They also encountered a memory error while training on a local CPU machine. The user has shared their model training script and mentioned that they were able to train the same architecture with a smaller dataset on their local GPU machine.",
        "Challenge_last_edit_time":1597613347630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63441299",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":13.2,
        "Challenge_reading_time":25.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":45.6704155556,
        "Challenge_title":"Pytorch CUDA OutOfMemory Error while training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2033.0,
        "Challenge_word_count":215,
        "Platform":"Stack Overflow",
        "Poster_created_time":1478616262200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kolkata, West Bengal, India",
        "Poster_reputation_count":375.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>Okay, so after 2 days of continuous debugging was able to find out the root cause.\nWhat I understood is Flair does not have any limitation on the sentence length, in the sense the word count, it is taking the highest length sentence as the maximum.\nSo there it was causing issue, as in my case there were few content with 1.5 lakh rows which is too much to load the embedding of into the memory, even a 16GB GPU.\nSo there it was breaking.<\/p>\n<p><strong>To solve this<\/strong>: For content with this much lengthy words, you can take chunk of n words(10K in my case) from these kind of content from any portion(left\/right\/middle anywhere) and trunk the rest, or simply ignore those records for training if it is very minimal in comparative count.<\/p>\n<p>After this I hope you will be able to progress with your training, as it happened in my case.<\/p>\n<p>P.S.: If you are following this thread and face similar issue feel free to comment back so that I can explore and help on your case of the issue.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":12.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":181.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":70.62626,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hello to all, <\/p>\n<p>I am working on a machine learning project, I have trained my model on azure auto ml studio, I would like to import it in onnx format, but it is not in the download options, so I want to modify the resulting code directly in azure auto ml, in the script.py I have modified the recording format of the model but I can't find how to execute this script because it tells me that the script.py is not found, <\/p>\n<p>Could you help me please ? <\/p>\n<p>thank you <\/p>\n<p>Lysa<\/p>",
        "Challenge_closed_time":1680506244446,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680251989910,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in modifying the template script on Azure Auto ML to import their model in ONNX format. They have modified the recording format of the model in script.py but are unable to execute the script as it is not found. They are seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1195000\/how-to-modify-the-template-script-on-azure-auto-ml",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.8,
        "Challenge_reading_time":6.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":70.62626,
        "Challenge_title":"how to modify the template script on azure auto ml?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=2d96c782-8477-4987-b5a4-5ea497b49eb9\">AMROUN Lysa<\/a> I believe you are looking to create a model wihich is ONNX compatible with AutoML as per your previous thread. In this case in the automl config if you setup <code>enable_onnx_compatible_models<\/code> to true in the automl config the model should be available for download. Something similar to what is provided as guidance in this <a href=\"https:\/\/github.com\/Azure\/azureml-examples\/blob\/main\/v1\/python-sdk\/tutorials\/automl-with-azureml\/classification-bank-marketing-all-features\/auto-ml-classification-bank-marketing-all-features.ipynb\">notebook<\/a>.<\/p>\n<p>I would recommend running through this notebook and then change your experiment settings in a similar way to retrieve the best onnx format model. <\/p>\n<pre><code>automl_settings = {\n    &quot;experiment_timeout_hours&quot;: 0.3,\n    &quot;enable_early_stopping&quot;: True,\n    &quot;iteration_timeout_minutes&quot;: 5,\n    &quot;max_concurrent_iterations&quot;: 4,\n    &quot;max_cores_per_iteration&quot;: -1,\n    # &quot;n_cross_validations&quot;: 2,\n    &quot;primary_metric&quot;: &quot;AUC_weighted&quot;,\n    &quot;featurization&quot;: &quot;auto&quot;,\n    &quot;verbosity&quot;: logging.INFO,\n    &quot;enable_code_generation&quot;: True,\n}\n\nautoml_config = AutoMLConfig(\n    task=&quot;classification&quot;,\n    debug_log=&quot;automl_errors.log&quot;,\n    compute_target=compute_target,\n    experiment_exit_score=0.9984,\n    blocked_models=[&quot;KNN&quot;, &quot;LinearSVM&quot;],\n    enable_onnx_compatible_models=True,\n    training_data=train_data,\n    label_column_name=label,\n    validation_data=validation_dataset,\n    **automl_settings,\n)\n\n<\/code><\/pre>\n<p>If this answers your query, do click <code>Accept Answer<\/code> and <code>Yes<\/code> for was this answer helpful. And, if you have any further query do let us know.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.8,
        "Solution_reading_time":24.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":146.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1441691980488,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":860.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":30.4880244444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We have created an endpoint in Vertex AI. We have got the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-custom-models\" rel=\"nofollow noreferrer\">client library<\/a> route working. However, we also want to figure out the gRPC route since that is closest to the gRPC route we had with self managed TF-Serving.\nCan someone provide a code pointer for Vertex AI model serving using gRPC (preferably in Python)?<\/p>",
        "Challenge_closed_time":1662511382056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662394780720,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an endpoint in Vertex AI and has successfully used the client library route, but is now seeking assistance in figuring out the gRPC route for model serving, preferably in Python. They are requesting code pointers or samples to help them achieve this.",
        "Challenge_last_edit_time":1662401625168,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73612214",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":32.38926,
        "Challenge_title":"Vertex AI model serving - gRPC access - code pointers \/ samples",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":36.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1280773677816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jersey City, NJ",
        "Poster_reputation_count":4339.0,
        "Poster_view_count":479.0,
        "Solution_body":"<p>gRPC can be used through Vertex Prediction private endpoint, but it is not yet officially supported. See sample here: <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/vertex_endpoints\/optimized_tensorflow_runtime\/tabular_optimized_online_prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/vertex_endpoints\/optimized_tensorflow_runtime\/tabular_optimized_online_prediction.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":41.7,
        "Solution_reading_time":7.23,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1468179475927,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation_count":795.0,
        "Answerer_view_count":210.0,
        "Challenge_adjusted_solved_time":1328.6339833334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to test Azure Machine Learning Studio. <\/p>\n\n<p>I want to use TensorFlow, but it is not installed on Jupyter notebook.<\/p>\n\n<p>How can I use some machine learning libraries like TensorFlow, Theano, Keras,... on the notebook?<\/p>\n\n<p>I tried this:<\/p>\n\n<pre><code>!pip install tensorflow \n<\/code><\/pre>\n\n<p>But, I got error as below:<\/p>\n\n<pre><code>Collecting tensorflow\n  Downloading tensorflow-0.12.0rc0-cp34-cp34m-manylinux1_x86_64.whl (43.1MB)\n    100% |################################| 43.1MB 27kB\/s \nCollecting protobuf==3.1.0 (from tensorflow)\n  Downloading protobuf-3.1.0-py2.py3-none-any.whl (339kB)\n    100% |################################| 348kB 3.7MB\/s \nCollecting six&gt;=1.10.0 (from tensorflow)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already satisfied: numpy&gt;=1.11.0 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: wheel&gt;=0.26 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: setuptools in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg (from protobuf==3.1.0-&gt;tensorflow)\nInstalling collected packages: six, protobuf, tensorflow\n  Found existing installation: six 1.9.0\n    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n    Uninstalling six-1.9.0:\n      Successfully uninstalled six-1.9.0\n  Rolling back uninstall of six\nException:\nTraceback (most recent call last):\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/commands\/install.py\", line 342, in run\n    prefix=options.prefix_path,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_set.py\", line 784, in install\n    **kwargs\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 851, in install\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 1064, in move_wheel_files\n    isolated=self.isolated,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 345, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 329, in clobber\n    os.utime(destfile, (st.st_atime, st.st_mtime))\nPermissionError: [Errno 1] Operation not permitted\n<\/code><\/pre>",
        "Challenge_closed_time":1485956468903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1481173386563,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to test Azure Machine Learning Studio and wants to use TensorFlow, but it is not installed on Jupyter notebook. The user tried to install TensorFlow using \"!pip install tensorflow\" command but encountered a permission error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41032108",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":36.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":1328.6339833334,
        "Challenge_title":"How to install TensorFlow in jupyter notebook on Azure Machine Learning Studio",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2886.0,
        "Challenge_word_count":235,
        "Platform":"Stack Overflow",
        "Poster_created_time":1279636903496,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":1540.0,
        "Poster_view_count":404.0,
        "Solution_body":"<p>As you noticed, the active user doesn't have permissions to write to the <code>site-packages<\/code> directory in Azure Machine Learning Studio notebooks. You could try installing the package to another directory where you do have write permissions (like the default working directory) and importing from there, but I recommend the following lower-hassle option.<\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\" rel=\"nofollow noreferrer\">Azure Notebooks<\/a> is a separate Jupyter Notebook service that will allow you to install tensorflow, theano, and keras. Like the notebooks in AML Studio, these notebooks will persist in your account. The primary downside is that if you want to access your workspace through e.g. the Python <code>azureml<\/code> package, you'll need to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow noreferrer\">provide your workspace id\/authorization token<\/a> to set up the connection. (In Azure ML Studio, those values are loaded automatically from the current workspace.) Otherwise I believe Azure Notebooks can do everything you are used to doing inside AML Studio only.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":14.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1232453837820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sweden",
        "Answerer_reputation_count":27150.0,
        "Answerer_view_count":6735.0,
        "Challenge_adjusted_solved_time":6.5479111111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a deep learning project with about 700GB of table-like time series data in thousands of .csv files (each about 15MB). <br>\nAll the data is on S3 and it needs some preprocessing before being fed into the model. The question is how to best go about automating the process of loading, preprocessing and training. <br><br>Is a custom keras generator with some built in preprocessing the best solution?<\/p>",
        "Challenge_closed_time":1537380210983,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537356638503,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is working on a deep learning project with 700GB of time series data in thousands of .csv files stored on S3. They need to preprocess the data before feeding it into the model and are looking for the best way to automate the process of loading, preprocessing, and training. They are considering using a custom keras generator with built-in preprocessing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52404879",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.5479111111,
        "Challenge_title":"Efficient management of large amounts of data with SageMaker for training a keras model",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":423.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488416220368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":142.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Preprocessing implies that this is something you might want to decouple from the model execution and run separately, possibly on a schedule or in response to new data flowing in.<\/p>\n\n<p>If so, you'll probably want to do the preprocessing outside of SageMaker. You could orchestrate it using <a href=\"https:\/\/aws.amazon.com\/glue\/\" rel=\"nofollow noreferrer\">Glue<\/a>, or you could write a custom job and run it through <a href=\"https:\/\/aws.amazon.com\/batch\/\" rel=\"nofollow noreferrer\">AWS Batch<\/a> or alternatively on an EMR cluster.<\/p>\n\n<p>That way, your Keras notebook can load the already preprocessed data, train and test through SageMaker.<\/p>\n\n<p>With a little care, you should be able to perform at least some of the heavy lifting incrementally in the preprocessing step, saving both time and cost downstream in the Deep Learning pipeline.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.1,
        "Solution_reading_time":10.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617627646000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The mlflow run status shows \"FINISHED\" instead of \"FAILED\" when the kedro run fails, making it difficult to distinguish between successful and failed runs in the mlflow ui. The potential solution suggested is to replace certain lines of code or retrieve the current run status from mlflow.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":121,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":431.6069444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThere are some missing details for how to connect to Neptune from a MacOS device, we should add them to our doc on connecting to neptune via ssh-tunnel found [here](https:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune)\r\n\r\nOne main piece that we are missing is that a host alias needs to be made in order to get things working properly.\r\n\r\n**Additional context**\r\nThis is coming from a bug report from connectivity not working as found in #40 ",
        "Challenge_closed_time":1608658157000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607104372000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an AttributeError when using PyTorchLightningPruningCallback to search for the best hyperparameters. The error message states that the 'AcceleratorConnector' object has no attribute 'distributed_backend'. The user has provided a code snippet and expects the code to run without any errors. The user has provided the environment details, including the PyTorch Lightning version, PyTorch version, Python version, OS, and CUDA\/cuDNN version.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.2,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":431.6069444444,
        "Challenge_title":"[BUG] Missing documentation on connecting to Neptune from MacOS",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":0.5572202778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am trying to train a model using Amazon Sagemaker (xgboost: eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'). But I always get the same error message shortly after starting the training job:<\/p>\n\n<blockquote>\n  <p>\"ClientError: Hidden file found in the data path! Remove that before\n  training.\"<\/p>\n<\/blockquote>\n\n<p>The S3 console shows that output path is empty (I also tried to create a new directory to no avail). Versioning is not enabled for the bucket.<\/p>\n\n<p>Surprisingly, google finds nothing under this error message.<\/p>\n\n<p>I have configured the input and outputs as follows:<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/train\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/validation\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        }\n    ],\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": \"s3:\/\/{}\/{}-xgboost-output\".format(s3_utils.bucket, LABEL)        },\n<\/code><\/pre>\n\n<p>The field<\/p>\n\n<pre><code>    \"RoleArn\": role,\n<\/code><\/pre>\n\n<p>where role comes from<\/p>\n\n<pre><code>    from sagemaker import get_execution_role\n    role = get_execution_role()\n<\/code><\/pre>\n\n<p>and is<\/p>\n\n<pre><code>    arn:aws:iam::&lt;ACCOUNT&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;HIDDEN&gt;\n<\/code><\/pre>\n\n<p>Here is a screenshot showing the data-path:\n<a href=\"https:\/\/i.stack.imgur.com\/bs8kl.png\" rel=\"nofollow noreferrer\">S3 dashboard view of data-path<\/a>. The two csv files is all there is. In particular, there is no empty \"directory\" which might be what \"hidden file\" could mean.<\/p>",
        "Challenge_closed_time":1531417705336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531339613883,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error message while trying to train a model using Amazon Sagemaker. The error message states that a hidden file has been found in the data path and needs to be removed before training. The user has checked the S3 console and found that the output path is empty, and versioning is not enabled for the bucket. The user has configured the input and output data paths correctly, and the role is also set up correctly. However, the error persists, and the user is unable to find any information about this error message online.",
        "Challenge_last_edit_time":1531415699343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51293471",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.0,
        "Challenge_reading_time":24.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":21.6920702778,
        "Challenge_title":"AWS Sagemaker - \"Hidden file found in the data path! Remove that before training.\"",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1578.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531338135003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":23.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Ok, the prefix you set in the <code>S3Uri<\/code> matters here. Based on your screenshot I think your bucket looks something like this (in tree form):<\/p>\n\n<pre><code>s3:\/\/bucket\n\u2514\u2500\u2500 LABEL-inputdata\n    \u251c\u2500\u2500 train.csv\n    \u2514\u2500\u2500 validation.csv\n<\/code><\/pre>\n\n<p>Based on your <code>InputDataConfig<\/code> above, SageMaker has to download it to folders on the filesystem for the <code>xgboost<\/code> training algorithm to run. It does so based on the channel names and on the <code>S3Uri<\/code> prefix you provided. The prefix is chopped off to determine the name of the folder\/file to download to. So, in your example, the <code>train<\/code> channel gets downloaded as:<\/p>\n\n<pre><code>\/opt\/ml\/input\/data\/train\/.csv\n<\/code><\/pre>\n\n<p>Finally, the <code>xgboost<\/code> implementation sees the <code>.csv<\/code> file as a hidden file and complains about it.<\/p>\n\n<p>To get it to work you could rearrange your data in s3 like so...<\/p>\n\n<pre><code>s3:bucket\n\u2514\u2500\u2500 LABEL-inputdata\n    \u251c\u2500\u2500 train\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 data.csv\n    \u2514\u2500\u2500 validation\n        \u2514\u2500\u2500 data.csv\n<\/code><\/pre>\n\n<p>.. and change your input data config to:<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/train\/\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": \"s3:\/\/{}\/{}-inputdata\/validation\/\".format(s3_utils.bucket, LABEL)\n                }\n            },\n            \"ContentType\": \"csv\",\n            \"CompressionType\": \"None\"\n        }\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":20.04,
        "Solution_score_count":5.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1556553177963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation_count":347.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":465.8558136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a mismatch in shapes between inputs and the model of my reinforcement learning project.<\/p>\n\n<p>I have been closely following the AWS examples, specifically the cartpole example. However I have built my own custom environment. What I am struggling to understand is how to change my environment so that it is able to work with the prebuilt Ray RLEstimator.<\/p>\n\n<p>Here is the code for the environment:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\n\nimport numpy as np\n\n# from float_space import FloatSpace\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        self.observation_space = Box(np.array(0.0),np.array(1000))\n#         self.observation_space = FloatSpace(price)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n        next_state = self.price\n#         next_state = self.observation_space.sample()\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n\n#         self.observation_space.value = np.random.rand()\n#         return self.observation_space.sample()\n        self.price = np.random.rand()\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        self.price += PRICE_ADJUSTMENT[action]\n\n    def _get_reward(self,price):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    env = gym.make('ArrivalSim-v0')\n\n    env.reset()\n    for _ in range(20):\n        test = env.action_space.sample()\n        print(test)\n        print(env.observation_space)\n        env.step(test)  # take a random action\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n\n<\/code><\/pre>\n\n<p>Here is the training script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nfrom ray.tune.registry import register_env\nfrom gym.envs.registration import register\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\n\n\nclass MyLauncher(SageMakerRayLauncher):\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n\n            }\n          }\n        }\n\nif __name__ == \"__main__\":\n    MyLauncher().train_main()\n<\/code><\/pre>\n\n<p>Here is the code I run in Jupyter:<\/p>\n\n<pre><code>metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\nenvironment = env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\n\nestimator = RLEstimator(entry_point=\"train.py\",\n                        source_dir='.',\n                        toolkit=RLToolkit.RAY,\n                        toolkit_version='0.6.5',\n                        framework=RLFramework.TENSORFLOW,\n                        dependencies=[\"sagemaker_rl\"],\n#                         image_name='price-response-ray-cpu',\n                        role=role,\n#                         train_instance_type=\"ml.c5.2xlarge\",\n                        train_instance_type='local',\n                        train_instance_count=1,\n#                         output_path=s3_output_path,\n#                         base_job_name=job_name_prefix,\n                        metric_definitions=metric_definitions\n#                         hyperparameters={\n                          # Attention scientists!  You can override any Ray algorithm parameter here:\n                          #\"rl.training.config.horizon\": 5000,\n                          #\"rl.training.config.num_sgd_iter\": 10,\n                        #}\n                    )\n\nestimator.fit(wait=True)\njob_name = estimator.latest_training_job.job_name\nprint(\"Training job: %s\" % job_name)\n<\/code><\/pre>\n\n<p>The error message I have been receiving has been the following:<\/p>\n\n<pre><code>algo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.1\/4.1 GB\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 2\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.4\/4.1 GB\nalgo-1-dxwxx_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-dxwxx_1  | Number of trials: 1 ({'RUNNING': 1})\nalgo-1-dxwxx_1  | RUNNING trials:\nalgo-1-dxwxx_1  |  - PPO_ArrivalSim-v0_0:   RUNNING\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,030  WARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,063   ERROR trial_runner.py:460 -- Error processing event.\nalgo-1-dxwxx_1  | Traceback (most recent call last):\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trial_runner.py\", line 409, in _process_trial\nalgo-1-dxwxx_1  |     result = self.trial_executor.fetch_result(trial)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/ray_trial_executor.py\", line 314, in fetch_result\nalgo-1-dxwxx_1  |     result = ray.get(trial_future[0])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/worker.py\", line 2316, in get\nalgo-1-dxwxx_1  |     raise value\nalgo-1-dxwxx_1  | ray.exceptions.RayTaskError: ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 83, in __init__\nalgo-1-dxwxx_1  |     restored, num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 135, in _build_layers_v2\nalgo-1-dxwxx_1  |     raise NotImplementedError\nalgo-1-dxwxx_1  | NotImplementedError\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | During handling of the above exception, another exception occurred:\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 276, in __init__\nalgo-1-dxwxx_1  |     Trainable.__init__(self, config, logger_creator)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trainable.py\", line 88, in __init__\nalgo-1-dxwxx_1  |     self._setup(copy.deepcopy(self.config))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 373, in _setup\nalgo-1-dxwxx_1  |     self._init()\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo.py\", line 77, in _init\nalgo-1-dxwxx_1  |     self.env_creator, self._policy_graph)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 506, in make_local_evaluator\nalgo-1-dxwxx_1  |     extra_config or {}))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 714, in _make_evaluator\nalgo-1-dxwxx_1  |     async_remote_worker_envs=config[\"async_remote_worker_envs\"])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 288, in __init__\nalgo-1-dxwxx_1  |     self._build_policy_map(policy_dict, policy_config)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 661, in _build_policy_map\nalgo-1-dxwxx_1  |     policy_map[name] = cls(obs_space, act_space, merged_conf)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo_policy_graph.py\", line 176, in __init__\nalgo-1-dxwxx_1  |     seq_lens=existing_seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 215, in get_model\nalgo-1-dxwxx_1  |     seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 255, in _get_model\nalgo-1-dxwxx_1  |     num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 86, in __init__\nalgo-1-dxwxx_1  |     input_dict[\"obs\"], num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/fcnet.py\", line 37, in _build_layers\nalgo-1-dxwxx_1  |     scope=label)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/framework\/python\/ops\/arg_scope.py\", line 182, in func_with_args\nalgo-1-dxwxx_1  |     return func(*args, **current_args)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/layers\/python\/layers\/layers.py\", line 1854, in fully_connected\nalgo-1-dxwxx_1  |     outputs = layer.apply(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 817, in apply\nalgo-1-dxwxx_1  |     return self.__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/layers\/base.py\", line 374, in __call__\nalgo-1-dxwxx_1  |     outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 730, in __call__\nalgo-1-dxwxx_1  |     self._assert_input_compatibility(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 1493, in _assert_input_compatibility\nalgo-1-dxwxx_1  |     str(x.shape.as_list()))\nalgo-1-dxwxx_1  | ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,064   INFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_ArrivalSim-v0_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,076   INFO trial_runner.py:497 -- Attempting to recover trial state from last checkpoint.\nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,041  INFO policy_evaluator.py:278 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n<\/code><\/pre>\n\n<p>I am not sure how to change the input the environment gives to the model or the models setup itself. It seems the documentations are quite obscure. I have a hunch that problem lies with the observation and action spaces<\/p>\n\n<p>Here is the reference to the original aws project example:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray<\/a><\/p>",
        "Challenge_closed_time":1567460466780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567158570927,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"the user is encountering a challenge with mismatched shapes between the inputs and model of their reinforcement learning project, and is struggling to understand how to change their environment to work with the prebuilt ray rlestimator.",
        "Challenge_last_edit_time":1567160893503,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57724414",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.2,
        "Challenge_reading_time":159.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":119,
        "Challenge_solved_time":83.8599591667,
        "Challenge_title":"How to make the inputs and model have the same shape (RLlib Ray Sagemaker reinforcement learning)",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1492.0,
        "Challenge_word_count":999,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565097950652,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p><strong>Possible reason:<\/strong><\/p>\n\n<p>The error message:<\/p>\n\n<p><code>ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]<\/code><\/p>\n\n<p>Your original environment obs space is <code>self.observation_space = Box(np.array(0.0),np.array(1000))<\/code>.<\/p>\n\n<p>Displaying the shape of your environment obs space gives:<\/p>\n\n<p><code>print(Box(np.array(0.0), np.array(1000), dtype=np.float32).shape)<\/code> = <code>()<\/code><\/p>\n\n<p>This could be indicated by <code>Full shape received: [None]<\/code> in the error message.<\/p>\n\n<p>If you pass the shape <code>(1,1)<\/code> into <code>np.zeros<\/code>, you get the expected  <code>min_ndim=2<\/code>:<\/p>\n\n<p><code>x = np.zeros((1, 1))\nprint(x)\n[[0.]]\nprint(x.ndim)\n2<\/code><\/p>\n\n<p><strong>Suggested solution:<\/strong><\/p>\n\n<p>I assume that you want your environment obs space to range from 0.0 to 1000.0 as indicated by the <code>self.price = np.random.rand()<\/code> in your <code>reset<\/code> function.<\/p>\n\n<p>Try using the following for your environment obs space:<\/p>\n\n<p><code>self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)<\/code><\/p>\n\n<p>I hope that by setting the <code>Box<\/code> with an explicit <code>shape<\/code> helps.<\/p>\n\n<p><strike>\n<strong>EDIT (20190903):<\/strong><\/p>\n\n<p>I have modified your training script. This modification includes new imports, custom model class, model registration &amp; addition of registered custom model to config. For readability, only sections added are shown below. The entire modified training script is available in this <a href=\"https:\/\/gist.github.com\/ChuaCheowHuan\/ddf70654bd928d70e5415c947d4d43f3\" rel=\"nofollow noreferrer\">gist<\/a>. Please run with the proposed obs space as describe above.<\/p>\n\n<p>New additional imports:<\/p>\n\n<pre><code># new imports\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\nfrom ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n\nfrom ray.rllib.utils import try_import_tf\nfrom ray.tune import grid_search\n\ntf = try_import_tf()\n# end new imports\n<\/code><\/pre>\n\n<p>Custom model class:<\/p>\n\n<pre><code># Custom model class (fcnet)\nclass CustomModel(TFModelV2):\n    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n\n    def __init__(self, obs_space, action_space, num_outputs, model_config,\n                 name):\n        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n                                          model_config, name)\n        self.model = FullyConnectedNetwork(obs_space, action_space,\n                                           num_outputs, model_config, name)\n        self.register_variables(self.model.variables())\n\n    def forward(self, input_dict, state, seq_lens):\n        return self.model.forward(input_dict, state, seq_lens)\n\n    def value_function(self):\n        return self.model.value_function()\n<\/code><\/pre>\n\n<p>Registered &amp; add custom model:<\/p>\n\n<pre><code>    def get_experiment_config(self):\n\n\n        # Register custom model\n        ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n\n\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n\n\n              \"model\": {\"custom_model\": \"my_model\"}, # Add registered custom model\n\n\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n            }\n          }\n        }\n<\/code><\/pre>\n\n<p><\/strike><\/p>\n\n<p><strong>EDIT 2 (20190910):<\/strong><\/p>\n\n<p>To show that it works, truncated output from Sagemaker (Jupyter notebook instance):<\/p>\n\n<pre><code>.\n.\n.\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 10.103020961393266 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 9.663759180463124 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.189261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.18926178] 5.67785342790426 False {}\nalgo-1-y2ayw_1  | price b = 0.189261780930142\nalgo-1-y2ayw_1  | price a = -0.06073821906985799\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [-0.06073822] 0 True {}\nalgo-1-y2ayw_1  | Result for PPO_ArrivalSim-v0_0:\nalgo-1-y2ayw_1  |   date: 2019-09-10_11-51-13\nalgo-1-y2ayw_1  |   done: true\nalgo-1-y2ayw_1  |   episode_len_mean: 126.72727272727273\nalgo-1-y2ayw_1  |   episode_reward_max: 15772.677709596366\nalgo-1-y2ayw_1  |   episode_reward_mean: 2964.4609668691965\nalgo-1-y2ayw_1  |   episode_reward_min: 0.0\nalgo-1-y2ayw_1  |   episodes: 5\nalgo-1-y2ayw_1  |   experiment_id: 5d3b9f2988854a0db164a2e5e9a7550f\nalgo-1-y2ayw_1  |   hostname: 2dae585dcc65\nalgo-1-y2ayw_1  |   info:\nalgo-1-y2ayw_1  |     cur_lr: 4.999999873689376e-05\nalgo-1-y2ayw_1  |     entropy: 1.0670874118804932\nalgo-1-y2ayw_1  |     grad_time_ms: 1195.066\nalgo-1-y2ayw_1  |     kl: 3.391784191131592\nalgo-1-y2ayw_1  |     load_time_ms: 44.725\nalgo-1-y2ayw_1  |     num_steps_sampled: 463\nalgo-1-y2ayw_1  |     num_steps_trained: 463\nalgo-1-y2ayw_1  |     policy_loss: -0.05383850634098053\nalgo-1-y2ayw_1  |     sample_time_ms: 621.282\nalgo-1-y2ayw_1  |     total_loss: 2194493.5\nalgo-1-y2ayw_1  |     update_time_ms: 145.352\nalgo-1-y2ayw_1  |     vf_explained_var: -5.519390106201172e-05\nalgo-1-y2ayw_1  |     vf_loss: 2194492.5\nalgo-1-y2ayw_1  |   iterations_since_restore: 2\nalgo-1-y2ayw_1  |   node_ip: 172.18.0.2\nalgo-1-y2ayw_1  |   pid: 77\nalgo-1-y2ayw_1  |   policy_reward_mean: {}\nalgo-1-y2ayw_1  |   time_since_restore: 4.55129861831665\nalgo-1-y2ayw_1  |   time_this_iter_s: 1.3484764099121094\nalgo-1-y2ayw_1  |   time_total_s: 4.55129861831665\nalgo-1-y2ayw_1  |   timestamp: 1568116273\nalgo-1-y2ayw_1  |   timesteps_since_restore: 463\nalgo-1-y2ayw_1  |   timesteps_this_iter: 234\nalgo-1-y2ayw_1  |   timesteps_total: 463\nalgo-1-y2ayw_1  |   training_iteration: 2\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000781a7b5b94a203683f8f789e593abbb1.\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000d3507bc6b41ee1c9fc36292eeae69557.\nalgo-1-y2ayw_1  | == Status ==\nalgo-1-y2ayw_1  | Using FIFO scheduling algorithm.\nalgo-1-y2ayw_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-y2ayw_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-y2ayw_1  | TERMINATED trials:\nalgo-1-y2ayw_1  |  - PPO_ArrivalSim-v0_0:   TERMINATED [pid=77], 4 s, 2 iter, 463 ts, 2.96e+03 rew\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | Saved model configuration.\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.extra_data as \/opt\/ml\/model\/checkpoint.extra_data\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.tune_metadata as \/opt\/ml\/model\/checkpoint.tune_metadata\nalgo-1-y2ayw_1  | Created LogSyncer for \/root\/ray_results\/PPO_ArrivalSim-v0_2019-09-10_11-51-13xdn_5i34 -&gt; None\nalgo-1-y2ayw_1  | 2019-09-10 11:51:13.941718: I tensorflow\/core\/common_runtime\/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | LocalMultiGPUOptimizer devices ['\/cpu:0']\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to save.\nalgo-1-y2ayw_1  | No assets to save.\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to write.\nalgo-1-y2ayw_1  | No assets to write.\nalgo-1-y2ayw_1  | INFO:tensorflow:SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | Saved TensorFlow serving model!\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000f352d985b807ca399460941fe2264899.\n\nalgo-1-y2ayw_1  | 2019-09-10 11:51:20,075 sagemaker-containers INFO\n\n Reporting training SUCCESS\n\ntmpwwb4b358_algo-1-y2ayw_1 exited with code 0\n\nAborting on container exit...\nFailed to delete: \/tmp\/tmpwwb4b358\/algo-1-y2ayw Please remove it manually.\n\n===== Job Complete =====\n<\/code><\/pre>\n\n<p>This time I make edits in all 3 files. Your environment, training script &amp; the Jupyter notebook but it turns out that there isn't a need to define custom models for your custom environment. However, that remains viable. And you're right, the main cause of the issue is still in the obs space.<\/p>\n\n<p>I set <code>self.price<\/code> to be a 1D numpy array to make it talk better with Ray RLlib. The creation of the custom environment in the training script was done in a simpler way as shown below. As for the notebook, I used version 0.5.3 instead of 0.6.5 for toolkit_version &amp; the training is done in local mode (in the docker container on the Sagemaker Jupyter notebook instance, still on AWS) with CPU only. However, it will also work with any ML instance (e.g ml.m4.xlarge) with GPU.<\/p>\n\n<p>The entire package along with all dependencies is in <a href=\"https:\/\/github.com\/ChuaCheowHuan\/sagemaker_Ray_RLlib_custom_env\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The edited env:<\/p>\n\n<pre><code># new\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n# end new\n\n\nfrom enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\nimport numpy as np\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        # original obs space:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space initially suggested:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space suggested in this edit:\n        self.observation_space = spaces.Box(np.array([0.0]), np.array([1000.0]), dtype=np.float32)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n\n        next_state = self.price\n        print('(self.price).shape =', (self.price).shape)\n        #next_state = self.observation_space.sample()\n\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n        #self.observation_space.value = np.random.rand()\n        #return self.observation_space.sample()\n\n        self.price = np.random.rand(1)\n\n        print('reset -&gt; (self.price).shape = ', (self.price).shape)\n\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        #print('price b =', self.price)\n        print('price b =', self.price[0])\n        #print('price b =', self.price[[0]])\n        #self.price += PRICE_ADJUSTMENT[action]\n        self.price[0] += PRICE_ADJUSTMENT[action]\n        #self.price[[0]] += PRICE_ADJUSTMENT[action]\n        #print('price a =', self.price)\n        print('price a =', self.price[0])\n        #print('price a =', self.price[[0]])\n\n    #def _get_reward(self, price):\n    def _get_reward(self):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        #self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        #return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    \"\"\"\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40.0}\n    )\n    env = gym.make('ArrivalSim-v0')\n    \"\"\"\n    env = ArrivalSim(30.0)\n\n    val = env.reset()\n    print('val.shape = ', val.shape)\n\n    for _ in range(5):\n        print('env.observation_space =', env.observation_space)\n        act = env.action_space.sample()\n        print('\\nact =', act)\n        next_state, reward, done, _ = env.step(act)  # take a random action\n        print('next_state = ', next_state)\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n<\/code><\/pre>\n\n<p>The edited training script:<\/p>\n\n<pre><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nimport ray.rllib.agents.a3c as a3c\nimport ray.rllib.agents.ppo as ppo\nfrom ray.tune.registry import register_env\nfrom mod_op_env import ArrivalSim\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\"\"\"\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\"\"\"\ndef create_environment(env_config):\n    price = 30.0\n    # This import must happen inside the method so that worker processes import this code\n    from mod_op_env import ArrivalSim\n    return ArrivalSim(price)\n\n\nclass MyLauncher(SageMakerRayLauncher):\n    def __init__(self):        \n        super(MyLauncher, self).__init__()\n        self.num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))\n        self.hosts_info = json.loads(os.environ.get(\"SM_RESOURCE_CONFIG\"))[\"hosts\"]\n        self.num_total_gpus = self.num_gpus * len(self.hosts_info)\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"training_iteration\": 3,\n            },\n\n            \"local_dir\": \"\/opt\/ml\/model\/\",\n            \"checkpoint_freq\" : 3,\n\n            \"config\": {                                \n              #\"num_workers\": max(self.num_total_gpus-1, 1),\n              \"num_workers\": max(self.num_cpus-1, 1),\n              #\"use_gpu_for_workers\": False,\n              \"train_batch_size\": 128, #5,\n              \"sample_batch_size\": 32, #1,\n              \"gpu_fraction\": 0.3,\n              \"optimizer\": {\n                \"grads_per_step\": 10\n              },\n            },\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 1), \"extra_cpu\": 0},\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 0),\n            #                    \"extra_cpu\": max(self.num_cpus-1, 1)},\n            \"trial_resources\": {\"cpu\": 1,\n                                \"extra_cpu\": max(self.num_cpus-1, 1)},              \n          }\n        }\n\nif __name__ == \"__main__\":\n    os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n    os.environ[\"LANG\"] = \"C.UTF-8\"\n    os.environ[\"RAY_USE_XRAY\"] = \"1\"\n    print(ppo.DEFAULT_CONFIG)\n    MyLauncher().train_main()\n\n<\/code><\/pre>\n\n<p>The notebook code:<\/p>\n\n<pre><code>!\/bin\/bash .\/setup.sh\n\nfrom time import gmtime, strftime\nimport sagemaker \nrole = sagemaker.get_execution_role()\n\nsage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3:\/\/{}\/'.format(s3_bucket)\nprint(\"S3 bucket path: {}\".format(s3_output_path))\n\njob_name_prefix = 'ArrivalSim'\n\nfrom sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n\nestimator = RLEstimator(entry_point=\"mod_op_train.py\", # Our launcher code\n                        source_dir='src', # Directory where the supporting files are at. All of this will be\n                                          # copied into the container.\n                        dependencies=[\"common\/sagemaker_rl\"], # some other utils files.\n                        toolkit=RLToolkit.RAY, # We want to run using the Ray toolkit against the ray container image.\n                        framework=RLFramework.TENSORFLOW, # The code is in tensorflow backend.\n                        toolkit_version='0.5.3', # Toolkit version. This will also choose an apporpriate tf version.                                               \n                        #toolkit_version='0.6.5', # Toolkit version. This will also choose an apporpriate tf version.                        \n                        role=role, # The IAM role that we created at the begining.\n                        #train_instance_type=\"ml.m4.xlarge\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_type=\"local\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_count=1, # Single instance will also work, but running distributed makes things \n                                                # fast, particularly in the case of multiple rollout training.\n                        output_path=s3_output_path, # The path where we can expect our trained model.\n                        base_job_name=job_name_prefix, # This is the name we setup above to be to track our job.\n                        hyperparameters = {      # Some hyperparameters for Ray toolkit to operate.\n                          \"s3_bucket\": s3_bucket,\n                          \"rl.training.stop.training_iteration\": 2, # Number of iterations.\n                          \"rl.training.checkpoint_freq\": 2,\n                        },\n                        #metric_definitions=metric_definitions, # This will bring all the logs out into the notebook.\n                    )\n\nestimator.fit()\n<\/code><\/pre>",
        "Solution_comment_count":19.0,
        "Solution_last_edit_time":1568837974432,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":222.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":215.0,
        "Solution_word_count":1601.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":4397.9938172222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Challenge_closed_time":1617119678127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1616764992750,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on a Machine Learning proof of concept on AWS with SageMaker and is confused about the differences between sagemaker and sagemaker_pyspark libraries. They are unsure if using sagemaker is equivalent to running a training job without taking advantage of the distributed computing capabilities of AWS. They are also experiencing slow training times with sagemaker_pyspark and are wondering if there is a way to reuse the same endpoint with different training jobs. The user is also unsure if sagemaker_pyspark supports custom algorithms and hyperparameter optimization. They are confused about the concept of entry_point and container\/image_name and are wondering if model containers are necessary. The user is also considering using a combination of Step Functions, Glue processes",
        "Challenge_last_edit_time":1618398204670,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66817781",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":47.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":98.5237158334,
        "Challenge_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":242.0,
        "Challenge_word_count":473,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376052312528,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Australia",
        "Poster_reputation_count":135.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1634230982412,
        "Solution_link_count":2.0,
        "Solution_readability":10.5,
        "Solution_reading_time":41.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":433.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":153.0623633333,
        "Challenge_answer_count":1,
        "Challenge_body":"1. Can we make a new code through the sagemaker studio?\n2. In my computer, GPU is GTX2080ti model, so if I use AWS sagemaker for paid service, can I get better performance?\n3. How much GPU performance can you improve compared to before?\n4. I want to proceed with object segmentation through AWS sagemaker, can I use the code I used through sagemaker studio?",
        "Challenge_closed_time":1661427331608,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660876307100,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is seeking information about using AWS SageMaker for coding and improving GPU performance. They also want to know if they can use their existing code for object segmentation in SageMaker Studio.",
        "Challenge_last_edit_time":1667925848878,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCKKplP7ES22DuZf8QJ38JA\/ask-aws-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":4.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":153.0623633333,
        "Challenge_title":"Ask AWS SageMaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":50.0,
        "Challenge_word_count":67,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"My apologies, I am not fully sure on all the questions. But let me still make an attempt to respond to see if it helps.\n\n1. Yes, you can write your own custom code through SageMaker studio.\n\n2. This may not be an apple to apple comparison. The main advantage in this context, is your able to scale out your training to multiple nodes and cores (if your underlying model supports that). Likewise you can scale out the deployment as well. Typically the studio notebook is backed by a lightweight EC2 instance, but there are a large range of EC2 instances for training on SageMaker. Please refer to the following links for further assistance. 1. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-instance-types.html 2. https:\/\/aws.amazon.com\/ec2\/instance-types\/\n\n3. Please refer to the response above for question # 2.\n4. Did you mean semantic segmentation? If yes, the answer is yes too.\n\nHope that helps!\n\nRegards,\nPunya",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1661427331608,
        "Solution_link_count":2.0,
        "Solution_readability":6.9,
        "Solution_reading_time":11.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":146.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":23.3159841667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run this on Amazon Sagemaker but I am getting this error while when I try to run it on my local machine, it works very fine.<\/p>\n<p>this is my code:<\/p>\n<pre><code>import tensorflow as tf\n\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\n    \ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)\n\ncontent_path = tf.keras.utils.get_file('YellowLabradorLooking_nw4.jpg', 'https:\/\/example.com\/IMG_20200216_163015.jpg')\n\n\nstyle_path = tf.keras.utils.get_file('kandinsky3.jpg','https:\/\/example.com\/download+(2).png')\n\n\ndef load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\ncontent_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\nimport tensorflow_hub as hub\nhub_module = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/1')\nstylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)\n\n\nfile_name = 'stylized-image5.png'\ntensor_to_image(stylized_image).save(file_name)\n<\/code><\/pre>\n<p>This is the exact error I get:<\/p>\n<pre><code>---------------------------------------------------------------------------\n<\/code><\/pre>\n<p>TypeError                                 Traceback (most recent call last)<\/p>\n<pre><code>&lt;ipython-input-24-c47a4db4880c&gt; in &lt;module&gt;()\n     53 \n     54 \n---&gt; 55 content_image = load_img(content_path)\n     56 style_image = load_img(style_path)\n     57 \n<\/code><\/pre>\n<p> in load_img(path_to_img)<\/p>\n<pre><code>     34 \n     35     shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n---&gt; 36     long_dim = max(shape)\n     37     scale = max_dim \/ long_dim\n     38 \n<\/code><\/pre>\n<p>~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/ops.py in <strong>iter<\/strong>(self)<\/p>\n<pre><code>    475     if not context.executing_eagerly():\n    476       raise TypeError(\n--&gt; 477           &quot;Tensor objects are only iterable when eager execution is &quot;\n    478           &quot;enabled. To iterate over this tensor use tf.map_fn.&quot;)\n    479     shape = self._shape_tuple()\n<\/code><\/pre>\n<p>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.<\/p>",
        "Challenge_closed_time":1594129639003,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594076057097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a Tensorflow error while trying to run a code on Amazon Sagemaker. The error message states that Tensor objects are only iterable when eager execution is enabled and suggests using tf.map_fn to iterate over the tensor. The code runs fine on the user's local machine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62765658",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":12.0,
        "Challenge_reading_time":41.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":14.8838627778,
        "Challenge_title":"Tensorflow error. TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1530.0,
        "Challenge_word_count":281,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565307837780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":570.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>Your error is being raised in this function <code>load_img<\/code>:<\/p>\n<pre><code>def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n<\/code><\/pre>\n<p>Specifically, this line:<\/p>\n<pre><code>    long_dim = max(shape)\n<\/code><\/pre>\n<p>You are passing a tensor to the <a href=\"https:\/\/docs.python.org\/3\/library\/functions.html#max\" rel=\"nofollow noreferrer\">built-in Python max function<\/a> in graph execution mode. You can only iterate through tensors in eager-execution mode. You probably want to use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/reduce_max\" rel=\"nofollow noreferrer\">tf.reduce_max<\/a> instead:<\/p>\n<pre><code>    long_dim = tf.reduce_max(shape)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1594159994640,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":13.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1440024011336,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":335.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":4.1729647222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy(local) using this line:<\/p>\n\n<pre><code>local_service = Model.deploy(ws, \"test\", [model], inference_config, deployment_config)\n<\/code><\/pre>\n\n<p>Then I get this output in the terminal:<\/p>\n\n<pre><code>tarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WXbPe.png\" rel=\"nofollow noreferrer\">Screenshot of the output<\/a><\/p>",
        "Challenge_closed_time":1570725963836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570710941163,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a local Azure ML model using the provided code. The error message \"tarfile.ReadError: file could not be opened successfully\" is displayed in the terminal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58323029",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":6.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.1729647222,
        "Challenge_title":"Azure ML deploy locally: tarfile.ReadError: file could not be opened successfully",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570704481987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>There was a bug with the retry logic when files were being uploaded. That bug has since been fixed, so updating your SDK should fix the issue.<\/p>\n\n<p>Similar post: <a href=\"https:\/\/stackoverflow.com\/questions\/57854136\/registering-and-downloading-a-fasttext-bin-model-fails-with-azure-machine-learn\">Registering and downloading a fastText .bin model fails with Azure Machine Learning Service<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":5.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359957233372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":86.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":2.74399,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When running an ML training job in Amazon SageMaker, the training script is \"deployed\" and given an ML training instance, which takes about 10 minutes to spin up and get the data it needs. <\/p>\n\n<p>I can only get one error message from the training job, then it dies and the instance is killed along with it. <\/p>\n\n<p>After I make a change to the training script to fix it, I need to deploy and run it which takes another 10 minutes or so.<\/p>\n\n<p>How can I accomplish this faster, or keep the training instance running?<\/p>",
        "Challenge_closed_time":1548282026927,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548272148563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in quickly debugging a SageMaker training script as the training instance takes about 10 minutes to spin up and get the data it needs. Additionally, the user can only get one error message from the training job, after which the instance is killed. The user is seeking ways to accomplish this faster or keep the training instance running.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54334462",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.74399,
        "Challenge_title":"How can I quickly debug a SageMaker training script?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1902.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416193017423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gensokyo",
        "Poster_reputation_count":880.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the \"local mode\" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to \"local\". More details about local mode can be found at <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview<\/a> and the blog post: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.7,
        "Solution_reading_time":13.03,
        "Solution_score_count":5.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":102.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1493314794172,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":844.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":13.1640722222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am training a Machine learning model in google colab, to be more specific I am training a GAN with PyTorch-lightning. The problem occurs is when I get disconnected from my current runtime due to inactivity. When I try to reconnect my Browser(tried on firefox and chrome) becomes first laggy and than freezes, my pc starts to lag so that I am not able to close my browser and it doesn't go away. I am forced to press the power button of my PC in order to restart the PC.\nI have no clue why this happens.\nI tried various batch sizes(also the size 1) but it still happens. It can't be that my dataset is too big either(since i tried it on a dataset with 10images for testing puposes).\nI hope someone can help me.<\/p>\n\n<p>Here is my code (For using the code you will need comet.nl and enter the comet.ml api key):<\/p>\n\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision  \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers\n\nimport numpy as np\nfrom numpy.random import choice\n\nfrom PIL import Image\n\nimport os\nfrom pathlib import Path\nimport shutil\n\nfrom collections import OrderedDict\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# randomly flip some labels\ndef noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n    # determine the number of labels to flip\n    n_select = int(p_flip * y.shape[0])\n    # choose labels to flip\n    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n    # invert the labels in place\n    y[flip_ix] = 1 - y[flip_ix]\n    return y\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=0.1):\n        self.std = std\n        self.mean = mean\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\ndef get_valid_labels(img):\n  return (0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1  # soft labels\n\ndef get_unvalid_labels(img):\n  return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)  # soft labels\n\nclass Generator(nn.Module):\n    def __init__(self, ngf, nc, latent_dim):\n        super(Generator, self).__init__()\n        self.ngf = ngf\n        self.latent_dim = latent_dim\n        self.nc = nc\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n             nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf, nc):\n        super(Discriminator, self).__init__()\n        self.nc = nc\n        self.ndf = ndf\n\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass DCGAN(pl.LightningModule):\n\n    def __init__(self, hparams, logger, checkpoint_folder, experiment_name):\n        super().__init__()\n        self.hparams = hparams\n        self.logger = logger  # only compatible with comet_logger at the moment\n        self.checkpoint_folder = checkpoint_folder\n        self.experiment_name = experiment_name\n\n        # networks\n        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n        # For experience replay\n        self.exp_replay_dis = torch.tensor([])\n\n        # creating checkpoint folder\n        dirpath = Path(self.checkpoint_folder)\n        if not dirpath.exists():\n          os.makedirs(dirpath, 0o755)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        # For adding Instance noise for more visit: https:\/\/www.inference.vc\/instance-noise-a-trick-for-stabilising-gan-training\/\n        std_gaussian = max(0, self.hparams.level_of_noise - ((self.hparams.level_of_noise * 1.5) * (self.current_epoch \/ self.hparams.epochs)))\n        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian) # the noise decays over time\n\n        imgs, _ = batch\n        imgs = AddGaussianNoiseInst(imgs) # Adding instance noise to real images\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n\n            # generate images\n            self.generated_imgs = self(z)\n            self.generated_imgs = AddGaussianNoiseInst(self.generated_imgs) # Adding instance noise to fake images\n\n            # Experience replay\n            # for discriminator\n            perm = torch.randperm(self.generated_imgs.size(0))  # Shuffeling\n            r_idx = perm[:max(1, self.hparams.experience_save_per_batch)]  # Getting the index\n            self.exp_replay_dis = torch.cat((self.exp_replay_dis, self.generated_imgs[r_idx]), 0).detach()  # Add our new example to the replay buffer\n\n            # ground truth result (ie: all fake)\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), get_valid_labels(self.generated_imgs)) # adversarial loss is binary cross-entropy\n\n            tqdm_dict = {'g_loss': g_loss}\n            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': log\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n            # how well can it label as real?\n            real_loss = self.adversarial_loss(self.discriminator(imgs), get_valid_labels(imgs))\n\n            # Experience replay\n            if self.exp_replay_dis.size(0) &gt;= self.hparams.experience_batch_size:\n              fake_loss = self.adversarial_loss(self.discriminator(self.exp_replay_dis.detach()), get_unvalid_labels(self.exp_replay_dis))  # train on already seen images\n\n              self.exp_replay_dis = torch.tensor([]) # Reset experience replay\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"d_exp_loss\": fake_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n            else:\n              fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), get_unvalid_labels(self.generated_imgs))  # how well can it label as fake?\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n        #                                 transforms.ToTensor(),\n        #                                 transforms.Normalize([0.5], [0.5])\n        #                                 ])\n\n        # train_dataset = torchvision.datasets.ImageFolder(\n        #     root=\".\/drive\/My Drive\/datasets\/ghibli_dataset_small_overfit\/\",\n        #     transform=transform\n        # )\n        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True, batch_size=self.hparams.batch_size)\n\n    def on_epoch_end(self):\n        z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n        # match gpu device (or keep as cpu)\n        if self.on_gpu:\n            z = z.cuda(self.last_imgs.device.index)\n\n        # log sampled images\n        sample_imgs = self.generator(z)\n        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n        self.logger.experiment.log_image(grid.permute(1, 2, 0), f'generated_images_epoch{self.current_epoch}', step=self.current_epoch)\n\n        # save model\n        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n          trainer.save_checkpoint(self.checkpoint_folder + \"\/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n          comet_logger.experiment.log_asset_folder(self.checkpoint_folder, step=self.current_epoch)\n\n          # Deleting the folder where we saved the model so that we dont upload a thing twice\n          dirpath = Path(self.checkpoint_folder)\n          if dirpath.exists() and dirpath.is_dir():\n                shutil.rmtree(dirpath)\n\n          # creating checkpoint folder\n          access_rights = 0o755\n          os.makedirs(dirpath, access_rights)\n\nfrom argparse import Namespace\n\nargs = {\n    'batch_size': 48,\n    'lr': 0.0002,\n    'b1': 0.5,\n    'b2': 0.999,\n    'latent_dim': 128, # tested value which worked(in V4_1): 100\n    'nc': 1,\n    'ndf': 32,\n    'ngf': 32,\n    'epochs': 10,\n    'save_model_every_epoch': 5,\n    'image_size': 64,\n    'num_workers': 2,\n    'level_of_noise': 0.15,\n    'experience_save_per_batch': 1, # this value should be very low; tested value which works: 1\n    'experience_batch_size': 50 # this value shouldnt be too high; tested value which works: 50\n}\nhparams = Namespace(**args)\n\n# Parameters\nexperiment_name = \"DCGAN_V4_2_MNIST\"\ndataset_name = \"MNIST\"\ncheckpoint_folder = \"DCGAN\/\"\ntags = [\"DCGAN\", \"MNIST\", \"OVERFIT\", \"64x64\"]\ndirpath = Path(checkpoint_folder)\n\n# init logger\ncomet_logger = loggers.CometLogger(\n    api_key=\"\",\n    rest_api_key=\"\",\n    project_name=\"gan\",\n    experiment_name=experiment_name,\n    #experiment_key=\"f23d00c0fe3448ee884bfbe3fc3923fd\"  # used for resuming trained id can be found in comet.ml\n)\n\n#defining net\nnet = DCGAN(hparams, comet_logger, checkpoint_folder, experiment_name)\n\n#logging\ncomet_logger.experiment.set_model_graph(str(net))\ncomet_logger.experiment.add_tags(tags=tags)\ncomet_logger.experiment.log_dataset_info(dataset_name)\n\ntrainer = pl.Trainer(#resume_from_checkpoint=\"GHIBLI_DCGAN_OVERFIT_64px_epoch_6000.ckpt\",\n                     logger=comet_logger,\n                     max_epochs=args[\"epochs\"]\n                     )\ntrainer.fit(net)\ncomet_logger.experiment.end()\n<\/code><\/pre>",
        "Challenge_closed_time":1587034953183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586987562523,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is training a machine learning model in Google Colab, specifically a GAN with PyTorch-lightning. The issue occurs when the user gets disconnected from the current runtime due to inactivity, and upon trying to reconnect, the browser becomes laggy and freezes, causing the PC to lag and forcing the user to restart it. The user has tried various batch sizes and datasets but the issue persists. The user has provided their code for reference.",
        "Challenge_last_edit_time":1587130797008,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61239274",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":149.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":145,
        "Challenge_solved_time":13.1640722222,
        "Challenge_title":"Google Colab freezes my browser and pc when trying to reconnect to a notebook",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1013.0,
        "Challenge_word_count":1119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493314794172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":844.0,
        "Poster_view_count":170.0,
        "Solution_body":"<p>I fixed it with importing this:<\/p>\n\n<pre><code>from IPython.display import clear_output \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1623879163643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":0.6793758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created my own model on a AWS SageMaker instance, with my own training and inference loops. I want to deploy it so that I can call the model for inference from AWS Lambda.<\/p>\n<p>I didn't use the SageMaker package to develop at all, but every tutorial (here is <a href=\"https:\/\/towardsdatascience.com\/using-aws-sagemaker-and-lambda-function-to-build-a-serverless-ml-platform-f14b3ec5854a%3E\" rel=\"nofollow noreferrer\">one<\/a>) I've looked at does so.<\/p>\n<p>How do I create an endpoint without using the SageMaker package.<\/p>",
        "Challenge_closed_time":1657053745440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657051299687,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a model on AWS SageMaker without using the SageMaker SDK and wants to set up an endpoint to call the model for inference from AWS Lambda without using the SageMaker package. They are seeking guidance on how to create an endpoint without using the SageMaker package.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72874937",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.6793758333,
        "Challenge_title":"Is it possible set up an endpoint for a model I created in AWS SageMaker without using the SageMaker SDK",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":66.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1657050754840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can use the boto3 library to do this.<\/p>\n<p>Here is an example of pseudo code for this -<\/p>\n<pre><code>import boto3\nsm_client = boto3.client('sagemaker')\ncreate_model_respose = sm_client.create_model(ModelName=model_name, ExecutionRoleArn=role, Containers=[container] )\n\ncreate_endpoint_config_response = sm_client.create_endpoint_config(EndpointConfigName=endpoint_config_name)\n\ncreate_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":25.5,
        "Solution_reading_time":7.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.8341666667,
        "Challenge_answer_count":1,
        "Challenge_body":"I'd like to set up Amazon SageMaker XGBoost to train datasets on multiple machines. Is that possible? If so, how?",
        "Challenge_closed_time":1583654787000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583496984000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of parallel training across multiple machines using Amazon SageMaker XGBoost and is seeking guidance on how to set it up.",
        "Challenge_last_edit_time":1668057386500,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOKZq2V_RQaaFzQkapcWpsA\/does-amazon-sagemaker-xgboost-support-parallel-training-across-multiple-machines",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":43.8341666667,
        "Challenge_title":"Does Amazon SageMaker XGBoost support parallel training across multiple machines?",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":119.0,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see [Docker registry paths and example code](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html) in the Amazon SageMaker developer guide. ",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925571827,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":3.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589738451347,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":179.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":99.0917625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to provision an AKS cluster that is connected to a vnet and has an internal load balancer on Azure. I am using code from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet?tabs=python\" rel=\"nofollow noreferrer\">here<\/a> that looks like this:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Verify that cluster does not exist already\ntry:\n    aks_target = AksCompute(workspace=ws, name=aks_cluster_name)\n    print(&quot;Found existing aks cluster&quot;)\n\nexcept:\n    print(&quot;Creating new aks cluster&quot;)\n\n    # Subnet to use for AKS\n    subnet_name = &quot;default&quot;\n    # Create AKS configuration\n    prov_config=AksCompute.provisioning_configuration(load_balancer_type=&quot;InternalLoadBalancer&quot;)\n    # Set info for existing virtual network to create the cluster in\n    prov_config.vnet_resourcegroup_name = &quot;myvnetresourcegroup&quot;\n    prov_config.vnet_name = &quot;myvnetname&quot;\n    prov_config.service_cidr = &quot;10.0.0.0\/16&quot;\n    prov_config.dns_service_ip = &quot;10.0.0.10&quot;\n    prov_config.subnet_name = subnet_name\n    prov_config.docker_bridge_cidr = &quot;172.17.0.1\/16&quot;\n\n    # Create compute target\n    aks_target = ComputeTarget.create(workspace = ws, name = &quot;myaks&quot;, provisioning_configuration = prov_config)\n    # Wait for the operation to complete\n    aks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>However, I get the following error<\/p>\n<pre><code>K8s failed to assign an IP for Load Balancer after waiting for an hour.\n<\/code><\/pre>\n<p>Is this because the AKS cluster does not yet have a 'network contributor' role for the vnet resource group? Is the only way to get this to work to first create AKS outside of AMLS, grant the network contributor role to the vnet resource group, then attach the AKS cluster to AMLS and configure the internal load balancer afterwards?<\/p>",
        "Challenge_closed_time":1604359053332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603997735143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to provision an AKS cluster with an internal load balancer on Azure using code from a Microsoft documentation page. However, they are encountering an error where K8s failed to assign an IP for Load Balancer after waiting for an hour. The user suspects that this is because the AKS cluster does not have a 'network contributor' role for the vnet resource group. They are wondering if the only solution is to create AKS outside of AMLS, grant the network contributor role to the vnet resource group, and then attach the AKS cluster to AMLS and configure the internal load balancer afterwards.",
        "Challenge_last_edit_time":1604002322987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64597526",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":25.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":100.3661636111,
        "Challenge_title":"Provision AKS with internal load balancer from AMLS on Azure",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":352.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589738451347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I was able to get this to work by first creating an AKS resource without an internal load balancer, then separately updating the load balancer following this code:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute.aks import AksUpdateConfiguration\nfrom azureml.core.compute import AksCompute\n\n# ws = workspace object. Creation not shown in this snippet\naks_target = AksCompute(ws,&quot;myaks&quot;)\n\n# Change to the name of the subnet that contains AKS\nsubnet_name = &quot;default&quot;\n# Update AKS configuration to use an internal load balancer\nupdate_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)\naks_target.update(update_config)\n# Wait for the operation to complete\naks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>No network contributor role was required.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":10.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0858333333,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi Team,  \nI'm trying to package sagemaker dependencies as external dependcies to upload to lambda.  \nBut I'm getting the max size limit error. Package size is more than allowed size limit i.e..  deployment package size is 50 MB.  \nAnd the reason I'm trying to do this is, 'get_image_uri' api is not accessible with boto3.  \nsample code for this api :   \n#Import the get_image_url utility function Amazon SageMaker Python SDK and get the location of the XGBoost container.  \n  \nimport sagemaker  \nfrom sagemaker.amazon.amazon_estimator import get_image_uri  \ncontainer = get_image_uri(boto3.Session().region_name, 'xgboost')  \n  \nAny reference would be of great help. Thank you.",
        "Challenge_closed_time":1568642184000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568641875000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in adding sagemaker dependencies as external dependencies to upload to lambda due to the max size limit error. The package size is more than the allowed size limit of 50 MB. The user is trying to access the 'get_image_uri' API, which is not accessible with boto3. The user has provided sample code for the API and is seeking any reference to resolve the issue.",
        "Challenge_last_edit_time":1668590352780,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUg5l3Jjl4SISDvnjIVYcqaA\/not-able-to-add-sagemaker-dependencies-as-external-dependencies-to-lambda",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.0858333333,
        "Challenge_title":"not able to add sagemaker dependencies as external dependencies to lambda",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":338.0,
        "Challenge_word_count":104,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Could you explain in more detail why do you want to have sagemaker inside of a lambda please?",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1568642184000,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1506516283190,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Torino, TO, Italia",
        "Answerer_reputation_count":875.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":433.2236119444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello Stackoverflowers,<\/p>\n\n<p>I'm using azureml and I'm wondering if it is possible to log a confusion matrix of the xgboost model I'm training, together with the other metrics I'm already logging. Here's a sample of the code I'm using:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nimport json\n\nwith open('.\/azureml.config', 'r') as f:\n    config = json.load(f)\n\nsvc_pr = ServicePrincipalAuthentication(\n   tenant_id=config['tenant_id'],\n   service_principal_id=config['svc_pr_id'],\n   service_principal_password=config['svc_pr_password'])\n\n\nws = Workspace(workspace_name=config['workspace_name'],\n                        subscription_id=config['subscription_id'],\n                        resource_group=config['resource_group'],\n                        auth=svc_pr)\n\ny_pred = model.predict(dtest)\n\nacc = metrics.accuracy_score(y_test, (y_pred&gt;.5).astype(int))\nrun.log(\"accuracy\",  acc)\nf1 = metrics.f1_score(y_test, (y_pred&gt;.5).astype(int), average='binary')\nrun.log(\"f1 score\",  f1)\n\n\ncmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\nrun.log_confusion_matrix('Confusion matrix', cmtx)\n<\/code><\/pre>\n\n<p>The above code raises this kind of error:<\/p>\n\n<pre><code>TypeError: Object of type ndarray is not JSON serializable\n<\/code><\/pre>\n\n<p>I already tried to transform the matrix in a simpler one, but another error occurred as before I logged a \"manual\" version of it (<code>cmtx = [[30000, 50],[40, 2000]]<\/code>).<\/p>\n\n<pre><code>run.log_confusion_matrix('Confusion matrix', [list([int(y) for y in x]) for x in cmtx])\n\nAzureMLException: AzureMLException:\n    Message: UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-    c5103b205379\/Confusion matrix already exists.\n    InnerException None\n    ErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/Confusion matrix already exists.\"\n    }\n}\n<\/code><\/pre>\n\n<p>This makes me think that I'm not properly handling the command <code>run.log_confusion_matrix()<\/code>. So, again, which is the best way I can log a confusion matrix to my azureml experiments?<\/p>",
        "Challenge_closed_time":1593519812260,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591960207257,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to log a confusion matrix of an xgboost model they are training on the AzureML platform using Python. They are encountering errors when trying to log the matrix using the `run.log_confusion_matrix()` command, and have tried transforming the matrix to a simpler version but still encounter errors. They are seeking advice on the best way to log a confusion matrix to their AzureML experiments.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62343056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":29.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":433.2236119444,
        "Challenge_title":"How to log a confusion matrix to azureml platform using python",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1418.0,
        "Challenge_word_count":216,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506516283190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Torino, TO, Italia",
        "Poster_reputation_count":875.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.<\/p>\n<p>You can find the proper function in this link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----<\/a>.<\/p>\n<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:<\/p>\n<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\ncmtx = {\n\n&quot;schema_type&quot;: &quot;confusion_matrix&quot;,\n&quot;parameters&quot;: params,\n &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],\n          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}\n}\nrun.log_confusion_matrix('Confusion matrix - error rate', cmtx)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.4,
        "Solution_reading_time":17.29,
        "Solution_score_count":6.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":126.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":70.55,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\n\nI successfully trained and deployed a pipeline in Vertex AI using Kubeflow for a retrieval model, Two Towers.\n\nNow I want to schedule this pipeline run every 8 minutes. Here's my code:\n\n\u00a0\n\nfrom kfp.v2.google.client import AIPlatformClient\napi_client = AIPlatformClient(project_id='my-project', region='us-central1')\n\napi_client.create_schedule_from_job_spec(\n    job_spec_path='vacantes_pipeline.json',\n    schedule=\"\/8 * * * *\", # every 8 minutes\n    time_zone='America\/Sao_Paulo',\n    parameter_values={\n        \"epochs_\": 5,\n    \"embed_length\":768,  \n        \"maxsplit_\" : 130\n    }\n)\n\n\u00a0\n\nThe JSON is successfuly created, but the Scheduler Job fails immediately.\n\nLogging tells me the httpRequest has an error 503 plus:\n\n\u00a0\n\njsonPayload: {\n@type: \"type.googleapis.com\/google.cloud.scheduler.logging.AttemptFinished\"\njobName: \"projects\/my-project\/locations\/us-central1\/jobs\/pipeline_vacantes-pipeline-with-deployment_c7e98a8f_59-14-a-a-a\"\nstatus: \"UNAVAILABLE\"\ntargetType: \"HTTP\"\nurl: \"https:\/\/us-central1-bogotatrabaja.cloudfunctions.net\/templated_http_request-v1\"\n}\n\n\u00a0\n\nAny ideas on how to solve this issue ?",
        "Challenge_closed_time":1683971520000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683717540000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error 503 while trying to schedule a pipeline run every 8 minutes in Vertex AI using Kubeflow. The JSON is successfully created, but the Scheduler Job fails immediately with an \"UNAVAILABLE\" status. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Scheduling-Vertex-AI-Pipeline-Error-503\/m-p\/552124#M1849",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.8,
        "Challenge_reading_time":14.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":70.55,
        "Challenge_title":"Scheduling Vertex AI Pipeline - Error 503",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":98,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I solved with Compute Engine and cron jobs.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":0.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":135.3537413889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi all,     <\/p>\n<p>I am following the steps on this tutorial:     <br \/>\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\">https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool<\/a>      <\/p>\n<p>I don't know what is the AML_MODEL_URI. I thought it was the REST endpoint or the Swagger URI from the endpoint.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/150362-image.png?platform=QnA\" alt=\"150362-image.png\" \/>    <\/p>\n<p>But it is not working. I am getting this error on Synapse: &quot;RuntimeError: Load model failed    <br \/>\nTraceback (most recent call last):&quot;    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/150308-image.png?platform=QnA\" alt=\"150308-image.png\" \/>    <\/p>\n<p>I appreciate you help.    <\/p>\n<p>Kind regards,     <br \/>\nAnaid     <\/p>",
        "Challenge_closed_time":1637667545272,
        "Challenge_comment_count":2,
        "Challenge_created_time":1637180271803,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is following a tutorial on scoring machine learning models with PREDICT in serverless Apache Spark pools but is encountering an error related to the AML_MODEL_URI. The user is unsure of what the AML_MODEL_URI is and has tried using the REST endpoint and Swagger URI but it is not working. The error message received is \"RuntimeError: Load model failed\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":135.3537413889,
        "Challenge_title":"What is AML_MODEL_URI - PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=4bb27b25-616e-491c-b986-136b5bf96f77\">@Anaid  <\/a>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>AML_MODEL_URL is the same name of the model in the ML workspace with (follow the format of <code>aml:\/\/<\/code> + Name of the Model).    <\/p>\n<\/blockquote>\n<p>Example: <code>aml:\/\/sklearn_regression_model:1<\/code> (follow the format of <code>aml:\/\/<\/code> + Name of the Model).    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/153599-image.png?platform=QnA\" alt=\"153599-image.png\" \/>    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.2,
        "Solution_reading_time":22.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":163.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":62.4823258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Good morning,  <br \/>\nI have a a dataset that consist of 99000 (256 x 256 pixels) images. I am trying to use this dataset to training a generative advesarial network (GAN) for at least a 1,000 epoch.   <br \/>\nCurrently, I am using a standard_NC24r (24 cores, 224 GB RAM, 1440 GB disk) GPU  (4 x NVIDIA Tesla K80) cluster but the training is slow. It takes about 3000 seconds to train 1 epoch. This implies it would take at least a month to complete training.  <br \/>\nIs a cluster that I can used to speed up training?  <\/p>\n<p>Thanks for your help in advance  <\/p>\n<p>Many thanks  <\/p>\n<p>Roland<\/p>",
        "Challenge_closed_time":1633928472660,
        "Challenge_comment_count":1,
        "Challenge_created_time":1633703536287,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has a dataset of 99,000 images and is trying to train a GAN for at least 1,000 epochs. They are currently using a standard_NC24r GPU cluster but the training is slow, taking 3000 seconds to train 1 epoch. The user is seeking advice on a faster cluster to speed up the training process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/583349\/best-compute-cluster-for-training-large-image-data",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":5.4,
        "Challenge_reading_time":7.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":62.4823258333,
        "Challenge_title":"Best compute cluster for training large image datasets !",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":117,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=5554be0b-de3f-4849-9c04-ab53140c4523\">@Okwen, Roland T  <\/a> Thanks, Instead of bigger machines with more memory, there are techniques to be used with Aml Compute for larger datasets. The <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/machine-learning-pipelines\/parallel-run\">Parallel Run<\/a> Step is an AzureML Pipeline Step which enables parallel processing or data partitions across multiple workers on multiple nodes. PRS (ParallelRunStep) is designed for embarrassingly parallel workload, e.g. train many models, batch inference, etc.    <\/p>\n<p>Also look into using some of the curated images provided for compute clusters.    <br \/>\nSpecifically look into the DASK image.    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/resource-curated-environments\">Curated environments - Azure Machine Learning | Microsoft Learn<\/a>    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":12.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1543010383463,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":404.8255119445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I send a TensorFlow training job to a SageMaker instance, what is the typical way to view training progress? Can I access TensorBoard for this launched EC2 instance? Is there some other alternative? What I'm looking for specifically are things like graphs of current training epoch and mAP.<\/p>",
        "Challenge_closed_time":1543010856183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541553484340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for ways to view the training progress of a TensorFlow job sent to a SageMaker instance, specifically graphs of current training epoch and mAP. They are asking if they can access TensorBoard or if there is another alternative.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53182436",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":4.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":404.8255119445,
        "Challenge_title":"SageMaker: visualizing training statistics",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":415.0,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>you can now specify metrics(metricName, Regex) that you want to track by using AWS management console or Amazon SageMaker Python SDK APIs. After the model training starts, Amazon SageMaker will automatically monitor and stream the specified metrics in real time to the Amazon CloudWatch console for visualizing time-series curves. <\/p>\n\n<p>Ref: \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_MetricDefinition.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":7.11,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":55.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1309439921808,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3050.0,
        "Answerer_view_count":106.0,
        "Challenge_adjusted_solved_time":3196.0599155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have wrapped my keras-tf-model into a Sklearn Pipeline, which also does some pre- and postprocessing. I want to serialize this model and capture its dependencies via MLflow.<\/p>\n\n<p>I have tried <code>mlflow.keras.save_model()<\/code>, which seems not appropriate. (it's not a \"pure\" keras model and as no <code>save()<\/code> attribute)<\/p>\n\n<p>I also tried <code>mlflow.sklearn.save_model()<\/code> and <code>mlflow.pyfunc.save_model()<\/code>, which both lead my to the same error: <\/p>\n\n<p><code>NotImplementedError: numpy() is only available when eager execution is enabled.<\/code><\/p>\n\n<p>(This error seems to stem from a clash between python and tensorflow, maybe?)<\/p>\n\n<p>I wonder, should it already\/ generally be possible to serialize these kind of \"hybrid\" models with mlflow?<\/p>\n\n<h3>Please finde a minimal example below<\/h3>\n\n<pre><code># In[1]:\n\n\nfrom mlflow.sklearn import save_model\nimport mlflow.sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\nfrom tensorflow.keras.models import Sequential\n\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\n\n# ### Save Keras Model\n\n# In[2]:\n\n\niris_data = load_iris() \n\nx = iris_data.data\ny_ = iris_data.target.reshape(-1, 1)\n\n# One Hot encode the class labels\nencoder = OneHotEncoder(sparse=False)\ny = encoder.fit_transform(y_)\n\n# Split the data for training and testing\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)\n\n# Build the model\nmodel = Sequential()\n\nmodel.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\nmodel.add(Dense(10, activation='relu', name='fc2'))\nmodel.add(Dense(3, activation='softmax', name='output'))\n\noptimizer = Adam(lr=0.001)\nmodel.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_x, train_y, verbose=2, batch_size=5, epochs=20)\n\n\n# In[3]:\n\n\nimport mlflow.keras\n\nmlflow.keras.save_model(model, \"modelstorage\/model40\")\n\n\n# ### Save Minimal SKlearn-Pipeline (with Keras)\n\n# In[4]:\n\n\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n\n# In[5]:\n\n\ndef define_model():\n    \"\"\"\n    Create fully connected network with given parameters.\n    \"\"\"\n    keras_model = Sequential()\n\n    keras_model.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\n    keras_model.add(Dense(10, activation='relu', name='fc2'))\n    keras_model.add(Dense(3, activation='softmax', name='output'))\n\n    optimizer = Adam(lr=0.001)\n    keras_model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n# In[6]:\n\n\n# target_encoder = TargetEncoder() \nscaler = StandardScaler()\nkeras_model = KerasClassifier(define_model, batch_size=5, epochs=20)\n\n\n# In[7]:\n\n\npipeline = Pipeline([\n#     ('encoding', target_encoder),\n    ('scaling', scaler),\n    ('modeling', keras_model)\n])\n\n\n# In[8]:\n\n\npipeline.fit(train_x, train_y)\n\n\n# In[9]:\n\n\nmlflow.keras.save_model(pipeline, \"modelstorage\/model42\")   #not working\n\n\n# In[10]:\n\n\nimport mlflow.sklearn\n\nmlflow.sklearn.save_model(pipeline, \"modelstorage\/model43\")\n\nOutput from modelstorage\/model43\/conda.yaml:\n\n======================\nchannels:\n- defaults\ndependencies:\n- python=3.6.7\n- scikit-learn=0.21.2\n- pip:\n  - mlflow\n  - cloudpickle==1.2.1\nname: mlflow-env\n======================\n\nDoesn't seem to capture Tensorflow.\n<\/code><\/pre>",
        "Challenge_closed_time":1572628704763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561111222150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has wrapped a Keras-tf-model into a Sklearn Pipeline, which also does some pre- and postprocessing. The user wants to serialize this model and capture its dependencies via MLflow. The user has tried mlflow.keras.save_model(), mlflow.sklearn.save_model(), and mlflow.pyfunc.save_model(), but all lead to the same error: \"NotImplementedError: numpy() is only available when eager execution is enabled.\" The user wonders if it is possible to serialize these kinds of \"hybrid\" models with MLflow.",
        "Challenge_last_edit_time":1561122889067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56701139",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":47.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":3199.3007258334,
        "Challenge_title":"Model-logging for \"hybrid models\" (e.g. SKlearn Pipeline including KerasWrapper) possible?",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1470.0,
        "Challenge_word_count":334,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336936830510,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":948.0,
        "Poster_view_count":132.0,
        "Solution_body":"<p>You can add extra dependencies when you save your model, for example if you have a keras step in your pipeline you can add keras &amp; tensorflow:<\/p>\n\n<pre><code>  conda_env = mlflow.sklearn.get_default_conda_env()\n  conda_env[\"dependencies\"] = ['keras==2.2.4', 'tensorflow==1.14.0'] + conda_env[\"dependencies\"]\n  mlflow.sklearn.log_model(pipeline, \"modelstorage\/model43\", conda_env = conda_env)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":5.36,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":39.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1489170718523,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Toronto, ON, Canada",
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":13.2178627778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I try to run the Sagemaker provided examples with PySpark in Sagemaker Studio<\/p>\n<pre><code>import os\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\n\nrole = get_execution_role()\n\n# Configure Spark to use the SageMaker Spark dependency jars\njars = sagemaker_pyspark.classpath_jars()\n\nclasspath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\n\n# See the SageMaker Spark Github repo under sagemaker-pyspark-sdk\n# to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\nspark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n    .master(&quot;local[*]&quot;).getOrCreate()\n<\/code><\/pre>\n<p>I get the following exception:<\/p>\n<pre><code>    ---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-6-c8f6fff0daaf&gt; in &lt;module&gt;\n     19 # to learn how to connect to a remote EMR cluster running Spark from a Notebook Instance.\n     20 spark = SparkSession.builder.config(&quot;spark.driver.extraClassPath&quot;, classpath)\\\n---&gt; 21     .master(&quot;local[*]&quot;).getOrCreate()\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    171                     for key, value in self._options.items():\n    172                         sparkConf.set(key, value)\n--&gt; 173                     sc = SparkContext.getOrCreate(sparkConf)\n    174                     # This SparkContext may be an existing one.\n    175                     for key, value in self._options.items():\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    361         with SparkContext._lock:\n    362             if SparkContext._active_spark_context is None:\n--&gt; 363                 SparkContext(conf=conf or SparkConf())\n    364             return SparkContext._active_spark_context\n    365 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\n    127                     &quot; note this option will be removed in Spark 3.0&quot;)\n    128 \n--&gt; 129         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    130         try:\n    131             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    310         with SparkContext._lock:\n    311             if not SparkContext._gateway:\n--&gt; 312                 SparkContext._gateway = gateway or launch_gateway(conf)\n    313                 SparkContext._jvm = SparkContext._gateway.jvm\n    314 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf)\n     44     :return: a JVM gateway\n     45     &quot;&quot;&quot;\n---&gt; 46     return _launch_gateway(conf)\n     47 \n     48 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/pyspark\/java_gateway.py in _launch_gateway(conf, insecure)\n    106 \n    107             if not os.path.isfile(conn_info_file):\n--&gt; 108                 raise Exception(&quot;Java gateway process exited before sending its port number&quot;)\n    109 \n    110             with open(conn_info_file, &quot;rb&quot;) as info:\n\nException: Java gateway process exited before sending its port number\n<\/code><\/pre>\n<p>Before running the example I installed pyspark and sagemaker_pyspark with pip from the notebook. I am also using SparkMagic kernel from the kernels library of SageMaker.<\/p>",
        "Challenge_closed_time":1611005573523,
        "Challenge_comment_count":2,
        "Challenge_created_time":1610957989217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an exception while trying to run Sagemaker provided examples with PySpark in Sagemaker Studio. The exception is related to the Java gateway process which exited before sending its port number. The user has installed pyspark and sagemaker_pyspark with pip and is using SparkMagic kernel from the kernels library of SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65770913",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":14.5,
        "Challenge_reading_time":43.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":13.2178627778,
        "Challenge_title":"Sagemaker Studio Pyspark example fails",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1827.0,
        "Challenge_word_count":316,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452190055592,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":247.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>Maybe, you are having this issue because this notebook was designed to run when you have an EMR cluster. I suggest you start a notebook with conda_python3 kernel on Sagemaker instead of the SparkMagic kernel. You will need to install <code>pyspark<\/code> and <code>sagemaker_pyspark<\/code> using pip, but it should work with the code you posted.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":4.39,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":38.1544444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Seems that the Neptune_catalyst.ipynb is failing. \r\nPerhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/neptune-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Challenge_closed_time":1625030635000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1624893279000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while running a ray tune job using the Sigopt suggester on a remote cluster. The Sigopt suggester object was found to be unserializable, but the stack trace did not indicate this. The severity of the issue is medium, as it is a significant difficulty but can be worked around.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/neptune-ai\/examples\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.0,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":160.0,
        "Challenge_repo_star_count":28.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":38.1544444444,
        "Challenge_title":"Neptune_catalyst.ipynb fails",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":22,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"on it. #43 fixing here fixed in #43 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":0.5,
        "Solution_reading_time":0.41,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0275,
        "Challenge_answer_count":0,
        "Challenge_body":"We are trying to save a model using log_model_ref and add a name to it, i.e. best_auc. Then we want to be able to retrieve this model from the latest run.\nHowever, if we use RunClient.client.runs_v1.get_runs_artifacts_lineage this returns all the artifacts ever generated for that project. And if we use RunClient.get_artifacts_tree, we do have more control about which run we are looking at, but we lose the name information we set when using log_model_ref?",
        "Challenge_closed_time":1649410238000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649410139000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to save a model using log_model_ref and add a name to it, but is facing challenges in retrieving the model from the latest run as the available methods either return all artifacts ever generated for the project or lose the name information set when using log_model_ref.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1485",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":7.9,
        "Challenge_reading_time":6.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.0275,
        "Challenge_title":"How to get model references logged by a specific run?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":83,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To get the logged model refs:\n\nfrom polyaxon.client import RunClient\n\nrun_client = RunClient(project=\"PROJECT_NAME\", run_uuid=\"RUN_UUID\")\n\n# Query the lineage information\nlineages = run_client.get_artifacts_lineage(query=\"kind: model\").results\n\n# Download the lineage assets\nfor lineage in lineages:\n    run_client.download_artifact_for_lineage(lineage=lineage)\n\nYou can restrict the ref to specific lineage by filtering further by name:\n\nlineages = run_client.get_artifacts_lineage(query=\"kind: model, name: best_auc\").results",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.6,
        "Solution_reading_time":6.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":47.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":73.4173611111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi! \n\nI'm using the [Feature Store Spark connector](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-ingestion-spark-connector-setup.html) to ingest data into the Sagemaker Feature Store and when we try to ingest data to a Feature Group with the online store enabled, the data is duplicated.\nIn the image bellow, \"customer_id\" is the ID feature, \"date_ref\" the event column. All the features are equal for the same ID and EventTime column, except the \"api_invocation_time\".\n\n ![Duplicated data](\/media\/postImages\/original\/IMhOq2A8JdTzm0WS21rLjyLQ)\n\nIf the feature group doesn't have the online store enabled, we ingest the data directly to the offline store without issues. But when we use the \"Ingest by default\" option in the connector (not specifying the \"target_stores\" in the connector, uses the PutRecord API), the data ingested is duplicated:\n\n```\nparams = {\n    \"input_data_frame\":dataframe,\n    \"feature_group_arn\": feature_group_arn            \n}\n\nif not online_store_enabled:\n    params[\"target_stores\"] = [\"OfflineStore\"]\n    logger.info(f\"Ingesting data to the offline store\")\n\npyspark_connector.ingest_data(**params)\nlogger.info(\"Finished the ingestion!\")\n\nfailed_records = pyspark_connector.get_failed_stream_ingestion_data_frame()\n```\n\nHow can I solve this issue using the connector?\n\n**EDIT:**\n\nApparently, the problem is in the \"get_failed_stream ingestion data frame\" method. This method, instead of just returning a dataframe, ingests the data again before returning the failed records. Removing the method from the ingestion pipeline resolves the issue, although we lose a form of validation.",
        "Challenge_closed_time":1677783327872,
        "Challenge_comment_count":0,
        "Challenge_created_time":1677163363698,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the Sagemaker Feature Store Spark connector where data is being duplicated when ingesting data to a Feature Group with the online store enabled. The issue is resolved by removing the \"get_failed_stream ingestion data frame\" method from the ingestion pipeline, although this results in the loss of a form of validation.",
        "Challenge_last_edit_time":1677519025372,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDdzO0n0cRKm5fLSuPdE3Qg\/sagemaker-feature-store-spark-connector-is-duplicating-data",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":21.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":172.2122705556,
        "Challenge_title":"Sagemaker Feature Store Spark connector is duplicating data",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":198,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue should be patched in 1.1.1. If you upgrade from 1.1.0, get_failed_stream_ingestion_data_frame should no longer trigger any re-computation now.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1677783327872,
        "Solution_link_count":0.0,
        "Solution_readability":4.7,
        "Solution_reading_time":1.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1477057589223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Columbus, OH, United States",
        "Answerer_reputation_count":547.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":20.3776283333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure webjob that is calling a ML training experiment via HttpRequests, leveraging the code generated in the ML webportal:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = \"azureStorageConnectionString\",\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/Model_2018421.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>However, the request fails with the following message:<\/p>\n\n<blockquote>\n  <p>The blob reference:\n  experiments\/experimentId\/TenantId\/Model_2018421.ilearner\n  has an invalid or missing file extension. Supported file extensions\n  for this output type are: \\\\\".csv, .tsv, .arff\\\\\"<\/p>\n<\/blockquote>\n\n<p>I'm pretty confused about this, since it's written right the documentation all over the place that if I'm expecting a trained model to use \".ilearner\" as the file extension for the model.<\/p>\n\n<p>I've seen <a href=\"https:\/\/stackoverflow.com\/questions\/47920098\/use-azure-data-factory-updating-azure-machine-learning-models\">this question<\/a> asking about the same error leveraging the DataFactory, and also <a href=\"https:\/\/datascience.stackexchange.com\/questions\/27397\/azure-machine-learning-model-retraining-problem\">this question on datascience.stackexchange<\/a>. Neither one had any clues, answers, or other follow up.<\/p>\n\n<p>Any insight on what I'm missing would be greatly appreciated!<\/p>",
        "Challenge_closed_time":1525787996892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525714637430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure ML Experiment Batch Webservice Call, where the request fails with an error message stating that the blob reference has an invalid or missing file extension. The user is confused as the file extension used for the model is \".ilearner\", which is mentioned in the documentation. The user is seeking insights to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50219664",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.0,
        "Challenge_reading_time":24.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":20.3776283333,
        "Challenge_title":"Azure ML Experiment Batch Webservice Call Fails with Invalid Output Extension",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477057589223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Columbus, OH, United States",
        "Poster_reputation_count":547.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>For anyone looking for your \"Don't Overthink It\" moment of the day:<\/p>\n\n<p>I needed to provide TWO output blob file references:<\/p>\n\n<pre><code>var request = new BatchExecutionRequest()\n            {\n                Inputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"input1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{trainingDataFileName}.csv\"\n                        }\n                    },\n                },\n\n                Outputs = new Dictionary&lt;string, AzureBlobDataReference&gt;() {\n                    {\n                        \"output1\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameCsv}.csv\"\n                        }\n                    },\n                    {\n                        \"output2\",\n                        new AzureBlobDataReference()\n                        {\n                            ConnectionString = _connectionString,\n                            RelativeLocation = $\"{_containerName}\/{experimentId}\/{tenantId}\/{outputFileNameIlearner}.ilearner\"\n                        }\n                    },\n                },\n\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n<\/code><\/pre>\n\n<p>There's an old saying in American English about not making assumptions, and I assumed the second output was an optional parameter used in batch operations. Since I'm not actually looking for more than one result from each call, I thought I was safe to remove the second output parameter.<\/p>\n\n<p>TL\/DR: Keep all the parameters the webservice portal's \"Consume\" tab generates, and make sure the first one is a .csv file reference.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":20.0,
        "Solution_reading_time":17.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1562578633160,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":92.103195,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to python programming. Following the AWS learning path:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult<\/a><\/p>\n<p>I am getting an error when excuting the following block (in conda_python3):<\/p>\n<pre><code>test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\nxgb_predictor.content_type = 'text\/csv' # set the data type for an inference\nxgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an \narray\nprint(predictions_array.shape)\n<\/code><\/pre>\n<blockquote>\n<p>AttributeError                            Traceback (most recent call last)\n in \n1 test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n----&gt; 2 xgb_predictor.content_type = 'text\/csv' # set the data type for an inference\n3 xgb_predictor.serializer = csv_serializer # set the serializer type\n4 predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n5 predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array<\/p>\n<\/blockquote>\n<blockquote>\n<p>AttributeError: can't set attribute<\/p>\n<\/blockquote>\n<p>I have looked at several prior questions but couldn't find much information related to this error when it comes to creating data types.<\/p>\n<p>Thanks in advance for any help.<\/p>",
        "Challenge_closed_time":1609399068572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609067497070,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an AttributeError while executing a block of code in AWS Sagemaker. The error occurs when trying to set the data type for an inference and the serializer type. The user has looked at prior questions but couldn't find much information related to this error when it comes to creating data types.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65465114",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":20.9,
        "Challenge_reading_time":27.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":92.103195,
        "Challenge_title":"AWS Sagemaker AttributeError: can't set attribute error",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2479.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1374284754808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":279.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.\nxgb_predictor.content_type = 'text\/csv'<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":1.87,
        "Solution_score_count":8.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":116.7776319444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I built a model in Rstudio, published in Azure ML Studio a web service using \"AzureML\" R package. When testing the web service on Azure ML Studio, I encountered an error:<\/p>\n\n<pre><code>Error: AzureML returns error code:\nHTTP status code : 400\nAzureML error code : LibraryExecutionError\nModule execution encountered an internal library error.\nThe following error occurred during evaluation of R script: R_tryEval: return error: Error: bad restore file magic number (file may be corrupted) -- no data loaded\n<\/code><\/pre>\n\n<p>Do you have any <strong>insights<\/strong> on how to solve such issue? Am I <strong>missing some important code in the R script<\/strong>?<\/p>\n\n<p>The model I used was a RandomForest to predicted Species on Iris dataset<\/p>\n\n<pre><code># Iris dataset\ndf = iris\nset.seed(100);\n\nindex = createDataPartition(df$Species, p = 0.7, list = FALSE)\nML.train = df[index,]; \nML.test = df[-index,];  rm(index)\n\nlibrary(randomForest)\nmodel = randomForest::randomForest(Species ~., data = ML.train)\n\nmypredict = function(newdata) {\n      require(randomForest)\n      predict(model, newdata, type = \"response\")\n}\n\n# Create workspace\nwsObj = AzureML::workspace(id = \"my Id\", auth = \"my token\")  # I omitted on purpose my Id and my token values\n\n# Publishing\nlibrary(devtools)\nlibrary(AzureML)\napi = AzureML::publishWebService(ws = wsObj,\n                                 fun = mypredict,\n                                 name = \"IrisWebService\",\n                                 inputSchema = ML.test %&gt;% select(-Species) )\n<\/code><\/pre>",
        "Challenge_closed_time":1575268924192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574853611397,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while testing a web service published via Rstudio RRS feed in Azure ML Studio. The error message indicated a LibraryExecutionError with an internal library error. The user is seeking insights on how to solve the issue and wondering if there is any important code missing in the R script. The model used was a RandomForest to predict Species on the Iris dataset.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59069043",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":18.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":115.3646652778,
        "Challenge_title":"LibraryExecutionError - testing a web service published via Rstudio RRS feed",
        "Challenge_topic":"MXNet Development",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":183,
        "Platform":"Stack Overflow",
        "Poster_created_time":1574853098927,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Dublin, Ireland",
        "Poster_reputation_count":13.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>AzureML for RStudio is now not supported as the package is removed from CRAN repository from <a href=\"https:\/\/cran.r-project.org\/web\/packages\/AzureML\/index.html\" rel=\"nofollow noreferrer\">2019-07-29<\/a>.\nAzure ML studio with this package will not work as the package(AzureML) is removed.<\/p>\n\n<p>Azure Machine Learning SDK for R can be Downloaded from CRAN at <a href=\"https:\/\/cloud.r-project.org\/web\/packages\/azuremlsdk\/index.html\" rel=\"nofollow noreferrer\">https:\/\/cloud.r-project.org\/web\/packages\/azuremlsdk\/index.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1575274010872,
        "Solution_link_count":3.0,
        "Solution_readability":14.3,
        "Solution_reading_time":7.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.3539691667,
        "Challenge_answer_count":1,
        "Challenge_body":"I am currently utilizing an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. The data consists of 7157 time series with 152 timesteps in the training set and 52 timesteps in the test set respectively. I estimate the run time for the tuning job on this specific instance type to take about 4-5 days. Looking to find out if DeepAR is engineered to take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. Also would be great for recommendations as to which Accelerated Computing instance would be optimal for this scenario.",
        "Challenge_closed_time":1644892187263,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644862112974,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is currently using an ml.c4.2xlarge instance type for a DeepAR use case to run an Automated Model Tuning job. They are looking for advice on whether DeepAR can take advantage of GPU computing for training and if it would be advisable to use a 'p' or 'g' compute instance instead for faster results. They are also seeking recommendations for the optimal Accelerated Computing instance for this scenario.",
        "Challenge_last_edit_time":1668563778934,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUnYV-WoO2R3KY4sNEq-Dshw\/optimal-notebook-instance-type-for-deepar-in-aws-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":8.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":8.3539691667,
        "Challenge_title":"Optimal notebook instance type for DeepAR in AWS Sagemaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":232.0,
        "Challenge_word_count":121,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"(As detailed further on the [algorithm details page](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar.html#deepar-instances)), **yes**, the SageMaker DeepAR algorithm implementation is able to train on GPU-accelerated instances to speed up more challenging jobs. There's also a handy [reference table here](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/common-info-all-im-models.html) listing all the SageMaker built-in algorithms and whether they're likely to be accelerated with GPU.\n\n**However**, to be clear, it shouldn't be the *notebook* instance type that affects this... Typically when training models on SageMaker, the notebook would provide your interactive compute environment but you'd run training in *training jobs* - for example using the [SageMaker Python SDK](https:\/\/sagemaker.readthedocs.io\/en\/stable\/) `Estimator` class as shown in the sample notebooks for DeepAR [electricity](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb) and [synthetic](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/introduction_to_amazon_algorithms\/deepar_synthetic\/deepar_synthetic.ipynb). The instance type you select for training is independent of the instance type you use for your notebook - for example in the electricity notebook it's set as follows:\n\n```python\nestimator = sagemaker.estimator.Estimator(\n    image_uri=image_name,\n    sagemaker_session=sagemaker_session,\n    role=role,\n    train_instance_count=1,  # <-- Setting training instance count\n    train_instance_type=\"ml.c4.2xlarge\",  # <-- Setting training instance type\n    base_job_name=\"deepar-electricity-demo\",\n    output_path=s3_output_path,\n)\n```\n\nSo normally I wouldn't expect you to need to change your *notebook* instance type to speed up training - just edit the configuration of your training job from within the notebook.\n\nSuggesting a particular type is tricky because [DeepAR hyperparameters](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar_hyperparameters.html) like `context_length`, `embedding_dimension`, and `mini_batch_size` will affect how much GPU capacity is needed for a particular run. Since you're coming from CPU-only baseline, I'd maybe suggest to start small with trying out single-GPU `g4dn.xlarge`, `g5.xlarge` or `p3.2xlarge` instances, perhaps starting with the lowest cost-per-hour? You can keep  an eye on your jobs' GPUUtilization and GPUMemoryUtilization metrics to check whether utilization is low on instances like p3 with \"bigger\" GPUs. Increasing `mini_batch_size` should help fill extra capacity on these and complete your job faster, but it will probably affect model convergence - so may need to tune other parameters like `learning_rate` to try and compensate. So considering all of this, you may find trade-offs between speed and total cost, or speed and accuracy, for good hyperparameter combinations on your dataset. Of course you could also scale up to multi-GPU instance types if you'd like to accelerate further.\n\nIf I understood right you're also using SageMaker Automatic Hyperparameter Tuning to search these parameters, something like [this XGBoost notebook](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/hyperparameter_tuning\/xgboost_random_log\/hpo_xgboost_random_log.ipynb) with the `HyperparameterTuner` class?\n\nIn that case would also mention:\n- Increasing the `max_parallel_jobs` parameter may accelerate the overall run time (by running more of the individual training jobs in parallel) - with a trade-off on how much information is available when each training job in the budget is kicked off.\n- If you're planning to run this training regularly on a dataset which evolves over time, you probably don't need to run HPO each time: Will likely see good results using your previously-optimized hyperparameters, unless something materially changes in the nature of the data and patterns.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1644892187264,
        "Solution_link_count":7.0,
        "Solution_readability":16.5,
        "Solution_reading_time":50.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":448.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1310699693432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2889.0,
        "Answerer_view_count":62.0,
        "Challenge_adjusted_solved_time":139.2068527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to replicate <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb<\/a><\/p>\n\n<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:<\/p>\n\n<pre><code>%%time\nimport boto3\n\nregion = boto3.Session().region_name\nsaved_model = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/model\/resnet\/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)\n\nimport sagemaker\nfrom sagemaker.tensorflow.serving import Model\n\nrole = sagemaker.get_execution_role()\n\ntensorflow_model = Model(model_data=saved_model,\nrole=role,\nframework_version='1.14')\ntf_predictor = tensorflow_model.deploy(initial_instance_count=1,\ninstance_type='local',\naccelerator_type='local_sagemaker_notebook')\n<\/code><\/pre>\n\n<p>I am getting following log in the notebook:<\/p>\n\n<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1\nalgo-1-7ynb1_1 | INFO:main:starting services\nalgo-1-7ynb1_1 | INFO:main:using default model name: Servo\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving model config:\nalgo-1-7ynb1_1 | model_config_list: {\nalgo-1-7ynb1_1 | config: {\nalgo-1-7ynb1_1 | name: \"Servo\",\nalgo-1-7ynb1_1 | base_path: \"\/opt\/ml\/model\/export\/Servo\",\nalgo-1-7ynb1_1 | model_platform: \"tensorflow\"\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:nginx config:\nalgo-1-7ynb1_1 | load_module modules\/ngx_http_js_module.so;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_processes auto;\nalgo-1-7ynb1_1 | daemon off;\nalgo-1-7ynb1_1 | pid \/tmp\/nginx.pid;\nalgo-1-7ynb1_1 | error_log \/dev\/stderr error;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_rlimit_nofile 4096;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | events {\nalgo-1-7ynb1_1 | worker_connections 2048;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | http {\nalgo-1-7ynb1_1 | include \/etc\/nginx\/mime.types;\nalgo-1-7ynb1_1 | default_type application\/json;\nalgo-1-7ynb1_1 | access_log \/dev\/stdout combined;\nalgo-1-7ynb1_1 | js_include tensorflow-serving.js;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream tfs_upstream {\nalgo-1-7ynb1_1 | server localhost:8501;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream gunicorn_upstream {\nalgo-1-7ynb1_1 | server unix:\/tmp\/gunicorn.sock fail_timeout=1;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | server {\nalgo-1-7ynb1_1 | listen 8080 deferred;\nalgo-1-7ynb1_1 | client_max_body_size 0;\nalgo-1-7ynb1_1 | client_body_buffer_size 100m;\nalgo-1-7ynb1_1 | subrequest_output_buffer_size 100m;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | set $tfs_version 1.14;\nalgo-1-7ynb1_1 | set $default_tfs_model Servo;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/tfs {\nalgo-1-7ynb1_1 | rewrite ^\/tfs\/(.) \/$1 break;\nalgo-1-7ynb1_1 | proxy_redirect off;\nalgo-1-7ynb1_1 | proxy_pass_request_headers off;\nalgo-1-7ynb1_1 | proxy_set_header Content-Type 'application\/json';\nalgo-1-7ynb1_1 | proxy_set_header Accept 'application\/json';\nalgo-1-7ynb1_1 | proxy_pass http:\/\/tfs_upstream;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ping {\nalgo-1-7ynb1_1 | js_content ping;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/invocations {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location ~ ^\/models\/(.)\/invoke {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/models {\nalgo-1-7ynb1_1 | proxy_pass http:\/\/gunicorn_upstream\/models;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ {\nalgo-1-7ynb1_1 | return 404 '{\"error\": \"Not Found\"}';\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | keepalive_timeout 3;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)\nalgo-1-7ynb1_1 | INFO:main:nginx version info:\nalgo-1-7ynb1_1 | nginx version: nginx\/1.16.1\nalgo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)\nalgo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016\nalgo-1-7ynb1_1 | TLS SNI support enabled\nalgo-1-7ynb1_1 | configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\nalgo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\nalgo-1-7ynb1_1 |\n!algo-1-7ynb1_1 | 172.18.0.1 - - [17\/Jun\/2020:05:02:10 +0000] \"GET \/ping HTTP\/1.1\" 200 3 \"-\" \"-\"\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-7ynb1_1 | EI Error Code: [3, 16, 8]\nalgo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | EI Client Version: 1.5.3\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\n`\n<\/code><\/pre>\n\n<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.<\/p>\n\n<p>I can see the docker of the model running\n<a href=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I try to infer\/predict from that model, I get error:<\/p>\n\n<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]\n\nalgo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-iikpj_1 | EI Error Code: [3, 16, 8]\nalgo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-iikpj_1 | EI Client Version: 1.5.3\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-iikpj_1 | INFO:main:tensorflow version info:\nalgo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-iikpj_1 | TensorFlow Library: 1.14.0\nalgo-1-iikpj_1 | EI Version: EI-1.4\nalgo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 172.18.0.1 - - [17\/Jun\/2020:05:29:47 +0000] \"POST \/invocations HTTP\/1.1\" 502 157 \"-\" \"-\"\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nJSONDecodeError Traceback (most recent call last)\nin ()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/serving.py in predict(self, data, initial_args)\n116 args[\"CustomAttributes\"] = self._model_attributes\n117\n--&gt; 118 return super(Predictor, self).predict(data, args)\n119\n120\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model)\n109 request_args = self._create_request_args(data, initial_args, target_model)\n110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n--&gt; 111 return self._handle_response(response)\n112\n113 def _handle_response(self, response):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in _handle_response(self, response)\n119 if self.deserializer is not None:\n120 # It's the deserializer's responsibility to close the stream\n--&gt; 121 return self.deserializer(response_body, response[\"ContentType\"])\n122 data = response_body.read()\n123 response_body.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in call(self, stream, content_type)\n578 \"\"\"\n579 try:\n--&gt; 580 return json.load(codecs.getreader(\"utf-8\")(stream))\n581 finally:\n582 stream.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n297 cls=cls, object_hook=object_hook,\n298 parse_float=parse_float, parse_int=parse_int,\n--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n300\n301\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 \"\"\"\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\n<\/code><\/pre>\n\n<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.\nI also tried Rest API post to docker deployed model:<\/p>\n\n<pre><code>curl -v -X POST \\ -H 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/Servo:predict\nbut still getting error:\n[![enter image description here][1]][1]\n<\/code><\/pre>\n\n<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.<\/p>\n\n<p>Please help me out. Thanks<\/p>",
        "Challenge_closed_time":1592876274340,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592375129670,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering challenges with deploying a tensorflow model with elastic inference on a notebook instance, resulting in errors such as \"jsondecodeerror: expecting value: line 1 column 1 (char 0)\" and \"curl -v -x post \\ -h 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/servo",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62422682",
        "Challenge_link_count":9,
        "Challenge_participation_count":1,
        "Challenge_readability":19.9,
        "Challenge_reading_time":261.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":164,
        "Challenge_solved_time":139.2068527778,
        "Challenge_title":"sagemaker notebook instance Elastic Inference tensorflow model local deployment",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":1504,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310699693432,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2889.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>Solved it. The error I was getting is due to roles\/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.2,
        "Solution_reading_time":4.99,
        "Solution_score_count":-1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4346333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi , i would like to know if it is possible to change the location of AzureML workspace after creating it ?  <br \/>\nRight now i do not find any option to change it manually on the UI. We want to move the server location to a different country.  <br \/>\nAny leads would be helpful. Thanks<\/p>",
        "Challenge_closed_time":1643796022067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643794457387,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on whether it is possible to change the location of an Azure ML workspace after it has been created, as they want to move the server location to a different country. They are unable to find an option to change it manually on the UI and are seeking assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/719406\/change-location-of-azure-ml-workspace",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4346333334,
        "Challenge_title":"change location of Azure ML workspace?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Based on the below document, ML workspace can't be moved across region. Probably, you will have to create a new resource in target region and move artifacts \/ pipelines \/ child resources to it (not so familiar with ML)     <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/move-support-resources#microsoftmachinelearning\">https:\/\/learn.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/move-support-resources#microsoftmachinelearning<\/a>    <\/p>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-workspace#limitations\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-workspace#limitations<\/a>    <\/p>\n<p>----------    <\/p>\n<p>Please don't forget to <strong>Accept Answer<\/strong> and <strong>Up-vote<\/strong> if the response helped -- Vaibhav<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.9,
        "Solution_reading_time":11.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":59.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":245.3151869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting the following error from the Evaluate Model module in Azure Machine Learning Designer:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/101409-screenshot-2021-06-01-at-100708-pm.png?platform=QnA\" alt=\"101409-screenshot-2021-06-01-at-100708-pm.png\" \/>    <\/p>\n<p>When I open the Assigned Data to Clusters module everything seems fine. I downloaded the output for Assigned Data to Clusters and played with cluster number 31 and there doesn't seem to be any issue. Additionally, I am using Azure Modules, so I am confused as to why this is failing. Please provide some clarity into this issue. This is a part of my pipeline:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/101399-screenshot-2021-06-01-at-104512-pm.png?platform=QnA\" alt=\"101399-screenshot-2021-06-01-at-104512-pm.png\" \/>    <\/p>\n<p>Additionally, it seems unless I successfully run the Evaluate Model module, I cannot create an inference pipeline. If this is untrue, please help me out here as well. There is no option for me to 'Create an Inference Pipeline' which shown in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\">this tutorial; step 1.<\/a>    <\/p>\n<p>Please let me know if you need any other information.    <\/p>\n<p>Thanks in advance.    <\/p>",
        "Challenge_closed_time":1623450957476,
        "Challenge_comment_count":3,
        "Challenge_created_time":1622567822803,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an ambiguous error in the Azure Machine Learning Designer's 'Evaluate Model' module. They have checked the output for the 'Assigned Data to Clusters' module and found no issues. The user is also confused about the inability to create an inference pipeline unless the 'Evaluate Model' module runs successfully. They are seeking clarity on the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/418016\/ambiguous-error-in-azure-machine-learning-designer",
        "Challenge_link_count":3,
        "Challenge_participation_count":4,
        "Challenge_readability":11.7,
        "Challenge_reading_time":18.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":245.3151869444,
        "Challenge_title":"Ambiguous error in Azure Machine Learning Designer 'Evaluate Model' Module",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":163,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Can you please check if the Assignment cluster 31 has NaN value? The <strong>Assign Data to Clusters<\/strong> leverages SKlearn, and from the error message, seems the Assignment column had NaN value which resulted in an error. If that's the case, let us know, so we can enable <strong>Evaluate<\/strong> Module module to deal with NaN values, and in the meantime, here's a short-term workaround:    <\/p>\n<ul>\n<li> Connect <strong>Clean Missing Data<\/strong> module to Assign Data to Cluster module, to clean the missing values.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/104943-image.png?platform=QnA\" alt=\"104943-image.png\" \/>    <\/li>\n<li> Use <strong>Edit Metadata<\/strong> module to convert Assignment to Integer and categorical type, this is because if Assignment column has NaN value before and its column type was double, we need to convert it to integer.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/104888-image.png?platform=QnA\" alt=\"104888-image.png\" \/>    <\/li>\n<li> Connect <strong>Edit Metadata<\/strong> to Evaluate Model module.    <\/li>\n<\/ul>\n<p>Hope this help!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":14.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":142.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":159.9731416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello :)     <\/p>\n<p>Do you have any kind of idea when Azure Machine Learning Python SDK V2 could support parallel computing? We are testing things out with the machine learning studio and we are in a bit confusing stage that should we go with the SDK V1 or V2, but seemingly the V2 is not yet supporting multiple nodes in compute clusters.    <\/p>\n<p>Best regards,    <br \/>\nTuomas<\/p>",
        "Challenge_closed_time":1657774403743,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657198500433,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about when Azure Machine Learning Python SDK V2 will support parallel computing as they are currently testing with the machine learning studio and are unsure whether to use SDK V1 or V2, as V2 does not yet support multiple nodes in compute clusters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/918129\/parallel-computing-with-python-sdk-v2",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.9,
        "Challenge_reading_time":5.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":159.9731416667,
        "Challenge_title":"Parallel computing with Python SDK V2",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=618a88ea-c762-4907-9a08-ae41864a250e\">@Tuomas Partanen  <\/a>     <\/p>\n<p>I have a good news for you, we are testing Parallel Run Step NOW in private preview of V2.     <\/p>\n<p>For your scenario, v1 is stable and serving all production customers. v2 (through DPv2) is still in private preview, and there are some dependency on new dataset\/mltable implementation. So if you want to seriously put some production traffic, I suggest guide to v1; but if you just want to have some prototypes, v2 may be better, as v2 is growing but v1 will not. Also, V2 will have the feature you want - Parallel.     <\/p>\n<p>The estimate time is not confirmed but should be around October.    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p><em>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/em>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":10.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":138.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.0508991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,  <\/p>\n<p>we have also found this example of using <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-use-databricks-as-compute-target.ipynb\">Databricks as a Compute Target for an Azure Machine Learning Pipeline<\/a>.  <\/p>\n<p>However, we want to use an existing Databricks Cluster as compute target within Azure Machine Learning Studio for our Azure Machine Learning Pipeline.  <br \/>\nCould you help us in accomplishing this, please?  <\/p>\n<p>With best regards  <br \/>\nAlex  <\/p>",
        "Challenge_closed_time":1654664851167,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654614267930,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking assistance in connecting an existing Databricks Cluster as a compute target within Azure Machine Learning Studio for their Azure Machine Learning Pipeline. They have found an example of using Databricks as a Compute Target for an Azure Machine Learning Pipeline but require guidance on how to use an existing cluster.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/880189\/connecting-to-an-existing-databricks-cluster-in-am",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.0508991667,
        "Challenge_title":"Connecting to an existing Databricks Cluster in AMLS",
        "Challenge_topic":"Cluster Computing",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@AlexanderPakakis-0994 Are you looking at adding the cluster from the UI of ML studio rather than using the SDK as mentioned in the notebook you referenced?    <br \/>\nIf Yes, you need to add the same attached compute.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209283-image.png?platform=QnA\" alt=\"209283-image.png\" \/>    <\/p>\n<p>Once you select Azure Databricks the following option to add the existing databricks workspace is seen.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/209260-image.png?platform=QnA\" alt=\"209260-image.png\" \/>    <\/p>\n<p>I hope this helps!!    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":10.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":13.2,
        "Solution_reading_time":13.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646758945343,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":0.2594925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been trying to train a BertSequenceForClassification Model using AWS Sagemaker. i'm using hugging face estimators. but I keep getting the error: <code>RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.73 GiB already allocated; 87.88 MiB free; 10.77 GiB reserved in total by PyTorch)<\/code> the same code runs fine on my laptop.<\/p>\n<ol>\n<li>how do I check what is occupying that 10GB of memory? my dataset is pretty small (68kb), so is my batch size (8) and epochs (1). When I run nvidia-smi, i can only see &quot;No processes running&quot; and the GPU memory usage is zero. When I run <code>print(torch.cuda.memory_summary(device=None, abbreviated=False))<\/code> from within my training script (right before it throws the error) it prints<\/li>\n<\/ol>\n<pre><code>|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|---------------------------------------------------------------------------|\n| Allocations           |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Active allocs         |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|===========================================================================|\n<\/code><\/pre>\n<p>but I have no idea what it means or how to interpret it<\/p>\n<ol start=\"2\">\n<li>when i run <code>!df -h<\/code> I can see:<\/li>\n<\/ol>\n<pre><code>Filesystem      Size  Used Avail Use% Mounted on\ndevtmpfs         30G   72K   30G   1% \/dev\ntmpfs            30G     0   30G   0% \/dev\/shm\n\/dev\/xvda1      109G   93G   16G  86% \/\n\/dev\/xvdf       196G   61M  186G   1% \/home\/ec2-user\/SageMaker\n<\/code><\/pre>\n<p>how is this memory different from the GPU? if theres 200GB in \/dev\/xvdf is there anyway I can just use that..? in my test script I tried<br \/>\n<code>model = BertForSequenceClassification.from_pretrained(args.model_name,num_labels=args.num_labels).to(&quot;cpu&quot;)<\/code>\nbut that just gives the same error<\/p>",
        "Challenge_closed_time":1646760128676,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646759194503,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"CUDA out of memory\" error while trying to train a BertSequenceForClassification model using AWS Sagemaker. The user is unsure of what is occupying the GPU memory and is unable to interpret the output of the memory summary. The user has also tried switching to CPU but still faces the same error. The user is also unsure of how the memory in \/dev\/xvdf is different from the GPU memory and if it can be used instead.",
        "Challenge_last_edit_time":1646767755128,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71398882",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":43.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":0.2594925,
        "Challenge_title":"CUDA: RuntimeError: CUDA out of memory - BERT sagemaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":1983.0,
        "Challenge_word_count":456,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597997723910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>A <code>CUDA out of memory<\/code> error indicates that your GPU RAM (Random access memory) is full. This is different from the storage on your device (which is the info you get following the <code>df -h<\/code> command).<\/p>\n<p>This memory is occupied by the model that you load into GPU memory, which is independent of  your dataset size. The GPU memory required by the model is at least twice the actual size of the model, but most likely closer to 4 times (initial weights, checkpoint, gradients, optimizer states, etc).<\/p>\n<p>Things you can try:<\/p>\n<ul>\n<li>Provision an instance with more GPU memory<\/li>\n<li>Decrease batch size<\/li>\n<li>Use a different (smaller) model<\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1646760473952,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":8.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":108.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":78.4313997222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to convert a numpy array to an amazon protobuf record using <code>sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set()<\/code> However, this is taking a really long time. <\/p>\n\n<p>I'm wondering how the function actually performs and how long it should take<\/p>\n\n<pre><code>from sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=get_execution_role(),\n                             train_instance_count=len(train_features),\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier',\n                                )\n<\/code><\/pre>\n\n<pre><code>numpy_array = np.array([[7.4727994e-01 9.5506465e-01 7.6940370e-01 8.2015032e-01 1.8113719e-01\n  7.8720862e-01 2.9677063e-01 2.6711187e-01 7.9498607e-01 4.4924998e-01\n  4.9533784e-01 2.6846960e-01 7.0506859e-01 4.1573554e-01 6.5843487e-01\n  3.2448095e-01 4.3870610e-01 7.2739214e-01 6.0914969e-01 5.5108833e-01\n  5.8835250e-01 5.5872935e-01 4.4392920e-01 6.8353373e-01 4.7664520e-01\n  5.6887656e-01 4.7034043e-01 4.1631639e-01 3.1357434e-01 5.5933639e-04]\n [5.7815754e-01 9.5828843e-01 7.7824914e-01 8.3188844e-01 2.3287645e-01\n  7.7196079e-01 2.5512937e-01 2.7032304e-01 7.8349811e-01 5.0130588e-01\n  4.8345023e-01 3.8397798e-01 5.9922373e-01 4.7720599e-01 6.7832541e-01\n  2.7788603e-01 4.6435007e-01 7.6100332e-01 7.7771670e-01 5.1536995e-01\n  5.8536130e-01 5.6407303e-01 5.0898582e-01 6.7815554e-01 3.0614817e-01\n  5.7353836e-01 3.8981739e-01 4.1474316e-01 3.1389123e-01 3.5031504e-04]]) \n<\/code><\/pre>\n\n<pre><code>record=model.record_set(numpy_array)\n<\/code><\/pre>\n\n<h2>Expected output<\/h2>\n\n<p>I expect the variable record to container a record ready for training with linearlearning model<\/p>",
        "Challenge_closed_time":1563444116136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563161763097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while converting a numpy array to an Amazon protobuf record using sagemaker.amazon.amazon_estimator.AmazonAlgorithmEstimatorBase.record_set(). The user is unsure about the time taken by the function to perform the conversion. The user expects the variable record to contain a record ready for training with a linear learning model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57032981",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.6,
        "Challenge_reading_time":23.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":78.4313997222,
        "Challenge_title":"how long does converting to amazon protobuf record using .record_set() take to complete",
        "Challenge_topic":"Model Training",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":119.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515320453680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangkok",
        "Poster_reputation_count":1848.0,
        "Poster_view_count":206.0,
        "Solution_body":"<p>I believe this is the problem:<\/p>\n\n<pre><code>train_instance_count=len(train_features)\n<\/code><\/pre>\n\n<p>This parameter is about infrastructure (how many SageMaker instances you want to train on), not about features. You should set it to 1.<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker import LinearLearner\nimport numpy as np\n\nmodel=LinearLearner(role=sagemaker.get_execution_role(),\n                             train_instance_count=1,\n                             train_instance_type='ml.t2.medium',\n                             predictor_type='binary_classifier')\n\nnumpy_array = np.array(...)\n\nrecord=model.record_set(numpy_array)\n# This takes &lt;100 ms on my t3 notebook instance\n\nprint(record)\n\n(&lt;class 'sagemaker.amazon.amazon_estimator.RecordSet'&gt;, {'s3_data':\n's3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-\n2019-07-18-09-48-21-639\/.amazon.manifest', 'feature_dim': 30, 'num_records': 2,\n's3_data_type': 'ManifestFile', 'channel': 'train'})\n<\/code><\/pre>\n\n<p>The manifest file lists the protobuf-encoded file(s):<\/p>\n\n<pre><code>[{\"prefix\": \"s3:\/\/sagemaker-eu-west-1-123456789012\/sagemaker-record-sets\/LinearLearner-2019-07-18-09-48-21-639\/\"}, \"matrix_0.pbr\"]\n<\/code><\/pre>\n\n<p>You can now use it for the training channel when you call fit(), re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.0,
        "Solution_reading_time":18.8,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":101.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1444758849803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11962.0,
        "Answerer_view_count":960.0,
        "Challenge_adjusted_solved_time":3.0605805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook? <\/p>\n\n<p>Like you would do a \"select * from tablename limit 10\" in an RDS using SQL, similarly is there a way to get a sense of the graph data through Jupyter Notebook?<\/p>",
        "Challenge_closed_time":1584902412783,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584891394693,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook, similar to how one can view data in an RDS using SQL.",
        "Challenge_last_edit_time":1637708419888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60801292",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.0,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.0605805556,
        "Challenge_title":"View Neptune Graph Schema using Jupyter notebook",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":831.0,
        "Challenge_word_count":56,
        "Platform":"Stack Overflow",
        "Poster_created_time":1584891066787,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:<\/p>\n\n<pre><code>g.V().groupCount().by(label)\ng.E().groupCount().by(label)\n<\/code><\/pre>\n\n<p>If you have a very large graph try putting something like <code>limit(100000)<\/code> before the <code>groupCount<\/code> step.<\/p>\n\n<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()<\/code> terminal step to the queries as in:<\/p>\n\n<pre><code>g.V().groupCount().by(label).next()\ng.E().groupCount().by(label).next()\n<\/code><\/pre>\n\n<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called \"person\".<\/p>\n\n<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()\n<\/code><\/pre>\n\n<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.81,
        "Solution_score_count":4.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":162.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405317204728,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":69.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":196.8378408333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an endpoint running a trained SageMaker model on AWS, which expects the data on a specific format.<\/p>\n<p>Initially, the data has been processed on the client side of the application, it means, the <code>API Gateway<\/code> (which receives the POST API calls on AWS) used to receive pre-processed data, but now there's a change, the <code>API Gateway<\/code> will receive <strong>raw data<\/strong> from the client, and the job of pre-processing this data before sending to our SageMaker model is up to our workflow.<\/p>\n<p>What is the best way to create a pre-processing job on this workflow, without needing to re-train the model? My pre-process is just a bunch of dataframe transformations, no standardization or calculation with the training set required (it would not need to save any model file).<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1602762793703,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602162886373,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has an endpoint running a trained SageMaker model on AWS, which expects data on a specific format. The API Gateway used to receive pre-processed data, but now it will receive raw data from the client, and the job of pre-processing this data before sending it to the SageMaker model is up to the workflow. The user is looking for the best way to create a pre-processing job on this workflow without needing to re-train the model.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64263330",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":10.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":166.640925,
        "Challenge_title":"Data Preprocessing on AWS SageMaker",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":450.0,
        "Challenge_word_count":134,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405317204728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":69.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>After some research, this is the solution I've followed:<\/p>\n<ul>\n<li>First I have created a <code>SKLearn<\/code> sagemaker model to do all the preprocess setup (I've built a Scikit-Learn custom class to handle all the preprocess steps, following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">AWS code<\/a>)<\/li>\n<li>Trained this preprocess model on my training data. My model, in specific, didn't need to be trained (it does not have any standardization or anything that would need to store training data parameters), but sagemaker requires the model to be trained.<\/li>\n<li>Loaded the trained legacy model that we had using the <code>Model<\/code> parameter.<\/li>\n<li>Created a <code>PipelineModel<\/code> with the preprocessing model and legacy model in cascade:<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>pipeline_model = PipelineModel(name=model_name,\n                               role=role,\n                               models=[\n                                    preprocess_model,\n                                    trained_model\n                               ])\n<\/code><\/pre>\n<ul>\n<li>Create a new endpoint, calling the <code>PipelineModel<\/code> and then changed the <code>Lambda<\/code> function to call this new endpoint. With this I could send the <strong>raw data<\/strong> directly for the same <code>API Gateway<\/code> and it would call only <strong>one<\/strong> endpoint, without needing to pay two endpoints 24\/7 to perform the entire process.<\/li>\n<\/ul>\n<p>I've found this to be a good and &quot;<em>economic<\/em>&quot; way to perform the preprocess outside the trained model, without having to do hard processing jobs on a <code>Lambda<\/code> function.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1602871502600,
        "Solution_link_count":1.0,
        "Solution_readability":18.0,
        "Solution_reading_time":22.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":200.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1319019150600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3073.0,
        "Answerer_view_count":341.0,
        "Challenge_adjusted_solved_time":4.8491525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use a XGBoost model in Sage Maker and use it to score for a large data stored in S3 using Batch Transform.<\/p>\n<p>I build the model using existing Sagemaker Container as follows:<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator(image_name=container, \n                                          hyperparameters=hyperparameters,\n                                          role=sagemaker.get_execution_role(),\n                                          train_instance_count=1, \n                                          train_instance_type='ml.m5.2xlarge', \n                                          train_volume_size=5, # 5 GB \n                                          output_path=output_path,\n                                          train_use_spot_instances=True,\n                                          train_max_run=300,\n                                          train_max_wait=600)\n\nestimator.fit({'train': s3_input_train,'validation': s3_input_test})\n<\/code><\/pre>\n<p>The following code is used to do Batch Transform<\/p>\n<pre><code> The location of the test dataset\nbatch_input = 's3:\/\/{}\/{}\/test\/examples'.format(bucket, prefix)\n\n# The location to store the results of the batch transform job\nbatch_output = 's3:\/\/{}\/{}\/batch-inference'.format(bucket, prefix)\n\ntransformer = xgb_model.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)\n\ntransformer.transform(data=batch_input, data_type='S3Prefix', content_type='text\/csv', split_type='Line')\n\ntransformer.wait()\n<\/code><\/pre>\n<p>The above code works fine in Development environment (Jupyter notebook) when the model is built in Jupyter. However, I would like to deploy the model and call its endpoint for Batch Transform.<\/p>\n<p>Most examples for SageMaker endpoint creation is for scoring on a single data and not for batch transform.<\/p>\n<p>Can someone point to how to deploy and use the endpoints for Batch Transform in SageMaker? Thank you<\/p>",
        "Challenge_closed_time":1603999757616,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603982300667,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in using AWS SageMaker Deployment for Batch Transform. They have successfully built a model using an existing SageMaker container and are able to perform Batch Transform in a development environment. However, they are unable to deploy the model and call its endpoint for Batch Transform. The user is seeking guidance on how to deploy and use endpoints for Batch Transform in SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64593327",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":21.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.8491525,
        "Challenge_title":"AWS SageMaker Deployment for Batch Transform",
        "Challenge_topic":"Batch Transform",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":2988.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1319019150600,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3073.0,
        "Poster_view_count":341.0,
        "Solution_body":"<p>The following link has an example of how to call a stored model in SageMaker to run Batch Transform job.<\/p>\n<p><a href=\"https:\/\/github.com\/YiranJing\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/xgboost_customer_churn\/customised_xgboost_customer_churn_batch_transform.ipynb\" rel=\"nofollow noreferrer\">Batch Transform Reference<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":23.2,
        "Solution_reading_time":5.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2124.5019444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**To Reproduce**\r\nI've deleted some of the unwanted notebooks from studio lab's files and now I am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Challenge_closed_time":1655626980000,
        "Challenge_comment_count":9,
        "Challenge_created_time":1647978773000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to open their project on Amazon Sagemaker, as the 'open project' button keeps loading indefinitely. The user has tried various solutions such as restarting the project, browser, laptop, clearing cache, and changing the environment from GPU to CPU, but nothing has worked. The user has attached a screenshot and requested assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.35,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2124.5019444444,
        "Challenge_title":"Unable to open database file, Unexpected error while saving file: d2l-pytorch-sagemaker-studio-lab\/dash\/Untitled.ipynb unable to open database file",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey there, I\u2019m not one of the devs sorry\r\n\r\nBut i wanna ask if you can start up a GPU runtime? Kindly Try and let me know thanks @lorazabora  while launching the GPU instance I am getting this as there is no runtime environment available available right now \r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159628994-38b1c339-ac43-4f6a-8ac7-56f32a8174f9.png)\r\n So then indeed everyone is experiencing the same issue\r\n\r\nIt\u2019s been over a week now and not yet fixed, CPU runtimes aren\u2019t enough for my workloads neither that they even make much sense since there\u2019s already many free cloud CPU options out there\r\n\r\nI hope they see and fix this soon @someshfengde Thank you for reporting the problem. Would you please tell us how did you delete the notebooks? Because only delete the specific files does not affect the behavior of Jupyter Lab. We need to know the procedure to reproduce your problem.\r\n\r\nIf you need the Studio Lab as soon as possible, recreate the account is one of the option.\r\n Yes I tried to delete all notebooks from directory maybe because of that\n\nHow can I recreate account?\n\n\nOn Thu, Mar 24, 2022, 13:48 Takahiro Kubo ***@***.***> wrote:\n\n> @someshfengde <https:\/\/github.com\/someshfengde> Thank you for reporting\n> the problem. Would you please tell us how did you delete the notebooks?\n> Because only delete the specific files does not affect the behavior of\n> Jupyter Lab. We need to know the procedure to reproduce your problem.\n>\n> If you need the Studio Lab as soon as possible, recreate the account is\n> one of the option.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94#issuecomment-1077354727>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AKBFX5JUOPOEUHKEZGXZF53VBQQN3ANCNFSM5RL44VJA>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @someshfengde You can delete the account from here.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/160840123-5f628318-f158-4e40-abba-29524ceb4248.png)\r\n Dear @someshfengde , to delete the account was worked for you? If you still have problem, please let us know. If you already solved the problem, please let us know by closing the this issue. Hi @icoxfog417  I resolved the issue by deleting and opening the account again .\r\n Thanks for your response :smile:  closing issue now :)  You are welcome! We are very glad if Studio Lab supports your data science learning.\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":7.6,
        "Solution_reading_time":30.5,
        "Solution_score_count":null,
        "Solution_sentence_count":25.0,
        "Solution_word_count":349.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1397184643572,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":74.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":91.3691802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy my model (container) on AWS SageMaker. I've pushed the container to AWS ECR.\nThen I use an AWS Lambda that basically runs <code>create_training_job()<\/code> via the <code>boto3<\/code> SageMaker client. It runs the container in <strong>train<\/strong> mode and puts the generated artifact to the S3. Like that:<\/p>\n<pre><code>sm = boto3.client('sagemaker')\n\nsm.create_training_job(\n        TrainingJobName=full_job_name,\n        HyperParameters={\n            'general': json.dumps(\n                {\n                    'environment': ENVIRONMENT,\n                    'region': REGION,\n                    'version': date_suffix,\n                    'hyperparameter_tuning': training_params.get('hyperparameter_tuning', False),\n                    'basket_analysis': training_params.get('basket_analysis', True),\n                    'init_inventory_cache': training_params.get('init_inventory_cache', True),\n                }\n            ),\n            'aws_profile': '***-dev',\n            'db_config': json.dumps(database_mapping),\n            'model_server_params': json.dumps(training_params.get('model_server_params', {}))\n        },\n\n        AlgorithmSpecification={\n            'TrainingImage': training_image,\n            'TrainingInputMode': 'File',\n        },\n        RoleArn=ROLE_ARN,\n        OutputDataConfig={\n            'S3OutputPath': S3_OUTPUT_PATH\n        },\n        ResourceConfig={\n            'InstanceType': INSTANCE_TYPE,\n            'InstanceCount': 1,\n            'VolumeSizeInGB': 20,\n        },\n        # VpcConfig={\n        #     'SecurityGroupIds': SECURITY_GROUPS.split(','),\n        #     'Subnets': SUBNETS.split(',')\n        # },\n        StoppingCondition={\n            'MaxRuntimeInSeconds': int(MAX_RUNTIME_SEC),\n            #        'MaxWaitTimeInSeconds': 1800\n        },\n        Tags=[ ],\n        EnableNetworkIsolation=False,\n        EnableInterContainerTrafficEncryption=False,\n        EnableManagedSpotTraining=False,\n    )\n<\/code><\/pre>\n<p>I have a logger inside the container that says that <code>opt\/ml\/input\/config\/hyperparameters.json<\/code> now exists. It has been added by SageMaker. Fine.<\/p>\n<p>But then, when I try to run the same container in <code>serve<\/code> mode (so basically to deploy it) I encounter that <code>opt\/ml\/input\/config\/hyperparameters.json<\/code> doesn't exist anymore. I deploy it this way:<\/p>\n<pre><code>     sm.create_model(\n        ModelName=model_name,\n        PrimaryContainer={\n            'Image': training_image,\n            'ModelDataUrl': model_artifact,\n            'Environment': {\n                'version': version\n            }\n        },\n        ExecutionRoleArn=role_arn,\n        Tags=[ ],\n        # VpcConfig = {\n        #     'SecurityGroupIds': os.environ['security_groups'].split(','),\n        #     'Subnets': os.environ['subnets'].split(',')\n        # }\n    )\n\n    sm.create_endpoint_config(\n        EndpointConfigName=config_name,\n        ProductionVariants=[\n            {\n                'VariantName': variant_name,\n                'ModelName': model_name,\n                'InitialInstanceCount': instance_count,\n                'InstanceType': instance_type,\n                'InitialVariantWeight': 1\n            },\n        ],\n        Tags=[ ],\n    )\n\n    existing_endpoints = sm.list_endpoints(NameContains=endpoint_name)\n\n    scaling_resource_id = f'endpoint\/{endpoint_name}\/variant\/{variant_name}'\n\n    if not existing_endpoints['Endpoints']:\n        sm.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=config_name\n        )\n    else:\n        if aas.describe_scalable_targets(\n                ServiceNamespace='sagemaker',\n                ResourceIds=[scaling_resource_id],\n                ScalableDimension='sagemaker:variant:DesiredInstanceCount')['ScalableTargets']:\n            aas.deregister_scalable_target(\n                ServiceNamespace='sagemaker',\n                ResourceId=scaling_resource_id,\n                ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n            )\n\n        sm.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=config_name\n        )\n<\/code><\/pre>\n<p>It is important since it seemed to be a convenient way to pass some parameters inside the container from outside (like management console).<\/p>\n<p>I thought that this file\/directory will still exist after the <strong>train<\/strong>. Any ideas?<\/p>",
        "Challenge_closed_time":1629579807236,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629250878187,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with AWS SageMaker where the directory that SageMaker was supposed to create automatically is not present when trying to run the container in serve mode. The user is using an AWS Lambda to run create_training_job() via the boto3 SageMaker client and has a logger inside the container that says that opt\/ml\/input\/config\/hyperparameters.json now exists. However, when trying to run the same container in serve mode, the user encounters that opt\/ml\/input\/config\/hyperparameters.json doesn't exist anymore. The user is looking for ideas on why this file\/directory is not present after the train.",
        "Challenge_last_edit_time":1629669900607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68825648",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":22.1,
        "Challenge_reading_time":47.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":91.3691802778,
        "Challenge_title":"Why is there no specific directory that SageMaker was supposed to create automatically?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Compute Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":262,
        "Platform":"Stack Overflow",
        "Poster_created_time":1496401724783,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":165.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>tl;dr: two options:<\/p>\n<ul>\n<li>Copy the <code>hyperparameters.json<\/code> file to <code>\/opt\/ml\/model<\/code> in the training logic and it will be packed with the model artifacts;<\/li>\n<li>Pass whatever parameters you want through the <code>PrimaryContainer<\/code> parameter's <code>Environment<\/code> property.<\/li>\n<\/ul>\n<p>Long version:<\/p>\n<p>That file, <code>opt\/ml\/input\/config\/hyperparameters.json<\/code>, (in fact the whole <code>\/opt\/ml\/input<\/code> folder) is mounted on the <strong>training<\/strong> container when it is created. It is provided by SageMaker, based on information you provide, <em>only<\/em> for training purposes. SageMaker does not change your container in any way, and it doesn't preserve this or any configuration file it passes to the training job once training is done. If you want to pass parameters to the inference endpoint, that is not the way.<\/p>\n<p>You <em>could<\/em> copy the <code>hyperparameters.json<\/code> file to the <code>\/opt\/ml\/model<\/code> folder, and it'd be packed with the model in the <code>model.tar.gz<\/code> tarball. Your infrence code could then use that - but that's not the prescribed way to pass parameters to an endpoint, and it cause problems with your framework.<\/p>\n<p>The generally prescribed way to pass parameters to SageMaker endpoints is through the <strong>environment<\/strong>. If you check the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">boto3 docs for create_model<\/a>, you'll see that there's an <code>Environment<\/code> key within the <code>PrimaryContainer<\/code> parameter (also for each of the <code>Containers<\/code> parameter). In fact, your code above already uses that to pass a <code>version<\/code> parameter. You should use that to pass any parameters to your model and, from there, to the endpoint based on it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.0,
        "Solution_reading_time":24.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":236.0,
        "Tool":"Amazon SageMaker"
    }
]