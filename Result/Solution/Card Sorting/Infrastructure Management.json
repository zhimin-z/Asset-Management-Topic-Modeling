[
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":455.8612047222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>I am doing the Challenge. <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/intro-to-azure-machine-learning-service\/\">https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/intro-to-azure-machine-learning-service\/<\/a>    <\/p>\n<p>Please see what I have installed:    <\/p>\n<blockquote>\n<p>pip install azureml-sdk    <\/p>\n<\/blockquote>\n<p>I am getting the following messages at the end:    <\/p>\n<blockquote>\n<p>ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.    <\/p>\n<p>We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.    <\/p>\n<p>jupyterlab 2.2.9 requires jupyterlab-server&lt;2.0,&gt;=1.1.5, which is not installed.    <br \/>\nSuccessfully installed applicationinsights-0.11.9 azure-identity-1.4.1 azureml-automl-core-1.19.0 azureml-dataprep-2.6.3 azureml-dataprep-native-26.0.0 azureml-dataprep-rslex-1.4.0 azureml-dataset-runtime-1.19.0.post1 azureml-pipeline-1.19.0 azureml-pipeline-core-1.19.0 azureml-pipeline-steps-1.19.0 azureml-sdk-1.19.0 azureml-telemetry-1.19.0 azureml-train-1.19.0 azureml-train-automl-client-1.19.0 azureml-train-core-1.19.0 azureml-train-restclients-hyperdrive-1.19.0 distro-1.5.0 dotnetcore2-2.1.20 fusepy-3.0.1 msal-1.8.0 msal-extensions-0.2.2 numpy-1.19.3 portalocker-1.7.1 pyarrow-1.0.1 pywin32-227    <\/p>\n<\/blockquote>\n<p>Now I am trying to start up and type the following in .py file in Visual Studio Code    <\/p>\n<blockquote>\n<p>from azureml.core import Workspace    <\/p>\n<\/blockquote>\n<p>This is the error message I am getting:    <\/p>\n<blockquote>\n<p> File &quot;c:\/Users\/User\/OneDrive\/Desktop\/New folder\/Build AI Solution\/automl_python.py&quot;, line 1, in &lt;module&gt;    <br \/>\n    from azureml.core import Workspace    <br \/>\nModuleNotFoundError: No module named 'azureml'    <\/p>\n<\/blockquote>\n<p>Please could you help me?    <\/p>\n<p>thanks,    <\/p>\n<p>Naveen<\/p>",
        "Challenge_closed_time":1610753174427,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609112074090,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/211503\/modulenotfounderror-no-module-named-azureml",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":26.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":455.8612047222,
        "Challenge_title":"ModuleNotFoundError: No module named 'azureml'",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":190,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>This is now solved. Thanks!<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-1.9,
        "Solution_reading_time":0.44,
        "Solution_score_count":29.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1114.6180555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\ndoing\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show-mlflow-logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![Screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![Screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![Screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### To reproduce\r\n\r\nsee above\r\n\r\n### Expected behavior\r\n\r\nconvert the experiment by ID\r\n\r\n### Environment\r\n\r\n- Aim Version 3.6\r\n- Python 3.8.1\r\n- pip3\r\n- Ubuntu 20.04.3 LTS\r\n",
        "Challenge_closed_time":1649939186000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645926561000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aimhubio\/aim\/issues\/1415",
        "Challenge_link_count":4,
        "Challenge_participation_count":6,
        "Challenge_readability":13.3,
        "Challenge_reading_time":14.53,
        "Challenge_repo_contributor_count":47.0,
        "Challenge_repo_fork_count":181.0,
        "Challenge_repo_issue_count":2399.0,
        "Challenge_repo_star_count":2909.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1114.6180555556,
        "Challenge_title":"aim convert mlflow --experiment fails for experiment id, works for experiment name",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @luisoala, thanks for reporting the issue!\r\n@devfox-se could you please take a look at this? Thanks for reporting this @luisoala, will take a look soon! Hey @luisoala! We've released `v3.6.2` containing the fix for mlflow converter. Please check it out and let me know if there are any issues. thanks @alberttorosyan working through a few other deadlines atm, aiming for a test ~ next tuesday, will share result here @luisoala Hi, have you had a chance to test this?:) Closing due to inactivity, feel free to reopen in case this still persists.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":6.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1473257955532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":145.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":26.3318861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have read similar questions regarding azure app service, but I still can't find an answer. I was trying to deploy a model in azure kubernetes service, but I came across an error when importing cv2 (which is essential to me).<\/p>\n<p>Opencv-python is included in my environment .yaml file:<\/p>\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  # You must list azureml-defaults as a pip dependency\n  - azureml-defaults&gt;=1.0.45\n  - Cython\n  - matplotlib&gt;=3.2.2\n  - numpy&gt;=1.18.5\n  - opencv-python&gt;=4.1.2\n  - pillow\n  - PyYAML&gt;=5.3\n  - scipy&gt;=1.4.1\n  - torch&gt;=1.6.0\n  - torchvision&gt;=0.7.0\n  - tqdm&gt;=4.41.0\nchannels:\n- conda-forge\n<\/code><\/pre>\n<p>I am deploying as follows:<\/p>\n<pre><code>aks_service = Model.deploy(ws,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_config=gpu_aks_config,\n                       deployment_target=aks_target,\n                       name=aks_service_name)\n<\/code><\/pre>\n<p>And I get this error:<\/p>\n<pre><code>    Traceback (most recent call last):\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n    self.load_wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n    return self.load_wsgiapp()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n    __import__(module)\n  File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n    from app import main\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 23, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 8, in &lt;module&gt;\n    import cv2\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/cv2\/__init__.py&quot;, line 5, in &lt;module&gt;\n    from .cv2 import *\nImportError: libGL.so.1: cannot open shared object file: No such file or directory\nWorker exiting (pid: 41)\n<\/code><\/pre>",
        "Challenge_closed_time":1603897093470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603802298680,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64554615",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":41.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":26.3318861111,
        "Challenge_title":"Import cv2 error when deploying in Azure Kubernetes Service - python",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":484.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473257955532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>This might go wrong at some point, but my workaround was installing opencv-python-headless instead of opencv.<\/p>\n<p>In the environment .yaml file, just replace:<\/p>\n<pre><code>- opencv-python&gt;=4.1.2\n<\/code><\/pre>\n<p>with:<\/p>\n<pre><code>- opencv-python-headless\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":3.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.8048411111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5.<\/p>\n<p>I use env variables to set the IP Address of my clearml-server.<\/p>\n<pre><code>export CLEARML_HOST_IP=127.0.0.1\nexport TRAINS_HOST_IP=127.0.0.1\n<\/code><\/pre>\n<p>But it still is available thorugh the external server IP.\nHow can I deactivate the listeners for external IP in clearml-server config?<\/p>\n<p>Edit:\nAccording to this:\nI use SSH Port forward to access local instance from my computer outside of the network. But I can't access custom uploaded images (task-&gt; debug samples) as they will not use my port forwarded URLs.<\/p>",
        "Challenge_closed_time":1610303095156,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610294996320,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1610296597728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65655382",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":2.2496766667,
        "Challenge_title":"ClearML server IP address not used with localhost and SSH port forwarding",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1604391794420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":89.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer: I'm a ClearML (Trains) team member<\/p>\n<p>Basically the docker-compose will expose only the API\/Web\/File server , you can further limit the exposure to your localhost only, by changing the following section in your ClearML server <a href=\"https:\/\/github.com\/allegroai\/clearml-server\/blob\/master\/docker\/docker-compose.yml\" rel=\"nofollow noreferrer\">docker-compose.yml<\/a><\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>networks:\n  backend:\n    driver:\n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n  frontend:\n    driver: \n      bridge\n    driver_opts:\n      com.docker.network.bridge.host_binding_ipv4: &quot;127.0.0.1&quot;\n\n<\/code><\/pre>\n<p>Based on docker's <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/network_create\/\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.9,
        "Solution_reading_time":11.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":62.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1524670343368,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Z\u00fcrich, Schweiz",
        "Answerer_reputation_count":460.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":415.3991708334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to run a test script on an existing compute instance of Azure using the Azure Machine Learning extension to the Azure CLI:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg\n<\/code><\/pre>\n<p>I get a Service Error with the following error message:<\/p>\n<pre><code>Unable to run conda package manager. AzureML uses conda to provision python\\nenvironments from a dependency specification. To manage the python environment\\nmanually instead, set userManagedDependencies to True in the python environment\\nconfiguration. To use system managed python environments, install conda from:\\nhttps:\/\/conda.io\/miniconda.html\n<\/code><\/pre>\n<p>But when I connect to the compute instance through the Azure portal and select the default Python kernel, <code>conda --version<\/code> prints 4.5.12. So conda is effectively already installed on the compute instance. This is why I do not understand the error message.<\/p>\n<p>Further information on the azure versions:<\/p>\n<pre><code>  &quot;azure-cli&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-core&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-telemetry&quot;: &quot;1.0.6&quot;,\n  &quot;extensions&quot;: {\n    &quot;azure-cli-ml&quot;: &quot;1.15.0&quot;\n  }\n<\/code><\/pre>\n<p>The image I use is:<\/p>\n<pre><code>mcr.microsoft.com\/azure-cli:latest\n<\/code><\/pre>\n<p>Can somebody please explain as to why I am getting this error and help me resolve the error? Thank you!<\/p>\n<p>EDIT: I tried to update the environment in which the <code>az ml run<\/code>-command is run.\nEssentially this is my GitLab job. The installation of miniconda is a bit complicated as the azure-cli uses an alpine Linux image (reference: <a href=\"https:\/\/stackoverflow.com\/questions\/47177538\/installing-miniconda-on-alpine-linux-fails\">Installing miniconda on alpine linux fails<\/a>). I replaced some names with ... and cut out some irrelevant pieces of code.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>test:\n  image: 'mcr.microsoft.com\/azure-cli:latest'\n  script:\n    - echo &quot;Download conda&quot;\n    - apk --update add bash curl wget ca-certificates libstdc++ glib\n    - wget -q -O \/etc\/apk\/keys\/sgerrand.rsa.pub https:\/\/raw.githubusercontent.com\/sgerrand\/alpine-pkg-node-bower\/master\/sgerrand.rsa.pub\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-2.23-r3.apk&quot; -o glibc.apk\n    - apk del libc6-compat\n    - apk add glibc.apk\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-bin-2.23-r3.apk&quot; -o glibc-bin.apk \n    - apk add glibc-bin.apk \n    - curl -L &quot;https:\/\/github.com\/andyshinn\/alpine-pkg-glibc\/releases\/download\/2.25-r0\/glibc-i18n-2.25-r0.apk&quot; -o glibc-i18n.apk\n    - apk add --allow-untrusted glibc-i18n.apk \n    - \/usr\/glibc-compat\/bin\/localedef -i en_US -f UTF-8 en_US.UTF-8 \n    - \/usr\/glibc-compat\/sbin\/ldconfig \/lib \/usr\/glibc\/usr\/lib\n    - rm -rf glibc*apk \/var\/cache\/apk\/*\n    - echo &quot;yes&quot; | curl -sSL https:\/\/repo.continuum.io\/miniconda\/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh\n    - echo &quot;Install conda&quot;\n    - (echo -e &quot;\\n&quot;; echo &quot;yes&quot;; echo -e &quot;\\n&quot;; echo &quot;yes&quot;) | bash -bfp miniconda.sh\n    - echo &quot;Installing Azure Machine Learning Extension&quot;\n    - az extension add -n azure-cli-ml\n    - echo &quot;Azure Login&quot;\n    - az login\n    - az account set --subscription ...\n    - az configure --defaults group=...\n    - az ml folder attach -w ... \n    - az ml run submit-script test.py --target ... --experiment-name hello_world --resource-group ...\n<\/code><\/pre>",
        "Challenge_closed_time":1603466200947,
        "Challenge_comment_count":6,
        "Challenge_created_time":1601897074350,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1601970763932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64207678",
        "Challenge_link_count":7,
        "Challenge_participation_count":9,
        "Challenge_readability":11.6,
        "Challenge_reading_time":48.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":435.8684991667,
        "Challenge_title":"How to avoid error \"conda --version: conda not found\" in az ml run --submit-script command?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1089.0,
        "Challenge_word_count":373,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524670343368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Z\u00fcrich, Schweiz",
        "Poster_reputation_count":460.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>One needs to pass the <code>--workspace-name<\/code> argument to be able to run it on Azure's compute target and not on the local compute target:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg --workspace-name test-ws\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":4.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5344444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi everyone,\n\nhow can i install custom OS libraries on Sagemaker studio?  When I open a terminal it states:\n\nroot@0f04278e59cf:~\/# yum install unzip\n\nbash: yum: command not found\n\n\nThanks!",
        "Challenge_closed_time":1592471900000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592469976000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668596322740,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUZZFjMw_gS5Cz8sh-TK4J3w\/custom-packages-in-sagemaker-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.5344444444,
        "Challenge_title":"Custom packages in Sagemaker studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":616.0,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"**Short answer:** [Studio UI] > File > New > Terminal > sudo yum install unzip  \nThen unzip away...\n\n**Long answer:**  \nYou can open a terminal in two different types of compute environment: \n\n 1. On a specific kernel you're running: [Studio UI] > kernal tab > Terminial icon.\n 2. On the compute studio (jupyter) itself: [Studio UI] > File > New > Terminal \n\nIn both options your personal files folder is accessible. In a kernel terminal: \/root. In a Jupyter terminal: \/home\/sagemaker-user.  \nWhen opening a kernel terminal you'll have access to the software that is part of the kernel's container (say tensorflow container). Which in your case is missing yum. You can of course try apt-get, and such to install more tools.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925551743,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":8.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":112.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":328.67735,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently deploying a model trained using AzureML to an AKS cluster as follows:<\/p>\n<pre><code>deployment_config_aks = AksWebservice.deploy_configuration(\n    cpu_cores = 1, \n    memory_gb = 1)\n\nservice = Model.deploy(ws, &quot;test&quot;, [model], inference_config, deployment_config_aks, aks_target)\n\n<\/code><\/pre>\n<p>I would like this service to be scheduled on a specific nodepool. With normal Kubernetes deployment, I can specify a <code>nodeSelector<\/code> like:<\/p>\n<pre><code>spec:\n  nodeSelector:\n    myNodeName: alpha\n<\/code><\/pre>\n<p>How do I specify a <code>nodeSelector<\/code> while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>",
        "Challenge_closed_time":1650522075520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649338837060,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71783228",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.0,
        "Challenge_reading_time":10.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":328.67735,
        "Challenge_title":"How do I specify nodeSelector while deploying an Azure ML model to an AKS Cluster?",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":289.0,
        "Challenge_word_count":107,
        "Platform":"Stack Overflow",
        "Poster_created_time":1338974093052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Beijing, China",
        "Poster_reputation_count":1830.0,
        "Poster_view_count":155.0,
        "Solution_body":"<blockquote>\n<p>How do I specify a nodeSelector while deploying an Azure ML model to an AKS Cluster? Or in general, is there a way to merge my custom pod spec with the one generated by Azure ML library?<\/p>\n<\/blockquote>\n<p>As per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-arc-kubernetes?tabs=studio\" rel=\"nofollow noreferrer\">Configure Kubernetes clusters for machine learning<\/a>:<\/p>\n<p><code>nodeSelector<\/code> : Set the node selector so the extension components and the training\/inference workloads will only be deployed to the nodes with all specified selectors.<\/p>\n<p>For example:<\/p>\n<p><code>nodeSelector.key=value<\/code> , <code>nodeSelector.node-purpose=worker<\/code> and <code>nodeSelector.node-region=eastus<\/code><\/p>\n<p>You can refer to <a href=\"https:\/\/kubernetes.io\/docs\/concepts\/scheduling-eviction\/assign-pod-node\/#built-in-node-labels\" rel=\"nofollow noreferrer\">Assigning Pods to Nodes<\/a> and <a href=\"https:\/\/github.com\/Azure\/AKS\/issues\/2866\" rel=\"nofollow noreferrer\">Cannot create nodepool with node-restriction.kubernetes.io\/ prefix label<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":17.9,
        "Solution_reading_time":14.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":102.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":295.4369444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\nInstalling `kedro-datasets[option]` installs a different set of dependencies than `kedro[option]`. It appears that `kedro-datasets[option]` is installing the superset of requirements for all datasets.\n\n## Context\nThis is currently blocking https:\/\/github.com\/kedro-org\/kedro\/issues\/1495\n\n## Steps to Reproduce\n1. `pip install \"kedro[pandas.CSVDataSet]\"; pip freeze > requirements-kedro.txt`\n2. `pip install \"kedro-datasets[pandas.CSVDataSet]\"; pip freeze > requirements-kedro-datasets.txt`\n3. Compare the requirements\n",
        "Challenge_closed_time":1667929584000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1666866011000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/64",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":12.7,
        "Challenge_reading_time":8.27,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":295.4369444444,
        "Challenge_title":"pip installing kedro-datasets[option] causes different dependencies to installing kedro[option]",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":1833.7168480556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I trained a model with the built-in RESnet18 docker image, and now I want to deploy the model to an endpoint and classify ~ 1 million images. I have all my training, validation, and test images stored on S3 in RecordIO format (converted with <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/basic\/data.html?highlight=im2rec\" rel=\"nofollow noreferrer\">im2rec.py<\/a>). According to the <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>The Amazon SageMaker Image Classification algorithm supports both RecordIO (application\/x-recordio) and image (application\/x-image) content types for training. The algorithm supports only\u00a0application\/x-image\u00a0for inference.<\/p>\n<\/blockquote>\n\n<p>So I cannot perform inference on my training data in RecordIO format. To overcome this I copied all the raw .jpg images (~ 2GB) onto my Sagemaker Jupyter Notebook instance and performed inference one at a time in the following way:<\/p>\n\n<pre><code>img_list = os.listdir('temp_data') # list of all ~1,000,000 images\n\nfor im in img_list:\n    with open('temp_data\/'+im, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n                                       ContentType='application\/x-image', \n                                       Body=payload)\n\n    etc...\n<\/code><\/pre>\n\n<p>Needless to say, transferring all the data onto my Notebook instance took a long time and I would prefer not having to do that before running inference. Why does the SageMaker Image Classification not support RecordIO for inference? And more importantly, what is the best way to run inference on many images without having to move them from S3?<\/p>",
        "Challenge_closed_time":1531369355823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524767975170,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50049928",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":22.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":1833.7168480556,
        "Challenge_title":"Sagemaker image classification: Best way to perform inference on many images in S3?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2600.0,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Poster_created_time":1474520506390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":832.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>The RecordIO format is designed to pack a large number of images into a single file, so I don't think it would work well for predicting single images.<\/p>\n\n<p>When it comes to prediction, you definitely don't have to copy images to a notebook instance or to S3. You just have to load them from anywhere and inline them in your prediction requests.<\/p>\n\n<p><strong>If you want HTTP-based prediction, here are your options:<\/strong><\/p>\n\n<p>1) Use the SageMaker SDK Predictor.predict() API on any machine (as long as it has proper AWS credentials) <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<p>2) Use the AWS Python SDK (aka boto3) API invoke_endpoint() on any machine (as long as it has proper AWS credentials)<\/p>\n\n<p>You can even build a simple service to perform pre-processing or post-processing with Lambda. Here's an example: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>\n\n<p><strong>If you want batch prediction:<\/strong>\n the simplest way is to retrieve the trained model from SageMaker, write a few lines of ad-hoc MXNet code to load it and run all your predictions. Here's an example: <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.1,
        "Solution_reading_time":20.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":177.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1852777778,
        "Challenge_answer_count":4,
        "Challenge_body":"Hi everybody,\r\n\r\nI am trying to use AWS built-in algorithms in Sagemaker Studio Lab. For that I need an execution role and region etc. \r\nWhen I try to run my code it outputs\r\n\r\nValueError: Must setup local AWS configuration with a region supported by SageMaker.\r\n\r\nIs it even possible to link access AWS resources in Studiolab?\r\n\r\nMany thanks in advance!\r\n\r\n\r\n",
        "Challenge_closed_time":1639666969000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639662702000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.16,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.1852777778,
        "Challenge_title":"Can't configure profile with AWS CLI for using AWS Built-in sagemaker algorithms ",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":72,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! The use case you are describing is exactly why we have this example notebook:\r\n- https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/connect-to-aws\/Access_AWS_from_Studio_Lab.ipynb \r\n\r\nYour net net is:\r\n1\/ Ensure you have proper training and authorization to use your AWS access and secret keys. If you are an AWS account admin, you can find this in your IAM console. If you are not, work with your admin to determine if this pattern is appropriate for you. \r\n\r\n2\/ If you are qualified to manage your AWS keys, create a new file called `~\/.aws\/credentials`. There, copy and paste in your access and secret keys.\r\n\r\n3\/ Remove any cells you ran in a notebook to create or verify those files.\r\n\r\n4\/ Create a SageMaker execution role. The easiest way to do this is via the console - you can create a new SageMaker execution role when create a notebook instance or a Studio user profile. Once this is done, paste in the arn (Amazon Resource Name), for this execution role.\r\n\r\n5\/ Go forth and scale up on SageMaker! After that,  once you are using the SageMaker Python SDK, you should be able to use all code-based features within SageMaker, such as training, hosting, tuning, autopilot, pipelines, etc.   Hi @EmilyWebber. Might you also have an example that doesn't require AWS account information yet avoids \"ValueError: Must setup local AWS configuration with a region supported by SageMaker\" in the code below for \"role\"?\r\n\r\n```\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n\ttransformers_version='4.6.1',\r\n\tpytorch_version='1.7.1',\r\n\tpy_version='py36',\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n```\r\nCode source: https:\/\/huggingface.co\/distilbert-base-uncased-finetuned-sst-2-english, where Deploy is Amazon SageMaker rather than Accelerated Inference \u2014 the latter [currently returns an error](https:\/\/discuss.huggingface.co\/t\/distilbert-accelerated-inference-error\/15027) with or without SageMaker\r\n\r\nSince the SageMaker Studio Lab website emphasizes no costs and no need to sign up for an AWS account, a team and I are exploring whether to have students try. Thank you in advance. Hi @derekschan - thanks for the comment. This is actually expected behavior right now - Studio Lab defaults to not having access to any AWS API's unless explicitly granted. To date, that is solved only by the pattern mentioned above in this issue, explicitly installing AWS key permissions to utilize them. \r\n\r\nShould that ever change in the future we will be sure to let you know! I'll mark your comment as a feature enhancement.  Thank you, @EmilyWebber.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.3,
        "Solution_reading_time":31.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":372.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1601004709207,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":1074.0,
        "Answerer_view_count":143.0,
        "Challenge_adjusted_solved_time":11.2046330556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running MLflow Project for a model using following command from my ubuntu 20.04 terminal<\/p>\n<pre><code>mlflow run . --no-conda -P alpha=0.5\n<\/code><\/pre>\n<p>My system doesn't have conda or python (It does however have python3). So, I added alias for python using terminal<\/p>\n<pre><code>alias python='python3'\n<\/code><\/pre>\n<p>After which I could open python in terminal using <code>python<\/code>. However, I still got the same error<\/p>\n<pre><code>2021\/11\/21 08:07:34 INFO mlflow.projects.utils: === Created directory \/tmp\/tmpp4h595ql for downloading remote URIs passed to arguments of type 'path' ===\n2021\/11\/21 08:07:34 INFO mlflow.projects.backend.local: === Running command 'python tracking.py 0.5 0.1' in run with ID 'e50ca47b3f8848a083906be6220c26fc' === \nbash: python: command not found\n2021\/11\/21 08:07:34 ERROR mlflow.cli: === Run (ID 'e50ca47b3f8848a083906be6220c26fc') failed ===\n<\/code><\/pre>\n<p>How to get rid of this error?<\/p>",
        "Challenge_closed_time":1637463754447,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637462657297,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70051420",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3047638889,
        "Challenge_title":"MLFlow projects; bash: python: command not found",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601004709207,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":1074.0,
        "Poster_view_count":143.0,
        "Solution_body":"<p>Change <code>python<\/code> to <code>python3<\/code> in the <code>MLproject<\/code> file to the resolve error.<\/p>\n<pre><code>command: &quot;python3 tracking.py {alpha} {l1_ratio}&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1637502993976,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":18.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1621410539876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3579.0,
        "Answerer_view_count":1775.0,
        "Challenge_adjusted_solved_time":4.1121516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>got a folder called data-asset which contains a yaml file with the following<\/p>\n<pre><code>type: uri_folder\nname: &lt;name_of_data&gt;\ndescription: &lt;description goes here&gt;\npath: &lt;path&gt;\n<\/code><\/pre>\n<p>In a pipeline am referencing this using azure cli inline script using the following command az ml data create -f .yml but getting error<\/p>\n<p>full error-D:\\a\\1\\s\\ETL\\data-asset&gt;az ml data create -f data-asset.yml\nERROR: 'ml' is misspelled or not recognized by the system.<\/p>\n<p>Examples from AI knowledge base:\naz extension add --name anextension\nAdd extension by name<\/p>\n<p>trying to implement this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI<\/a><\/p>\n<p>how can a resolve this?<\/p>",
        "Challenge_closed_time":1659361184080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659348144177,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73192053",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":12.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.6221952778,
        "Challenge_title":"azure cli not recognizing the following command az ml data create -f <file-name>.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606756004663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>One of the workaround you can follow to resolve the above issue;<\/p>\n<p>Based on this <a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/21390#issuecomment-1161782243\" rel=\"nofollow noreferrer\"><em><strong>GitHub issue<\/strong><\/em><\/a> as suggested by @<em>adba-msft<\/em> .<\/p>\n<blockquote>\n<p><strong>Please make sure that you have upgraded your azure cli to latest and<\/strong>\n<strong>Azure CLI ML extension v2 is being used.<\/strong><\/p>\n<\/blockquote>\n<p>To check and upgrade the cli we can use the below <code>cmdlts<\/code>:<\/p>\n<pre><code>az version\n\naz upgrade\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Uopde.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Uopde.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For more information please refer this similar <a href=\"https:\/\/stackoverflow.com\/questions\/73110661\/create-is-misspelled-or-not-recognized-by-the-system-on-az-ml-dataset-create\"><em><strong>SO THREAD|'create' is misspelled or not recognized by the system on az ml dataset create<\/strong><\/em><\/a> .<\/p>\n<p>I did observe the same issue after trying the aforementioned suggestion by @<em>Dor Lugasi-Gal<\/em> it works for me with (in my case <code>az ml -h<\/code>) after installed the extension with  <code>az extension add -n ml -y<\/code> can able to get the result of <code>az ml -h<\/code> without any error.<\/p>\n<p><em><strong>SCREENSHOT FOR REFERENCE:-<\/strong><\/em><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/39LHa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/39LHa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659362947923,
        "Solution_link_count":6.0,
        "Solution_readability":12.5,
        "Solution_reading_time":21.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":161.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":15.5657958333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have done quite a few google searches but have not found a clear answer to the following use case. Basically, I would rather use cloud 9 (most of the time) as my IDE rather than Jupyter. What I am confused\/not sure about is, how I could executed long running jobs like (Bayesian) hyper parameter optimisation from there. Can I use Sagemaker capabilities? Should I use docker and deploy to ECR (looking for the cheapest-ish option)? Any pointers w.r.t. to this particular issue would be very much appreciated. Thanks.<\/p>",
        "Challenge_closed_time":1641735920212,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641644416603,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1641679883347,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70632239",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":25.4176691667,
        "Challenge_title":"cloud 9 and sagemaker - hyper parameter optimisation",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>You could use whatever IDE you choose (including your laptop).<br \/>\nSaegMaker tuning job (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex.html\" rel=\"nofollow noreferrer\">example<\/a>) is <strong>asynchronous<\/strong>, so you can safely close your IDE after launching it. You can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-monitor.html\" rel=\"nofollow noreferrer\">monitor the job the AWS web console,<\/a> or with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeHyperParameterTuningJob.html\" rel=\"nofollow noreferrer\">DescribeHyperParameterTuningJob API call<\/a>.<\/p>\n<p>You can launch TensorFlow, PyTorch, XGBoost, Scikit-learn, and other <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/frameworks.html\" rel=\"nofollow noreferrer\">popular ML frameworks<\/a>, using one of the built-in framework containers, avoiding the extra work of bringing your own container.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":20.2,
        "Solution_reading_time":13.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505166133223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":43.793195,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to pull the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pre-built-docker-containers-frameworks.html\" rel=\"noreferrer\">pre-built docker images<\/a> for SageMaker. I am able to successfully <code>docker login<\/code> to ECR (my AWS credentials). When I try to pull the image I get the standard <code>no basic auth credentials<\/code>.<\/p>\n\n<p>Maybe I'm misunderstanding... I assumed those ECR URLs were public.<\/p>\n\n<pre><code>$(aws ecr get-login --region us-west-2 --no-include-email)\n\ndocker pull 246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn\n<\/code><\/pre>",
        "Challenge_closed_time":1557174090812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557016435310,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1557175542127,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55987935",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":43.793195,
        "Challenge_title":"How do I pull the pre-built docker images for SageMaker?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2738.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343237570416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California",
        "Poster_reputation_count":1325.0,
        "Poster_view_count":131.0,
        "Solution_body":"<p>Could you show your ECR login command and pull command in the question?<\/p>\n\n<p>For SageMaker pre-built image 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/p>\n\n<p>What I do is:<\/p>\n\n<ol>\n<li>Log in ECR<\/li>\n<\/ol>\n\n<p><code>$(aws ecr get-login --no-include-email --registry-ids 520713654638 --region us-west-2)<\/code><\/p>\n\n<ol start=\"2\">\n<li>Pull the image<\/li>\n<\/ol>\n\n<p><code>docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/code><\/p>\n\n<p>These images are public readable so you can pull them from any AWS account. I guess the reason you failed is that you did not specify --registry-ids in your login. But it's better if you can provide your scripts for others to identify what's wrong.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":9.83,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1498123491552,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"\u4e2d\u56fdJiangsu Sheng",
        "Answerer_reputation_count":22369.0,
        "Answerer_view_count":3121.0,
        "Challenge_adjusted_solved_time":112.0978552778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would really like to get access to some of the updated functions in pandas 0.19, but Azure ML studio uses pandas 0.18 as part of the Anaconda 4.0 bundle. Is there a way to update the version that is used within the \"Execute Python Script\" components?<\/p>",
        "Challenge_closed_time":1505456513227,
        "Challenge_comment_count":2,
        "Challenge_created_time":1505401641617,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46222606",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.7,
        "Challenge_reading_time":3.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":15.2421138889,
        "Challenge_title":"Updating pandas to version 0.19 in Azure ML Studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1903.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421081882987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":588.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>I offer the below steps for you to show how to update the version of pandas  library in <code>Execute Python Script<\/code>.<\/p>\n\n<p><strong><em>Step 1<\/em><\/strong> : Use the <code>virtualenv<\/code> component to create an independent python runtime environment in your system.Please install it first with command <code>pip install virtualenv<\/code> if you don't have it.<\/p>\n\n<p>If you installed it successfully ,you could see it in your python\/Scripts file.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step2<\/em><\/strong> : Run the commad to create independent python runtime environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 3<\/em><\/strong> : Then go into the created directory's Scripts folder and activate it (this step is important , don't miss it)<\/p>\n\n<p>Please don't close this command window and use <code>pip install pandas==0.19<\/code> to download external libraries in this command window.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Wj857.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Wj857.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 4<\/em><\/strong> : Compress all of the files in the Lib\/site-packages folder into a zip package (I'm calling it pandas - package here)<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 5<\/em><\/strong> \uff1aUpload the zip package into the Azure Machine Learning WorkSpace DataSet.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/efRkK.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/efRkK.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>specific steps please refer to the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/\" rel=\"noreferrer\">Technical Notes<\/a>.<\/p>\n\n<p>After success, you will see the uploaded package in the DataSet List<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 6<\/em><\/strong> \uff1a Before the defination of method <code>azureml_main<\/code> in the Execute Python Script module, you need to remove the old <code>pandas<\/code> modules &amp; its dependencies, then to import <code>pandas<\/code> again, as the code below.<\/p>\n\n<pre><code>import sys\nimport pandas as pd\nprint(pd.__version__)\ndel sys.modules['pandas']\ndel sys.modules['numpy']\ndel sys.modules['pytz']\ndel sys.modules['six']\ndel sys.modules['dateutil']\nsys.path.insert(0, '.\\\\Script Bundle')\nfor td in [m for m in sys.modules if m.startswith('pandas.') or m.startswith('numpy.') or m.startswith('pytz.') or m.startswith('dateutil.') or m.startswith('six.')]:\n    del sys.modules[td]\nimport pandas as pd\nprint(pd.__version__)\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n<\/code><\/pre>\n\n<p>Then you can see the result from logs as below, first print the old version <code>0.14.0<\/code>, then print the new version <code>0.19.0<\/code> from the uploaded zip file.<\/p>\n\n<pre><code>[Information]         0.14.0\n[Information]         0.19.0\n<\/code><\/pre>\n\n<p>You could also refer to these threads: <a href=\"https:\/\/stackoverflow.com\/questions\/45749479\/access-blob-file-using-time-stamp-in-azure\/45814318#45814318\">Access blob file using time stamp in Azure<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/12669546\/reload-with-reset\">reload with reset<\/a>.<\/p>\n\n<p>Hope it helps you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1505805193896,
        "Solution_link_count":15.0,
        "Solution_readability":10.2,
        "Solution_reading_time":51.18,
        "Solution_score_count":6.0,
        "Solution_sentence_count":43.0,
        "Solution_word_count":375.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.5115797222,
        "Challenge_answer_count":1,
        "Challenge_body":"I see Autosave Documents is checked in the Settings menu of my notebook, but I'm curious what the default interval is and if that's configurable?",
        "Challenge_closed_time":1673973257172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1673960615485,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1674306814308,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPNplairnRPWo3zA_otNpWA\/is-there-a-way-to-configure-the-auto-save-interval-on-sagemaker-studio-notebooks",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":3.5115797222,
        "Challenge_title":"Is there a way to configure the auto-save interval on SageMaker Studio Notebooks?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":37,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"SageMaker Studio uses the default JupyterLab configuration, i.e., auto save every 2 minutes. \n\nYou can modify it through the `%autosave` magic command, or use an extension. You can see a medium article with details [here](https:\/\/medium.com\/nabla-squared\/how-to-change-the-autosave-interval-in-jupyter-notebooks-2ab996fe4446).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1673973257172,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":4.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.3068894444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The Python package <a href=\"https:\/\/pypi.org\/project\/azureml-train\/\">azureml-train<\/a> is deprecated and seems basically a wrapper around <a href=\"https:\/\/pypi.org\/project\/azureml-train-core\/\">azureml-train-core<\/a>. When installing <code>azureml-train<\/code>, if I'm correct it tries to install the <code>azureml-train-core<\/code> package with the same version number. However, on the 24th of August 2021 the <code>azureml-train<\/code> package was <a href=\"https:\/\/pypi.org\/project\/azureml-train\/#history\">updated<\/a> to version 1.33.1 whereas <code>azureml-train-core<\/code> <a href=\"https:\/\/pypi.org\/project\/azureml-train-core\/#history\">wasn't updated<\/a>. This causes the installation of <code>azureml-train<\/code> to fail.   <\/p>\n<p>I would suggest to remove version 1.33.1 of <code>azureml-train<\/code> such that it still can be installed.  <br \/>\nOtherwise, I'm curious why this situation is the case.  <\/p>",
        "Challenge_closed_time":1629981340732,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629973035930,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/528959\/(bug)-azureml-train-python-package-deprecated-but",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":2.3068894444,
        "Challenge_title":"[Bug] azureml-train Python package deprecated but did receive an update which is not in line with azureml-train-core",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b3ae76bb-30c9-45f0-9c89-d61eac6d87f6\">@SjoerdGn  <\/a> Yes, this is a bug in the release cycle that was pushed to pypi, This also caused other packages to get updated.    <\/p>\n<p><a href=\"https:\/\/pypi.org\/project\/azureml-train-automl-runtime\/1.33.1.post1\/\">https:\/\/pypi.org\/project\/azureml-train-automl-runtime\/1.33.1.post1\/<\/a>    <br \/>\n<a href=\"https:\/\/pypi.org\/project\/azureml-train-automl\/1.33.1\/\">https:\/\/pypi.org\/project\/azureml-train-automl\/1.33.1\/<\/a>    <\/p>\n<p><a href=\"https:\/\/pypi.org\/project\/azureml-sdk\/#history\">azureml-sdk 1.33.0.post1<\/a> is released now to ensure the correct versions are installed with the SDK. As you mentioned above azureml-train 1.33.1 is not required and can be removed.     <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.0,
        "Solution_reading_time":9.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2342.6461111111,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to use Azure Computer Vision's OCR API in an Azure Machine Learning Notebook. However there seems to be an error when trying to call the Computer Vision API from an Azure Machine Learning Notebook. The same code works when I'm running it on a local machine.\r\nI'm following Azure Computer Vision's OCR Quickstart: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/quickstarts-sdk\/client-library?tabs=visual-studio&pivots=programming-language-python\r\n\r\nWhen running the following code, `computervision_client.read(read_image_url, raw=True)` does not return but throws an exception.\r\nException:\r\n`ClientRequestError: Error occurred in request., ConnectionError: HTTPSConnectionPool(host='some-host.cognitiveservices.azure.com', port=443): Max retries exceeded with url: \/vision\/v3.2\/read\/analyze?model-version=latest&readingOrder=basic (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f59e0102820>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))`\r\n\r\nCode:\r\n```python\r\nfrom azure.cognitiveservices.vision.computervision import ComputerVisionClient\r\nfrom azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\r\nfrom azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\r\nfrom msrest.authentication import CognitiveServicesCredentials\r\n\r\nfrom array import array\r\nimport os\r\nfrom PIL import Image\r\nimport sys\r\nimport time\r\n\r\n'''\r\nAuthenticate\r\nAuthenticates your credentials and creates a client.\r\n'''\r\nsubscription_key = os.environ[\"COMPUTERVISION_KEY\"]\r\nendpoint = os.environ[\"COMPUTERVISION_URL\"]\r\n\r\ncomputervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\r\n\r\n'''\r\nOCR: Read File using the Read API, extract text - remote\r\nThis example will extract text in an image, then print results, line by line.\r\nThis API call can also extract handwriting style text (not shown).\r\n'''\r\nprint(\"===== Read File - remote =====\")\r\n# Get an image with text\r\nread_image_url = \"https:\/\/raw.githubusercontent.com\/MicrosoftDocs\/azure-docs\/master\/articles\/cognitive-services\/Computer-vision\/Images\/readsample.jpg\"\r\n\r\n# Call API with URL and raw response (allows you to get the operation location)\r\nread_response = computervision_client.read(read_image_url,  raw=True) # <- THROWS EXCEPTION\r\n```\r\n\r\nUsed azure packages:\r\n```\r\nazure-ai-textanalytics                        5.1.0b7\r\nazure-cognitiveservices-vision-computervision 0.9.0\r\nazure-common                                  1.1.27\r\nazure-core                                    1.14.0\r\nazure-cosmos                                  4.2.0\r\nazure-graphrbac                               0.61.1\r\nazure-identity                                1.4.1\r\nazure-mgmt-authorization                      0.61.0\r\nazure-mgmt-containerregistry                  8.0.0\r\nazure-mgmt-core                               1.2.2\r\nazure-mgmt-keyvault                           2.2.0\r\nazure-mgmt-resource                           13.0.0\r\nazure-mgmt-storage                            11.2.0\r\nazure-storage-blob                            12.8.0\r\nazureml-automl-core                           1.29.0\r\nazureml-contrib-dataset                       1.29.0\r\nazureml-core                                  1.29.0.post1\r\nazureml-dataprep                              2.15.1\r\nazureml-dataprep-native                       33.0.0\r\nazureml-dataprep-rslex                        1.13.0\r\nazureml-dataset-runtime                       1.29.0\r\nazureml-pipeline-core                         1.29.0\r\nazureml-pipeline-steps                        1.29.0\r\nazureml-telemetry                             1.29.0\r\nazureml-train-automl-client                   1.29.0\r\nazureml-train-core                            1.29.0\r\nazureml-train-restclients-hyperdrive          1.29.0\r\nazureml-widgets                               1.29.0.post1\r\n```\r\n\r\nMaybe related to #1107",
        "Challenge_closed_time":1631108652000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622675126000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1501",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":43.59,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":2342.6461111111,
        "Challenge_title":"'ClientRequestError' when trying to use Azure Computer Vision API from Azure Machine Learning Notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":292,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"After a long while I've tried the exact same script in the same compute instance again. I don't experience the connection error anymore so I will close this ticket. I don't know what changed.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":2.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1313736279736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":207794.0,
        "Answerer_view_count":16864.0,
        "Challenge_adjusted_solved_time":4.9399880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to figure out whether this behaviour on IPython (v7.12.0, on Amazon SageMaker) is a bug or I'm missing some proper way \/ documented constraint...<\/p>\n<p>Say I have some Python variables like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>NODE_VER = &quot;v16.14.2&quot;\nNODE_DISTRO = &quot;linux-x64&quot;\n<\/code><\/pre>\n<p>These commands both work fine in a notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo $PATH\n# Shows **contents of system path**\n!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:\n# Shows \/usr\/local\/lib\/nodejs\/node-v16.14.2-linux-x64\/bin\n<\/code><\/pre>\n<p>...But this does not:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n# Shows:\n# \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:**contents of system path**\n<\/code><\/pre>\n<p>I've tried a couple of combinations of e.g. using <code>$NODE_VER<\/code> syntax instead (which produces <code>node--\/<\/code> instead of <code>node-{NODE_VER}-{NODE_DISTRO}\/<\/code>, but seems like any combination using both shell variables (PATH) and Python variables (NODE_VER\/NODE_DISTRO) fails.<\/p>\n<p>Can anybody help me understand why and how to work around it?<\/p>\n<p>My end goal, as you might have guessed already, is to actually add this folder to the PATH rather than just echoing it - something like:<\/p>\n<pre><code>!export PATH=\/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n<\/code><\/pre>",
        "Challenge_closed_time":1648184137556,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648174324543,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71611419",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.2,
        "Challenge_reading_time":20.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2.7258369444,
        "Challenge_title":"Mixing shell variables and python variables in IPython '!command'",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":160.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587281590603,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":473.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/69194172\/how-to-reference-both-a-python-and-environment-variable-in-jupyter-bash-magic\">How to reference both a python and environment variable in jupyter bash magic?<\/a><\/p>\n<p>Try<\/p>\n<pre><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$$PATH\n<\/code><\/pre>\n<p><code>$$PATH<\/code> forces it to use the system variable rather than try to find a Python\/local one.<\/p>\n<p>Various examples:<\/p>\n<pre><code>In [130]: foo = 'foo*.txt'\nIn [131]: HOME = 'myvar'\nIn [132]: !echo $foo\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt\nIn [133]: !echo $foo $HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt myvar\nIn [134]: !echo $foo $$HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\nIn [135]: !echo $foo $PWD\n\/home\/paul\/mypy\nIn [136]: !echo $foo $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\nIn [137]: !echo {foo} $PWD\n{foo} \/home\/paul\/mypy\nIn [138]: !echo {foo} $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\n<\/code><\/pre>\n<p>Any variable not locally defined forces the behavior you see:<\/p>\n<pre><code>In [139]: !echo $abc\n\nIn [140]: !echo {foo} $abc\n{foo}\n<\/code><\/pre>\n<p>It may put the substitution in a <code>try\/except<\/code> block, and &quot;give up&quot; if there's any <code>NameError<\/code>.<\/p>\n<p>This substitution can occur in most of the magics, not just <code>!<\/code>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1648192108500,
        "Solution_link_count":1.0,
        "Solution_readability":7.6,
        "Solution_reading_time":18.76,
        "Solution_score_count":3.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":160.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":98.9333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"I was able to create a deep learning VM from the marketplace and when I open up the VM instance in the Console I see a metadata tag called `proxy-url` which has a format like `https:\/\/[alphanumeric string]-dot-us-central1.notebooks.googleusercontent.com\/lab`\n\nClicking on that link takes me to a JupyterLab UI that is running on my VM. Amazing! Unfortunately, when I try opening that link on an incognito window, I'm asked to sign in. If I sign in, I get a 403 forbidden.\n\nMy question now is, how can I make that link available to someone else?",
        "Challenge_closed_time":1643637480000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643281320000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Make-deep-learning-VM-JupyterLab-publicly-available\/td-p\/386576\/jump-to\/first-unread-message",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":98.9333333333,
        "Challenge_title":"Make deep learning VM JupyterLab publicly available?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":134.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi gopalv\n\nAs far as I understand, it sounds like your Jupyter Notebook isn't configured for remote access. since it doesn't work when trying to access it from the incognito window with a 403 error.\n\nYou can try looking here and here for details on how to set up a publicly accessible\/remote access notebook. There are additional troubleshooting steps in our documentation here as well.\n\nHope this helps!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":73.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1405882600928,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":115.0,
        "Challenge_adjusted_solved_time":0.3938825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an existing project, cloned with <code>git clone<\/code>.<\/p>\n\n<p>After I <code>pip install kedro<\/code> I can run <code>kedro info<\/code> fine but I dont seem to have access to the projects CLI for example if I try to run<code>kedro install<\/code> I get the following error:<\/p>\n\n<pre><code>Usage: kedro [OPTIONS] COMMAND [ARGS]...\nTry 'kedro -h' for help.\n\nError: No such command 'install'.\n<\/code><\/pre>\n\n<p>Any clues on what to do for existing projects are much appreciated.<\/p>\n\n<p>Not sure if this matters but I am working inside a conda environment which is inside a docker container.<\/p>",
        "Challenge_closed_time":1589565622150,
        "Challenge_comment_count":2,
        "Challenge_created_time":1589564204173,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1589721006283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61825202",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3938825,
        "Challenge_title":"Accessing Kedro CLI from an existing project",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1175.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1387377887347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Edinburgh, United Kingdom",
        "Poster_reputation_count":1240.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>Project CLIs are available if you run <code>kedro<\/code> at your Kedro project directory. <\/p>\n\n<ol>\n<li><p>Run <code>kedro new<\/code> to create a Kedro project<\/p><\/li>\n<li><p><code>cd &lt;your-kedro-project&gt;<\/code><\/p><\/li>\n<li><p><code>kedro<\/code> at the project directory<\/p><\/li>\n<\/ol>\n\n<p>And you should see the project level CLIs<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Also for your existing project, check if you have <code>kedro_cli.py<\/code> at your project directory.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":8.23,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1622117284230,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":316.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":21.7817686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running this notebook in my managed notebooks environment on Google Cloud and I'm getting the following error when trying to install the packages: &quot;WARNING: The script google-oauthlib-tool is installed in '\/home\/jupyter\/.local\/bin' which is not on PATH.\nConsider adding this directory to PATH.&quot;<\/p>\n<p>Here is the python code that I'm trying to run for reference. <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb<\/a><\/p>\n<p>Any suggestions on how I can update the package installation so it is on path and resolve the error? I'm currently working on GCP user-managed notebooks on a Mac.<\/p>\n<p>Thanks so much for any tips!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>",
        "Challenge_closed_time":1658596861407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658518447040,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73085293",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":21.7817686111,
        "Challenge_title":"Library is not installed on PATH - How can I install on path?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1621620820567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Open up your shell config file (likely .zshrc because the default shell on Mac is now zsh and that's the name of the zsh config file) located at your home directory in a text editor (TextEdit, etc) and add the path to the  executable.\nLike this:\nOpen the file:\n<code>open -e ~\/.zshrc<\/code>\nEdit the file:\nAdd this line at the top (may vary, check the documentation):\n<code>export PATH=&quot;\/home\/jupyter\/.local\/bin&quot;<\/code>\nThat may not work, try this:\n<code>export PATH=&quot;$PATH:\/home\/jupyter\/.local\/bin&quot;<\/code>\nYour best bet is to read the package documentation.<\/p>\n<p>After saving the config file, run <code>source ~\/.zshrc<\/code> and replace .zshrc with the config file name if it's different OR open a new terminal tab.<\/p>\n<p>What this does is tells the shell that the command exists and where to find it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.8,
        "Solution_reading_time":10.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":126.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":107.1936713889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<br>\nsometimes I meet with this kind of error, when I run on a remote device:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85.png\" data-download-href=\"\/uploads\/short-url\/78TKyZQzgGPLZDlHXk14ZJlglYV.png?dl=1\" title=\"Screenshot from 2023-04-23 08-40-26\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_690x93.png\" alt=\"Screenshot from 2023-04-23 08-40-26\" data-base62-sha1=\"78TKyZQzgGPLZDlHXk14ZJlglYV\" width=\"690\" height=\"93\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_690x93.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_1035x139.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/3210bee6d41f470899393ebd804a54d0b8eacb85_2_1380x186.png 2x\" data-dominant-color=\"2A2319\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2023-04-23 08-40-26<\/span><span class=\"informations\">1482\u00d7200 40.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\n<code>Error: Run xxx errored: AssertionError()<\/code><br>\nDubugging tool in vscode cannot locate it. It just pops up and tells you the run is failed. I can find which line is the reason of this error, but have no idea what happens.<br>\nAfter checking for much time, I can do nothing but disconnect with remote device and connect again. And the problem is gone\u2026 Is there a more specific reason?<br>\nThanks a lot!<br>\nJialei Li<\/p>",
        "Challenge_closed_time":1682618207071,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682232309854,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/wandb-assertion-errror\/4260",
        "Challenge_link_count":5,
        "Challenge_participation_count":4,
        "Challenge_readability":16.1,
        "Challenge_reading_time":26.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":107.1936713889,
        "Challenge_title":"Wandb assertion errror",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":50.0,
        "Challenge_word_count":139,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi,<br>\nI found the reason now. It is relative with other part of my project\u2019s program. It has nothing to do with wandb actually\u2026 Still thanks a lot for your willingness to check error for me <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":4.77,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":44.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":47.865,
        "Challenge_answer_count":6,
        "Challenge_body":"**Describe the bug**\r\nGetting error \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" while running \"mlflow ui\".\r\n\r\n**To Reproduce**\r\nRun \"mlflow ui\"\r\n\r\n\r\n**Expected behavior**\r\nIt should run without any issues\r\n\r\n\r\n**Versions**\r\n2.3.10\r\n\r\nNot sure if this is the right forum to post this issue. If it is not, please ignore.\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1650449956000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650277642000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2425",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":5.9,
        "Challenge_reading_time":4.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":47.865,
        "Challenge_title":"[BUG] mlflow ui never runs",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@maverick-scientist there is not enough information here to recreate or debug the issue. Please provide a complete reproducible example so we can debug. Also, note that if the data is proprietary, you can create a fake dataset yourself to recreate the issue. Refer to the following for more details:\r\n\r\n\ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\r\n\ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\r\n\r\n- The do's and dont's have an example of how to create the fake data - minimal to reproduce the problem.\r\n- Alternately, you could try to reproduce the issue with a publicly available dataset or one available in pycaret itself.\r\n\r\nThanks!\r\n Hi Nikhil,\nCan we please discuss this over teams call? I can share a python file with\nyou for this issue but I feel if we can discuss this on a call it would\nsave me some time.\n\nWhat do you think?\n\nPlease advise.\n\nOn Mon, 18 Apr 2022 at 22:55, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> there is not\n> enough information here to recreate or debug the issue. Please provide a\n> complete reproducible example so we can debug. Also, note that if the data\n> is proprietary, you can create a fake dataset yourself to recreate the\n> issue. Refer to the following for more details:\n>\n> \ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\n> \ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\n>\n>    - The do's and dont's have an example of how to create the fake data -\n>    minimal to reproduce the problem.\n>    - Alternately, you could try to reproduce the issue with a publicly\n>    available dataset or one available in pycaret itself.\n>\n> Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1101586641>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRACMIA55LOVAGIZZKPLVFWLKHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @maverick-scientist Honestly, I would prefer not to do that since it sets the wrong precedent for the open-source community and is not sustainable in the long run. The globally accepted best practice is to provide a minimal reproducible example and I would encourage you to do that.\r\n\r\nThanks! Hi Nikhil,\r\nAttaching the code file to reproduce this issue. Could you please check and tell me what's wrong with it or my machine?\r\n\r\nThanks & Regards,\r\nAbhinav\r\n\r\n[Simple MLflow.zip](https:\/\/github.com\/pycaret\/pycaret\/files\/8511837\/Simple.MLflow.zip)\r\n\r\n @maverick-scientist The example that you posted has no reference to pycaret. It is a generic MLFlow example. How is it related to pycaret and this repo? Hi Nikhil,\nThank you for your reply. However, if you read the issue carefully, I\u2019d\nclearly mentioned my dilemma whether it was the right forum to post this\nissue.\n\nAnyways, thanks for your support. I\u2019d try and see what\u2019s preventing the\nMLFlow to run properly on my machine.\n\nOn Wed, 20 Apr 2022 at 15:21, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> The example\n> that you posted has no reference to pycaret. It is a generic MLFlow\n> example. How is it related to pycaret and this repo?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1103727978>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRADHBVLOBH4SACXZHNLVF7HTHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_readability":7.7,
        "Solution_reading_time":43.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":477.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617627646000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":121,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":19.9010738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I ran into this issue yesterday, while trying to use the same sqlite script I used in \"Apply SQL Transformation\" module in Azure ML, in Sqlite over Python module in Azure ML:<\/p>\n\n<pre><code>with tbl as (select * from t1)\nselect * from tbl\n<\/code><\/pre>\n\n<p>Here is the error I got:<\/p>\n\n<pre><code>[Critical]     Error: Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\n  File \"C:\\server\\invokepy.py\", line 169, in batch\ndata:text\/plain,Caught exception while executing function: Traceback (most recent call last):\n    odfs = mod.azureml_main(*idfs)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 388, in read_sql\n  File \"C:\\temp\\azuremod.py\", line 193, in azureml_main\n    results = pd.read_sql(query,con)\n    coerce_float=coerce_float, parse_dates=parse_dates)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1017, in execute\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1022, in read_sql\n    cursor = self.execute(*args)\n    raise_with_traceback(ex)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\io\\sql.py\", line 1006, in execute\n---------- End of error message from Python  interpreter  ----------\n    cur.execute(*args)\nDatabaseError: Execution failed on sql:  with tbl as (select * from t1)\n                    select * from tbl\n<\/code><\/pre>\n\n<p>and the Python code:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n    import pandas as pd\n    import sqlite3 as lite\n    import sys\n    con = lite.connect('data1.db')\n    con.text_factory = str\n    with con:\n        cur = con.cursor()\n\n        if (dataframe1 is not None):\n            cur.execute(\"DROP TABLE IF EXISTS t1\")\n            dataframe1.to_sql('t1',con)\n        query = '''with tbl as (select * from t1)\n                    select * from tbl'''                      \n        results = pd.read_sql(query,con)    \n\n    return results,\n<\/code><\/pre>\n\n<p>when replacing the query with:<\/p>\n\n<pre><code>select * from t1\n<\/code><\/pre>\n\n<p>It worked as expected.\nAs you probably know, Common table expressions is a key feature in Sqlite, the ability to run recursive code is a \"must have\" in any functional language such as Sqlite.<\/p>\n\n<p>I also tried to run my Python script in Jupyter Notebook in Azure, that also worked as expected.<\/p>\n\n<p>Is it possible we have a different configuration for Sqlite in the Python module than in Jupyter Notebook and in \"Apply SQL Transformation\" module?<\/p>",
        "Challenge_closed_time":1456485065183,
        "Challenge_comment_count":1,
        "Challenge_created_time":1456413421317,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1456812883803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35631267",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":31.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":19.9010738889,
        "Challenge_title":"Azure ML in \"Execute Python Script\" module :Common table expressions is not supported in sqlite3",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":280.0,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320061998252,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":778.0,
        "Poster_view_count":89.0,
        "Solution_body":"<p>I reproduced your issue and reviewed the <code>SQL Queries<\/code> doc of <code>pandas.io.sql<\/code> at <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html#sql-queries\" rel=\"nofollow\">http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html#sql-queries<\/a>. I tried to use <code>read_sql_query<\/code> to solve it, but failed.<\/p>\n\n<p>According to the <code>pandas<\/code> doc, tt seems that <code>Pandas<\/code> not support the usage for this SQL syntax.<\/p>\n\n<p>Base on my experience and according to your SQL, I tried to do the SQL <code>select * from (select * from t1) as tbl<\/code> instead of your SQL that work for <code>Pandas<\/code>.<\/p>\n\n<p>Hope it helps. Best Regards. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.6,
        "Solution_reading_time":8.87,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1247721040396,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Letterkenny, Ireland",
        "Answerer_reputation_count":2017.0,
        "Answerer_view_count":166.0,
        "Challenge_adjusted_solved_time":47.612795,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained an automated machine learning model on an Azure ML compute cluster.<\/p>\n\n<p>I am trying to use that remote model in my Azure hosted Jupyter notebook. <\/p>\n\n<p>This is the code in the workbook that tries to load the remote model:<\/p>\n\n<pre><code>remote_run = AutoMLRun(experiment = experiment, run_id = '... Experiment id ...')\nremote_best_run, remote_fitted_model = remote_run.get_output()\n<\/code><\/pre>\n\n<p>This code fails with the following error:<\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError                       Traceback (most recent call\n  last)  in \n        2 # remote_run.wait_for_completion(show_output = True)\n        3 import pandas as pd\n  ----> 4 remote_best_run, remote_fitted_model = remote_run.get_output()\n        5 #!pip list<\/p>\n  \n  <p>~\/anaconda3_501\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/run.py\n  in get_output(self, iteration, metric)\n      406 \n      407         with open(model_local, \"rb\") as model_file:\n  --> 408             fitted_model = pickle.load(model_file)\n      409         return curr_run, fitted_model\n      410 <\/p>\n  \n  <p>ModuleNotFoundError: No module named 'pandas._libs.tslibs.timestamps'<\/p>\n<\/blockquote>\n\n<p>Presumably there is a version difference between what is installed on the Azure ML compute cluster vs what is installed in the kernel of the Jupyter notebook, or I have a package missing. <\/p>\n\n<p>How can I make this remote model work?<\/p>\n\n<p>For additional reference, I am following this tutorial: <a href=\"https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI<\/a><\/p>\n\n<p><strong>Note 1<\/strong> I can also reproduce this error by running the following code in my jupyter notebook: <\/p>\n\n<pre><code>import pickle\n\nwith open('model.pkl', 'rb') as p_f:\n    data = pickle.load(p_f)\n<\/code><\/pre>",
        "Challenge_closed_time":1545041889310,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544869470737,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1544870483248,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53791461",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":23.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":47.8940480556,
        "Challenge_title":"Import error when using remote Azure Automated Machine Learning model in Azure notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":353.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1247721040396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Letterkenny, Ireland",
        "Poster_reputation_count":2017.0,
        "Poster_view_count":166.0,
        "Solution_body":"<p>I emailed the Auto ML helpdesk and they solved the problem.<\/p>\n\n<p>Quote from them: <\/p>\n\n<blockquote>\n  <p>We have a bug where the AutoML inferencing fails because the pandas\n  version is 0.22.0 which doesn\u2019t have some API support.<\/p>\n<\/blockquote>\n\n<p>I upgraded pandas on my hosted notebook to version 0.23.4, and after this the model unpickles and works successfully<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":4.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":129.76,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nAttempting to deploy the docker image build yaml file available under https:\/\/github.com\/polyaxon\/polyaxon-examples\/blob\/master\/in_cluster\/build_image\/build-ml.yaml using the polyaxon cli returns the following error:\n\n>polyaxon run -f build_docker_image.yaml -l\nPolyaxonfile is not valid.\nError message: The Polyaxonfile's version specified is not supported by your current CLI.Your CLI support Polyaxonfile versions between: 1.1 <= v <= 1.1.You can run `polyaxon upgrade` and check documentation for the specification..\n\nTo reproduce\n\nThe contents of build_docker_image.yaml\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: python:3.8.8-buster\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nExpected behavior\n\nA build operation should begin that results in an image being saved to the docker registry connected under destination: connection:\n\nEnvironment\n\nPolyaxon 1.8.0\nPolyaxon CLI 1.8.0",
        "Challenge_closed_time":1618580258000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618113122000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1303",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.9,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":129.76,
        "Challenge_title":"Issue building docker image using example build config",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":140,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Very strange, and I could not reproduce. Did you try other files in the same examples folder https:\/\/github.com\/polyaxon\/polyaxon-examples\/tree\/master\/in_cluster\/build_image ?\nNot sure if quoting all values would work on your system:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: \"polyaxon-examples:ml\"\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"python:3.8.8-buster\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - 'pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nFinally you may try with polyaxon -v ... to trigger some additional debug information.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":8.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":78.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.3186111111,
        "Challenge_answer_count":3,
        "Challenge_body":"**SSL Connection to remote Neptune not working**\r\nI am unable to figure out how can I specify the correct certificate SFSRootCAG2.pem when running queries against SSL-enabled Neptune.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. I set up SSH tunnel via bastion to the Neptune cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -N -L 8182:yourneptuneendpoint:8182_'\r\n2. I start graph-notebook as '_jupyter notebook notebook\/destination_neptune_'. This gives me the output _Jupyter Notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_\r\n3. I open my notebook and run the following magic commands \r\n_'%%graph_notebook_config\r\n{\r\n  \"host\": \"localhost\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"iam_credentials_provider_type\": \"ROLE\",\r\n  \"load_from_s3_arn\": \"\",\r\n  \"aws_region\": <myregion>,\r\n  **\"ssl\": true**\r\n}'_\r\n4. I run the command \r\n_%%sparql        \r\nSELECT * WHERE {?s ?p ?o} LIMIT 1_\r\n\r\n5. It gives me the error\r\n**{'error': SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'localhost\\', port=8182): Max retries exceeded with url: \/sparql (Caused by SSLError(SSLCertVerificationError(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............**\r\n\r\n**Expected behavior**\r\nI expect to be able to connect to a remote neptune that has ssl enabled.\r\n\r\n**Screenshots**\r\nNone\r\n\r\n**Desktop (please complete the following information):**\r\n - macOS 10.15.7 Catalina\r\n - Browser Chrome\r\n - Version 86.0.4240.198 (Official Build) (x86_64)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.None",
        "Challenge_closed_time":1607104425000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606948478000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/40",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":20.85,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":43.3186111111,
        "Challenge_title":"[BUG] No documentation on how to connect local notebook to remote Neptune SSL",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":192,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Looks like we have a missing piece in our walkthrough for connecting to Neptune:\r\n\r\nhttps:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune\r\n\r\nCould you set your hostname from localhost to your Neptune endpoint:\r\n\r\n```\r\n%graph_notebook_host <your endpoint here>\r\n```\r\n\r\nAnd give it a try? I  updated \/etc\/hosts on my Mac and added an alias for localhost as\r\n\r\n> _27.0.0.1       localhost    yourneptuneendpoint_\r\n\r\nFlushed DNS cache.\r\nSet the hostname in Jupyter notebook graph_notebook_config command to **yourneptuneendpoint**.\r\nRan sparql query and it successfully completed.\r\n\r\n\r\n Glad it worked! I have filed an issue for us to expand our documentation to cover the steps you had to take. Closing this but feel free to open or submit a new issue if you need further assistance",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.9,
        "Solution_reading_time":9.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":110.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":44.3683333333,
        "Challenge_answer_count":6,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen trying to install mlflow chart I'm trying to migrate from old mlflow version to the new one. I'm using `backendStore.databaseMigration: true` value for that. But mlflow pod failed to start with error:\r\n```\r\nmlflow.exceptions.MlflowException: Detected out-of-date database schema (found version c48cb773bb87, but expected cc1f77228345). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n```\r\n\r\nFrom the looks of things migration Job should have `pre-install,pre-upgrade` hooks instead of `post-install,post-upgrade` but I can be wrong here. \r\n\r\nRunning Job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release.\r\n\r\nThanks!\n\n### What's your helm version?\n\nv3.9.3\n\n### What's your kubectl version?\n\nv1.24.3\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.6.0\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\nDB migration job should run before mlflow pod upgrade. \n\n### How to reproduce it?\n\n1. Install mlflow with old DB schema (1.23.1)\r\n2. Try to upgrade with 0.6.0 helm chart\n\n### Enter the changed values of values.yaml?\n\n```\r\nmlflow:\r\n  nodeSelector:\r\n    redacted: Shared\r\n  \r\n  ingress:\r\n    enabled: true\r\n  \r\n  artifactRoot:\r\n    s3:\r\n      enabled: true\r\n      bucket: \"redacted\"\r\n      awsAccessKeyId: \"\"\r\n      awsSecretAccessKey: \"\"\r\n  \r\n  extraEnvVars:\r\n    AWS_DEFAULT_REGION: eu-central-1\r\n    MLFLOW_S3_ENDPOINT_URL: https:\/\/bucket.redacted.s3.eu-central-1.vpce.amazonaws.com\r\n  \r\n  backendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    mysql:\r\n      enabled: true\r\n      host: \"redacted.eu-central-1.rds.amazonaws.com\"\r\n      database: \"mlflow\"\r\n      user: \"\"\r\n      password: \"\"\r\n```\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm upgrade --install --values override.yaml --wait --create-namespace --atomic --timeout 15m0s -f secrets:\/\/secrets.yaml shared-services .\/shared-services\n\n### Anything else we need to know?\n\nChart was installed as a part of another umbrella chart",
        "Challenge_closed_time":1660908206000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660748480000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/35",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":7.0,
        "Challenge_reading_time":27.79,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":44.3683333333,
        "Challenge_title":"[mlflow] Migration Job should run before upgrade",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":277,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @faceless7171 \r\n\r\nThank you very much for reporting the issue. Yes, your suggestion can work. Let me create a PR and test it. Well, we can't use the pre-hook option because we need a configuration file for the DB connection. And I don't want to make secrets visible in the container.\r\n\r\n```console\r\n\u2502 Events:                                                                                                                                                          \u2502\r\n\u2502   Type     Reason       Age               From               Message                                                                                             \u2502\r\n\u2502   ----     ------       ----              ----               -------                                                                                             \u2502\r\n\u2502   Normal   Scheduled    22s               default-scheduler  Successfully assigned default\/mlflow-bzb8s to minikube                                              \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"migrations-config\" : configmap \"mlflow-migrations\" not found   \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"dbchecker\" : configmap \"mlflow-dbchecker\" not found            \u2502\r\n\u2502                                                                                                                                                                  \u2502\r\n```\r\n\r\nSo, we have another option. Maybe we can use the init container pattern for this purpose. Let me try. @all-contributors please add @faceless7171  for bug @burakince \n\nI've put up [a pull request](https:\/\/github.com\/community-charts\/helm-charts\/pull\/37) to add @faceless7171! :tada: Hi @faceless7171 \r\n\r\nCould you please try again with mlflow chart minimum 0.7.0 version? @burakince tested on 0.7.1 version. Everything is working now. Thanks for the fix.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.2,
        "Solution_reading_time":15.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":161.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.5692297222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I  was deploying a real-time inference pipeline into an AKS compute in East US region today. The endpoint deployment state was stuck at Transitioning for over 2 hours and never finished and I had to delete it. A separate deployment to region East US 2 got stuck as well.  I was able to deploy the same pipeline to East US  the day before yesterday.  <\/p>\n<p> I wonder if this is likely an error related to my account\/resources or a system wide issue? Did anyone else encounter the similar issue?  <\/p>\n<p>thanks in advance!  <\/p>",
        "Challenge_closed_time":1591940540980,
        "Challenge_comment_count":6,
        "Challenge_created_time":1591823291753,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/34653\/azure-ml-real-time-inference-endpoint-deloyment-st",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":7.6,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":32.5692297222,
        "Challenge_title":"Azure ML real-time inference endpoint deloyment stuck - with deployment state as Transitioning for over 2 hours.",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello All,  <\/p>\n<p>We have deployed a fix now to all regions and this should be fixed. Could you please retry and let us know if there are any issues.  <\/p>\n<p>-Rohit<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":431.6069444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThere are some missing details for how to connect to Neptune from a MacOS device, we should add them to our doc on connecting to neptune via ssh-tunnel found [here](https:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune)\r\n\r\nOne main piece that we are missing is that a host alias needs to be made in order to get things working properly.\r\n\r\n**Additional context**\r\nThis is coming from a bug report from connectivity not working as found in #40 ",
        "Challenge_closed_time":1608658157000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607104372000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.2,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":431.6069444444,
        "Challenge_title":"[BUG] Missing documentation on connecting to Neptune from MacOS",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1536318818623,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"\u0130zmit, Kocaeli, T\u00fcrkiye",
        "Answerer_reputation_count":1033.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":1138.2136686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute the Azure ml sdk from the local system using the Jupyter notebook. When I run the below code i am getting an error.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\n\nModuleNotFoundError: No module named 'ruamel' \n<\/code><\/pre>",
        "Challenge_closed_time":1626743937907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622646368700,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67807756",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":4.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1138.2136686111,
        "Challenge_title":"ModuleNotFoundError: No module named 'ruamel' when excuting from azureml.core",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Poster_created_time":1599816833352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New Delhi, Delhi, India",
        "Poster_reputation_count":329.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>You have to add pip 20.1.1<\/p>\n<p>Conda ruamel needs higher version of pip<\/p>\n<pre><code>conda install pip=20.1.1\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2039.8183333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nError Building SageMaker Types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerFile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.DeepCopy undefined (type Tag has no field or method DeepCopy)\r\nmake: *** [vet] Error 2**\r\n\r\n**What you expected to happen**:\r\nPackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nImport of sagemaker types in Go Client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n- Operator version (controller image tag): v1.1.0\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Challenge_closed_time":1599678360000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592335014000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/122",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":21.73,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2039.8183333333,
        "Challenge_title":"Error Building SageMaker Types due to missing types in common\/manual_deepcopy",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":180,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I believe this may be caused due to the generated deepcopy code not being checked in to the v1.1.0 branch. I attempted to backport this code previously but I don't think the go modules ever picked this up for some reason. I might suggest attempting this pinning to the `master` branch rather than `v1.1.0`. The APIs are backwardly compatible (while `master` is still pointed at a `v1.X`).",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.5,
        "Solution_reading_time":4.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":41.0340352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use a Jupyter Notebook instance on Sagemaker to run a code that took around 3 hours to complete. Since I pay for hour use, I would like to automatically \"Close and Halt\" the Notebook as well as stop the \"Notebook instance\" after running that code. Is this possible?<\/p>",
        "Challenge_closed_time":1568364157240,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568216434713,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57892580",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":4.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":41.0340352778,
        "Challenge_title":"Is there a Jupyter code that I can use \"to stop\" a Notebook Instances on Sagemaker, after running any code?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1393.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Jupyter Notebook are predominantly designed for exploration and\n   development. If you want to launch long-running or scheduled jobs on\n   ephemeral hardware, it will be a much better experience to use the\n   training API, such as the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\"><code>create_training_job<\/code><\/a> in boto3 or the\n   <code>estimator.fit()<\/code> of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>. The code passed to training jobs\n   can be completely arbitrary - not necessarily ML code - so whatever\n   you write in jupyter could likely be scheduled and ran in those\n   training jobs. See the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">random forest sklearn demo here<\/a> for an\n   example. That being said, if you still want to programmatically shut\n   down a SageMaker notebook instance, you can use that boto3 call:<\/p>\n\n<pre><code>import boto3\n\nsm = boto3.client('sagemaker')\nsm.stop_notebook_instance(NotebookInstanceName='string')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.9,
        "Solution_reading_time":16.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.5216666667,
        "Challenge_answer_count":1,
        "Challenge_body":"### What happened + What you expected to happen\n\nNotebook is broken due to missing permission first, maybe more issues down the road. @Yard1 looked into it earlier and we're creating this issue to keep track of it.\n\n### Versions \/ Dependencies\n\nmaster\n\n### Reproduction script\n\n`bazel test \/\/doc\/source\/tune\/examples:sigopt_example`\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Challenge_closed_time":1659051271000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659034993000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/27203",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.5216666667,
        "Challenge_title":"[AIR] Fix  \/\/doc\/source\/tune\/examples:sigopt_example",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":61,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Duplicate of https:\/\/github.com\/ray-project\/ray\/issues\/26567",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":29.2,
        "Solution_reading_time":0.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"SigOpt"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":4.4646297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have one Linux machine and one Windows machine for developments. For data sharing, we have set up a shared Windows directory in another Windows machine, which both my Linux and Windows can access.<\/p>\n<p>I am now using <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> for version control of the shared data. To make it easy, I mount the shared Windows folder both in Windows and in Linux development machine. In Windows, it looks like<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>In Linux, it looks like:<\/p>\n<pre><code>[core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>As you can see, Windows and Linux have different mounting points. So my question is: is there a way to make that both Windows and Linux have the same <code>\u00f9rl<\/code> in the DVC configuration file?<\/p>\n<p>If this is impossible, is there another alternative solution for DVC keeps data in remote shared Windows folder? Thanks.<\/p>",
        "Challenge_closed_time":1641179335767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641163263100,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1641199464667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70560288",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":14.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4.4646297222,
        "Challenge_title":"DVC Shared Windows Directory Setup",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":1331553057367,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":10643.0,
        "Poster_view_count":504.0,
        "Solution_body":"<p>If you are using a local remote this way, you won't be able to have to the same <code>url<\/code> on both platforms since the mount points are different (as you already realized).<\/p>\n<p>The simplest way to configure this would be to pick one (Linux or Windows) <code>url<\/code> to use as your default case that gets git-committed into <code>.dvc\/config<\/code>. On the other platform you (or your users) can override that <code>url<\/code> in the local configuration file: <code>.dvc\/config.local<\/code>.<\/p>\n<p>(Note that <code>.dvc\/config.local<\/code> is a git-ignored file and will not be included in any commits)<\/p>\n<p>So if you wanted Windows to be the default case, in <code>.dvc\/config<\/code> you would have:<\/p>\n<pre><code> [core]\n    analytics = false\n    remote = remote_storage\n['remote &quot;remote_storage&quot;']\n    url = \\\\my_shared_storage\\project_dir\n<\/code><\/pre>\n<p>and on your Linux machine you would add the file <code>.dvc\/config.local<\/code> containing:<\/p>\n<pre><code>['remote &quot;remote_storage&quot;']\n    url = \/mnt\/mount_point\/project_dir\n<\/code><\/pre>\n<p>See the DVC docs for <code>dvc config --local<\/code> and <code>dvc remote modify --local<\/code> for more details:<\/p>\n<ul>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/config#description<\/a><\/li>\n<li><a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#command-options-flags<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.4,
        "Solution_reading_time":20.54,
        "Solution_score_count":3.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":160.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1614882423070,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":1.5591075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For some reason the jupyter notebooks on my VM are in the wrong environment (ie stuck in <code>(base)<\/code>). Furthermore, I can change the environment in the terminal but not in the notebook. Here is what happens when I attempt <code>!conda activate desired_env<\/code> in the notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n# conda environments:\n#\nbase                  *  \/anaconda\nazureml_py36             \/anaconda\/envs\/azureml_py36\nazureml_py38             \/anaconda\/envs\/azureml_py38\nazureml_py38_pytorch     \/anaconda\/envs\/azureml_py38_pytorch\nazureml_py38_tensorflow     \/anaconda\/envs\/azureml_py38_tensorflow\n<\/code><\/pre>\n<p>I tried the answers <a href=\"https:\/\/stackoverflow.com\/questions\/61915607\/commandnotfounderror-your-shell-has-not-been-properly-configured-to-use-conda\">here<\/a> (e.g., first running <code>!source \/anaconda\/etc\/profile.d\/conda.sh<\/code>).<\/p>\n<p>I also tried activating the environment using <code>source<\/code> rather than 'conda activate': <code>!source \/anaconda\/envs\/desired_env\/bin\/activate<\/code>. This runs but doesn't actually do anything when I see the current environment in <code>conda env list<\/code><\/p>\n<p>Edit: also adding that if I install a package in the <code>(base)<\/code> environment in the terminal, I still don't have access to it in jupyter notebook.<\/p>",
        "Challenge_closed_time":1636652640790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636646694700,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1636647028003,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69931411",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.6516916667,
        "Challenge_title":"Can't change virtual environment within Azure ML notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":672.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586979502910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":361.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>I'm the PM that released AzureML Notebooks, you can't activate a Conda env from a cell, you have to create a new kernel will the Conda Env. Here are the instructions: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels<\/a><\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":5.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":204.9339161111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When attempting to select the Dependencies file in the AML GUI - the file selector window that opens has the wrong file extension selected (*.py) - it should be either <em>.<\/em> or potentially <em>.yml\/<\/em>.yaml - thanks    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/272512-capture3.png?platform=QnA\" alt=\"272512-capture3.png\" \/>    <\/p>",
        "Challenge_closed_time":1672281348808,
        "Challenge_comment_count":2,
        "Challenge_created_time":1671543586710,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1136021\/selecting-the-dependencies-file-in-the-studio-gui",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":204.9339161111,
        "Challenge_title":"Selecting the Dependencies file in the Studio GUI for deploying to an endpoint is bugged - wrong file format specified",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=0ec06fb6-513e-4f5c-9aff-281bc5e44e22\">@Neil McAlister  <\/a>     <\/p>\n<p>Thanks for your waiting due to the holiday season and sorry for the confusion. This buttion is actually for adding <strong>code dependencies<\/strong>, i.e., my scoring script requires dependency.py. For package dependencies, those need to be included in the environment to use for the deployment.    <\/p>\n<p>The button name &quot;Add Dependencies&quot; is confused here, and product team is working on changing the name or add more explanation for this button.     <\/p>\n<p>In the UI here, you must have already created an environment to do a deployment. Providing a conda.yml file in the endpoints create dialog will not register a new environment for you.  The environment must be selected in the bottom section of the page as below -    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/274589-microsoftteams-image-12.png?platform=QnA\" alt=\"274589-microsoftteams-image-12.png\" \/>    <\/p>\n<p>If your environment doesn't exist in this list, or in the custom environments, then you'll need to go to the environments tab on the left side of the portal and create an enviornment.    <\/p>\n<p>I am sorry for the confusion and product team will fix it in the near future.    <\/p>\n<p>I hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly acceept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":18.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":204.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1469978721363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":73.1672544445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We can now publish Docker images to AWS ECR directly from <strong>SageMaker Studio<\/strong> using this code <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli<\/a>\nI did follow the easy installation instructions:<\/p>\n<pre><code>!pip install sagemaker-studio-image-build\nsm-docker build .\n<\/code><\/pre>\n<p>Also Trust policy and permissions have been set as described in the instructions.\nBut I'm getting the error &quot;<strong>Command did not exit successfully docker push<\/strong>&quot; at the stage where it is pushing the Docker image to AWS ECR. Any idea why? Here are the details print as output:<\/p>\n<pre><code>[Container] 2021\/05\/04 06:57:20 Running command echo Pushing the Docker image...\nPushing the Docker image...\n\n[Container] 2021\/05\/04 06:57:20 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG\nThe push refers to repository [752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml]\nAn image does not exist locally with the tag: 752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml\n\n[Container] 2021\/05\/04 06:57:20 Command did not exit successfully docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG exit status 1\n[Container] 2021\/05\/04 06:57:20 Phase complete: POST_BUILD State: FAILED\n[Container] 2021\/05\/04 06:57:20 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG. Reason: exit status 1\n<\/code><\/pre>",
        "Challenge_closed_time":1620378926343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620115524227,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67380942",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":23.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":73.1672544445,
        "Challenge_title":"Pushing docker image in AWS ECR from SageMaker Studio using AWS CLI",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1046.0,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469978721363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":415.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>In the Dockerfile, there was a reference to another file that was not present in the directory from where the command <code>sm-docker build .<\/code> was launched.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":2.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1544524371740,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cologne Germany",
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":3.6875811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running into a ModuleNotFoundError for pandas while using the following code to orchestrate my Azure Machine Learning Pipeline:<\/p>\n<pre><code># Loading run config\nprint(&quot;Loading run config&quot;)\ntask_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \n\ntask_1_script_run_config = ScriptRunConfig(\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    run_config=task_1_run_config    \n)\n\ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name=task_1_script_run_config.script,\n    source_directory=task_1_script_run_config.source_directory,\n    compute_target=compute_target\n)\n\npipeline_run_config = Pipeline(workspace=workspace, steps=[task_1_py_script_step])#, task_2])\n\npipeline_run = Experiment(workspace, 'Test_Run_New_Pipeline').submit(pipeline_run_config)\npipeline_run.wait_for_completion()\n<\/code><\/pre>\n<p>The environment.yml<\/p>\n<pre><code>name: phinmo_pipeline_env\ndependencies:\n- python=3.8\n- pip:\n  - pandas\n  - azureml-core==1.43.0\n  - azureml-sdk\n  - scipy\n  - scikit-learn\n  - numpy\n  - pyyaml==6.0\n  - datetime\n  - azure\nchannels:\n  - conda-forge\n<\/code><\/pre>\n<p>The loaded RunConfiguration in T01_Test_Task.yml looks like this:<\/p>\n<pre><code># The script to run.\nscript: T01_Test_Task.py\n# The arguments to the script file.\narguments: [\n  &quot;--test&quot;, False,\n  &quot;--date&quot;, &quot;2022-07-26&quot;\n]\n# The name of the compute target to use for this run.\ncompute_target: phinmo-compute-cluster\n# Framework to execute inside. Allowed values are &quot;Python&quot;, &quot;PySpark&quot;, &quot;CNTK&quot;, &quot;TensorFlow&quot;, and &quot;PyTorch&quot;.\nframework: Python\n# Maximum allowed duration for the run.\nmaxRunDurationSeconds: 6000\n# Number of nodes to use for running job.\nnodeCount: 1\n\n#Environment details.\nenvironment:\n  # Environment name\n  name: phinmo_pipeline_env\n  # Environment version\n  version:\n  # Environment variables set for the run.\n  #environmentVariables:\n  #  EXAMPLE_ENV_VAR: EXAMPLE_VALUE\n  # Python details\n  python:\n    # user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.\n    userManagedDependencies: false\n    # The python interpreter path\n    interpreterPath: python\n    # Path to the conda dependencies file to use for this run. If a project\n    # contains multiple programs with different sets of dependencies, it may be\n    # convenient to manage those environments with separate files.\n    condaDependenciesFile: environment.yml\n    # The base conda environment used for incremental environment creation.\n    baseCondaEnvironment: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n  # Docker details\n  \n# History details.\nhistory:\n  # Enable history tracking -- this allows status, logs, metrics, and outputs\n  # to be collected for a run.\n  outputCollection: true\n  # Whether to take snapshots for history.\n  snapshotProject: true\n  # Directories to sync with FileWatcher.\n  directoriesToWatch:\n  - logs\n# data reference configuration details\ndataReferences: {}\n# The configuration details for data.\ndata: {}\n# Project share datastore reference.\nsourceDirectoryDataStore:\n<\/code><\/pre>\n<p>I already tried a few things like overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml. But I am at a loss at what else to try. the T01_Test_Task.py runs without issues on its own. But putting it into a pipeline just does not seem to work.<\/p>",
        "Challenge_closed_time":1658937635368,
        "Challenge_comment_count":1,
        "Challenge_created_time":1658922798813,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1658924360076,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73137433",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":47.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":4.1212652778,
        "Challenge_title":"ModuleNotFoundError while using AzureML pipeline with yml file based RunConfiguration and environment.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":366,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544524371740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cologne Germany",
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Okay I found the issue.\nI am unnecessarily using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. I was able to see that only in the Task description in the Azure Machine Learning Studio UI.<\/p>\n<p>I was able to just remove that part and now it works:<\/p>\n<pre><code>task_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name='T01_Test_Task.py',\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    runconfig=task_1_run_config, \n    compute_target=compute_target\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.0403669444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>The following documentation specifies this option...    <\/p>\n<blockquote>\n<p>To reference an existing environment, use the azureml:&lt;environment-name&gt;:&lt;environment-version&gt; syntax.    <\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-gb\/azure\/machine-learning\/reference-yaml-deployment-managed-online#:%7E:text=To%20reference%20an%20existing%20environment%2C%20use%20the%20azureml%3A%3Cenvironment%2Dname%3E%3A%3Cenvironment%2Dversion%3E%20syntax\">https:\/\/learn.microsoft.com\/en-gb\/azure\/machine-learning\/reference-yaml-deployment-managed-online#:~:text=To%20reference%20an%20existing%20environment%2C%20use%20the%20azureml%3A%3Cenvironment%2Dname%3E%3A%3Cenvironment%2Dversion%3E%20syntax<\/a>.    <\/p>\n<p>Can anyone help me work out what I need to put into the YAML deployment config file please - tried multiple things and nothing works. I tried to highlight this over at the documentation GitHub but they told me to come here - not a documentation issue apparently! There are no example around.    <\/p>\n<p>This is my deployment YAML file - I want to reference the Azure Machine Learning existing environment which is called <strong>AzureML-sklearn-1.0-ubuntu20.04-py38-cpu<\/strong> version <strong>32<\/strong>    <\/p>\n<pre><code>$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json  \nmodel: azureml:my-model:1  \ncode_configuration:  \n  code: models\/my-model\/  \n  scoring_script: score.py  \nenvironment:   \n ???????  \n  conda_file: models\/my-model\/conda.yml  \ninstance_type: Standard_DS2_v2  \ninstance_count: 2  \napp_insights_enabled: true  \n<\/code><\/pre>\n<p>Thank you in advance    <\/p>",
        "Challenge_closed_time":1671596306488,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671542161167,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1135929\/cli-v2-managed-online-endpoint-deployment-yaml-fil",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.8,
        "Challenge_reading_time":23.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":15.0403669444,
        "Challenge_title":"CLI v2 managed online endpoint deployment YAML file referencing an existing inbuilt environment - how?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=0ec06fb6-513e-4f5c-9aff-281bc5e44e22\">@Neil McAlister  <\/a> Thanks for the question. Here is the sample to create a Deployment YAML Definition.    <\/p>\n<pre><code>$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json \u200b  \n  \nname: blue \u200b  \n  \nendpoint_name: my-endpoint \u200b  \n  \nmodel: \u200b  \n  \n    path: ..\/..\/model-1\/model\/ \u200b  \n  \ncode_configuration: \u200b  \n  \n    code: ..\/..\/model-1\/onlinescoring\/ \u200b  \n  \n    scoring_script: score.py \u200b  \n  \nenvironment: \u200b  \n  \n    conda_file: ..\/..\/model-1\/environment\/conda.yml \u200b  \n  \n    image: mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:20210727.v1 \u200b  \n  \ninstance_type: Standard_DS2_v2 \u200b  \n  \ninstance_count: 1  \n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.1,
        "Solution_reading_time":8.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":91.9480555556,
        "Challenge_answer_count":2,
        "Challenge_body":"'m going through this notebook: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\r\n\r\nI need to start the training using the docker image from my local registry. I provided all required data in the environment I created:\r\n\r\nconda_env.docker.enabled = True\r\nconda_env.docker.base_image = \"tf_od_api:latest\"\r\nconda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\"\r\nconda_env.docker.base_image_registry.username = \"MyToken\"\r\nconda_env.docker.base_image_registry.password = \"MyPassword\"\r\n\r\nconda_env.python.user_managed_dependencies = True\r\n\r\nsrc = ScriptRunConfig(source_directory='azureml-examples\/workflows\/train\/fastai\/pets\/src',\r\n                      script='aml_wrapper.py',\r\n                      compute_target=attached_dsvm_compute,\r\n                      environment=conda_env)\r\nrun = exp.submit(config=src)\r\nrun.wait_for_completion(show_output=True)\r\n\r\nAnd when I start the pipeline I got: \"FailedPullingImage: Unable to pull docker image\\n\\timageName: Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"\r\n\r\nIf I set conda_env.python.user_managed_dependencies = False\r\n\r\nthen the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. But on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Challenge_closed_time":1614604742000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1614273729000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1371",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":21.99,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":91.9480555556,
        "Challenge_title":"Azure ML Run docker command to pull public image failed ",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":179,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"please try this \r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1247#issuecomment-738887772 Seems like it solved the issue. Thanks!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":45.7504811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a Custom Training Job in Google Cloud Platform's Vertex AI Training service.<\/p>\n<p>The job is based on <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-train-and-tune-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">a tutorial from Google that fine-tunes a pre-trained BERT model<\/a> (from HuggingFace).<\/p>\n<p>When I use the <code>gcloud<\/code> CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:<\/p>\n<pre><code>$BASE_GPU_IMAGE=&quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest&quot;\n$BUCKET_NAME = &quot;my-bucket&quot;\n\ngcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=&quot;--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier&quot; `\n--worker-pool-spec=&quot;machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task&quot;\n<\/code><\/pre>\n<p>... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.<\/p>\n<p>Granted <a href=\"https:\/\/console.cloud.google.com\/gcr\/images\/deeplearning-platform-release\/GLOBAL\/pytorch-gpu.1-7?tag=nightly-2021-03-28\" rel=\"nofollow noreferrer\">the base image is around 6.5GB<\/a> but <strong>where do the additional &gt;10GB come from and is there a way for me to avoid this &quot;image bloat&quot;?<\/strong><\/p>\n<p>Please note that my job loads the training data using the <code>datasets<\/code> Python package at run time and AFAIK does not include it in the auto-packaged docker image.<\/p>",
        "Challenge_closed_time":1646123694692,
        "Challenge_comment_count":1,
        "Challenge_created_time":1645958992960,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71284125",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.4,
        "Challenge_reading_time":23.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":45.7504811111,
        "Challenge_title":"GCP Vertex AI Training: Auto-packaged Custom Training Job Yields Huge Docker Image",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":478.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Poster_created_time":1225618447787,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv, Israel",
        "Poster_reputation_count":15116.0,
        "Poster_view_count":1182.0,
        "Solution_body":"<p>The image size shown in the UI is the virtual size of the image. It is the compressed total image size that will be downloaded over the network. Once the image is pulled, it will be extracted and the resulting size will be bigger. In this case, the <a href=\"https:\/\/console.cloud.google.com\/artifacts\/docker\/vertex-ai\/us\/training\/pytorch-gpu.1-7\/sha256:0d990ebc4fc376880fd3ee375015e43594450d11d9791ac203cf2d044871917f\" rel=\"nofollow noreferrer\">PyTorch image's virtual size<\/a> is 6.8 GB while the actual size is 17.9 GB.<\/p>\n<p>Also, when a <code>docker push<\/code> command is executed, the progress bars show the uncompressed size. The actual amount of data that\u2019s pushed will be <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/push\/#extended-description\" rel=\"nofollow noreferrer\">compressed before sending<\/a>, so the uploaded size will not be reflected by the progress bar.<\/p>\n<p>To cut down the size of the docker image, custom containers can be used. Here, only the necessary components can be configured which would result in a smaller docker image. More information on custom containers <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/containers-overview\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.0,
        "Solution_reading_time":15.97,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":146.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.8786111111,
        "Challenge_answer_count":1,
        "Challenge_body":"",
        "Challenge_closed_time":1615314058000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615292895000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/175",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":1.84,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":5.8786111111,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":12,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Closing in favour of #174 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":0.5,
        "Solution_reading_time":0.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8976.4125,
        "Challenge_answer_count":2,
        "Challenge_body":"I got **ImportError** when trying to use AutoGluon in a SageMaker instance (ml.c5d.4xlarge), with kernel being **conda_python3**.\r\n\r\nThe error I got is:\r\n```\r\nfrom autogluon import TabularPrediction as task\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-6f7d1b4fed2f> in <module>()\r\n----> 1 from autogluon import TabularPrediction as task\r\n\r\nImportError: cannot import name 'TabularPrediction'\r\n```\r\n\r\nIf I try\r\n```\r\nimport autogluon as ag\r\nag.TabularPrediction.Dataset(file_path='data\/nbc_golf_model_1_training.csv')\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-a8e4ec84df4b> in <module>()\r\n----> 1 ag.TabularPrediction.Dataset(file_path='nbc_golf_model_1_training.csv')\r\n\r\nAttributeError: module 'autogluon' has no attribute 'TabularPrediction'\r\n```\r\n\r\nFor your reference:\r\n\r\nI installed AutoGluon by using Version PIP in the notebook as usual.\r\n```\r\n!pip install --upgrade mxnet\r\n!pip install autogluon\r\n```\r\n\r\n```\r\nCollecting mxnet\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/6c\/c6e5562f8face683cec73f5d4d74a58f8572c0595d54f1fed9d923020bbd\/mxnet-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25.4MB 1.9MB\/s eta 0:00:01\r\nRequirement not upgraded as not directly required: requests<3,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (2.20.0)\r\nRequirement not upgraded as not directly required: graphviz<0.9.0,>=0.8.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (0.8.4)\r\nRequirement not upgraded as not directly required: numpy<2.0.0,>1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (1.16.4)\r\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\r\nRequirement not upgraded as not directly required: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\r\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2019.9.11)\r\nRequirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\r\nInstalling collected packages: mxnet\r\nSuccessfully installed mxnet-1.5.1.post0\r\n```\r\nThere are 2 errors in the second installation step:\r\n\r\n**ERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.**\r\n```\r\nCollecting autogluon\r\n  Downloading autogluon-0.0.5-py3-none-any.whl (328 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 328 kB 18.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: tornado>=5.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.0.2)\r\nRequirement already satisfied: cryptography>=2.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.8)\r\nCollecting lightgbm==2.3.0\r\n  Downloading lightgbm-2.3.0-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 33.4 MB\/s eta 0:00:01\r\nRequirement already satisfied: paramiko>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.6.0)\r\nCollecting scipy>=1.3.3\r\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 32.8 MB\/s eta 0:00:01\r\nCollecting boto3==1.9.187\r\n  Downloading boto3-1.9.187-py2.py3-none-any.whl (128 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 36.5 MB\/s eta 0:00:01\r\nRequirement already satisfied: cython in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.28.2)\r\nCollecting scikit-optimize\r\n  Downloading scikit_optimize-0.7.1-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 10.7 MB\/s eta 0:00:01\r\nRequirement already satisfied: Pillow<=6.2.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.2.0)\r\nCollecting catboost\r\n  Downloading catboost-0.21-cp36-none-manylinux1_x86_64.whl (64.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64.0 MB 36.8 MB\/s eta 0:00:01\r\nCollecting gluonnlp==0.8.1\r\n  Downloading gluonnlp-0.8.1.tar.gz (236 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 236 kB 63.1 MB\/s eta 0:00:01\r\nRequirement already satisfied: psutil>=5.0.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.6.3)\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.24.2)\r\nRequirement already satisfied: graphviz in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.8.4)\r\nCollecting dask==2.6.0\r\n  Downloading dask-2.6.0-py3-none-any.whl (760 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 760 kB 66.0 MB\/s eta 0:00:01\r\nRequirement already satisfied: requests in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.20.0)\r\nCollecting scikit-learn==0.21.2\r\n  Downloading scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.7 MB 32.2 MB\/s eta 0:00:01\r\nCollecting distributed==2.6.0\r\n  Downloading distributed-2.6.0-py3-none-any.whl (560 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560 kB 70.9 MB\/s eta 0:00:01\r\nRequirement already satisfied: matplotlib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (3.0.3)\r\nCollecting ConfigSpace<=0.4.10\r\n  Downloading ConfigSpace-0.4.10.tar.gz (882 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 882 kB 72.3 MB\/s eta 0:00:01\r\nCollecting tqdm>=4.38.0\r\n  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 10.6 MB\/s eta 0:00:01\r\nCollecting gluoncv>=0.5.0\r\n  Downloading gluoncv-0.6.0-py2.py3-none-any.whl (693 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 693 kB 69.3 MB\/s eta 0:00:01\r\nRequirement already satisfied: numpy>=1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (1.16.4)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.5)\r\nRequirement already satisfied: six>=1.4.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.0)\r\nRequirement already satisfied: bcrypt>=3.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (3.1.7)\r\nRequirement already satisfied: pynacl>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (1.3.0)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.9.4)\r\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.2.1)\r\nCollecting botocore<1.13.0,>=1.12.187\r\n  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7 MB 47.6 MB\/s eta 0:00:01\r\nCollecting pyaml\r\n  Downloading pyaml-19.12.0-py2.py3-none-any.whl (17 kB)\r\nCollecting joblib\r\n  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 294 kB 55.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: plotly in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from catboost->autogluon) (4.2.1)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2018.4)\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2.7.3)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2.6)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (1.23)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2019.9.11)\r\nRequirement already satisfied: tblib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.3.2)\r\nRequirement already satisfied: pyyaml in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (3.12)\r\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.5.10)\r\nRequirement already satisfied: msgpack in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.6.0)\r\nRequirement already satisfied: zict>=0.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.1.3)\r\nRequirement already satisfied: toolz>=0.7.4 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.9.0)\r\nRequirement already satisfied: cloudpickle>=0.2.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.5.3)\r\nRequirement already satisfied: click>=6.6 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (6.7)\r\nRequirement already satisfied: cycler>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (1.0.1)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (2.2.0)\r\nRequirement already satisfied: typing in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from ConfigSpace<=0.4.10->autogluon) (3.6.4)\r\nCollecting portalocker\r\n  Downloading portalocker-1.5.2-py2.py3-none-any.whl (14 kB)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->autogluon) (2.18)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from botocore<1.13.0,>=1.12.187->boto3==1.9.187->autogluon) (0.14)\r\nRequirement already satisfied: retrying>=1.3.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from plotly->catboost->autogluon) (1.3.3)\r\nRequirement already satisfied: heapdict in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from zict>=0.1.3->distributed==2.6.0->autogluon) (1.0.0)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from kiwisolver>=1.0.1->matplotlib->autogluon) (39.1.0)\r\nBuilding wheels for collected packages: gluonnlp, ConfigSpace\r\n  Building wheel for gluonnlp (setup.py) ... done\r\n  Created wheel for gluonnlp: filename=gluonnlp-0.8.1-py3-none-any.whl size=289392 sha256=3eba5a08b1bdd7719e9e6d869c3029e8aae5eb848f58c3f30ad5d42fe0969b9f\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/cb\/1c\/e6fb5e5eefcd5fe8ee2163f27c79a63c96d9a956e8d93fb496\r\n  Building wheel for ConfigSpace (setup.py) ... done\r\n  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.10-cp36-cp36m-linux_x86_64.whl size=3000873 sha256=35ce111cf113601a2e6543690fb721b2449622e0c010e0b6bc094a498890edc4\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/71\/a2\/00ca7cb0f71294d73e8791d6fe5cd0c7401066ec3b7e1026db\r\nSuccessfully built gluonnlp ConfigSpace\r\nERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.\r\nInstalling collected packages: scipy, joblib, scikit-learn, lightgbm, botocore, boto3, pyaml, scikit-optimize, catboost, gluonnlp, dask, distributed, ConfigSpace, tqdm, portalocker, gluoncv, autogluon\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.2.1\r\n    Uninstalling scipy-1.2.1:\r\n      Successfully uninstalled scipy-1.2.1\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 0.20.3\r\n    Uninstalling scikit-learn-0.20.3:\r\n      Successfully uninstalled scikit-learn-0.20.3\r\n  Attempting uninstall: botocore\r\n    Found existing installation: botocore 1.13.19\r\n    Uninstalling botocore-1.13.19:\r\n      Successfully uninstalled botocore-1.13.19\r\n  Attempting uninstall: boto3\r\n    Found existing installation: boto3 1.10.19\r\n    Uninstalling boto3-1.10.19:\r\n      Successfully uninstalled boto3-1.10.19\r\n  Attempting uninstall: dask\r\n    Found existing installation: dask 0.17.5\r\n    Uninstalling dask-0.17.5:\r\n      Successfully uninstalled dask-0.17.5\r\n  Attempting uninstall: distributed\r\n    Found existing installation: distributed 1.21.8\r\n    Uninstalling distributed-1.21.8:\r\n      Successfully uninstalled distributed-1.21.8\r\nSuccessfully installed ConfigSpace-0.4.10 autogluon-0.0.5 boto3-1.9.187 botocore-1.12.253 catboost-0.21 dask-2.6.0 distributed-2.6.0 gluoncv-0.6.0 gluonnlp-0.8.1 joblib-0.14.1 lightgbm-2.3.0 portalocker-1.5.2 pyaml-19.12.0 scikit-learn-0.21.2 scikit-optimize-0.7.1 scipy-1.4.1 tqdm-4.42.1\r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1613263024000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580947939000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/268",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.4,
        "Challenge_reading_time":192.92,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":230,
        "Challenge_solved_time":8976.4125,
        "Challenge_title":"ImportError for TabularPrediction in SageMaker Notebook Instance",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":999,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for submitting this issue!\r\n\r\nWe haven't looked closely at which boto versions are functional with AutoGluon, but I would suspect using the newer version wouldn't cause issues.\r\n\r\nWe will take a look.\r\n @zhuwenzhen In the latest mainline of AutoGluon, the boto version limitation has been removed. This may resolve your issue if you install AutoGluon from source, or wait for AutoGluon 0.1 to be released.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":15023.6469805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can someone explain or help me install <a href=\"https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/tree\/master\/python\" rel=\"nofollow noreferrer\">vowpalwabbit<\/a> (I'm interested in the python bindings) on an Amazon linux machine, either EC2 or SageMaker?\nfor some reason it is very hard and I can't find anything about it online...<\/p>\n\n<p>a <code>pip install vowpalwabbit<\/code> returns a <\/p>\n\n<pre><code>Using cached https:\/\/files.pythonhosted.org\/packages\/d1\/5a\/9fcd64fd52ad22e2d1821b2ef871e8783c324b37e2103e7ddefa776c2ed7\/vowpalwabbit-8.8.0.tar.gz\nBuilding wheels for collected packages: vowpalwabbit\n  Building wheel for vowpalwabbit (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0]= '\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-x0j85ac_ --python-tag cp36\n       cwd: \/tmp\/pip-install-tvp1174t\/vowpalwabbit\/\n<\/code><\/pre>\n\n<p>lower in the error I can also see a:<\/p>\n\n<pre><code>CMake Error at \/usr\/lib64\/python3.6\/dist-packages\/cmake\/data\/share\/cmake-3.13\/Modules\/FindBoost.cmake:2100 (message):\n    Unable to find the requested Boost libraries.\n\n    Boost version: 1.53.0\n\n    Boost include path: \/usr\/include\n\n    Could not find the following Boost libraries:\n\n            boost_python3\n\n    Some (but not all) of the required Boost libraries were found.  You may\n    need to install these additional Boost libraries.  Alternatively, set\n    BOOST_LIBRARYDIR to the directory containing Boost libraries or BOOST_ROOT\n    to the location of Boost.\n<\/code><\/pre>",
        "Challenge_closed_time":1634653018680,
        "Challenge_comment_count":1,
        "Challenge_created_time":1580567889550,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60017893",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":15.4,
        "Challenge_reading_time":25.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15023.6469805556,
        "Challenge_title":"How to install Vowpal Wabbit on Amazon EC2 or SageMaker? (amazon linux)",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442180190107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3203.0,
        "Poster_view_count":400.0,
        "Solution_body":"<p>Tested again 1.5 years later, and a <code>pip install vowpalwabbit<\/code> works fine on notebook instance. In training job, adding vowpalwabbit in a <code>requirements.txt<\/code> send to an AWS-managed Scikit learn container (<code>141502667606.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3<\/code>) also installs successfully. Both tested with vowpalwabbit-8.11.0<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1.2687819444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Suppose I have a server where many users run different experiments, possibly with different Trains Servers.<\/p>\n<p>I know about the <code>TRAINS_CONFIG_FILE<\/code> environment variable, but I wonder if this can be made more flexible in one of the following ways:<\/p>\n<ol>\n<li>Specifying the Trains config file dynamically, i.e. during runtime of the training script?<\/li>\n<li>Storing a config file in each of the training repos and specifying its path relatively to the running script path (instead of relatively to <code>~\/<\/code>)?<\/li>\n<\/ol>",
        "Challenge_closed_time":1592997291032,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592992723417,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609531417760,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62552414",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":7.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.2687819444,
        "Challenge_title":"Can Trains config file be specified dynamically or relative to the running script path?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311330349880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation_count":3784.0,
        "Poster_view_count":342.0,
        "Solution_body":"<p>Disclaimer: I'm a member of Allegro Trains team<\/p>\n<ol>\n<li>Loading of the configuration is done at import time. This means that if you set the os environment before importing the package, you should be fine:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>os.environ['TRAINS_CONFIG_FILE']='~\/repo\/trains.conf'\nfrom trains import Task\n<\/code><\/pre>\n<ol start=\"2\">\n<li>The configuration file is loaded based on the current working directory, this means that if you have <code>os.environ['TRAINS_CONFIG_FILE']='trains.conf'<\/code> the trains.conf file will be loaded from the running directory at the time the import happens (which usually is the folder where your script is executed from). This means you can have it as part of the repository, and always set the <code>TRAINS_CONFIG_FILE<\/code> to point to it.<\/li>\n<\/ol>\n<p>A few notes:<\/p>\n<ul>\n<li>What is the use case for different configuration files ?<\/li>\n<li>Notice that when running with <a href=\"https:\/\/github.com\/allegroai\/trains-agent\" rel=\"nofollow noreferrer\">trains-agent<\/a> , this method will override the configuration that the <em>trains-agent<\/em> passes to the code.<\/li>\n<\/ul>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":14.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":149.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":1229.6615519445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Azure ML. I am having some doubts .Could anyone please clarify my doubts listed below.<\/p>\n\n<ol>\n<li>What is the difference between Azure ML service Azure ML experimentation service.<\/li>\n<li>What is the difference between Azure ML workbench and Azure ML Studio.<\/li>\n<li>I want to use azure ML Experimentation service for building few models and creating web API's. Is it possible to do the same with ML studio. <\/li>\n<li>And also ML Experimentation service requires me to have a docker for windows installed for creating web services.\nCan i create web services without using docker?<\/li>\n<\/ol>",
        "Challenge_closed_time":1525629753687,
        "Challenge_comment_count":1,
        "Challenge_created_time":1521202972100,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1554674738927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49320679",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1229.6615519445,
        "Challenge_title":"Difference between Azure ML and Azure ML experimentation",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":763.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1422010576070,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom House, London, UK",
        "Poster_reputation_count":388.0,
        "Poster_view_count":124.0,
        "Solution_body":"<ol>\n<li><p>The AML Experimentation is one of our many new ML offerings, including data preparation, experimentation, model management, and operationalization. Workbench is a PREVIEW product that provides a GUI for some of these services. But it is just a installer\/wrapper for the CLI that is needed to run. The services are Spark and Python based. Other Python frameworks will work, and you can get a little hacky to call Java\/Scala from Python. Not really sure what you mean by an \"Azure ML Service\", perhaps you are referring to the operationalization service I mentioned above. This will quickly let you create new Python based APIs using Docker containers, and will connect with the model management account to keep track of the linage between your models and your services. All services here are still in preview and may breaking change before GA release. <\/p><\/li>\n<li><p>Azure ML Studio is an older product that is perhaps simpler for some(myself an engineer not a data scientist). It offers a drag and drop experience, but is limited in it's data size to about 10G. This product is GA. <\/p><\/li>\n<li><p>It is, but you need smaller data sizes, and the job flow is not spark based. I use this to do rapid PoC's. Also you will less control over the scalability of your scoring (batch or real time), because it is PaaS, compared to the newer service which is more IaaS. I would recommend looking at the new service instead of studio for most use cases. <\/p><\/li>\n<li><p>The web services are completely based on Docker. Needing docker for experimentation is more about running things locally, which I myself rarely do. But, for the real time service, everything you package is placed into a docker container so it can be deployed to an ACS cluster. <\/p><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":21.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":299.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":145.9862022222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am quite new to docker. My problem in short is to create a docker file that contains python with sklearn and pandas which can be used on aws sagemaker.<\/p>\n<p>My current docker file looks like the following:<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>However when i try to create this image I get an error at line <code>pip3 install sagemaker-training<\/code>. The error is the following:<\/p>\n<pre><code>error: command 'gcc' failed: No such file or directory\n\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-o5rzjscd\/install-record.txt --single-version-externally-managed --compile --install-headers \/opt\/conda\/include\/python3.9\/sagemaker-training Check the logs for full command output.\n\nThe command '\/bin\/bash -o pipefail -c pip3 install sagemaker-training' returned a non-zero code: 1\n<\/code><\/pre>\n<p>If there is a more suitable base image can someone point that out to me? I am generally trying to follow this page <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit<\/a>.<\/p>\n<p>Note: I realise I can use some sagemaker pre-built containers without using my own docker file. However I am trying to do this for my own learning so I know what to do for projects that can't utilise them.<\/p>",
        "Challenge_closed_time":1635439092896,
        "Challenge_comment_count":5,
        "Challenge_created_time":1634912008467,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1634913542568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69678393",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":11.5,
        "Challenge_reading_time":29.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":146.4123413889,
        "Challenge_title":"Creating docker file which installs python with sklearn and pandas that can be used on sagemaker",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":161.0,
        "Challenge_word_count":227,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737124656,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milton Keynes",
        "Poster_reputation_count":738.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>I adjusted your Dockerfile and it builds successfully for me.<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\nARG defaultuser=jovyan\nUSER root\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update &amp;&amp; \\\n    apt-get -y install gcc mono-mcs &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\nUSER $defaultuser\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>(I had to adjust for the fact that the default user from the base container isn't root, when installing GCC)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":6.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1444035983568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":2377.1102455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My code is in R. And I need to excess external database. I am storing database credentials in AWS Secret Manager.<\/p>\n<p>So I first tried using paws library to get aws secrets in R but that would require storing access key, secret id and session token, and I want to avoid that.<\/p>\n<p>Is there a better way to do this? I have created IAM role for Sagemaker. Is it possible to pass secrets as environment variables?<\/p>\n<p>Edit: I wanted to trigger Sagemaker Processing<\/p>",
        "Challenge_closed_time":1593501738047,
        "Challenge_comment_count":2,
        "Challenge_created_time":1591862670733,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1593501623132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62319753",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.4,
        "Challenge_reading_time":6.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":455.2964761111,
        "Challenge_title":"How to I pass secrets stored in AWS Secret Manager to a Docker container in Sagemaker?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1063.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444035983568,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>I found a simple solution to it. Env variables can be passed via Sagemaker sdk. It minimizes the dependencies.<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>\n<p>As another answer suggested, paws can be used as well to get secrets from aws. This would be a better approach<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1602059220016,
        "Solution_link_count":2.0,
        "Solution_readability":12.8,
        "Solution_reading_time":5.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.3733694445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pip install pyenchant, but It doesn't seem to be working.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/169225-image.png?platform=QnA\" alt=\"169225-image.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/169252-image.png?platform=QnA\" alt=\"169252-image.png\" \/>    <\/p>\n<p>Is there any other way?    <br \/>\n<a href=\"https:\/\/stackoverflow.com\/questions\/21083059\/enchant-c-library-not-found-while-installing-pyenchant-using-pip-on-osx\">https:\/\/stackoverflow.com\/questions\/21083059\/enchant-c-library-not-found-while-installing-pyenchant-using-pip-on-osx<\/a>    <br \/>\nI looked it up but do not know where to put it    <br \/>\nThanks!<\/p>",
        "Challenge_closed_time":1643357543967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643330999837,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/713217\/machine-learning-conda-env-package(pyenchant)",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":20.1,
        "Challenge_reading_time":9.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7.3733694445,
        "Challenge_title":"machine learning conda env package(pyenchant)",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c427e306-40a4-4da4-b489-b3f6aae251d7\">@Yongchao Liu (Neusoft America Inc)  <\/a> Based on the error it looks like you also need to ensure the enchant C library is available to use for the package. Based on the pip install <a href=\"https:\/\/pyenchant.github.io\/pyenchant\/install.html#installation\">page<\/a> of pyenchant, the package will not work directly out of the box using pip.    <\/p>\n<blockquote>\n<p>In general, PyEnchant will not work out of the box after having been installed with pip. See the Installation section for more details.    <\/p>\n<\/blockquote>\n<p>Since you are using Linux, this is the guidance on the installation page.    <\/p>\n<blockquote>\n<p>The quickest way is to install libenchant using the package manager of your current distribution. PyEnchant tries to be compatible with a large number of libenchant versions. If you find an incompatibility with your libenchant installation, feel free to open a bug report.    <\/p>\n<p>To detect the libenchant binaries, PyEnchant uses ctypes.util.find_library(), which requires ldconfig, gcc, objdump or ld to be installed. This is the case on most major distributions, however statically linked distributions (like Alpine Linux) might not bring along binutils by default.    <\/p>\n<\/blockquote>\n<p>I believe you are using the ubuntu flavor of the azureml base image, In this case I think adding <a href=\"https:\/\/ubuntu.pkgs.org\/20.04\/ubuntu-main-amd64\/libenchant-2-dev_2.2.8-1_amd64.deb.html\">libenchant-2-dev<\/a> as dependency in your YAML should work.     <\/p>\n<pre><code>-libenchant-2-dev=2.2.8  \n<\/code><\/pre>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.1,
        "Solution_reading_time":25.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":236.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":29.8171413889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I hope I am not missing something obvious here. I am using the new azure ml studio designer. I am able to use to create datasets, train models and use them just fine.<\/p>\n\n<p>azure ml studio allows creation of Jupyter notebooks (also) and use them to do machine learning. I am able to do that too. <\/p>\n\n<p>So, now, I am wondering, can I build my ML pipeline\/experiment in ML studio designer, and once it is in good shape, export it as a python and jupyter notebook? then, use it in the same designer provided notebook option or may be use it locally?<\/p>",
        "Challenge_closed_time":1582139574710,
        "Challenge_comment_count":4,
        "Challenge_created_time":1582134046887,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60306240",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.5355063889,
        "Challenge_title":"export azure ml studio designer project as jupyter notebook?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":789.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442334437952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":2272.0,
        "Poster_view_count":516.0,
        "Solution_body":"<p>This is not currently supported, but I am 80% sure it is in the roadmap.\nAn alternative would be to use the SDK to create the same pipeline using <code>ModuleStep<\/code> where  I <em>believe<\/em> you can reference a Designer Module by its name to use it like a <code>PythonScriptStep<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1582241388596,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":3.66,
        "Solution_score_count":6.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1276712500692,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tandil, Buenos Aires Province, Argentina",
        "Answerer_reputation_count":7226.0,
        "Answerer_view_count":637.0,
        "Challenge_adjusted_solved_time":7952.5209027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train my model using Bring your own container technique in sagemaker. My model training runs correctly without any issues locally. But my docker image takes env-file as an input that could change at different runs. But in sagemaker when passing the ECR image, I don't know how to pass this env-file. So instead, inside the <code>train<\/code> script, which is called by the sagemaker, I added <code>export KEY=value<\/code> statements to create my variables. Even that did not expose my variables. Another way I tried it was by executing <code>RUN source file.env<\/code> while building my image. Even this approach did not work out as I got an error <code>\/bin\/sh: 1: source: not found<\/code>.<\/p>\n<p>I could try <code>ENV<\/code> while building my image and that would probably work but this approach won't be flexible as my variables could change at different runs. Is there any way to pass docker run arguments from a sagemaker estimator or notebook? I checked out the documentation but I couldn't find anything.<\/p>",
        "Challenge_closed_time":1662661497360,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634032422110,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69538469",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7952.5209027778,
        "Challenge_title":"Is there a way to pass arguments to our own docker container in sagemaker?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":292.0,
        "Challenge_word_count":180,
        "Platform":"Stack Overflow",
        "Poster_created_time":1633433905427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train<\/code> method states that:<\/p>\n<pre><code>environment (dict[str, str]) : Environment variables to be set for\n            use during training job (default: ``None``): \n<\/code><\/pre>\n<p>For reference, the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/5bc3ccf\/src\/sagemaker\/session.py#L569\" rel=\"nofollow noreferrer\">SDK source<\/a>.<\/p>\n<p>Because the SDK is a wrapper on top of <a href=\"https:\/\/pypi.org\/project\/boto3\/\" rel=\"nofollow noreferrer\">Boto3<\/a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=\"https:\/\/aws.amazon.com\/developer\/tools\/#SDKs\" rel=\"nofollow noreferrer\">Amazon Services SDK<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.2,
        "Solution_reading_time":11.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":92.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":529.4461111111,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\nA SageMaker Notebook-v3 workspace that was working fine on Friday today appears with the status as \"Unknown\". \r\nWhen clicking on connect the new window pop up but is empty, and when going back to the SWB page, we see the message, \"We have a problem! Something went wrong\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to 'Workspaces'\r\n2. Look for the workspace that was expected to be \"Stoped\"\r\n2. Click on 'connect'\r\n4. See error\r\n\r\n**Expected behavior**\r\nThat the workspace was \"Stopped\" and when clicking on Connect we can access to the workspace. \r\n\r\n**Screenshots**\r\n![Screen Shot 2021-09-13 at 1 27 57 PM](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**Versions (please complete the following information):**\r\nRelease Version installed: 3.3.1\r\n\r\n**Additional context**\r\nThe workspace was working fine all previous week, autostop and connect without any issue. Unknown status found today.",
        "Challenge_closed_time":1633460282000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631554276000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/708",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":529.4461111111,
        "Challenge_title":"[Bug] SageMaker Notebook-v3 Workspace changed to \"Unknown\" status and cannot connect anymore",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":148,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think there's a good chance this instance was autostopped, but that information was not propagated to DDB correctly.\r\n\r\nCan you log onto the hosting account for that Sagemaker instance and check if it's currently in the `Stopped` state. If yes, the latest code fixes that issue.\r\nhttps:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/8cb199b8093f5e799d2d87c228930a4929ebebb7 Hi @nguyen102 yes, I can confirm that the Sagemaker instance is  in the Stopped sate. So then, the latest code that you mention should fix the issue. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":6.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1351534968768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":823.0,
        "Answerer_view_count":114.0,
        "Challenge_adjusted_solved_time":43.0164288889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know how to access packages in <code>R<\/code> scripts in <code>Azure machine<\/code> learning by either using the <code>Azure<\/code> supported ones or by zipping up the packages.<\/p>\n\n<p>My problem now is that <code>Azure<\/code> machine learning does not support the <code>h2o package<\/code> and when I tried using the zipped file - it gave an <code>error<\/code>. <\/p>\n\n<p>Has anyone figured out how to use <code>h2o<\/code> in <code>R<\/code> in <code>Azure machine<\/code> learning?<\/p>",
        "Challenge_closed_time":1487237805672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487082559403,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1487082946528,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42228715",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":43.1239636111,
        "Challenge_title":"Running h2o in R script in Azure machine learning",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351534968768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":823.0,
        "Poster_view_count":114.0,
        "Solution_body":"<p>So since there was no reply to my question, I made some research and came up with the following:<\/p>\n\n<p>H2O cannot be ran in a straightforward manner in Azure machine learning embedded R scripts. A workaround the problem is to consider using an Azure created environment - specially for H2O. The options available are:<\/p>\n\n<ol>\n<li>Spinning up an H2O Artificial Intelligence Virtual Machine solution<\/li>\n<li>Using an H2O application for HDInsight<\/li>\n<\/ol>\n\n<p>For more reading, you can go to: <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html\" rel=\"nofollow noreferrer\">http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":8.43,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.9591811111,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm following [this guide](https:\/\/aws.amazon.com\/blogs\/machine-learning\/migrate-your-work-to-amazon-sagemaker-notebook-instance-with-amazon-linux-2\/) for transitioning to Amazon Linux 2 provided by AWS \n\nI've set up the two needed lifecycle configurations and created a new S3 Bucket to store the backup. I've also ensured the IAM roles have the required S3 permissions and updated the notebook with the ebs-backup-bucket tag per the instructions.\n\nWhen I run the notebook with the new configuration I get the following error: \n\"Notebook Instance Lifecycle Config [LIFECYCLE ARN] for Notebook Instance [NOTEBOOK ARN] took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.\n\nLooking at the logs I get the error: \n`\/bin\/bash: \/tmp\/OnStart_2022-11-09-01-51ontlqcqt: \/bin\/bash^M: bad interpreter: No such file or directory`\n\nAny thoughts on how to resolve this issue? The code for the backup lifecycle configuration can be found [here](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/migrate-ebs-data-backup\/on-start.sh)",
        "Challenge_closed_time":1667972357492,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667961704440,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668335812740,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPsaLux6EQcqvHW1HtNlkIw\/directory-error-when-running-sagemaker-backup-ebs-lifecycle-for-amazon-linux-2-transition",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":15.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2.9591811111,
        "Challenge_title":"Directory Error when running SageMaker backup-ebs lifecycle for Amazon Linux 2 transition",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":99.0,
        "Challenge_word_count":146,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The extra `^M` symbol (i.e. Ctrl-M) stopped the whole scrip from being interpreted properly. \n\nThis issue is normally seen in scripts prepared in MSDOS\/Windows based system but used in Linux system due to difference of line endings.\n\nIn Unix based OS, lines end with `\\n` but MSDOS\/Win based system ends with `\\r\\n`\n\nIn Linux based system, you could show your prepared scripts by running\n```\ncat -e some-script.sh \n``` \nThe results would be something similar to\n```\n#!\/bin\/bash^M$\n... ...^M$\n```\n`$` is normal Unix end-of-line symbol. Windows uses an extra one `^M` and this symbol is not recognized by Unix system. That's why, in SageMaker Notebook Lifecycle Configuration, which is running Linux, your script was interpreted as `\/bin\/bash^M`\n\n\nTo mitigate the issue, please convert the scripts to Unix based ending and update life cycle configuration. To achieve this, you could use `Notepad++` in Windows. You can go to the `Edit` menu, select the `EOL Conversion` submenu, and from the options that come up select `UNIX\/OSX Format`. The next time you save the file, its line endings will, all going well, be saved with UNIX-style line endings. \n\nAlternatively, you could put the script in a Linux environment, e.g. EC2 instance with Amazon Linux 2 and install `dos2unix` via `sudo yum install dos2unix`. After installation, you could convert your files via\n```\ndos2unix -n file.sh  output.sh \n```\nAfter the conversion, please update LCC with the new scripts. You could verify that `^M` has been removed via \n```\ncat -e your_script.sh\n```\nThe output will print all special characters directly without hiding.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1668093487360,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":19.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":250.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.6244272222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Sorry if this is covered by docs  couldn\u2019t understand something about wandb jobs (wandb launch);<\/p>\n<p>Basically, does the Dockerfile itself must have an <code>Entrypoint<\/code> attr, which runs a program? Or is it enough to have a docker image which has WANDB_DOCKER, wandb_api_key env variable set, having a python directory codebase which can start wandb runs?? (can\u2019t create a job this way though\u2026), if possible without logging the code.<\/p>",
        "Challenge_closed_time":1679692322111,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679664874173,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/confused-about-wandb-launch-usage-with-docker\/4119",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":6.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7.6244272222,
        "Challenge_title":"Confused about wandb launch usage with docker",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":74,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Cleared:<br>\nYeah, the Dockerfile should be complete end to end such that just running the docker file should run the program, it must have <code>Entrypoint<\/code> attr in the Dockerfile, which actually can be changed from the overrides when we pass it to queue.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.7,
        "Solution_reading_time":3.33,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":43.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0708333333,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a customer who wants to install LightGBM on SageMaker notebooks, as they are currently using it outside of SageMaker.\n\nRight now, they are interested in the ability to SSH into the instance, but it would be great if we could provide them a way to install LightGBM right now.\n\nCheers",
        "Challenge_closed_time":1516633097000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516632842000,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1668497411264,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPwkZcylKQR6-u0pghgrseA\/lightgbm-on-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":3.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.0708333333,
        "Challenge_title":"LightGBM on SageMaker",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":483.0,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It's possible to do, I have used it myself for gradient boosting, from within Jupyter you can simply run:\n\n    !conda install -y -c conda-forge lightgbm\n\nWithin a selected conda environment. No terminal access is needed, however it must be done, On the top right of the Jupyter notebook you can choose a terminal environment which will give you a shell to the backend instance and you can install there.\n\nHowever if you want the notebook to be immutable\/transferable you can do the install within the notebook .\n\nThanks",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925593440,
        "Solution_link_count":0.0,
        "Solution_readability":10.6,
        "Solution_reading_time":6.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":87.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505194585676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":44.2415530556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to deploy a model to Kubernetes in Azure Machine Learning Studio, it was working for a while, but now, it fails during deployment, the error message is as follows:<\/p>\n<pre><code>Deploy: Failed on step WaitServiceCreating. Details: AzureML service API error. \nYour container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r.\nFrom the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally.\nPlease refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\n<\/code><\/pre>\n<pre class=\"lang-js prettyprint-override\"><code>{\n    &quot;code&quot;: &quot;KubernetesDeploymentFailed&quot;,\n    &quot;statusCode&quot;: 400,\n    &quot;message&quot;: &quot;Kubernetes Deployment failed&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n            &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1596104635683,
        "Challenge_comment_count":2,
        "Challenge_created_time":1595913837627,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1595945366092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63127521",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":21.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":52.99946,
        "Challenge_title":"Deploying Model to Kubernetes",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":178,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505194585676,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>It seems it was a bug, got corrected by itself today. Closing this question now<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.9,
        "Solution_reading_time":1.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":15.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1808333333,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to connect a Sagemaker notebook to Glue Catalog, but is not allowed to use developer endpoints because of security constraints.\n\nI can't seem to find documentation on the Glue Catalog API that would allow this, or examples of how this might be done.  Any links or pointers would be greatly appreciated.",
        "Challenge_closed_time":1594232347000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594231696000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668594120152,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUoiI3L85FT6OmPewooCH4lQ\/how-to-connect-a-sagemaker-notebook-to-glue-catalog",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":4.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1808333333,
        "Challenge_title":"How to connect a Sagemaker Notebook to Glue Catalog",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1030.0,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"So there is the catalog API which allows you to describe databases, tables, etc. Documentation regarding the calls and data structures can be found here:\n\n- https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-api-catalog-tables.html\n\nBoto3 for get_table\n- https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/glue.html#Glue.Client.get_table\n\nIf they have a restrictive security posture (as suggested by the avoidance of Dev Endpoints) you may also suggest a Glue VPC-E:  https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/vpce-interface.html\n\nI would ask what are they accessing the catalog for, as the Dev Endpoint isn't entirely about the Glue Catalog, but about the compute resources andSparkMagic.\n\nAlso, think about steering them towards AWS Data Wrangler for interacting with Glue Catalog if they are using Pandas. Helpful snippets can be found here: \n\n- https:\/\/github.com\/awslabs\/aws-data-wrangler\/blob\/master\/tutorials\/005%20-%20Glue%20Catalog.ipynb",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1598510602164,
        "Solution_link_count":4.0,
        "Solution_readability":15.0,
        "Solution_reading_time":12.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":659.0783333333,
        "Challenge_answer_count":4,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No --> Yes\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Open remote connection to Azure Machine Learning Compute Instance\r\n\r\nThis does not seem to cause any issues, but it's annoying to see the error message every time.\r\n\r\nAction: azureAccount.onSubscriptionsChanged\r\nError type: REQUEST_SEND_ERROR\r\nError Message: request to redacted:url failed, reason: getaddrinfo ENOTFOUND redacted:idworkspace.westeurope.api.azureml.ms\r\n\r\n\r\nVersion: 0.8.2\r\nOS: linux\r\nOS Release: 5.4.0-1068-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.66.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nnew t extension.js:2:486489\r\nt.<anonymous> extension.js:2:470040\r\nextension.js:2:2450576\r\nObject.throw extension.js:2:2450681\r\nc extension.js:2:2449471\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":1652117002000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649744320000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1541",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":659.0783333333,
        "Challenge_title":"Reoccurring error on opening connection to Azure Machine Learning Compute Instance",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@evakkuri thanks for filing this issue. Is this happening every time you open a remote connection? Are you connecting to Compute Instance through the ML Studio? Currently this happens every time I connect. I'm not connecting via ML Studio, instead through VS Code with the Azure Machine Learning extension. @sevillal Can you please follow up here :) ? @evakkuri we have published version v0.10.0 of the extension. Could you please upgrade and retry to check if you issue is still reproducible? Please reopen this issue if that's the case.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":82.4554527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a locally trained RandomForest model into Azure Machine Learning Studio.<\/p>\n<p><strong>training code (whentrain.ipynb) :<\/strong><\/p>\n<pre><code>#import libs and packages\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom math import sqrt\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom azureml.core import Workspace, Dataset\n\n# get existing workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# get the datastore to upload prepared data\ndatastore = workspace.get_default_datastore()\n\n# load the dataset which is placed in the data folder\ndataset = Dataset.Tabular.from_delimited_files(datastore.path('UI\/12-23-2021_023530_UTC\/prepped_data101121.csv'))\ndataset = dataset.to_pandas_dataframe()\n\n# Create the outputs directories to save the model and images\nos.makedirs('outputs\/model', exist_ok=True)\nos.makedirs('outputs\/output', exist_ok=True)\ndataset['Date'] = pd.to_datetime(dataset['Date'])\ndataset = dataset.set_index('Date')\n###\nscaler = MinMaxScaler()\n\n#inputs\nX = dataset.iloc[:, 1:]\n#output\ny = dataset.iloc[:, :1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42, shuffle=True)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n###\n\nmodel1 = RandomForestRegressor(n_estimators = 6,\n                                   max_depth = 10,\n                                   min_samples_leaf= 1,\n                                   oob_score = 'True',\n                                   random_state=42)\nmodel1.fit(X_train, y_train.values.ravel())\n\ny_pred2 = model1.predict(X_test)\n<\/code><\/pre>\n<p><strong>And here is the code on the estimator part (estimator.ipynb):<\/strong><\/p>\n<pre><code>from azureml.core import Experiment\nfrom azureml.core import Workspace\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.train.dnn import TensorFlow\nfrom azureml.widgets import RunDetails\n\nimport os\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\nexp = Experiment(workspace=workspace, name='azure-exp')\ncluster_name = &quot;gpucluster&quot;\n\ntry:\n    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2',\n                                                           max_nodes=1)\n\n    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)  # , min_node_count=None, timeout_in_minutes=20)\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\nfrom azureml.core import ScriptRunConfig\nsource_directory = os.getcwd()\n\nfrom azureml.core import Environment\n\nmyenv = Environment(&quot;user-managed-env&quot;)\nmyenv.python.user_managed_dependencies =True\nfrom azureml.core import Dataset\ntest_data_ds = Dataset.get_by_name(workspace, name='prepped_data101121')\n\nsrc = ScriptRunConfig(source_directory=source_directory,\n                      script='whentrain.ipynb',\n\n                      arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],\n                      compute_target=compute_target,\n                      environment=myenv)\nrun = exp.submit(src)\nRunDetails(run).show()\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>The error that happens in <strong>run.wait_for_completion<\/strong> states :<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 107, in &lt;module&gt;\n[stderr]    &quot;notebookHasBeenCompleted&quot;: true\n[stderr]NameError: name 'true' is not defined\n[stderr]\n<\/code><\/pre>\n<p>As you can see in my whentrain.ipynb, it does not even reach line 107, and I could not find where this error come from. So how do I fix it?<\/p>\n<p>I'm running the Notebook on Python 3.<\/p>\n<p><strong>UPDATE:<\/strong><\/p>\n<p>Okay, after a little adjustment that should not affect the whole code (I just removed some extra columns, added model save code in whentrain.ipynb making use of import os) it's now giving me somewhat the same error.<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 115, in &lt;module&gt;\n[stderr]    &quot;source_hidden&quot;: false,\n[stderr]NameError: name 'false' is not defined\n[stderr]\n<\/code><\/pre>",
        "Challenge_closed_time":1640628230640,
        "Challenge_comment_count":2,
        "Challenge_created_time":1640331391010,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/674712\/nameerror-when-trying-to-run-an-scriptrunconfig-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":61.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":82.4554527778,
        "Challenge_title":"NameError when trying to run an ScriptRunConfig in Azure Machine Learning",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":413,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b72317d8-d212-487c-8510-7d965e8d135f\">@Ash  <\/a> Ok, I think the issue is here.     <\/p>\n<pre><code>src = ScriptRunConfig(source_directory=source_directory,  \n                       script='whentrain.ipynb',  \n                            \n                       arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],  \n                       compute_target=compute_target,  \n                       environment=myenv)  \n<\/code><\/pre>\n<p>The script parameter is set to the notebook &quot;whentrain.ipynb&quot;, This should be a python script *.py which can train your model. Since you are using the notebook filename the entire source of jupyter notebook is loaded and it fails with these errors. You can lookup samples on azure ml notebook github repo for <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-amlcompute\/train.py\">reference<\/a>. I think if you can convert your whentrain.ipynb file to a python script whentrain.py and save it the current folder structure you should be able to use it in this step.    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":12.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":106.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":3.8717241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created a docker image using AWS SageMaker and am now trying to push said image to ECR. When I do <code>docker push ${fullname}<\/code> it retries a couple of times and then errors.<\/p>\n<p>In CloudTrail I can see that I'm getting an access denied error with message:<\/p>\n<p>&quot;User: arn:aws:sts::xxxxxxxxxx:assumed-role\/AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx\/SageMaker is not authorized to perform: ecr:InitiateLayerUpload on resource: arn:aws:ecr:us-east-x:xxxxxxxxxx:repository\/image because no identity-based policy allows the ecr:InitiateLayerUpload action&quot;<\/p>\n<p>I have full permissions, but from the error message above it thinks the user is SageMaker and not me.<\/p>\n<p>How do I change the user? I'm guessing that's the problem.<\/p>",
        "Challenge_closed_time":1658175998627,
        "Challenge_comment_count":2,
        "Challenge_created_time":1658162060420,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73025706",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.7,
        "Challenge_reading_time":10.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.8717241667,
        "Challenge_title":"AccessDenied error when pushing docker image from SageMaker to ECR",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":143.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1657050754840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>When you're running commands from SageMaker, you're executing them as the SageMaker execution role, instead of your role. There are two options -<\/p>\n<ol>\n<li>[Straighforward solution] Add <em>ecr:InitiateLayerUpload<\/em> permissions to the <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx<\/code> role<\/li>\n<li>Assume a different role using sts (in that case, <code>AmazonSageMaker-ExecutionRole-xxxxxxxxxxxx<\/code> needs to have permissions to assume your Admin role) and then run <code>docker push<\/code> command.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.5,
        "Solution_reading_time":6.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.3555730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to create my own Kedro starter. I have tried to replicate the relevant portions of the pandas iris starter. I have a <code>cookiecutter.json<\/code> file with what I believe are appropriate mappings, and I have changed the repo and package directory names as well as any references to Kedro version such that they work with cookie cutter.<\/p>\n<p>I am able to generate a new project from my starter with <code>kedro new --starter=path\/to\/my\/starter<\/code>. <strong>However, the newly created project uses the default values for the project, package, and repo names, without prompting me for any input in the terminal<\/strong>.<\/p>\n<p>Have I misconfigured something? How can I create a starter that will prompt users to override the defaults when creating new projects?<\/p>\n<p>Here are the contents of <code>cookiecutter.json<\/code> in the top directory of my starter project:<\/p>\n<pre><code>{\n    &quot;project_name&quot;: &quot;default&quot;,\n    &quot;repo_name&quot;: &quot;{{ cookiecutter.project_name }}&quot;,\n    &quot;python_package&quot;: &quot;{{ cookiecutter.repo_name }}&quot;,\n    &quot;kedro_version&quot;: &quot;{{ cookiecutter.kedro_version }}&quot;\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1641488156140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641486876077,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70610418",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":15.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.3555730556,
        "Challenge_title":"Why doesn't my Kedro starter prompt for input?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416091573812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Texas, USA",
        "Poster_reputation_count":197.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>I think you may be missing <code>prompts.yml<\/code>\n<a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml<\/a><\/p>\n<p>Full instructions can be found here:\n<a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":38.0,
        "Solution_reading_time":7.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":21.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":15.1942297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Vertex AI managed notebook in <code>europe-west1-d<\/code>. I try both (locally and in the notebook):<\/p>\n<pre><code>gcloud notebooks instances list --location=europe-west1-d\ngcloud compute instances list --filter=&quot;my-notebook-name&quot; --format &quot;[box]&quot;\n<\/code><\/pre>\n<p>and both return nothing. What am I missing?<\/p>",
        "Challenge_closed_time":1653610592520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653555893293,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72389357",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":5.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":15.1942297222,
        "Challenge_title":"Why does gcloud not see my managed notebook?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>When creating a Vertex AI managed notebook, it is expected that the instance won't appear on your project. By design, the notebook instance is created in a Google managed project and is not visible to the end user.<\/p>\n<p>But if you want to see instance details of your managed notebooks, you can use the Notebooks API to send a request to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/reference\/rest\/v1\/projects.locations.runtimes\/list\" rel=\"nofollow noreferrer\">runtimes.list<\/a>. See example request:<\/p>\n<pre><code>project=your-project-here\nlocation=us-central1 #adjust based on your location\n\ncurl -X GET -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/notebooks.googleapis.com\/v1\/projects\/${project}\/locations\/${location}\/runtimes&quot;\n<\/code><\/pre>\n<p>Response output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.5,
        "Solution_reading_time":14.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":102.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":26.6152038889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to know what is the OPTIMAL way to store the result of a Google BigQuery table query, to Google Cloud storage. My code, which is currently being run in some Jupyter Notebook (in Vertex AI Workbench, same project than both the BigQuery data source, as well as the Cloud Storage destination), looks as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># CELL 1 OF 2\n\nfrom google.cloud import bigquery\nbqclient = bigquery.Client()\n\n# The query string can vary:\nquery_string = &quot;&quot;&quot;\n        SELECT *  \n        FROM `my_project-name.my_db.my_table` \n        LIMIT 2000000\n        &quot;&quot;&quot;\n\ndataframe = (\n    bqclient.query(query_string)\n    .result()\n    .to_dataframe(\n        create_bqstorage_client=True,\n    )\n)\nprint(&quot;Dataframe shape: &quot;, dataframe.shape)\n\n# CELL 2 OF 2:\n\nimport pandas as pd\ndataframe.to_csv('gs:\/\/my_bucket\/test_file.csv', index=False)\n<\/code><\/pre>\n<p>This code takes around 7.5 minutes to successfully complete.<\/p>\n<p><strong>Is there a more OPTIMAL way to achive what was done above?<\/strong> (It would mean <em>faster<\/em>, but maybe something else could be improved).<\/p>\n<p>Some additional notes:<\/p>\n<ol>\n<li>I want to run it &quot;via a Jupyter Notebook&quot; (in Vertex AI Workbench), because sometimes some data preprocessing, or special filtering must be done, which cannot be easily accomplished via SQL queries.<\/li>\n<li>For the first part of the code, I have discarded <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas.read_gbq<\/a>, as it was giving me some weird EOF errors, when (experimentally) &quot;storing as .CSV and reading back&quot;.<\/li>\n<li>Intuitively, I would focus the optimization efforts in the second half of the code (<code>CELL 2 OF 2<\/code>), as the first one was borrowed from <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/bigquery-storage-python-pandas\" rel=\"nofollow noreferrer\">the official Google documentation<\/a>. I have tried <a href=\"https:\/\/stackoverflow.com\/a\/57404119\/16706763\">this<\/a> but it does not work, however in the same thread <a href=\"https:\/\/stackoverflow.com\/a\/60644694\/16706763\">this<\/a> option worked OK.<\/li>\n<li>It is likley that this code will be included in some Docker image afterwards, so &quot;as little libraries as possible&quot; must be used.<\/li>\n<\/ol>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1651612810316,
        "Challenge_comment_count":2,
        "Challenge_created_time":1651600671333,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72103557",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":30.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":3.3719397222,
        "Challenge_title":"Save the result of a query in a BigQuery Table, in Cloud Storage",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1409.0,
        "Challenge_word_count":292,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>After some experiments, I think I have got to a solution for my original post. First, the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd  # Just one library is imported this time\n\n# This SQL query can vary, modify it to match your needs\nquery_string = &quot;&quot;&quot;\nSELECT *\nFROM `my_project.my_db.my_table`\nLIMIT 2000000\n&quot;&quot;&quot;\n\n# One liner to query BigQuery data.\ndownloaded_dataframe = pd.read_gbq(query_string, dialect='standard', use_bqstorage_api=True)\n\n# Data processing (OPTIONAL, modify it to match your needs)\n# I won't do anything this time, just upload the previously queried data\n\n# Data store in GCS\ndownloaded_dataframe.to_csv('gs:\/\/my_bucket\/uploaded_data.csv', index=False)\n<\/code><\/pre>\n<p>Some final notes:<\/p>\n<ol>\n<li>I have not done an &quot;in-depth research&quot; about the processing speed VS the number of rows existing in a BigQuery table, however I saw that the processing time with the updated code and the original query, now takes ~6 minutes; that's enough for the time being. <em>This answer might have some room for further improvements<\/em> therefore, but it's better than the original situation.<\/li>\n<li>The EOF error I mentioned in  my original post was: <code>ParserError: Error tokenizing data. C error: EOF inside string starting at row 70198<\/code>. In the end I got to realize that it did not have anything to do with <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas_gbq<\/a> function, but with &quot;how I was saving the data&quot;. See, <em>I was 'experimentally' storing the .csv file in the Vertex AI Workbench local storage, then downloading it to my local device, and when trying to open that data from my local device, I kept stumbling upon that error, however not getting the same when downloading the .csv data from Cloud Storage<\/em> ... Why? Well, it happens that if you download the .csv data &quot;very quickly&quot; after &quot;it gets generated&quot; (i.e., after few seconds), from Vertex AI Workbench local storage, the data is simply still incomplete, but it does not give any error or warning message: it will simply &quot;let you start with the download&quot;. For this reason, I think it is safer to export your data to Cloud Storage, and then download safely from there. This behaviour is more noticeable on large files (i.e. my own generated file, which had ~3.1GB in size).<\/li>\n<\/ol>\n<p>Hope this helps.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1651696486067,
        "Solution_link_count":1.0,
        "Solution_readability":9.6,
        "Solution_reading_time":31.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":357.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.7277777778,
        "Challenge_answer_count":4,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nFirst of all, thanks to everyone creating this Helm Chart as it is really good and easy to use.\r\n\r\nHowever, I encountered a problem when choosing to include ServiceMonitor and Prometheus metrics along the Deployment. Generally, the created ServiceMonitor for MLFlow is correct, yet in the current form it does not work for me.\r\nI use the latest Prometheus deployed using the official Helm Chart and the MLFlow metrics did not show up in the Targets, yet it was visible in Service Discovery panel in Prometheus Dashboard, but appeared as `0\/1 active targets`.\r\n\r\nAfter a couple of hours of educated debugging I changed manually the `targetPort: 80` to `port: http` in the deployed ServiceMonitor manifest. It worked straightaway! \r\n\r\n\r\nWhat I propose is a simple fix:\r\nAccording to official Prometheus Troubleshooting docs the port specified in ServiceMonitor should use `name` instead of port number ([Link to docs](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/troubleshooting.md#using-textual-port-number-instead-of-port-name)) \r\nSimple fix would be to change `targetPort: 80` to `port: http` in `templates\/servicemonitor.yaml`. Port name `http` is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name.\r\nI am aware that port number of type Integer should also work...\r\n\n\n### What's your helm version?\n\n3.6.0\n\n### What's your kubectl version?\n\n1.19\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.21\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\n helm install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1658854769000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658844949000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":9.3,
        "Challenge_reading_time":25.56,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.7277777778,
        "Challenge_title":"[mlflow] Use port name instead of port number in ServiceMonitor ",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":286,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @mikwieczorek \r\n\r\nThanks to inform us. Yes, probably it's my mistake. I changed it to the name some time ago. Let's write some tests and fix the problem. Well, it looks like your link refers to the port (service port) rather than targetPort (pod's port. This is currently what we use.). But we can even make it optional (port or targetPort selection) and use a name rather than a port number. It looks like it works with the latest version of Prometheus but I think we need to support all versions together.\r\n\r\nAnd [this is the full schema of endpoints field](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/api.md#monitoring.coreos.com\/v1.Endpoint).\r\n\r\nI will do some additional manual tests and send the PR. Hi @mikwieczorek \r\n\r\nChart version 0.3.0 should solve your problem. If it still accrues, feel free to reopen this issue. You can use the following command to update your deployment without the need for additional changes.\r\n\r\n```\r\nhelm repo update\r\nhelm upgrade --install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\r\n```\r\n\r\nBest,\r\nBurak Thank you @burakince for your prompt fix. It works correctly after the update. \r\nNext time, I will make an MR instead of just reporting the issue",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.3,
        "Solution_reading_time":15.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":187.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":150.3343644444,
        "Challenge_answer_count":11,
        "Challenge_body":"<p>I can't use ml real-time inference endpoint becouse it's stuck on transitioning status (more than 20 hours). Could you help me with that?<\/p>",
        "Challenge_closed_time":1600780351532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600239147820,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/96645\/azure-ml-real-time-inference-endpoint-deloyment-st",
        "Challenge_link_count":0,
        "Challenge_participation_count":11,
        "Challenge_readability":8.6,
        "Challenge_reading_time":2.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":150.3343644444,
        "Challenge_title":"Azure ML real-time inference endpoint deloyment stuck on transitioning status",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":32,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I've chacked in on some different algorithms and the issue appears when i'm using n-grams block for getting features. When i'm using feature hashing for example it looks like working well.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":0.6604344445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to overwrite properties taken from the parameters.yaml file within a Kedro notebook?<\/p>\n\n<p>I am trying to dynamically change parameter values within a notebook. I would like to be able to give users the ability to run a standard pipeline but with customizable parameters. I don't want to change the YAML file, I just want to change the parameter for the life of the notebook.<\/p>\n\n<p>I have tried editing the params within the context but this has no affect.<\/p>\n\n<pre><code>context.params.update({\"test_param\": 2})\n<\/code><\/pre>\n\n<p>Am I missing something or is this not an intended use case?<\/p>",
        "Challenge_closed_time":1582114523347,
        "Challenge_comment_count":1,
        "Challenge_created_time":1582112145783,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1583420995670,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60299478",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":8.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.6604344445,
        "Challenge_title":"Setting parameters in Kedro Notebook",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":586.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582111403048,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Kedro supports specifying <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/03_configuration.html#specifying-extra-parameters\" rel=\"nofollow noreferrer\">extra parameters<\/a> from the command line by running<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>kedro run --params \"key1:value1,key2:value2\"\n<\/code><\/pre>\n\n<p>which solves your second use case.<\/p>\n\n<p>As for the notebook use case, updating <code>context.params<\/code> does not have any effect since the context does not store the parameters on <code>self<\/code> but rather pulls them from the config every time the property is being called.<\/p>\n\n<p>However you can still add extra parameters to the context object after it being instantiated:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>extra_params = context._extra_params or {}\nextra_params.update({\"test_param\": 2})\ncontext._extra_params = extra_params\n<\/code><\/pre>\n\n<p>This will update extra parameters that are applied on top of regular parameters coming from the config.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.4,
        "Solution_reading_time":13.31,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":106.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.6107855556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have the following error while running <a href=\"http:\/\/wandb.me\/prompts-quickstart\" rel=\"noopener nofollow ugc\">W&amp;B_Prompts_Quickstart notebook<\/a> <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/8\/88da9eb34b5ef62baf4fcd367d69c08d5579c825.png\" data-download-href=\"\/uploads\/short-url\/jwFkh0P5adLx7fGnRM3YtxxLn4p.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/8\/88da9eb34b5ef62baf4fcd367d69c08d5579c825.png\" alt=\"image\" data-base62-sha1=\"jwFkh0P5adLx7fGnRM3YtxxLn4p\" width=\"690\" height=\"289\" data-dominant-color=\"F4F4F4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">774\u00d7325 8.32 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1684232017936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684222619108,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/error-in-w-b-prompts-quickstart-notebook\/4411",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":28.9,
        "Challenge_reading_time":15.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.6107855556,
        "Challenge_title":"Error in W&B_Prompts_Quickstart notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":24.0,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tinsae\">@tinsae<\/a> thanks for reporting this issue. There\u2019s a breaking change with newer LangChain version, and the Growth team is working on a fix.<\/p>\n<p>You could run the Prompts Quickstart notebook for now by pinning a previous LangChain version which I just tested and seems to be working fine. Could you please change the installation section as follows:<\/p>\n<pre><code class=\"lang-auto\">!pip install \"wandb&gt;=0.15.2\" -qqq\n!pip install \"langchain==v0.0.158\" openai\n<\/code><\/pre>\n<p>Would this work for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":7.04,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":0.1047952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was following a guide on mounting EFS in SageMaker studio, but when using the following as a notebook cell:<\/p>\n<pre><code>%%sh \n\nsudo mount -t nfs \\\n    -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\\n    172.31.5.227:\/ \\\n    ..\/efs\n\nsudo chmod go+rw ..\/efs\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>sh: 2: sudo: not found\nsh: 7: sudo: not found\n<\/code><\/pre>\n<p>Even in the terminal ('image terminal'), sudo is not found: <code># sudo \/bin\/sh: 1: sudo: not found<\/code><\/p>",
        "Challenge_closed_time":1596811524960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596811147697,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1596823164336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63304005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1047952778,
        "Challenge_title":"sudo: not found on AWS Sagemaker Studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":980.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>I managed to get sudo working in the &quot;System Terminal&quot; instead. The image terminals don't seem to have access to sudo.<\/p>\n<p>Unrelated: But then when I tried to mount EFS onto the SageMaker studio app, it simply failed, saying mount target is not a directory. Looks like I'm not using Sagemaker Studio this year.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1596815281143,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1494171603136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":73187.0,
        "Answerer_view_count":8473.0,
        "Challenge_adjusted_solved_time":0.2887638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure ML Workspace which comes by default with some pre-installed packages.<\/p>\n<p>I tried to install<\/p>\n<pre><code>!pip install -U imbalanced-learn\n<\/code><\/pre>\n<p>But I got this error<\/p>\n<pre><code>Requirement already up-to-date: scikit-learn in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (0.24.2)\nRequirement already satisfied, skipping upgrade: scipy&gt;=0.19.1 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.18.5)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (2.1.0)\nCollecting imbalanced-learn\n  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: scipy&gt;=1.1.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (1.4.1)\nERROR: Could not find a version that satisfies the requirement scikit-learn&gt;=1.0.1 (from imbalanced-learn) (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\nERROR: No matching distribution found for scikit-learn&gt;=1.0.1 (from imbalanced-\n<\/code><\/pre>\n<p>learn)<\/p>\n<p>Not sure how to solve this, I have read in other posts to use conda, but that didnt work either.<\/p>",
        "Challenge_closed_time":1644935036627,
        "Challenge_comment_count":1,
        "Challenge_created_time":1644933997077,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1644960360047,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71127858",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":31.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":0.2887638889,
        "Challenge_title":"Cant install imbalanced-learn on an Azure ML Environment",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":219.0,
        "Challenge_word_count":225,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p><a href=\"https:\/\/pypi.org\/project\/scikit-learn\/1.0.1\/\" rel=\"nofollow noreferrer\"><code>scikit-learn<\/code> 1.0.1<\/a> and up require Python &gt;= 3.7; you use Python 3.6. You need to upgrade Python or downgrade <code>imbalanced-learn<\/code>. <a href=\"https:\/\/pypi.org\/project\/imbalanced-learn\/0.8.1\/\" rel=\"nofollow noreferrer\"><code>imbalanced-learn<\/code> 0.8.1<\/a> allows Python 3.6 so<\/p>\n<pre><code>!pip install -U &quot;imbalanced-learn &lt; 0.9&quot;\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.3,
        "Solution_reading_time":6.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":53.7558333333,
        "Challenge_answer_count":5,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\r\n\r\nI have local minikube cluster. I installed the helm chart with some changed settings. See below for the changed values. Everthing else is same as per default values yaml file. For db backend I am using `bitnami\/postgresql` and for s3 storage minio instance. I also have created a initial bucket named \"mlflow\" in minio. \r\n\r\nAnd then I created a simple k8s pod to run the simple training example from mlflow docs. This pod has env variables set as : `MLFLOW_TRACKING_URI=http:\/\/mlflow.airflow.svc.cluster.local:5000` [Here ](https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. I can see the metadata about the model in UI however , artifact section in UI is empty and also the bucket is empty. \r\n\r\n### What's your helm version?\r\n\r\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\r\n\r\n### What's your kubectl version?\r\n\r\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n\r\n### Which chart?\r\n\r\nmlflow\r\n\r\n### What's the chart version?\r\n\r\nlatest\r\n\r\n### What happened?\r\n\r\n_No response_\r\n\r\n### What you expected to happen?\r\n\r\nI would expect the artifacts in minio bucket.\r\n\r\n### How to reproduce it?\r\n\r\ninstall the helm chart with minio and postgresql config. Run a simple exmple frpom docs. \r\n\r\n### Enter the changed values of values.yaml?\r\n\r\n```\r\nbackendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    postgres:\r\n      enabled: true\r\n      host: mlflow-postgres-postgresql.airflow.svc.cluster.local\r\n      database: mlflow_db\r\n      user: mlflow\r\n      password: mlflow\r\nartifactRoot:\r\n  proxiedArtifactStorage: true\r\n  s3:\r\n    enabled: true\r\n    bucket: mlflow\r\n    awsAccessKeyId: {{ requiredEnv \"MINIO_USERNAME\" }}\r\n    awsSecretAccessKey: {{ requiredEnv \"MINIO_PASSWORD\" }}\r\nextraEnvVars:\r\n  MLFLOW_S3_ENDPOINT_URL: minio.airflow.svc.cluster.local\r\n```\r\n\r\n### Enter the command that you execute and failing\/misfunctioning.\r\n\r\nhelm install mlflow-release community-charts\/mlflow --values values.yaml\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_",
        "Challenge_closed_time":1660345866000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660152345000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/32",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":8.7,
        "Challenge_reading_time":30.0,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":53.7558333333,
        "Challenge_title":"[mlflow] model artifacts not saved in remote s3 artifact store",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":261,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @mohittalele ,\r\n\r\nThank you very much for reporting the error. Could you please share your mlflow pod and training pod logs with me?\r\n\r\nBest,\r\nBurak Hi @mohittalele \r\n\r\nI think you have a misconfiguration. I added a [full example to here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/bitnami-postgresql-and-bitnami-minio-sklearn-training-example). Simply, your `MLFLOW_S3_ENDPOINT_URL` configuration is wrong. URL must be `http:\/\/minio.airflow.svc.cluster.local:9000`. Could you please fix your configuration and try again?\r\n\r\nBest,\r\nBurak @burakince  Here is the log of the training container. Somehow the trining container does not \"know\" of the s3 endpoint and it is using the local path. \r\n\r\n```\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:26 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - The git executable must be specified in one of the following ways:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be included in your $PATH\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be set via $GIT_PYTHON_GIT_EXECUTABLE\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - explicitly set via git.refresh()\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - All git commands will error until this is rectified.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - This initial warning can be silenced or aggravated in the future by setting the\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - quiet|q|silence|s|none|n|0: for no warning or exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - warn|w|warning|1: for a printed warning\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - error|e|raise|r|2: for a raised exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Example:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     export GIT_PYTHON_REFRESH=quiet\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Elasticnet model (alpha=0.500000, l1_ratio=0.500000):\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   RMSE: 0.793164022927685\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   MAE: 0.6271946374319586\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   R2: 0.10862644997792636\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_artifact_uri ::  .\/mlruns\/0\/3b376331bcaa4894a0723fe4b690658f\/artifacts\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_registry_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_tracking_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Registered model 'ElasticnetWineModel' already exists. Creating a new version of this model...\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:29 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: ElasticnetWineModel, version 11\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Created version '11' of model 'ElasticnetWineModel'.\r\n[2022-08-11, 13:53:30 CEST] {kubernetes_pod.py:453} INFO - Deleting pod: mlflow-example-1-7e89c5e6540645e3822fbf34410c6b99\r\n[2022-08-11, 13:53:30 CEST] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=mlflow, task_id=mlflow_example_1, execution_date=20220811T115225, start_date=20220811T115226, end_date=20220811T115330\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:156} INFO - Task exited with return code 0\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check\r\n```\r\n\r\n\r\n\r\nmlflow logs are quite, There is nothing logged there. I will try out the example. Thanks for the example. \r\n\r\nedit : with 9000 port number specifie, there is no improvement @burakince The setup now works. Actually there was problem with VPN setting since I was deploying mlflow behind mlflow. We can close the issue :) Hi @mohittalele,\r\n\r\nI'm glad to hear the problem was resolved. If you need anything else, please don't hesitate to open a new issue.\r\n\r\nBest,\r\nBurak",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":7.5,
        "Solution_reading_time":60.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":66.0,
        "Solution_word_count":507.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.9199194444,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I\u2019m using wandb version 0.14.0 in an ipynb file using vscode as part of assignment 1 of the \u2018Effective MLOps\u2019 course (logging dataset as artifact and visualising data with a table)<\/p>\n<p>When I execute <code>run.finish()<\/code> at the end of my file the cell hangs indefinitely with the message<\/p>\n<pre><code class=\"lang-console\">Waiting for W&amp;B process to finish... (success).\n<\/code><\/pre>",
        "Challenge_closed_time":1678983024296,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678979712586,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/run-finish-hangs\/4069",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.8,
        "Challenge_reading_time":5.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.9199194444,
        "Challenge_title":"Run.finish() hangs",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>How much data are you logging? It might still be uploading in the background. You can check one of the <code>debug.log<\/code> or <code>debug-internal.log<\/code> files in the <code>wandb<\/code> folder to see if there is any upload activity happening<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":3.22,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":37.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1524630627116,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":154.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":47.0271861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to install a package <code>aclocal<\/code> on <code>Amazon SageMmaker<\/code> (which is a requirement for <code>teseract<\/code>) using the command <code>sudo yum install aclocal<\/code><\/p>\n<p>But it gives me the following error.<\/p>\n<p><code>No package aclocal available<\/code><\/p>",
        "Challenge_closed_time":1663134824987,
        "Challenge_comment_count":2,
        "Challenge_created_time":1662965527117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73685446",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":47.0271861111,
        "Challenge_title":"No package aclocal available",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524630627116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":154.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>To install <code>tesseract<\/code> in SageMaker you can simply follow the instructions here: <a href=\"https:\/\/tesseract-ocr.github.io\/tessdoc\/Compiling.html#linux\" rel=\"nofollow noreferrer\">https:\/\/tesseract-ocr.github.io\/tessdoc\/Compiling.html#linux<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":24.1,
        "Solution_reading_time":3.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1553882107003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":294.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":715.1033155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Somewhere this spring the behaviour of the sagemaker docker image changed and I cannot find the way I need to construct it now.<\/p>\n<p><strong>Directory structure<\/strong><\/p>\n<pre><code>\/src\/some\/package\n\/project1\n    \/some_entrypoint.py\n    \/some_notebook.ipynb\n\/project2\n    \/another_entrypoint.py\n    \/another_notebook.ipynb\nsetup.py\n<\/code><\/pre>\n<p><strong>Docker file<\/strong><\/p>\n<p>Note that I want to shift tensorflow version, so I changed the <code>FROM<\/code> to the latest version. This was the\nbreaking change.<\/p>\n<pre><code># Core\nFROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com\/tensorflow-training:2.3.0-cpu-py37-ubuntu18.04\n\nCOPY . \/opt\/ml\/code\/all\/\nRUN pip install \/opt\/ml\/code\/all\/\n\nWORKDIR &quot;\/opt\/ml\/code&quot;\n<\/code><\/pre>\n<p><strong>Python code<\/strong><\/p>\n<p>This code should start the entrypoints, for example here we have the code of some_notebook.ipynb. I tried all possible combinations of working directory + source_dir (None, '.', or '..'), entry_point (with or without \/), dependencies ('src')...<\/p>\n<ul>\n<li>if setup is present it tries to call my project as a module (python -m some_entrypoint)<\/li>\n<li>if not, it often is not able to find my entrypoint. Which I don't understand because the TensorFlow is supposed to add it to the container, isn't it?<\/li>\n<\/ul>\n<pre><code>estimator = TensorFlow(\n   entry_point='some_entrypoint.py', \n   image_name='ECR.dkr.ecr.eu-west-1.amazonaws.com\/overall-project\/sagemaker-training:latest',\n   source_dir='.',\n#    dependencies=['..\/src\/'],\n   script_mode=True,\n\n   train_instance_type='ml.m5.4xlarge',\n   train_instance_count=1,\n   train_max_run=60*60,  # seconds * minutes\n   train_max_wait=60*60,  # seconds * minutes. Must be &gt;= train_max_run\n   hyperparameters=hyperparameters,\n   metric_definitions=metrics,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/some-data\/&quot;}\n#   , wait=False\n)\n<\/code><\/pre>\n<p>Ideally I would want to understand the logic within: what is called given what settings?<\/p>",
        "Challenge_closed_time":1600272249596,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597694559337,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597697877660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63457857",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":26.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":716.0250719444,
        "Challenge_title":"What is called within a sagemaker custom (training) container?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>when the training container runs, your entry_point script will be executed.<\/p>\n<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir<\/code> should just be &quot;.&quot;<\/p>\n<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":5.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":38.1544444444,
        "Challenge_answer_count":3,
        "Challenge_body":"Seems that the Neptune_catalyst.ipynb is failing. \r\nPerhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/neptune-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Challenge_closed_time":1625030635000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624893279000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/neptune-ai\/examples\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.0,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":160.0,
        "Challenge_repo_star_count":28.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":38.1544444444,
        "Challenge_title":"Neptune_catalyst.ipynb fails",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":22,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"on it. #43 fixing here fixed in #43 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":0.5,
        "Solution_reading_time":0.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1619204963587,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1442.0,
        "Answerer_view_count":879.0,
        "Challenge_adjusted_solved_time":172.2395325,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>There is a gcloud command to create a user-managed notebook instance.<\/p>\n<pre><code>gcloud notebooks instances create \n<\/code><\/pre>\n<p>Is is possible to create a managed notebook with gcloud?<\/p>\n<p>It looks to be possible in the <a href=\"https:\/\/stackoverflow.com\/questions\/70223161\/how-to-create-a-new-workbench-managed-notebook-using-rest-api\">API<\/a>. I can't find a gcloud reference.<\/p>",
        "Challenge_closed_time":1660204307547,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659584245230,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73230096",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":172.2395325,
        "Challenge_title":"How to create a new VertexAI Workbench Managed Notebook using gcloud",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359519204743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Melbourne, Australia",
        "Poster_reputation_count":3943.0,
        "Poster_view_count":339.0,
        "Solution_body":"<p>As mentioned by @gogasca, the gcloud SDK for creating managed notebook is currently under work. Meanwhile you can try client libraries and REST API to create the same.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.7819525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a compute cluster in Azure ML and  I am able to see it in designer but not in the notebook. Can someone let me know the reason for this?<\/p>",
        "Challenge_closed_time":1608293911592,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608276696563,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/203139\/compute-clusters-in-azure-ml-notebook",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":4.7819525,
        "Challenge_title":"Compute Clusters in Azure ML Notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":36,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c0af539e-5b19-433b-85a1-2ca81772cd40\">@Srinivasan G  <\/a> This is an expected behavior while using notebooks on Azure ML portal. Compute clusters are used to train models and run experiments using the designer or pipelines. These cannot be used with notebooks.     <br \/>\nNotebooks are integrated to run on an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#notebookvm\">compute instance<\/a> which was previously termed as notebook VM. You can start\/stop the compute instance while using your notebook from Azure ML portal.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":7.71,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":13.3893163889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to run sagemaker locally for tensorflow in script mode. It seems like I cannot pull the docker container. I have ran the code below from a sagemaker notebook instance and everything ran fine. But when running it on my machine it doesn't work.<\/p>\n\n<p>How can I download the container, so I can debug things locally?<\/p>\n\n<pre><code>import os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\n\nhyperparameters = {}\nrole = 'arn:aws:iam::xxxxxxxx:role\/yyyyyyy'\nestimator = TensorFlow(\n    entry_point='train.py',\n    source_dir='.',\n    train_instance_type='local',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>I get this output<\/p>\n\n<pre><code>INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-\nscriptmode-2019-01-28-18-51-57-787\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n\nsubprocess.CalledProcessError: Command 'docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3' returned non-zero exit status 1.\n<\/code><\/pre>\n\n<p>The warning looks like the output you get when using the docker login stuff <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/Registries.html\" rel=\"nofollow noreferrer\">here<\/a>. If I follow these steps to register to the directory with tensorflow container it says login success<\/p>\n\n<pre><code>Invoke-Expression -Command (aws ecr get-login --no-include-email --registry-ids 520713654638 --region eu-west-2)\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\n<\/code><\/pre>\n\n<p>But then I still cannot pull it<\/p>\n\n<pre><code>docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.11.0-cpu-py3\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n<\/code><\/pre>",
        "Challenge_closed_time":1548750603276,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548702401737,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54408673",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.1,
        "Challenge_reading_time":29.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":13.3893163889,
        "Challenge_title":"Getting sagemaker container locally",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2235.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352206833663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":893.0,
        "Poster_view_count":185.0,
        "Solution_body":"<p>the same sequence works for me locally : 'aws ecr get-login', 'docker login', 'docker pull'. <\/p>\n\n<p>Does your local IAM user have sufficient credentials to pull from ECR? The 'AmazonEC2ContainerRegistryReadOnly' policy should be enough: <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html<\/a><\/p>\n\n<p>Alternatively, you can grab the container from Github and build it: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.7,
        "Solution_reading_time":9.06,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":36.4333333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Have installed Google Cloud via pip and CLI installer, yet programs cannot seem to see import statements from Google.cloud, returning the following error:\n\nline 9, in <module>\nfrom google.cloud import vision\nModuleNotFoundError: No module named 'google.cloud'\n\nPlease advise and thank you for your time.",
        "Challenge_closed_time":1672393260000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1672262100000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Simple-but-frustrating-error-Google-cloud-module-not-found\/td-p\/504285\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":36.4333333333,
        "Challenge_title":"Simple but frustrating error: Google.cloud module not found",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":51,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Have you tried installing google cloud vision? You can also check what python version you are using, this package is only supported in python versions 3.7 and up.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.37,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1306085731476,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cleveland, TN",
        "Answerer_reputation_count":7737.0,
        "Answerer_view_count":454.0,
        "Challenge_adjusted_solved_time":3.9352741667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to follow a Python tutorial and I have been able to execute almost everything, until the point of Deploying an endpoint to Azure with python.<\/p>\n\n<p>In order to give some context I have uploaded the scripts to my git account:\n<a href=\"https:\/\/github.com\/levalencia\/MLTutorial\" rel=\"nofollow noreferrer\">https:\/\/github.com\/levalencia\/MLTutorial<\/a><\/p>\n\n<p>File 1 and 2 Work perfectly fine<\/p>\n\n<p>However the following section in File 3 fails:<\/p>\n\n<pre><code>%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\n\ninference_config = InferenceConfig(runtime= \"python\", \n                                   entry_script=\"score.py\",\n                                   conda_file=\"myenv.yml\")\n\nservice = Model.deploy(workspace=ws, \n                       name='keras-mnist-svc2', \n                       models=[amlModel], \n                       inference_config=inference_config, \n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>with below error:<\/p>\n\n<pre><code>ERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n\nERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n<\/code><\/pre>\n\n<p>When I download the logs, I got this:<\/p>\n\n<pre><code>wheel-0.34.2         | 24 KB     |            |   0% [0m[91m\nwheel-0.34.2         | 24 KB     | ########## | 100% [0m\nDownloading and Extracting Packages\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... failed\n[91m\nERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'.\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\nAttempting to roll back.\n\n[0mRolling back transaction: ...working... done\n[91m\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\n\n\n[0mThe command '\/bin\/sh -c ldconfig \/usr\/local\/cuda\/lib64\/stubs &amp;&amp; conda env create -p \/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56 -f azureml-environment-setup\/mutated_conda_dependencies.yml &amp;&amp; rm -rf \"$HOME\/.cache\/pip\" &amp;&amp; conda clean -aqy &amp;&amp; CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; rm -rf \"$CONDA_ROOT_DIR\/pkgs\" &amp;&amp; find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + &amp;&amp; ldconfig' returned a non-zero code: 1\n2020\/03\/11 12:28:11 Container failed during run: acb_step_0. No retries remaining.\nfailed to run step ID: acb_step_0: exit status 1\n\nRun ID: cb3 failed after 2m21s. Error: failed during run, err: exit status 1\n<\/code><\/pre>\n\n<p>Update 1:<\/p>\n\n<p>I tried to run:\nconda list    --name base  conda<\/p>\n\n<p>inside the notebook and I got this:<\/p>\n\n<pre><code> # packages in environment at \/anaconda:\n    #\n    # Name                    Version                   Build  Channel\n    _anaconda_depends         2019.03                  py37_0  \n    anaconda                  custom                   py37_1  \n    anaconda-client           1.7.2                    py37_0  \n    anaconda-navigator        1.9.6                    py37_0  \n    anaconda-project          0.8.4                      py_0  \n    conda                     4.8.2                    py37_0  \n    conda-build               3.17.6                   py37_0  \n    conda-env                 2.6.0                         1  \n    conda-package-handling    1.6.0            py37h7b6447c_0  \n    conda-verify              3.1.1                    py37_0  \n\n    Note: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n\n<p>However in the deployment log I got this:<\/p>\n\n<pre><code>Solving environment: ...working... \ndone\n[91m\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.5.11\n  latest version: 4.8.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n<\/code><\/pre>",
        "Challenge_closed_time":1584015256823,
        "Challenge_comment_count":6,
        "Challenge_created_time":1583931341127,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1584001089836,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60636558",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":12.9,
        "Challenge_reading_time":59.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":23.3099155556,
        "Challenge_title":"ERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":5656.0,
        "Challenge_word_count":421,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>Unfortunately there seems to an issue with this version of Conda (4.5.11). To complete this task in the tutorial, you can just update the dependency for Tensorflow and Keras to be from <code>pip<\/code> and not <code>conda<\/code>. There are reasons why this is less than ideal for a production environment. The Azure ML <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py\" rel=\"nofollow noreferrer\">documentation states<\/a>:<\/p>\n\n<blockquote>\n  <p>\"If your dependency is available through both Conda and pip (from\n  PyPi), use the Conda version, as Conda packages typically come with\n  pre-built binaries that make installation more reliable.\"<\/p>\n<\/blockquote>\n\n<p>In this case though, if you update the following code block:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"tensorflow\")\nmyenv.add_conda_package(\"keras\")\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n\n# Review environment file\nwith open(\"myenv.yml\",\"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>To be the following:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_pip_package(\"tensorflow==2.0.0\")\nmyenv.add_pip_package(\"azureml-defaults\")\nmyenv.add_pip_package(\"keras\")\n\nwith open(\"myenv.yml\", \"w\") as f:\n    f.write(myenv.serialize_to_string())\n\nwith open(\"myenv.yml\", \"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>The tutorial should be able to be completed.  Let me know if any of this does not work for you once this update has been made.<\/p>\n\n<p>I have also reported this issue to Microsoft (in regards to the Conda version).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":22.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":187.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1277777778,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi, I\u2019m trying to deploy Polyaxon CE with helm\/argocd and the polyaxon-api keeps logging in loop:\n\nSystem check identified some issues:\n\nPreparing...\n\n\nand the pod never becomes ready.\nAny idea what could cause that?\n\nMore\n\nIt seems the polyaxon helm chart installation always fails with the message:\n\nError: timed out waiting for the condition\nThis is what I see:\n\npolyaxon-polyaxon-api-d8d7c8b5f-spsb2           1\/1     Running   0          3d5h\npolyaxon-polyaxon-api-dbb84b79c-6jqm9           0\/1     Running   2          13m\n\n\nThe polyaxon api pods take a long time to become ready. It fails liveness probe:\n\nEvents:\n  Type     Reason     Age                   From               Message\n  ----     ------     ----                  ----               -------\n  Normal   Scheduled  15m                   default-scheduler  Successfully assigned polyaxon\/polyaxon-polyaxon-api-dbb84b79c-6jqm9 to ip-192-168-165-30.us-east-2.compute.internal\n  Normal   Pulling    14m                   kubelet            Pulling image \"polyaxon\/polyaxon-api:xx\"\n  Normal   Pulled     14m                   kubelet            Successfully pulled image \"polyaxon\/polyaxon-api:xx\" in 33.387487209s\n  Normal   Created    14m                   kubelet            Created container polyaxon-api\n  Normal   Started    14m                   kubelet            Started container polyaxon-api\n  Normal   Killing    8m49s                 kubelet            Container polyaxon-api failed liveness probe, will be restarted\n  Warning  Unhealthy  8m37s (x10 over 13m)  kubelet            Readiness probe failed: Get \"[http:\/\/192.168.184.124:80\/healthz\/](http:\/\/192.168.184.124\/healthz\/)\": dial tcp 192.168.184.124:80: connect: connection refused\n  Normal   Pulled     8m19s                 kubelet            Container image \"polyaxon\/polyaxon-api:1.9.5\" already present on machine\n  Warning  Unhealthy  4m49s (x16 over 13m)  kubelet            Liveness probe failed: Get \"[http:\/\/192.168.184.124:80\/healthz\/](http:\/\/192.168.184.124\/healthz\/)\": dial tcp 192.168.184.124:80: connect: connection refused",
        "Challenge_closed_time":1649329378000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649328918000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1473",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":10.4,
        "Challenge_reading_time":22.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.1277777778,
        "Challenge_title":"Polyaxon CE fresh deployment never finishes",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":196,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The firts step is to cehck if you are using the --wait flag with Helm, if yes the deployment will not pass because there's helm hook.\n\nThe reason of the deadlock: helm waits forever for API to be healthy and the API is waiting for the hook to initialize the database.\n\nMore info about the helm deadlock issue: helm\/helm#5118\n\nFor ArgoCD, the flag is set to true automatically: argoproj\/argo-cd#6880\n\nFor the Terraform provider for helm release, the flag is set to true by default: https:\/\/registry.terraform.io\/providers\/hashicorp\/helm\/latest\/docs\/resources\/release#wait and should be set it to false.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":92.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1511190340208,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":385.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":1001.86839,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I noticed my Sagemaker (Amazon aws) jupyter notebook has an outdated version of the sklearn library.<\/p>\n<p>when I run <code>! pip freeze<\/code> I get:<\/p>\n<pre><code>sklearn==0.0\n<\/code><\/pre>\n<p>and when I run (with python) <code>print(sklearn.__version__)<\/code> I get<\/p>\n<pre><code>0.24.1\n<\/code><\/pre>\n<p>I'm not sure which one is my real version but I need 1.0.0 in order to use the <code>from_predictions()<\/code> method.<\/p>\n<p>But when I am trying to run <code>! \/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/bin\/python -m pip install --upgrade sklearn<\/code> I am getting the following output:<\/p>\n<blockquote>\n<p>Requirement already satisfied: sklearn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(0.0) Requirement already satisfied: scikit-learn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from sklearn) (0.24.1) Requirement already satisfied: scipy&gt;=0.19.1\nin\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.5.3) Requirement already satisfied:\njoblib&gt;=0.11 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.0.1) Requirement already satisfied:\nthreadpoolctl&gt;=2.0.0 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (2.1.0) Requirement already satisfied:\nnumpy&gt;=1.13.3 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.19.5)<\/p>\n<\/blockquote>\n<p>This is a very pupular library so it's weird if sagemaker cant upgrade it. Anyone has an idea what am I doing wrong?<\/p>",
        "Challenge_closed_time":1641036273827,
        "Challenge_comment_count":2,
        "Challenge_created_time":1637426688750,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1637429547623,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70047920",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.1,
        "Challenge_reading_time":23.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1002.6625213889,
        "Challenge_title":"How to upgrade the sklearn library in sagemaker",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":634.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1381413304940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":593.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>I managed to update sklearn to version 0.24.2 via the following command:<\/p>\n<pre><code>!conda update scikit-learn --yes\n<\/code><\/pre>\n<p>To further update it, you probably have to also update Python, which is version 3.6 in the current conda_python3 kernel on Sagemaker.<\/p>\n<p>It also looks promising to create your custom conda environment, as explained here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":7.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1366551199192,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1789.0,
        "Answerer_view_count":102.0,
        "Challenge_adjusted_solved_time":1.5206341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am interested if one can import sagemaker packages on your own local Python environment or whether they are restricted to AWS Sagemaker?<\/p>\n<pre><code>from sagemaker_automl import AutoMLInteractiveRunner, AutoMLLocalCandidate\n<\/code><\/pre>\n<p>For instance can I somehow download the sagemaker_automl?<\/p>\n<p>I know the there are no sagemaker packages available in the conda repository. Perhaps there is some other way to get them.<\/p>",
        "Challenge_closed_time":1606601111440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606595637157,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65054187",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":6.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5206341667,
        "Challenge_title":"Can you use sagemaker Python libraries on my localhost?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":162.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310821356880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":14913.0,
        "Poster_view_count":1093.0,
        "Solution_body":"<p>The Sagemaker Python SDK is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">open source and on GitHub<\/a>, as well as published on <a href=\"https:\/\/pypi.org\/project\/sagemaker\/\" rel=\"nofollow noreferrer\">Pypi<\/a>.<\/p>\n<p>You can install it by running:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install sagemaker\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.0,
        "Solution_reading_time":4.99,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1531231343652,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"St. Louis, MO, USA",
        "Answerer_reputation_count":676.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":1504.1879255556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an AWS endpoint using a Docker container (I followed <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers.html\" rel=\"nofollow noreferrer\">this<\/a>).<\/p>\n<p>Everything is working perfectly but now I need to put it in production and define an auto scaling strategy.<\/p>\n<p>I tried 2 things:<\/p>\n<ol>\n<li><p>AWS console but the auto scaling button is greyed\nout.<\/p>\n<\/li>\n<li><p>The method described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-code-apply.html\" rel=\"nofollow noreferrer\">here<\/a>. My endpoint name\nis <code>EmbeddingEndpoint<\/code> and my variant name is <code>SimpleVariant<\/code>. So my\nfinal command is<\/p>\n<\/li>\n<\/ol>\n<pre><code>aws application-autoscaling put-scaling-policy \\\n--policy-name scalable_policy_for_embedding \\\n--policy-type TargetTrackingScaling \\\n--resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant \\\n--service-namespace sagemaker \\\n--scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n--target-tracking-scaling-policy-configuration file:\/\/policy_config.json\n<\/code><\/pre>\n<p>but I get this result :<\/p>\n<pre><code>An error occurred (ObjectNotFoundException) when calling the PutScalingPolicy operation: \nNo scalable target registered for service namespace: sagemaker, resource ID: \nendpoint\/EmbeddingEndpoint\/variant\/SimpleVariant, scalable dimension: \nsagemaker:variant:DesiredInstanceCount\n<\/code><\/pre>\n<p>Does someone have another solution, or is it that I didn't set the variable well ?\nThank you in advance !<\/p>",
        "Challenge_closed_time":1630466729772,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625051653240,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1630503140507,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68193708",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":19.2,
        "Challenge_reading_time":21.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1504.1879255556,
        "Challenge_title":"Unable to Define Auto Scaling for SageMaker Endpoint",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570620624976,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Your <code>sagemaker<\/code> service-namespace does not have any registered scaling targets. You need to first run <code>register-scalable-target<\/code> before running <code>put-scaling-policy<\/code>.<\/p>\n<pre><code>aws application-autoscaling register-scalable-target \\\n    --service-namespace sagemaker \\\n    --scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n    --resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":29.5,
        "Solution_reading_time":6.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6838888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nI am trying to launch an endpoint locally, to do couple inferences from my dev notebook (without having to wait for instanciation time of actual endpoint or batch training). I am running the following code:\n\n    # get trained model from s3\n    trained_S2S = SM.model.Model(\n        image=seq2seq,\n        model_data=('s3:\/\/XXXXXXXXXXXXX\/'\n            + 'output\/seq2seq-2018-07-30-16-55-12-521\/output\/model.tar.gz'),\n        role=role) \n    \n    S2S = trained_S2S.deploy(1, instance_type='local')    \n\nI get the following error (several hundreds of lines repeated):\n\n    WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa5c61eaac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': \/ping \n`RuntimeError: Giving up, endpoint: seq2seq-2018-08-01-14-18-06-555 didn't launch correctly`",
        "Challenge_closed_time":1533135863000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533133401000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668556657875,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU54PWM3V9QoybCiO5GvB03g\/sagemaker-local-deployment-runtimeerror-giving-up-endpoint-didn-t-launch-correctly",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":13.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.6838888889,
        "Challenge_title":"Sagemaker local deployment: \"RuntimeError: Giving up, endpoint: didn't launch correctly\"",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":424.0,
        "Challenge_word_count":102,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Currently, SageMaker local is supported only for SageMaker framework containers (MXNet, TensorFlow, PyTorch, Chainer and Spark) and not for the Builtin algorithms",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925544815,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1497960178323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":12088.0,
        "Answerer_view_count":3630.0,
        "Challenge_adjusted_solved_time":16.0933286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to creating similar machine learning experiment as on found on github at below link.<\/p>\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb<\/a><\/p>\n<p>what could I do to resolve the ImportEror:?<\/p>",
        "Challenge_closed_time":1606803664676,
        "Challenge_comment_count":1,
        "Challenge_created_time":1606745728693,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65075228",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":26.7,
        "Challenge_reading_time":9.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.0933286111,
        "Challenge_title":"How could I resolve ImportError: no module named 'azureml', I am using Azure databricks notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1063.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483978710216,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":17.0,
        "Solution_body":"<blockquote>\n<p>To resolve this issue, I would request you to install azureml library from PyPi packages.<\/p>\n<\/blockquote>\n<p>To make third-party or locally-built code available to notebooks and jobs running on your clusters, you can install a library. Libraries can be written in Python, Java, Scala, and R. You can upload Java, Scala, and Python libraries and point to external packages in PyPI, Maven, and CRAN repositories.<\/p>\n<p><strong>Steps to install third-party libraries:<\/strong><\/p>\n<p><strong>Step1:<\/strong> Create Databricks Cluster.<\/p>\n<p><strong>Step2:<\/strong> Select the cluster created.<\/p>\n<p><strong>Step3:<\/strong> Select Libraries =&gt; Install New =&gt; Select Library Source = &quot;PYPI&quot; =&gt; Package = &quot;azureml-sdk[databricks]&quot;.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DafqR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DafqR.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wCjUG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wCjUG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HD6VC.gif\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HD6VC.gif\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Reference:<\/strong> <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-databricks-automl-environment\" rel=\"nofollow noreferrer\">Set up a development environment with Azure Databricks and autoML in Azure Machine Learning<\/a><\/p>\n<p>For different methods to install packages in Azure Databricks: <a href=\"https:\/\/stackoverflow.com\/questions\/60543850\/how-to-install-a-library-on-a-databricks-cluster-using-some-command-in-the-noteb\/60557852#60557852\">How to install a library on a databricks cluster using some command in the notebook?<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":8.0,
        "Solution_readability":12.1,
        "Solution_reading_time":24.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":167.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1518706063680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":100.6858180556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When AzureML creates a python environment and runs <code>pip install<\/code>, I'd like it to use additional non-public indices. Is there a way to do that?<\/p>\n\n<p>I'm running my python script on an AzureML compute. The environment is created from pip requirements as per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-use-environments#conda-and-pip-specification-files\" rel=\"nofollow noreferrer\">docs<\/a>. The script now references a package in a private index. To run the script on a local or build machine I just specify <code>PIP_EXTRA_INDEX_URL<\/code> environment variable with credentials to the index before running <code>pip install -c ...<\/code>. How to enable same functionality on AzureML environment prep process?<\/p>\n\n<p>AzureML docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-use-environments#private-wheel-files\" rel=\"nofollow noreferrer\">suggest<\/a> that I directly supply wheel files instead of package names. That means I have to manually do all the work that pip is built for: identify private packages among other requirements, choose right versions and platform, download them.<\/p>\n\n<p>Ideally, I would have to just write something like this:<\/p>\n\n<pre><code>myenv = Environment.from_pip_requirements(\n    name = \"myenv\",\n    file_path = \"path-to-pip-requirements-file\",\n    extra-index-url = [\"url1\", \"url2\"])\n<\/code><\/pre>",
        "Challenge_closed_time":1572973318072,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572610849127,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58659160",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":19.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":100.6858180556,
        "Challenge_title":"How to specify pip extra-index-url when creating an azureml environment?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":5362.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518706063680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>It appears, there is a <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py#set-pip-option-pip-option-\" rel=\"nofollow noreferrer\"><code>set_pip_option<\/code> method<\/a> in the SDK which sorts out the problem with one single extra-index-url, e.g.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.environment import CondaDependencies\ndep = CondaDependencies.create(pip_packages=[\"pyyaml\", \"param\"])\ndep.set_pip_option(\"--extra-index-url https:\/\/user:password@extra.index\/url\")\n<\/code><\/pre>\n\n<p>Unfortunately, second call to this function replaces the first value with the new one. For the <code>--extra-index-url<\/code> option this logic should be changed in order to support search in more than 2 indices (one public, one private).<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.6,
        "Solution_reading_time":11.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1556965222392,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3025.0,
        "Answerer_view_count":175.0,
        "Challenge_adjusted_solved_time":0.2146566667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I currently use Conda to capture my dependencies for a python project in a <code>environment.yml<\/code>.<\/p>\n\n<p>When I build a docker service from the project I need to reinstall these dependencies. I would like to get around, having to add (mini-)conda to my docker image.<\/p>\n\n<p>Is it possible to parse <code>environment.yml<\/code> with pip\/pipenv or transform this into a corresponding <code>requirements.txt<\/code>?<\/p>\n\n<p>(I don't want to leave conda just yet, as this is what MLflow captures, when I log models)<\/p>",
        "Challenge_closed_time":1561110403227,
        "Challenge_comment_count":1,
        "Challenge_created_time":1561109630463,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1561111068256,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56700687",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.3,
        "Challenge_reading_time":7.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2146566667,
        "Challenge_title":"Installing dependencies from (Conda) environment.yml without Conda?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1601.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336936830510,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":948.0,
        "Poster_view_count":132.0,
        "Solution_body":"<p>Nope.<\/p>\n\n<ol>\n<li><p><code>conda<\/code> automatically installs dependencies of conda packages. These are resolved differently by <code>pip<\/code>, so you'd have to resolve the Anaconda dependency tree in your transformation script.<\/p><\/li>\n<li><p>Many <code>conda<\/code> packages are non-Python. You couldn't install those dependencies with <code>pip<\/code> at all.<\/p><\/li>\n<li><p>Some <code>conda<\/code> packages contain binaries that were compiled with the Anaconda compiler toolchain. Even if the corresponding <code>pip<\/code> package can compile such binaries on installation, it wouldn't be using the Anaconda toolchain. What you'd get would be fundamentally different from the corresponding <code>conda<\/code> package.<\/p><\/li>\n<li><p>Some <code>conda<\/code> packages have fixes applied, which are missing from corresponding <code>pip<\/code> packages.<\/p><\/li>\n<\/ol>\n\n<p>I hope this is enough to convince you that your idea won't fly.<\/p>\n\n<p>Installing Miniconda isn't really a big deal. Just do it :-)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.2,
        "Solution_reading_time":13.15,
        "Solution_score_count":3.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":124.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1526004205792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"China",
        "Answerer_reputation_count":28087.0,
        "Answerer_view_count":3298.0,
        "Challenge_adjusted_solved_time":2.4671630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>After having solved <a href=\"https:\/\/stackoverflow.com\/questions\/55347910\/\">Why does my ML model deployment in Azure Container Instance still fail?<\/a> and having deployed on ACI, I am using Azure Machine Learning Service to deploy a ML model as web service on AKS.<\/p>\n\n<p>My current (working) ACI-deployment code is<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core.image import ContainerImage\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                      memory_gb=8, \n                      tags={\"data\": \"text\",  \"method\" : \"NB\"}, \n                      description='Predict something')\n\n\nimage_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                      docker_file=\"Dockerfile\",\n                      runtime=\"python\", \n                      conda_file=\"myenv.yml\")\n\nimage = ContainerImage.create(name = \"scorer-image\",\n                      models = [model],\n                      image_config = image_config,\n                      workspace = ws\n                      )\n\nservice_name = 'scorer-svc'\nservice = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                        image = image,\n                                        name = service_name,\n                                        workspace = ws)\n<\/code><\/pre>\n\n<p>I would like to modify it so to deploy on AKS, but looks more convoluted than I expected, as I imagined moving from ACI to AKS (i.e. from test to production) to be a routine operation. Still, it seems to need a bit more of changes in the code than I thought: <\/p>\n\n<ul>\n<li>AKS seems to require an <code>InferenceConfig<\/code> object (?) <\/li>\n<li>with AKS there's no method like <code>deploy_from_image<\/code> for deployment from my existing Docker <code>image<\/code> (?)<\/li>\n<\/ul>\n\n<p>Can deployment be done on AKS by performing minimal changes to the ACI code instead?  <\/p>",
        "Challenge_closed_time":1557660771803,
        "Challenge_comment_count":2,
        "Challenge_created_time":1557392548383,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1557651890016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56055868",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":11.5,
        "Challenge_reading_time":21.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":74.5065055556,
        "Challenge_title":"What's the easiest way to move from ACI to AKS deployment?",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":460.0,
        "Challenge_word_count":192,
        "Platform":"Stack Overflow",
        "Poster_created_time":1415722650716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation_count":4811.0,
        "Poster_view_count":713.0,
        "Solution_body":"<p>From the code that you have provided, when you deploy the application in the ACI using the method <code>Webservice.deploy_from_image<\/code> with the parameters <code>deployment_config<\/code> and container image. The deployment_config makes by the <code>AciWebservice.deploy_configuration<\/code>.<\/p>\n\n<p>When you take a look at the ML about AKS, you can also find the method <code>AksWebservice.deploy_configuration<\/code>. So you just need to change the method <code>AciWebservice.deploy_configuration<\/code> into <code>AksWebservice.deploy_configuration<\/code>, then the application can be deployed from ACI into AKS. And it's the minimal changes. Also, it can deploy from the docker image.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":9.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1322.0291666667,
        "Challenge_answer_count":4,
        "Challenge_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\nI started to use amundsen metadata with Neptune database. Initially I used the metadata docker image to interact with the database, but every tested route gave me a 500 internal server error. So I tested it locally, using a VPN to connect to neptune db, and I found 2 problems. I'll do a PR linked to the issue that solves the problems\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nWhen calling a route of the metadata api for the neptune service, the server should respond without problem\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\n1. When calling the api to retrieve (for example) a table description, there's an error `got an unexpected keyword argument 'read_timeout'`. This error has already be identified in https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1382\r\n2. After the correction of 1, another error during the same request\r\n```json\r\n{\r\n    \"detailedMessage\": \"Failed to interpret Gremlin query: Query parsing failed at line 1, character position at 208, error message : token recognition error at: 'dec'\",\r\n    \"code\": \"MalformedQueryException\",\r\n    \"requestId\": \"25542307-96bb-40d2-9585-5a340b8d868c\"\r\n}\r\n```\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix\/reason for the bug -->\r\n1. Initialize `TornadoTransport` class properly, removing `read_timeout` and `write_timeout` in  `gremlin_proxy.py` file\r\n2. Move `Order.decr`to `Order.desc` for `_get_table_columns` and `_get_popular_tables_uris` functions in `gremlin_proxy.py` file. The Order.decr and Order.incr are deprecated and don't work with neptune\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1. Call the `\/table\/{table_uri}` metadata route using the gremlin metadata service with AWS Neptune db\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Amunsen version used: last (metadata-3.10.0)\r\n* Data warehouse stores: snowflake\r\n* Deployment (k8s or native):\r\n* Link to your fork or repository: https:\/\/github.com\/ggirodda\/amundsen\/tree\/main",
        "Challenge_closed_time":1664069854000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659310549000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1946",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":31.58,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":1322.0291666667,
        "Challenge_title":"Neptune MalformedQueryException",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":345,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for opening your first issue here!\n The PR that solves the issue in my case https:\/\/github.com\/amundsen-io\/amundsen\/pull\/1947\/files This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n This issue has been automatically closed for inactivity. If you still wish to make these changes, please open a new pull request or reopen this one.\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.7,
        "Solution_reading_time":5.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1746825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello community,     <br \/>\nI'm facing a problem, my ACR in my resource group was deleted and I couldn't create any instance. I created again and now I can create instances but i'm having problems to run the dataset profile. It's failing to pull the image docker.    <\/p>\n<p>This is the output    <\/p>\n<pre><code>AzureMLCompute job failed.  \nFailedPullingImage: Unable to pull docker image  \n\timageName: 19acd0cdf57549bcace363c924cf045b.azurecr.io\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f  \n\terror: Run docker command to pull public image failed with error: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.  \n.  \n\tReason: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.  \n  \n\tInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.  \n<\/code><\/pre>\n<p>The ML Studio has the following permissions on the ACR permissions    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/132698-unbenannt.png?platform=QnA\" alt=\"132698-unbenannt.png\" \/>    <\/p>\n<p>The docker image appears in the repositories of the ACR    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/132781-unbenannt2.png?platform=QnA\" alt=\"132781-unbenannt2.png\" \/>    <\/p>\n<p>Any hint how can i solve this problem?    <\/p>\n<p>Thanks in advance    <\/p>",
        "Challenge_closed_time":1631803071767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631798842910,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/555024\/not-able-to-pull-docker-image-from-container-regis",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":23.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1.1746825,
        "Challenge_title":"Not able to pull docker image from Container Registry",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":175,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=5565f700-af3b-41a9-b47f-9b8a6276d8fd\">@Moresi, Marco  <\/a> Does this container registry have the admin account enabled? A requirement while creating a workspace with an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace-cli?tabs=bringexistingresources1%2Cvnetpleconfigurationsv1cli#create-a-workspace\">existing container registry<\/a> is to have the admin account enabled.     <\/p>\n<p>If you have already enabled it then a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace-cli?tabs=bringexistingresources1%2Cvnetpleconfigurationsv1cli#sync-keys-for-dependent-resources\">re-sync of keys<\/a> might be required for your workspace.    <\/p>\n<pre><code>az ml workspace sync-keys -w &lt;workspace-name&gt; -g &lt;resource-group-name&gt;  \n<\/code><\/pre>\n<p>Deleting the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace?tabs=python#deleting-the-azure-container-registry\">default container registry<\/a> used by the workspace can also cause the workspace to break.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.3,
        "Solution_reading_time":14.97,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":168.8386111111,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nUsers get the error \"null is not an object\" when pop-ups are enabled in SWB (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620))\r\nThis error is illegible to the user and causes confusion. Can we make the error message more clear such as:\r\n\"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Diable pop-ups \r\n2. Connect to a workspace\r\n\r\n**Expected behavior**\r\nIf the workspace is unable to open, a more legible error message should be shown, such as \"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v4.3.1 and v5.0.0]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671209314000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670601495000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1081",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.6,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":168.8386111111,
        "Challenge_title":"[Bug] More descriptive error message for \"null is not an object\" while trying to connect to Sagemaker notebook. ",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @simranmakwana, thank you for creating this issue. There are currently no plans to enrich the error messages in the UI; the recommendation is for you to customize the error messages within your installation of SWB as you see fit. Please reply back if there are any concerns with this approach. Thank you! ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":3.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":2.2327661111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to bundle the Python package <code>kedro<\/code> which provides a command line interface (<code>kedro<\/code>). In addition I'd like to put the Python package <code>kedro-docker<\/code> into the snap as well. This second package extends the first package's command line interface (<code>kedro docker<\/code>). But when I create a snap with the <code>snapcraft.yaml<\/code> below I get only the command line interface of the first package:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>name: kedro\nbase: core18\nversion: latest\ndescription: |\n    Kedro is a development workflow framework that implements software\n    engineering best-practice for data pipelines with an eye towards\n    productionising machine learning models.\n\ngrade: devel\nconfinement: devmode\n\narchitectures:\n  - build-on: [amd64]\n\napps:\n  kedro:\n    command: kedro\n    plugs:\n      - home\n      - network\n      - network-bind\n      - docker\n    environment: {\n      LANG: C.UTF-8,\n      LC_ALL: C.UTF-8\n    }\n\nparts:\n  kedro:\n    plugin: python\n    python-version: python3\n    python-packages:\n      - kedro==0.15.9\n      - kedro-docker==0.1.1\n<\/code><\/pre>\n\n<p>How can I get the extended command line interface (<code>kedro docker<\/code>) into the snap?<\/p>",
        "Challenge_closed_time":1588362730510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588354211093,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1588354692552,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61547494",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":15.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2.3665047223,
        "Challenge_title":"How to snap a python package with plugin packages?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":147,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441627039648,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Augsburg, Germany",
        "Poster_reputation_count":3635.0,
        "Poster_view_count":383.0,
        "Solution_body":"<p>I'm no expert and never used <code>snapcraft<\/code>, therefore just a hypothesis here. Kedro-Docker exposes only project-specific commands that won't show up unless you are in the root of the project. So if you run <code>kedro new<\/code> and then <code>cd &lt;project-dir&gt; &amp;&amp; kedro<\/code>, you should (ideally) see a <code>docker<\/code> group of commands:<\/p>\n\n<pre><code>Global commands from Kedro\nCommands:\n  docs  See the kedro API docs and introductory tutorial.\n  info  Get more information about kedro.\n  new   Create a new kedro project.\n\nProject specific commands from Docker\nCommands:\n  docker  Dockerize your Kedro project.\n\nProject specific commands from &lt;project-dir&gt;\/kedro_cli.py\nCommands:\n  activate-nbstripout  Install the nbstripout git hook to automatically...\n  build-docs           Build the project documentation.\n  build-reqs           Build the project dependency requirements.\n  install              Install project dependencies from both...\n  ipython              Open IPython with project specific variables loaded.\n  jupyter              Open Jupyter Notebook \/ Lab with project specific...\n  lint                 Run flake8, isort and (on Python &gt;=3.6) black.\n  package              Package the project as a Python egg and wheel.\n  run                  Run the pipeline.\n  test                 Run the test suite.\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":15.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":164.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1431525955023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cherry Hill, NJ, United States",
        "Answerer_reputation_count":2069.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":8487.4160213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the mlflow databricks integration, specifically the tracking API. Normally, I can view past runs info in the handy sidebar of a notebook, as you can see <a href=\"https:\/\/i.stack.imgur.com\/JWY1c.png\" rel=\"nofollow noreferrer\">here<\/a> and which I got from the <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/quick-start.html#mlflow-mlflow-quick-start-python\" rel=\"nofollow noreferrer\">tutorial<\/a>. However, what I want now is to use multiple notebooks to send runs to the same experiment. Additionally, I would like to view the results of all these common runs in each of the notebooks. To do this, I need to change the (default) experiment tracked by the \"runs\" tab. <\/p>\n\n<p>Ultimately, my question boils down to the following: how can I set the experiment being tracked by the \"runs\" tab? I have tried using <code>mlflow.set_tracking_uri<\/code> and <code>mlflow.set_experiment(mlflow_experiment_name)<\/code><\/p>",
        "Challenge_closed_time":1566518807736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562089677383,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56857709",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":12.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1230.3139869444,
        "Challenge_title":"How to change experiment being tracked by Databricks \"runs\" tab?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":257.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464482523396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Canada",
        "Poster_reputation_count":2168.0,
        "Poster_view_count":1631.0,
        "Solution_body":"<p>I don't believe this is possible today, as the design choice is to associate the runs tab to the notebook experiment.  From the <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/tracking.html#notebook-experiments\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>Every Python and R notebook in a Databricks workspace has its own experiment. When you use MLflow in a notebook, it records runs in the notebook experiment.<\/p>\n<p>A notebook experiment shares the same name and ID as its corresponding notebook. The notebook ID is the numerical identifier at the end of a Notebook URL.<\/p>\n<\/blockquote>\n<p>You can create experiments independent of the notebook experiment and log runs to it from different sources.  You'll still have to open up the tracking UI to explore the results though.<\/p>\n<p>In other words, you can send multiple runs from different notebooks to the same experiment, but today you cannot log multiple runs to the 'Runs' tab in a specific notebook.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1592644375060,
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":12.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":144.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4421.2883333333,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nOccasionally after starting a Sagemaker workspace, clicking 'Connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> We have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. The notebook window is not opened after clicking on 'Connect'.\r\n\r\n**To Reproduce**\r\nThe error is intermittent. I *think* it may happen after the SW window has been open a while, because I noticed that the SW window automatically logged me out shortly after seeing this error.\r\n\r\n1. Click 'Start' for Sagemaker workspace and wait for the status to change to 'Available'. \r\n2. Click 'Connections', then 'Connect'\r\n3. See error\r\n\r\nWhen I logged out and back into Service Workbench, and was able to connect to the workspace successfully. \r\n\r\n**Expected behavior**\r\nA new window should open with a Jupyter\/Sagemaker notebook in a new window. \r\n\r\n**Versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Challenge_closed_time":1643923114000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628006476000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.78,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":4421.2883333333,
        "Challenge_title":"\"null is not an object\" while trying to connect to Sagemaker notebook.",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":164,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @tom-christie, we believe the issue mentioned is due to access token getting expired. Please feel free to use the latest version with the fix ([v3.3.1](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v3.3.1)). We're seeing this issue on 4.1.1 as well. However, it appears to be persistent (i.e. it happens every time we connect to a SageMaker workspace). So far, we've only tested a single workspace config, but the error consistently shows up when we try to connect to different workspace instances using the same config. The workspace instances are new and running, at least as shown in the SWB UI. We haven't verified if the instances are available in the SageMaker console, however. Is it possible this is related to a popup blocker as reported in GALI-1224? It creates a similar error message.\r\nhttps:\/\/sim.amazon.com\/issues\/CHAMDOC-17 Yeah, I've seen the error happen because popups are disabled for the SWB domain. Once you enable popup for the SWB domain, it should allow you to connect to Sagemaker. Feel free to reopen this ticket if enabling popups didn't resolve your issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.0,
        "Solution_reading_time":13.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":35.0730002778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Challenge_closed_time":1579273412888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579147150087,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":20.5,
        "Challenge_reading_time":212.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":223,
        "Challenge_solved_time":35.0730002778,
        "Challenge_title":"AWS Sagemaker scikit_bring_your_own example",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":341.0,
        "Challenge_word_count":1082,
        "Platform":"Stack Overflow",
        "Poster_created_time":1241005356852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Malaysia",
        "Poster_reputation_count":15794.0,
        "Poster_view_count":1032.0,
        "Solution_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":2.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1312912826288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":5843.0,
        "Answerer_view_count":153.0,
        "Challenge_adjusted_solved_time":0.9586563889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have deployed a build of mlflow to a pod in my kubernetes cluster. I'm able to port forward to the mlflow ui, and now I'm attempting to test it. To do this, I am running the following test on a jupyter notebook that is running on another pod in the same cluster.<\/p>\n<pre><code>import mlflow\n\nprint(&quot;Setting Tracking Server&quot;)\ntracking_uri = &quot;http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000&quot;\n\nmlflow.set_tracking_uri(tracking_uri)\n\nprint(&quot;Logging Artifact&quot;)\nmlflow.log_artifact('\/home\/test\/mlflow-example-artifact.png')\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p>When I run this though, I get<\/p>\n<pre><code>ConnectionError: HTTPConnectionPool(host='mlflow-tracking-server.default.svc.cluster.local', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get? (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>\n<p>The way I have deployed the mlflow pod is shown below in the yaml and docker:<\/p>\n<p>Yaml:<\/p>\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: &lt;ECR_IMAGE&gt;\n        ports:\n        - containerPort: 5000\n        env:\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;S3_BUCKET&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\n  annotations:\n    service.beta.kubernetes.io\/aws-load-balancer-type: nlb\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  selector:\n    app: mlflow-tracking-server\n  ports:\n    - name: http\n      port: 5000\n      targetPort: http\n<\/code><\/pre>\n<p>While the dockerfile calls a script that executes the mlflow server command: <code>mlflow server --default-artifact-root ${AWS_MLFLOW_BUCKET} --host 0.0.0.0 --port 5000<\/code>, I cannot connect to the service I have created using that mlflow pod.<\/p>\n<p>I have tried using the tracking uri <code>http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000<\/code>, I've tried using the service EXTERNAL-IP:5000, but everything I tried cannot connect and log using the service. Is there anything that I have missed in deploying my mlflow server pod to my kubernetes cluster?<\/p>",
        "Challenge_closed_time":1587498737656,
        "Challenge_comment_count":6,
        "Challenge_created_time":1587495286493,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1599477403816,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61351024",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":14.0,
        "Challenge_reading_time":34.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.9586563889,
        "Challenge_title":"Kubernetes MLflow Service Pod Connection",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":276,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417012835812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":945.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>Your <strong>mlflow-tracking-server<\/strong> service should have <em>ClusterIP<\/em> type, not <em>LoadBalancer<\/em>. <\/p>\n\n<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer<\/em> Service type.<\/p>\n\n<blockquote>\n  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that\u2019s outside of your cluster.\n  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.<\/p>\n  \n  <p>Type values and their behaviors are:<\/p>\n  \n  <ul>\n  <li><p><strong>ClusterIP<\/strong>: Exposes the Service on a cluster-internal IP. Choosing this\n  value makes the Service only reachable from within the cluster. This\n  is the default ServiceType. <\/p><\/li>\n  <li><p><strong>NodePort<\/strong>: Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll > be able to contact the NodePort Service, from outside the cluster, by\n  requesting :. <\/p><\/li>\n  <li><strong>LoadBalancer<\/strong>: Exposes the Service\n  externally using a cloud provider\u2019s load balancer. NodePort and\n  ClusterIP Services, to which the external load balancer routes, are\n  automatically created. <\/li>\n  <li><strong>ExternalName<\/strong>: Maps the Service to the contents\n  of the externalName field (e.g. foo.bar.example.com), by returning a\n  CNAME record with its value. No proxying of any kind is set up.<\/li>\n  <\/ul>\n  \n  <p><a href=\"https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#publishing-services-service-types\" rel=\"nofollow noreferrer\">kubernetes.io<\/a><\/p>\n<\/blockquote>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1587499353943,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":21.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":206.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3836.5522222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently the `.drone.yaml` is referencing the k8s secret `mlflow-server-secret` which doesn't exist by default.\r\n\r\nWe have noticed that `{{ .ProjectID }}-mlflow-secret` secret is created when a `kdlProject` resource is created.\r\n\r\nTo solve this issue the name of the `mlflow-server-secret` must be changed into `{{ .ProjectID }}-mlflow-secret`",
        "Challenge_closed_time":1664791991000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650980403000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/810",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3836.5522222222,
        "Challenge_title":"Project template mlflow secret bad name",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1636044845360,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":241.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":2833.1668325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook.<\/p>\n<p>After having prepared the correct Dockerfile and the URI where to push the image that contains my train.py script, I try to push the image directly in a notebook cell.<\/p>\n<p>The exact command I try to execute is: <code>!docker build .\/ -t $IMAGE_URI<\/code>, where IMAGE_URI is the environmental variable previously defined. However I try to run this command I get the error: <code>\/bin\/bash: docker: command not found<\/code>. I also tried to execute it with the magic cell %%bash, importing the subprocess library and also execute the command stored in a .sh file.<\/p>\n<p>Unfortunately none of the above solutions work, they all return the same <strong>command not found<\/strong> error with code 127.<\/p>\n<p>If instead I run the command from a bash present in the Jupyterlab it works fine as expected.<\/p>\n<p>Is there any workaround to make the push execute inside the jupyter notebook? I was trying to keep the whole custom training process inside the same notebook.<\/p>",
        "Challenge_closed_time":1661350818332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651142988007,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651151719750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72042363",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":2835.5084236111,
        "Challenge_title":"Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1648140384823,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>If you follow this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/create-user-managed-notebooks-instance-console-quickstart\" rel=\"nofollow noreferrer\">guide<\/a> to create a user-managed notebook from Vertex AI workbench and select Python 3, then it comes with Docker available.<\/p>\n<p>So you will be able to use Docker commands such as <code>! docker build .<\/code> inside the user-managed notebook.<\/p>\n<p>Example:\n<a href=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" alt=\"Vertex AI Managed Notebook\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661351120347,
        "Solution_link_count":3.0,
        "Solution_readability":12.3,
        "Solution_reading_time":8.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":57.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1583491811220,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Baku, Azerbaijan",
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":95.6068944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Challenge_closed_time":1633946464143,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633602279323,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69479488",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":44.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":95.6068944445,
        "Challenge_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":418.0,
        "Challenge_word_count":296,
        "Platform":"Stack Overflow",
        "Poster_created_time":1583491811220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Baku, Azerbaijan",
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":6.6,
        "Solution_reading_time":11.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":112.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.8833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\u00a0\n\nI'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.\u00a0 I have enabled\u00a0aiplatform.googleapis.com.\n\nCommand:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifier\n\nUsing endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.\n\nPlease suggest what am I missing.",
        "Challenge_closed_time":1661575080000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661561100000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project\/td-p\/460565\/jump-to\/first-unread-message",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.8833333333,
        "Challenge_title":"Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":310.0,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":3.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":51.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1534872376852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":90.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":149.6238352778,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I am trying to link my s3 bucket to a notebook instance, however i am not able to:<\/p>\n\n<p>Here is how much I know:<\/p>\n\n<pre><code>from sagemaker import get_execution_role\n\nrole = get_execution_role\nbucket = 'atwinebankloadrisk'\ndatalocation = 'atwinebankloadrisk'\n\ndata_location = 's3:\/\/{}\/'.format(bucket)\noutput_location = 's3:\/\/{}\/'.format(bucket)\n<\/code><\/pre>\n\n<p>to call the data from the bucket:<\/p>\n\n<pre><code>df_test = pd.read_csv(data_location\/'application_test.csv')\ndf_train = pd.read_csv('.\/application_train.csv')\ndf_bureau = pd.read_csv('.\/bureau_balance.csv')\n<\/code><\/pre>\n\n<p>However I keep getting errors and unable to proceed.\nI haven't found answers that can assist much.<\/p>\n\n<p>PS: I am new to this AWS<\/p>",
        "Challenge_closed_time":1534873380927,
        "Challenge_comment_count":1,
        "Challenge_created_time":1534334735120,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51858379",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.5,
        "Challenge_reading_time":9.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":149.6238352778,
        "Challenge_title":"how to link s3 bucket to sagemaker notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":13863.0,
        "Challenge_word_count":84,
        "Platform":"Stack Overflow",
        "Poster_created_time":1505379841387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kampala, Uganda",
        "Poster_reputation_count":45.0,
        "Poster_view_count":20.0,
        "Solution_body":"<p>You can load S3 Data into AWS SageMaker Notebook by using the sample code below. Do make sure the Amazon SageMaker role has policy attached to it to have access to S3. <\/p>\n\n<p>[1] <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html<\/a> <\/p>\n\n<pre><code>import boto3 \nimport botocore \nimport pandas as pd \nfrom sagemaker import get_execution_role \n\nrole = get_execution_role() \n\nbucket = 'Your_bucket_name' \ndata_key = your_data_file.csv' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \n\npd.read_csv(data_location) \n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.5,
        "Solution_reading_time":8.45,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":55.1218747222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello MS team,<\/p>\n<p>I am using Azure devOps pipeline to submit a control script to the Azure-ML workspace. This control script in turn kicks off the Azure-ML pipeline containing pythonscriptsteps and hyperdrive step.<\/p>\n<p><strong>My directory structure:<\/strong><\/p>\n<p>.  <br \/>\n\u251c\u2500\u2500\u2500.vscode  <br \/>\n\u251c\u2500\u2500\u2500Automation  <br \/>\n\u251c\u2500\u2500\u2500Build  <br \/>\n\u2514\u2500\u2500\u2500Source  <br \/>\n\u251c\u2500\u2500\u2500.azureml  <br \/>\n\u251c\u2500\u2500\u2500.vscode  <br \/>\n\u251c\u2500\u2500\u2500amlcode  <br \/>\n\u2502 \u251c\u2500\u2500\u2500projectcode  <br \/>\n\u2502 \u2514\u2500\u2500\u2500<strong>pycache<\/strong>  <br \/>\n\u251c\u2500\u2500\u2500config  <br \/>\n\u251c\u2500\u2500\u2500Data  <br \/>\n\u251c\u2500\u2500\u2500setup  <br \/>\n\u251c\u2500\u2500\u2500tests  <br \/>\n\u2502 \u251c\u2500\u2500\u2500.pytest_cache  <br \/>\n\u2502 \u2502 \u2514\u2500\u2500\u2500v  <br \/>\n\u2502 \u2502 \u2514\u2500\u2500\u2500cache  <br \/>\n\u2502 \u2514\u2500\u2500\u2500<strong>pycache<\/strong>  <br \/>\n\u2514\u2500\u2500\u2500<strong>pycache<\/strong><\/p>\n<p>So here one of the azure cli task in Azure DevOps pipeline uses:<\/p>\n<p><em>az ml folder attach -w $(azureml.workspaceName) -g $(azureml.resourceGroup)<\/em><\/p>\n<p><strong>This command attaches my whole directory to the AML workspace and automatically creates &quot;.amlignore&quot; and &quot;.azureml&quot; is automatically added to that.<\/strong><\/p>\n<p>So it is throwing an authentication error as the <em>config.json()<\/em> is not found because it is generally put in the path <em>\/.azureml<\/em>.<\/p>\n<p>Where to put the config.json() then? What is the best practice?<\/p>",
        "Challenge_closed_time":1643613417496,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643414978747,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/714713\/the-azure-cli-command-az-ml-attach-folder-is-direc",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.2,
        "Challenge_reading_time":18.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":55.1218747222,
        "Challenge_title":"The azure cli command \"az ml attach folder\" is directly adding .azureml directory to .amlignore , so where to put config.json when using Azure devops pipeline to submit script to aml workspace?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":178,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=6755dac2-30f1-48a2-9d0d-4d2c96edc5d4\">@Shivapriya Katta  <\/a> The command az ml folder attach will create the directories and add the config file to .azureml to ensure the workspace resources are easily accessible. You can lookup the note section of the command for <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/reference-azure-machine-learning-cli\">reference<\/a>.    <\/p>\n<blockquote>\n<p>This command creates a .azureml subdirectory that contains example runconfig and conda environment files. It also contains a config.json file that is used to communicate with your Azure Machine Learning workspace.    <\/p>\n<\/blockquote>\n<p>The authentication error in your case could be because <code>az login<\/code> command might have been missed which allows the cli to authenticate interactively or service principal or MI and then run rest of the commands. You can try to run <a href=\"https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/authenticate-azure-cli\">this<\/a> and check if the attach works successfully.     <\/p>\n<p>Also, with the devops pipeline I am not sure if <code>az devops login<\/code>  is required to be run but if the above command fails even after <code>az login<\/code> authentication you can try <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/devops\/cli\/?view=azure-devops\">az devops login<\/a>.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":11.7,
        "Solution_reading_time":22.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":189.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":1.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2608333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Challenge_closed_time":1593108137000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593107198000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668530553628,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sagemaker-studio",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":7.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.2608333333,
        "Challenge_title":"Notebook Instance Types for SageMaker Studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":829.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The instance types you are seeing are Fast Launch Instances ( which are instance types designed to launch in under two minutes).\n\nIn order to see all the types of instances, click on the switch on top of the instance type list that says \"Fast Launch\", that should display the rest of available instances.\n\nHere is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html\n\nHope it helps!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925564584,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":5.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":2.7001211111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to ask if it is possible to use DVC with several accounts on the same machine. At the moment, all commands (<code>dvc pull<\/code>, <code>dvc push<\/code>, ...) are executed under my name. But after several people joined this project too, I do not want them to execute commands under my name.<\/p>\n<p>When I was alone on this project I generated ssh key:<\/p>\n<pre><code>ssh-keygen\n<\/code><\/pre>\n<p>Connected to server where DVC remote data is stored:<\/p>\n<pre><code>ssh-copy-id username@server_IP\n<\/code><\/pre>\n<p>Created config file which lets me execute all <code>dvc<\/code> commands using ssh:<\/p>\n<pre><code>[core]\n    remote = storage_server\n['remote &quot;storage_server&quot;']\n    url = ssh:\/\/username@server_IP:\/home\/DVC_remote\/DVC_project\n<\/code><\/pre>\n<p>What I should do so that several people could execute commands on their own name?<\/p>",
        "Challenge_closed_time":1643651143316,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643641422880,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70928144",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":11.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.7001211111,
        "Challenge_title":"Multiple users in DVC",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":181.0,
        "Challenge_word_count":115,
        "Platform":"Stack Overflow",
        "Poster_created_time":1457469301700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":585.0,
        "Poster_view_count":54.0,
        "Solution_body":"<p>You need to make the &quot;username&quot; part of the config personalized based on who is running the command. There are a few options to do this (based on <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noreferrer\">this document<\/a>, see the SSH part):<\/p>\n<h2>Basic options are:<\/h2>\n<ul>\n<li>User defined in the SSH config file (e.g. <code>~\/.ssh\/config<\/code>) for this host (URL);<\/li>\n<li>Current system user;<\/li>\n<\/ul>\n<p>So, the simplest even options could be just remove it from the URL and rely on the current system user?<\/p>\n<h2>Local (git-ignored or per-project DVC config) config<\/h2>\n<p>You could do is to remove the <code>username<\/code> part from the <code>url<\/code> and run something like this:<\/p>\n<pre><code>dvc remote modify --local storage_server user username\n<\/code><\/pre>\n<p><code>--local<\/code> here means that DVC will create a separate additional config that will be ignored by Git. This way if every user runs this command in every project they use they will customize the username.<\/p>\n<hr \/>\n<p>Let me know if that helps or something doesn't work. I'll try to help.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":160.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":384.0669444444,
        "Challenge_answer_count":2,
        "Challenge_body":"### System Specs\r\n**Operating System:** Windows 10\r\n**Python Version:** 3.9.5 64-bit\r\n\r\nWhen I run the command:\r\n\r\n```terminal\r\npip install azureml-core\r\n```\r\n\r\nI get an error during the installation, specifically on the `ruamel.yaml` package. I guess the first question I have is there any reason we are restricted to that specific version of `ruamel.yaml`? I was able to install the latest version **(0.17.10)** no problem, so if we could use a later version that would be the easiest fix.\r\n\r\n### Partial Log\r\n```terminal\r\nAttempting uninstall: ruamel.yaml\r\nFound existing installation: ruamel.yaml 0.17.10\r\nUninstalling ruamel.yaml-0.17.10:\r\nSuccessfully uninstalled ruamel.yaml-0.17.10\r\nRunning setup.py install for ruamel.yaml ... error\r\nERROR: Command errored out with exit status 1:\r\n```\r\n\r\n### Full Log\r\n[Error Log From Installation Run](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/6913613\/error.log)",
        "Challenge_closed_time":1629244160000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1627861519000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1564",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":12.03,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":384.0669444444,
        "Challenge_title":"pip install `azureml-core` fails on `ruamel.yaml`",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":119,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"0.17.5 introduced a breaking change hence there is an upperbound Thank you @vizhur! @areed1192, I'm closing this issue. Please reopen if you still have questions.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.03,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":26.2894686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use WanDB but when i use wandb.init() there is nothing.!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am waiting a lot of time.<\/p>\n<p>However, there is nothing in window.<\/p>\n<p>This is working well in Kernel.<\/p>\n<p>please.. help me guys<\/p>",
        "Challenge_closed_time":1655111602567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655016960480,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72590067",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":5.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":26.2894686111,
        "Challenge_title":"jupyterLab Wandb does not iterative",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1609152494583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"South Korea",
        "Poster_reputation_count":72.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I work at Weights &amp; Biases. If you're in a notebook the quickest thing you can do to get going with <code>wandb<\/code> is simply:<\/p>\n<pre><code>wandb.init(project=MY_PROJECT, entity=MY_ENTITY)\n<\/code><\/pre>\n<p>No <code>!wandb login<\/code>, <code>wandb.login()<\/code> or <code>%%wandb<\/code> needed. If you're not already logged in then <code>wandb.init<\/code> will ask you for you API key.<\/p>\n<p>(curious where you found <code>%%wandb<\/code> by the way?)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":6.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":57.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1659144422092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":166.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":1.1739147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>l'am running a ipynb file on sagemaker, however the error of occurs.\nl have used 'pip install tqdm' in terminals to install the tqdm so l've no idea what's happening. Is it running in a different environment?\nThanks for any answer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bO6zS.png\" rel=\"nofollow noreferrer\">error report from my ipynb file <\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5tP1J.png\" rel=\"nofollow noreferrer\">what l've done in terminal<\/a><\/p>",
        "Challenge_closed_time":1659155782776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659151516027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1659151556683,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73172644",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":6.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.1852080556,
        "Challenge_title":"Running ipynb file, but can't import a certain module",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1658300099523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>There is a possibility you may be executing &quot;pip&quot; in a different environment.<\/p>\n<p>Try executing &quot;!pip install tqdm&quot; or &quot;!pip3 install tqdm&quot; as a code cell in the Sagemaker document itself.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":2.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.7562972223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running CI\/CD pipelines in Azure Dev Ops in order to deploy my machine learning workloads on components of a Azure Machine Learning Workspace (e.g. AzureML pipelines, endpoints, etc). I recently came across an issue in my pipelines and wanted to validate that the versions of the Azure CLI and Azure CLI ml extension didn't cause this issue.<\/p>\n<p>Because the Azure CLI is a tool that is developed in python, it is relatively easy to install an older version of the cli. I can just run <code>pip install azure-cli==2.44.1<\/code>. However, I'm having trouble figuring out how to install a specific version of the ml extension. According to this documentation, all extensions are packaged as python wheels, so theoretically if I had the URL of the build wheel, I could just target that. But I'm having trouble finding where the ml extension code is hosted. I found the pypi package for <a href=\"https:\/\/pypi.org\/project\/azure-cli-ml\/\">v1 of the extension<\/a>. Does anyone know where v2 is hosted?<\/p>\n<p>The version of the extension I would like to install is <code>2.13.0<\/code>.<\/p>",
        "Challenge_closed_time":1677220157550,
        "Challenge_comment_count":3,
        "Challenge_created_time":1677177834880,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1183753\/how-do-i-install-an-older-version-of-the-azure-cli",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":7.7,
        "Challenge_reading_time":14.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":11.7562972223,
        "Challenge_title":"How do I install an older version of the Azure CLI ml extension?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":186,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=1837dca2-1954-4413-9ba8-ffcc3afb6ce4\">Claire Salling<\/a> You can install a specific version of <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-cli?tabs=private\">Azure ML cli extension<\/a> using these commands.<\/p>\n<p>Remove any existing extensions of azure-cli-ml(v1) or ml(v2)<\/p>\n<p><code>az extension remove -n azure-cli-ml <\/code>  <br \/>\n<code>az extension remove -n ml<\/code><\/p>\n<p>Add the required version:<\/p>\n<p><code>az extension add --name ml --version 2.13.0<\/code><\/p>\n<p>List the extension to confirm the version.<\/p>\n<p><code>az extension list<\/code><\/p>\n<pre><code> {\n    &quot;experimental&quot;: false,\n    &quot;extensionType&quot;: &quot;whl&quot;,\n    &quot;name&quot;: &quot;ml&quot;,\n    &quot;path&quot;: &quot;C:\\\\Users\\\\user\\\\.azure\\\\cliextensions\\\\ml&quot;,\n    &quot;preview&quot;: false,\n    &quot;version&quot;: &quot;2.13.0&quot;\n  }\n<\/code><\/pre>\n<p>If this answers your query, do click <code>Accept Answer<\/code> and <code>Yes<\/code> for was this answer helpful. And, if you have any further query do let us know.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":14.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":12.0333325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to construct an <code>Explanation<\/code> object for a unit test, but can't seem to get it to work. Here's what I'm trying:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.compat.types.explanation_v1.Explanation(\n    attributions=aiplatform.compat.types.explanation_v1.Attribution(\n        {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n    )\n)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>&quot;.venv\/lib\/python3.7\/site-packages\/proto\/message.py&quot;, line 521, in __init__\n    super().__setattr__(&quot;_pb&quot;, self._meta.pb(**params))\nTypeError: Value must be iterable\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/413#issuecomment-872094378\" rel=\"nofollow noreferrer\">this<\/a> on github, but I'm not sure how to apply that workaround here.<\/p>",
        "Challenge_closed_time":1644385932888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644344024883,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71038823",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":11.6411125,
        "Challenge_title":"Cannot construct an Explanation object",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519933306780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3576.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>As the error mentioned value to be passed at <code>attributions<\/code> should be <strong>iterable<\/strong>. See <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.Explanation\" rel=\"nofollow noreferrer\">Explanation attributes documentation<\/a>.<\/p>\n<p>I tried your code and placed the <code>Attribution<\/code> object in a list and the error is gone. I assigned your objects in variables just so the code is readable.<\/p>\n<p>See code and testing below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\ntest = {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n\nattributions=aiplatform.compat.types.explanation_v1.Attribution(test)\nx  = aiplatform.compat.types.explanation_v1.Explanation(\n    attributions=[attributions]\n)\nprint(x)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>attributions {\n  baseline_output_value: 0.9280818700790405\n  instance_output_value: 0.6717480421066284\n  feature_attributions {\n    struct_value {\n      fields {\n        key: &quot;feature_1&quot;\n        value {\n          number_value: -0.0410824716091156\n        }\n      }\n      fields {\n        key: &quot;feature_2&quot;\n        value {\n          number_value: 0.01155053575833639\n        }\n      }\n    }\n  }\n  output_index: 0\n  output_display_name: &quot;true&quot;\n  approximation_error: 0.010399332817679649\n  output_name: &quot;scores&quot;\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1644387344880,
        "Solution_link_count":1.0,
        "Solution_readability":17.4,
        "Solution_reading_time":22.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":111.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1985.0091666667,
        "Challenge_answer_count":2,
        "Challenge_body":"If you run any command that uses azureml (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message:\r\n\r\n```\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), Requirement.parse('flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"')).\r\n```\r\n\r\n**Expected Behavior**\r\nNo warning message should be printed.\r\n\r\n**Steps to Reproduce the Issue**\r\n1. From latest master branch in a fresh virtualenv run: `make build install`\r\n2. `cd \/path\/to\/azure\/a2ml-project`\r\n3. `a2ml experiment leaderboard`\r\n4. Observe the warning message above.\r\n\r\n\r\n**Environment Details:**\r\n - OS: macOS 10.15\r\n - A2ML Version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34\r\n - Python Version: 3.7.7\r\n",
        "Challenge_closed_time":1597072927000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589926894000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/173",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":12.25,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":614.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1985.0091666667,
        "Challenge_title":"Warning message about hyperdrive loading with azureml_run_type_providers",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":99,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"try again pls, I cannot reproduce it with latest azure ml Not able to reproduce now.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.8151808334,
        "Challenge_answer_count":2,
        "Challenge_body":"I am running a Sagemaker Notebook instance. How can I tell if my Notebook is frozen or just taking a long time? I am using a 24xlarge and querying from Athena in parallel and it seems to be stuck on the same query for a long time. How can I tell if I need more Memory or more VCPUs?",
        "Challenge_closed_time":1683362949783,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683306015132,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1683644479959,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUK_hTODBBTeWX_c-lcO34mg\/how-can-i-tell-if-my-notebook-instance-is-frozen",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.9,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":15.8151808334,
        "Challenge_title":"How can I tell if my Notebook instance is frozen?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":66.0,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there,\n\nGreetings for the day!\n\nI understand that you wanted to know how can you determine if you need more VCPU or memory when your SageMaker Notebook Instance is frozen or just taking a long time?\n\nI\u2019d like to inform you that If your Sagemaker Notebook instance is taking a long time, you can check if it is frozen or still running by monitoring the CPU and memory usage. If the CPU usage is low or zero, it may be frozen. \n\nIf the CPU usage is high but the memory usage is low, you may need more VCPUs and If the memory usage is high, you may need more memory. \n\nYou can check it via SageMaker Notebook Instance terminal:\n\nTo see the memory and CPU information in detail , kindly follow the below instructions:-\n\n[1] Start Your Notebook Instance\n[2] Go to the  Jupyter Home Page \n[3] Right hand side ,Click on DropDown Option \u201cNew\u201d \n[4] Select \u201cTerminal\u201d.\n\nIn the Jupyter terminal, Run the below commands to see the information of Memory and CPUs.\n\n[+] To see the memory information:\n\n$ free -h\n\n=> output of \u201cfree -h\u201d will provide the information of total memory, used memory, free memory, shared memory etc in human readable form.\n\n[+] To see the CPU information, you can run any of the commands:\n\n$ mpstat -u\n\n=> Output of \u201cmpstat -u\u201d consists of different fields like %guest, %gnice, %steal etc.\n\n%steal\nShow the percentage of time spent in involuntary wait by the virtual CPU or CPUs while the hyper\u2010\nvisor was servicing another virtual processor.\n\n%guest\nShow the percentage of time spent by the CPU or CPUs to run a virtual processor.\n\n%idle\nShow the percentage of time that the CPU or CPUs were idle and the system did not have an out\u2010\nstanding disk I\/O request.\n\nmany more.. You can find more detail about the each field of mpstat command by visiting the manual page of it. To see the manual page of mpstat command, use \u201c$ man mpstst\u201d.\n\nAlong with \u201cmpstat -u\u201d, you can also try the below listed commands to get information about the cpu:\n\n$ lscpu\n$ cat \/proc\/cpuinfo\n$ top\n\nAdditionally, You can also check the cloudwatch logs for any errors or warnings that may indicate the cause of the issue. Most of the time, Cloud watch logs helps to find out the root cause of the issue.\n\nYou can find the CloudWatch logs under CloudWatch \u2192 Log Groups \u2192 \/aws\/sagemaker\/NotebookInstances -> Notebook Instance Name\n\nBased on the analysis , you can select different Notebook Instance type, You can find more detail of SageMaker Instance Here [1].\n\nI request you to kindly follow the above suggested workarounds. \n\nIf you have any difficulty or run into any issue, Please reach out to AWS Support[+] (Sagemaker), along with your issue\/use case in details and we would be happy to assist you further.\n\n[+] Creating support cases and case management - https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-casehttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-case \n\nI hope this information would be useful to you.\n\nThank you!\n\nREFERENCES:\n\n[1] https:\/\/aws.amazon.com\/sagemaker\/pricing\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1683362949783,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":37.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":488.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1577353307072,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":194.2405855556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to go through the following tutorial published <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-train-tensorflow\" rel=\"nofollow noreferrer\">here<\/a> but get the error below when I run these lines fo code:<\/p>\n\n<pre><code>run = exp.submit(est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>ERROR:<\/p>\n\n<pre><code>\"message\": \"Could not import package \\\"azureml-dataprep\\\". Please ensure it is installed by running: pip install \\\"azureml-dataprep[fuse,pandas]\\\"\"\n<\/code><\/pre>\n\n<p>However, I have already installed the required packages:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/PZBAt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PZBAt.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am running this through Jupyter Notebooks in an Anacoda Python 3.7 environment.<\/p>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>Tried creating a new conda environment as specified <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/resource-known-issues\" rel=\"nofollow noreferrer\">here<\/a> but still get the same error.<\/p>\n\n<pre><code>conda create -n aml python=3.7.3\n<\/code><\/pre>\n\n<p>After installing all the required packages, I am able to reproduce the exeception by executing the following:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/3Jysg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3Jysg.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":1577353328880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576648684500,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1576654062772,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59386244",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":15.6,
        "Challenge_reading_time":20.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":195.73455,
        "Challenge_title":"Azure machine learning could not import package azureml-dataprep",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2558.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310270742512,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2297.0,
        "Poster_view_count":244.0,
        "Solution_body":"<p>Sorry for this. Take a look at the Jupyter Notebook version of the same tutorial:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/ml-frameworks\/tensorflow\/deployment\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/ml-frameworks\/tensorflow\/deployment\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb<\/a><\/p>\n\n<p>When configuring estimator, you need to specify the pip packages u wanna install on the remote compute. In this case, azureml-dataprep[fuse, blob]. Installing the package to your local computer is not useful since the training script is executed on the remote compute target, which doesn't have the required package installed yet. <\/p>\n\n<pre><code>est = TensorFlow(source_directory=script_folder,\n             script_params=script_params,\n             compute_target=compute_target,\n             entry_script='tf_mnist.py',\n             use_gpu=True,\n             pip_packages=['azureml-dataprep[pandas,fuse]'])\n<\/code><\/pre>\n\n<p>Can you pls try the fix and let us know whether it solves your issue :) In the mean time, I will update the public documentation to include pip_packages in estimator config.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":22.8,
        "Solution_reading_time":17.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":110.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1655773889523,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":390.0,
        "Answerer_view_count":240.0,
        "Challenge_adjusted_solved_time":6.1325663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Vertex AI notebook that contains a lot of python and jupyter notebook as well as pickled data files in it.  I need to move these files to another notebook.  There isn't a lot of documentation on google's help center.<\/p>\n<p>Has someone had to do this yet?  I'm new to GCP.<\/p>",
        "Challenge_closed_time":1658194851972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658172774733,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73027674",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.2,
        "Challenge_reading_time":4.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.1325663889,
        "Challenge_title":"How does one move python and other types of files from one GCP notebook instance to another?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":114.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455270976143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Grand Rapids, MI, USA",
        "Poster_reputation_count":1269.0,
        "Poster_view_count":261.0,
        "Solution_body":"<p>Can you try these steps in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/migrate\" rel=\"nofollow noreferrer\">article<\/a>. It says you can copy your files to a <a href=\"https:\/\/cloud.google.com\/storage\/\" rel=\"nofollow noreferrer\">Google Cloud Storage Bucket<\/a> then move it to a new notebook by using gsutil tool.<\/p>\n<p>In your notebook's terminal run this code to copy an object to your Google Cloud storage bucket:<\/p>\n<pre><code>gsutil cp -R \/home\/jupyter\/* gs:\/\/BUCKET_NAMEPATH\n<\/code><\/pre>\n<p>Then open a new terminal to the target notebook and run this command to copy the directory to the notebook:<\/p>\n<pre><code>gsutil cp gs:\/\/BUCKET_NAMEPATH* \/home\/jupyter\/\n<\/code><\/pre>\n<p>Just change the <code>BUCKET_NAMEPATH<\/code> to the name of your cloud storage bucket.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":10.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":98.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.7286111111,
        "Challenge_answer_count":0,
        "Challenge_body":"Context\n\nLet's say I have following structure\n\nroot\n\u251c\u2500\u2500model_files\n\u251c\u2500\u2500polyaxonfiles\n\u2502      \u251c\u2500\u2500build.yml\n\u2502      \u2514\u2500\u2500run.yml\n\u251c\u2500\u2500utils\n\u2514\u2500\u2500dockerfiles\n         \u251c\u2500\u2500Dockerfile.0\n         \u2514\u2500\u2500Dockerfile.1\n\n\nmy build looks as simple as:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  context:\n    value: \"{{ globals.run_artifacts_path }}\/uploads\"\n  destination:\n    connection: docker-registry\n    value: machine-learning\/polyaxon-tutorial:1\n\nhubRef: kaniko\n\nRunning this with command\n\npolyaxon run -f polyaxonfiles\/build.yml -u\n\nGives me very much expected error:\n\nError: error resolving dockerfile path: please provide a valid path to a Dockerfile within the build context with --dockerfile\n\nSo the Kaniko expects Dockerfile to exist in root of the context by default. Does the polyaxonfile specification exposes a way to overwrite the default as suggested with error message?",
        "Challenge_closed_time":1620665885000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620659662000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1315",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":11.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.7286111111,
        "Challenge_title":"(How) Can I pass path to Dockerfile using Kaniko?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @captainCapitalism, next release we will be pushing a new set of guides, including guides for building containers and how to pass a custom dockerfile context.\n\nWe will update this thread as soon as we start updating the documentation's guides section.",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":3.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1356359759000,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dubai - United Arab Emirates",
        "Answerer_reputation_count":436.0,
        "Answerer_view_count":73.0,
        "Challenge_adjusted_solved_time":0.4312602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The code below works fine in a sagemaker notebook in the cloud. Locally I also have aws credentials created via the aws cli. Personally, I do not like notebooks (unless I do some EDA or the like). So I wonder if one can also fire this code up from a local machine (e.g. in visual studio code) as it only tells sagemaker what to do anyway? I guess it is only a question of authenticating and getting the session object? Thanks!<\/p>\n<pre><code>import boto3\nimport os\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker import image_uris\n\nregion_name = boto3.session.Session().region_name\ns3_bucket_name = 'bucket_name'\n# this image cannot be used below !!! there must be an issue with sagemaker ?\ntraining_image_name = image_uris.retrieve(framework='xgboost', region=region_name, version='latest')\nrole = get_execution_role()\ns3_prefix = 'my_model'\ntrain_file_name = 'sagemaker_train.csv'\nval_file_name = 'sagemaker_val.csv'\nsagemaker_session = sagemaker.Session()\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;10&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>",
        "Challenge_closed_time":1650001048760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649999496223,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651138032863,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71880336",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":28.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":0.4312602778,
        "Challenge_title":"can one run sagemaker notebook code locally in visual studio code",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":425.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>On a local machine,<\/p>\n<ul>\n<li>Make sure to install AWS CLI: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html<\/a><\/li>\n<li>Create an access key id and secret access key to access Sagemaker services locally: <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html<\/a><\/li>\n<li>Set these credentials using <code>aws configure<\/code> command locally.<\/li>\n<li>The code should work fine except getting the execution role. You can either hard code the Sagemaker role in the code (not best practice) or store it in the Parameter store and access it from there.<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.9,
        "Solution_reading_time":11.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1562691895952,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Norway",
        "Answerer_reputation_count":3183.0,
        "Answerer_view_count":981.0,
        "Challenge_adjusted_solved_time":1204.3708430556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have written a code for ML in my local machine in jupyter notebook. I notice that AWS Sagemaker has its own notebook instances too. '<\/p>\n<ol>\n<li>Can I upload my existing local jupyter notebook in AWS Sagemaker directly?\nor<\/li>\n<li>Do I need to type the entire code again in instance of AWS Sagemaker jupyter notebook?<\/li>\n<\/ol>",
        "Challenge_closed_time":1598173972672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593838237637,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62725467",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":5.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1204.3708430556,
        "Challenge_title":"AWS Sagemaker, how to connect existing local jupyter notebook with ML algorithms to AWS",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":155.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562691895952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":3183.0,
        "Poster_view_count":981.0,
        "Solution_body":"<ol>\n<li><p>Can I upload my existing local jupyter notebook in AWS Sagemaker directly? or\nYes you can upload from local disk or computer<\/p>\n<\/li>\n<li><p>Do I need to type the entire code again in instance of AWS Sagemaker jupyter notebook?\nNo<\/p>\n<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":165.5280555556,
        "Challenge_answer_count":1,
        "Challenge_body":"Is there a way to configure Amazon ECR containers so that they can't be changed once they're created? Here are our requirements:\n* Containers can't be changed after their built.\n* Containers can't receive updates.\n* Changes in the containerized application must require the building and deployment of a new container image.\n* Runtime data and configurations must be stored outside of the container environment.",
        "Challenge_closed_time":1608306412000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1607710511000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926185084,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUUPiBylRCSe6_ax_u_4g-oA\/can-you-configure-amazon-ecr-containers-to-be-immutable",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":165.5280555556,
        "Challenge_title":"Can you configure Amazon ECR containers to be immutable?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yes, you can configure Amazon ECR containers to be immutable. Amazon ECR uses resource-based permissions to control access to repositories. The resource-based permissions let you specify which IAM users or roles have access to a repository and what actions they can perform on it. By default, only the repository owner has access to a repository. \n\nFor more information, see [Repository policies](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/repository-policies.html) and [Image tag mutability](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/image-tag-mutability.html) in the Amazon ECR user guide.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925587432,
        "Solution_link_count":2.0,
        "Solution_readability":14.3,
        "Solution_reading_time":7.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.2510238889,
        "Challenge_answer_count":1,
        "Challenge_body":"I was working on a sagemaker studio for ML work, I attached Lifecycle Configuration with it, which was creating problem. Then I deleted the lifecycle configuration without detaching it, and this problem is happening. Can't start sagemaker studio notebook and this is shown.\n\n![Enter image description here](\/media\/postImages\/original\/IMg3hralubRIO8ITzuV3La8Q)\n\nAny suggestion to fix this ?",
        "Challenge_closed_time":1667785787444,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667756083758,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1668480117644,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU91ywEwTsRRqmHKZJ1yVrrA\/sagemaker-studio-is-not-opening-after-deleting-lifecycle-configuration",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.2510238889,
        "Challenge_title":"Sagemaker Studio is not opening after deleting lifecycle configuration",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can try detaching the LCC script using the CLI. You can use the [CloudShell](https:\/\/aws.amazon.com\/cloudshell\/) from console, since your console role is able to perform updates on the domain. \n\nUse the [update-domain](https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/update-domain.html) CLI call, and provide an empty configuration for the default user settings, something like- \n```\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'\n```",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1667797891160,
        "Solution_link_count":2.0,
        "Solution_readability":13.6,
        "Solution_reading_time":7.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":56.8840472222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/I8c93.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I8c93.png\" alt=\"enter image description here\"><\/a>I am attempting to install Gluonnlp to a sagemaker jupyter notebook. Im using the command <code>!sudo pip3 install gluonnlp<\/code> to install.  Which is successful.  However on import I get <code>ModuleNotFoundError: No module named 'gluonnlp'<\/code><\/p>\n\n<p>I got the same issue when attempting to install mxnet with pip in the same notebook.  It was resolved when I conda installed mxnet instead.  However conda install has not been working for gluonnlp as it cannot find the package.  I can't seem to find a way to conda install gluonnlp.  Any suggestions would be highly appreciated.<\/p>\n\n<p>Here are some of the commands I have tried<\/p>\n\n<p><code>!sudo pip3 install gluonnlp<\/code><\/p>\n\n<p><code>!conda install gluonnlp<\/code> --> Anaconda cant find the package in any channels<\/p>\n\n<pre><code>!conda install pip --y\n!sudo pip3 install gluonnlp\n\n!sudo pip3 install gluonnlp\n\n!conda install -c conda-forge gluonnlp --y\n<\/code><\/pre>\n\n<p>All these commands on my import \nimport warnings<\/p>\n\n<pre><code>warnings.filterwarnings('ignore')\n\nimport io\nimport random\nimport numpy as np\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom bert import data, model\n<\/code><\/pre>\n\n<p>result in the error<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'gluonnlp'\n<\/code><\/pre>",
        "Challenge_closed_time":1564315813180,
        "Challenge_comment_count":6,
        "Challenge_created_time":1564111030610,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1564413127092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57212696",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":9.6,
        "Challenge_reading_time":18.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":56.8840472222,
        "Challenge_title":"Gluonnlp installation not found on Sagemaker jupyter notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2015.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>this is as simple as creating a Jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing:<\/p>\n\n<pre><code>!pip install gluonnlp\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.7938888889,
        "Challenge_answer_count":1,
        "Challenge_body":"## Which example? Describe the issue\r\n\r\nexample:  az ml online-deployment create --name blue --endpoint-name amlarc-runner-simple-849b --resource-group lt-westus2-r6-amlarc-rg --workspace-name lt-westus2-r6-arc-ws --file azureml-examples\/cli\/endpoints\/online\/\/amlarc\/blue-deployment.yml --all-traffic\r\ndescription:\r\nFile \"\/var\/azureml-server\/entry.py\", line 1, in <module>\r\n    import create_app\r\n  File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\r\n    import aml_framework\r\n  File \"\/var\/azureml-server\/aml_framework.py\", line 9, in <module>\r\n    from synchronous.framework import *\r\n  File \"\/var\/azureml-server\/synchronous\/framework.py\", line 3, in <module>\r\n    from flask import Flask, request, g, Request, Response, Blueprint\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/__init__.py\", line 21, in <module>\r\n    from .app import Flask, Request, Response\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/app.py\", line 26, in <module>\r\n    from . import cli, json\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/json\/__init__.py\", line 21, in <module>\r\n    from itsdangerous import json as _json\r\nImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)\r\n\r\n## Additional context\r\n\r\nhttps:\/\/ml.azure.com\/endpoints\/realtime\/amlarc-runner-simple-849b\/logs?wsid=\/subscriptions\/589c7ae9-223e-45e3-a191-98433e0821a9\/resourcegroups\/lt-westus2-r6-amlarc-rg\/workspaces\/lt-westus2-r6-arc-ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\r\n\r\n-\r\n",
        "Challenge_closed_time":1645604942000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645497684000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/977",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.3,
        "Challenge_reading_time":24.77,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":650.0,
        "Challenge_repo_issue_count":1974.0,
        "Challenge_repo_star_count":882.0,
        "Challenge_repo_watch_count":2756.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":29.7938888889,
        "Challenge_title":"ImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue should be mitigated once the PR is merged: https:\/\/github.com\/Azure\/azureml-examples\/pull\/981",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":1.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":3103.0390416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Challenge_closed_time":1650761211590,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648821712310,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71708147",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":15.0,
        "Challenge_reading_time":52.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":43,
        "Challenge_solved_time":538.7498,
        "Challenge_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":936.0,
        "Challenge_word_count":377,
        "Platform":"Stack Overflow",
        "Poster_created_time":1648820658172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659992652860,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.82,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1034.8597222222,
        "Challenge_answer_count":11,
        "Challenge_body":"> These are reported by @tapadipti (thanks). I'm moving here to discuss and follow: \r\n\r\nI was running experiments by following the docs (https:\/\/dvc.org\/doc\/start\/experiments) and encountered the following issues. Sharing here for any required action.\r\n1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n2. `dvc pull` gave this error:\r\n   ```\r\n   ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   Is your cache up to date?\r\n   <https:\/\/error.dvc.org\/missing-files>\r\n   ```\r\n\r\n3. `dvc exp run` lists all the image when running the `extract` stage. Would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `If you used dvc repro before` section in the doc is a little unclear. Does `dvc exp run` replace `dvc repro`? If yes, can we state this clearly? Also would be great to change this statement `We use dvc repro to run the pipeline...` to `dvc repro runs the pipeline...`",
        "Challenge_closed_time":1642605521000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638880026000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/98",
        "Challenge_link_count":2,
        "Challenge_participation_count":11,
        "Challenge_readability":5.9,
        "Challenge_reading_time":13.98,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1034.8597222222,
        "Challenge_title":"Various issues in `example-dvc-experiments`",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":176,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This seems high priority. We can remove `bug` and change to `p1` after 2. is addressed at least, I think. > 1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n\r\nThis was a bit intentional to let the users install DVC themselves, and a bit to prevent version conflicts. There are some conditions (like installing DVC to system and venv both with different dependencies) that cause weird behavior. \r\n\r\nWe can go on to this route though, it's a single line of change. Is it better to add `dvc` to the `requirements.txt` @shcheklein?  If this was intentional and we don't want to include `dvc` in `requirements.txt`, then we should add an instruction that the user should install `dvc`. Currently, such an instruction is missing. It is unlikely that many people will reach the experiments page of the tutorial without first having installed `dvc`. But in case they try to work a new venv, it can be a `lil confusing. I remembered why I left `-v` in `tar`, it was taking some time after `extract` to start running and the experiment looks like it's frozen. I've now updated the project not to use `-v` in `tar`, and also updated `model.h5` in the remote. (We had a bug in DVC that was preventing to upload experiments.) Could you now check whether the project works as intended? @tapadipti \r\n\r\nI'll create separate PRs in the docs for content updates. Thank you.  Thanks @iesahin \r\n\r\n`dvc pull` gave this error:\r\n```                                                                                                                    \r\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\nIs your cache up to date?\r\n<https:\/\/error.dvc.org\/missing-files>\r\n```\r\nSo looks like `metrics` worked but not `model.h5`. And this time, the full file path is displayed.\r\n\r\nRemoving `-v` worked. The files are not listed anymore.\r\n\r\n ```\r\n> ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\n```\r\n\r\nInteresting. I double checked yesterday that the script pushing the artifacts has completed successfully. Now, I've checked again and it says:\r\n\r\n```\r\ndvc push\r\nEverything is up to date.\r\n```\r\n\r\nCould you check the MD5 line in `dvc.lock`, corresponding to this line: https:\/\/github.com\/iterative\/example-dvc-experiments\/blob\/main\/dvc.lock#L36\r\n\r\nWhat's the MD5 hash value there, in your installation?\r\n Also, I've checked after cloning the repository: \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/476310\/145449841-16ae8f43-7ce3-4459-a0d3-225d67214ab0.png)\r\n\r\n@tapadipti  The current staging version in https:\/\/github.com\/iterative\/example-dvc-staging resolves all of these issues. I think we can push it to `example-dvc-experiments`.  @iesahin sounds good. The most recent https:\/\/github.com\/iterative\/example-dvc-experiments resolves all these issues. The codification changes are in #97. Closing this. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":7.3,
        "Solution_reading_time":37.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":426.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":524.3688888889,
        "Challenge_answer_count":10,
        "Challenge_body":"Hello,\r\n\r\nWhen running the experiment, the error message **Environment name can not start with the prefix AzureML** was displayed. How can I set the name of the environment? I'm following the GitHub tutorial and haven't found anything about it.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117882725-01767d80-b281-11eb-8df5-36d8683523e7.png)\r\n\r\nCode used:\r\n\r\n- Registering Dataset\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883192-81044c80-b281-11eb-9dec-d73431948061.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883230-8c577800-b281-11eb-8445-060839369fe5.png)\r\n\r\n- Training Pipeline\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883313-a5602900-b281-11eb-818d-3972111d7f9c.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883356-b315ae80-b281-11eb-99d9-1ac6c0989186.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883429-c7f24200-b281-11eb-88de-3979570adb55.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883465-d2144080-b281-11eb-8b9c-f74756bedd01.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883535-e48e7a00-b281-11eb-9d31-e035f44a0871.png)\r\n\r\nReferences:\r\n\r\n- https:\/\/github.com\/microsoft\/solution-accelerator-many-models\/tree\/master\/Automated_ML\/02_AutoML_Training_Pipeline\r\n- https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/65770\r\n\r\nBest regards,\r\nCristina\r\n\r\n\r\n---\r\n#### Detalhes do documento\r\n\r\n\u26a0 *N\u00e3o edite esta se\u00e7\u00e3o. \u00c9 necess\u00e1rio para a vincula\u00e7\u00e3o do problema do docs.microsoft.com \u279f GitHub.*\r\n\r\n* ID: 49399a7d-d4e8-370e-ce62-d60a6b64e412\r\n* Version Independent ID: 782d8ba4-75dd-27c3-5a46-a921c3ead4bf\r\n* Content: [azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.automlpipelinebuilder?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr.pt-BR\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1622654301000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620766573000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1468",
        "Challenge_link_count":12,
        "Challenge_participation_count":10,
        "Challenge_readability":29.1,
        "Challenge_reading_time":35.07,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":524.3688888889,
        "Challenge_title":"AutoMLPipelineBuilder.get_many_models_train_steps - Error \"Environment name can not start with the prefix AzureML...\"",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\r\n\r\nI have some updates:\r\n\r\n- I put the code below to set the environment.\r\n\r\nfrom azureml.core.environment import Environment\r\n\r\nenv = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\r\nmyenv = env.clone(\"automl_env\")\r\n\r\ntrain_steps = AutoMLPipelineBuilder.get_many_models_train_steps(experiment=experiment,\r\n                                                                automl_settings=automl_settings,\r\n                                                                train_data=dataset_input,\r\n                                                                compute_target=compute_target,\r\n                                                                partition_column_names=partition_column_names,\r\n                                                                node_count=1,\r\n                                                                process_count_per_node=2,\r\n                                                                run_invocation_timeout=3700,\r\n                                                                train_env=myenv)\r\n\r\n- The environment problem has been resolved, but now the process displays the message **ValueError: None is not in list**. I don't know what this means.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/118014360-82894f80-b329-11eb-8e6a-558d6606d7b1.png)\r\n\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 3.0.0 (\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages), Requirement.parse('pyarrow<2.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\r\nTraceback (most recent call last):\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 212, in <module>\r\n    logs = run(data_file_path, args, automl_settings, current_step_run)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 100, in run\r\n    data = pd.read_csv(file_path, parse_dates=[timestamp_column])\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 676, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 448, in _read\r\n    parser = TextFileReader(fp_or_buf, **kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 880, in __init__\r\n    self._make_engine(self.engine)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1114, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1949, in __init__\r\n    self._set_noconvert_columns()\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2015, in _set_noconvert_columns\r\n    _set(val)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2005, in _set\r\n    x = names.index(x)\r\nValueError: None is not in list\r\n\r\nBest regards,\r\nCristina @crisansou is there any error surfaced in 70_driver_log? \r\nHi @shbijlan ,\r\n\r\nI deleted the workspace. I tried to reproduce the steps again but I couldn't even create the experiment, below is the error message. Can you tell which is the recommended version to use this solution?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119876912-c752e000-befe-11eb-9a18-c8f03afae48c.png)\r\n\r\nI don't know if it's related, but I realized that now compute instance is using version 1.29.\r\n\r\n!pip install --upgrade azureml-sdk[automl]\r\n!pip install azureml-contrib-automl-pipeline-steps\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119877097-03864080-beff-11eb-8134-07d22a243284.png)\r\n\r\n\r\n\r\n\r\n> @crisansou is there any error surfaced in 70_driver_log?\r\n\r\n from azureml.core. import Environment\r\ntrain_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n\r\nCan you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n\r\nAlso this solution only supports 'forecasting' and needs a time_column_name passed in automl settings Hi @deeptim123 ,\r\n\r\nThanks for the instructions! After including the environment I was able to run the cell, but the pipeline ended with an error because I am using a regression model.\r\n\r\nIn the next release, in addition to fixing the bug, will it be possible to use regression?\r\n\r\n> from azureml.core. import Environment\r\n> train_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n> \r\n> Can you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n> \r\n> Also this solution only supports 'forecasting' and needs a time_column_name passed in automl settings\r\n\r\n There are currently no plans to support regression. @cartacioS  for visibility of this ask @deeptim123 ,\r\n\r\nThanks for the info. I think it's important to add this functionality for regression and classification as well.\r\n\r\n> There are currently no plans to support regression. @cartacioS for visibility of this ask\r\n\r\n @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at sabina.cartacio@microsoft.com and we can further discuss.\r\n\r\nThanks! Hi @cartacioS ,\r\n\r\nGot it, thanks for the info! The project is starting now, but if it is really necessary to use the multiple models solution for regression I'll send you an email.\r\n\r\n> @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at [sabina.cartacio@microsoft.com](mailto:sabina.cartacio@microsoft.com) and we can further discuss.\r\n> \r\n> Thanks!\r\n\r\n Closing as this is being tracked offline as a feature request for MANY MODELS, by Sabina.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.4,
        "Solution_reading_time":82.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":57.0,
        "Solution_word_count":664.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1467035387036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":1277.0,
        "Answerer_view_count":42.0,
        "Challenge_adjusted_solved_time":191.1962875,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create an Azure DEVOPS ML Pipeline. The following code works 100% fine on Jupyter Notebooks, but when I run it in Azure Devops I get this error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;src\/my_custom_package\/data.py&quot;, line 26, in &lt;module&gt;\n    ws = Workspace.from_config()\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.8.7\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/workspace.py&quot;, line 258, in from_config\n    raise UserErrorException('We could not find config.json in: {} or in its parent directories. '\nazureml.exceptions._azureml_exception.UserErrorException: UserErrorException:\n    Message: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;\n    }\n}\n<\/code><\/pre>\n<p>The code is:<\/p>\n<pre><code>#import\nfrom sklearn.model_selection import train_test_split\nfrom azureml.core.workspace import Workspace\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.experiment import Experiment\nfrom datetime import date\nfrom azureml.core import Workspace, Dataset\n\n\n\nimport pandas as pd\nimport numpy as np\nimport logging\n\n#getdata\nsubscription_id = 'mysubid'\nresource_group = 'myrg'\nworkspace_name = 'mlplayground'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='correctData')\n\n\n#auto ml\nws = Workspace.from_config()\n\n\nautoml_settings = {\n    &quot;iteration_timeout_minutes&quot;: 2880,\n    &quot;experiment_timeout_hours&quot;: 48,\n    &quot;enable_early_stopping&quot;: True,\n    &quot;primary_metric&quot;: 'spearman_correlation',\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO,\n    &quot;n_cross_validations&quot;: 5,\n    &quot;max_concurrent_iterations&quot;: 4,\n    &quot;max_cores_per_iteration&quot;: -1,\n}\n\n\n\ncpu_cluster_name = &quot;computecluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\nprint(compute_target)\nautoml_config = AutoMLConfig(task='regression',\n                             compute_target = compute_target,\n                             debug_log='automated_ml_errors.log',\n                             training_data = dataset,\n                             label_column_name=&quot;paidInDays&quot;,\n                             **automl_settings)\n\ntoday = date.today()\nd4 = today.strftime(&quot;%b-%d-%Y&quot;)\n\nexperiment = Experiment(ws, &quot;myexperiment&quot;+d4)\nremote_run = experiment.submit(automl_config, show_output = True)\n\nfrom azureml.widgets import RunDetails\nRunDetails(remote_run).show()\n\nremote_run.wait_for_completion()\n<\/code><\/pre>",
        "Challenge_closed_time":1612865840128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1612177533493,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65991587",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":16.0,
        "Challenge_reading_time":40.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":191.1962875,
        "Challenge_title":"AzureDevOPS ML Error: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2339.0,
        "Challenge_word_count":265,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>There is something weird happening on your code, you are getting the data from a first workspace (<code>workspace = Workspace(subscription_id, resource_group, workspace_name)<\/code>), then using the resources from a second one (<code>ws = Workspace.from_config()<\/code>). I would suggest avoiding having code relying on two different workspaces, especially when you know that an underlying datasource can be registered (linked) to multiple workspaces (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#create-and-register-datastores\" rel=\"nofollow noreferrer\">documentation<\/a>).<\/p>\n<p>In general using a <code>config.json<\/code> file when instantiating a <code>Workspace<\/code> object will result in an interactive authentication. When your code will be processed and you will have a log asking you to reach a specific URL and enter a code. This will use your Microsoft account to verify that you are authorized to access the Azure resource (in this case your <code>Workspace('mysubid', 'myrg', 'mlplayground')<\/code>). This has its limitations when you start deploying the code onto virtual machines or agents, you will not always manually check the logs, access the URL and authenticate yourself.<\/p>\n<p>For this matter it is strongly recommended setting up more advanced authentication methods and personally I would suggest using the service principal one since it is simple, convinient and secure if done properly.\nYou can follow Azure's official documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#configure-a-service-principal\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.8,
        "Solution_reading_time":21.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":197.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":1.785035,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create my own custom Sagemaker Framework that runs a custom python script to train a ML model using the entry_point parameter.<\/p>\n\n<p>Following the Python SDK documentation (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>), I wrote the simplest code to run a training job just to see how it behaves and how Sagemaker Framework works.<\/p>\n\n<p>My problem is that I don't know how to properly build my Docker container in order to run the entry_point script.<\/p>\n\n<p>I added the <code>train.py<\/code> script into the container that only logs the folders and files paths as well as the variables in the containers environment.<\/p>\n\n<p>I was able to run the training job, but I couldn't find any reference of the entry_point script neither in environment variable nor the files in the container.<\/p>\n\n<p>Here is the code I used:<\/p>\n\n<ul>\n<li><strong>Custom Sagemaker Framework Class:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.estimator import Framework\n\nclass Doc2VecEstimator(Framework):\n    def create_model():\n        pass\n<\/code><\/pre>\n\n<ul>\n<li><strong>train.py:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nimport os\nfrom datetime import datetime\n\n\ndef log(*_args):\n    print('[log-{}]'.format(datetime.now().isoformat()), *_args)\n\n\ndef listdir_rec(path):\n    ls = os.listdir(path)\n    print(path, ls)\n\n    for ls_path in ls:\n        if os.path.isdir(os.path.join(path, ls_path)):\n            listdir_rec(os.path.join(path, ls_path))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--debug_size', type=int, default=None)\n\n    # # I commented the lines bellow since I haven't configured the environment variables in my container\n    #     # Sagemaker specific arguments. Defaults are set in the environment variables.\n    #     parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    #     parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    #     parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args, _ = parser.parse_known_args()\n\n    log('Received arguments {}'.format(args))\n\n    log(os.environ)\n\n    listdir_rec('.')\n\n<\/code><\/pre>\n\n<ul>\n<li><strong>Dockerfile:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-sh prettyprint-override\"><code>FROM ubuntu:18.04\n\nRUN apt-get -y update \\\n    &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n        wget \\\n        python3 \\\n        python3-pip \\\n        nginx \\\n        ca-certificates \\\n    &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN pip3 install --upgrade pip setuptools \\\n    &amp;&amp; \\\n    pip3 install \\\n        numpy \\\n        scipy \\\n        scikit-learn \\\n        pandas \\\n        flask \\\n        gevent \\\n        gunicorn \\\n        joblib \\\n        pyAthena \\\n        pandarallel \\\n        nltk \\\n        gensim \\\n    &amp;&amp; \\\n    rm -rf \/root\/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\n\nCOPY train.py \/train.py\n\nENTRYPOINT [\"python3\", \"-u\", \"train.py\"]\n<\/code><\/pre>\n\n<ul>\n<li><strong>Training Job Execution Script:<\/strong><\/li>\n<\/ul>\n\n<pre><code>framework = Doc2VecEstimator(\n    image_name=image,\n    entry_point='train_doc2vec_model.py',\n    output_path='s3:\/\/{bucket_prefix}'.format(bucket_prefix=bucket_prefix),\n\n    train_instance_count=1,\n    train_instance_type='ml.m5.xlarge',\n    train_volume_size=5,\n\n    role=role,\n    sagemaker_session=sagemaker_session,\n    base_job_name='gensim-doc2vec-train-100-epochs-test',\n\n    hyperparameters={\n        'epochs': '100',\n        'debug_size': '100',\n    },\n)\n\nframework.fit(s3_input_data_path, wait=True)\n<\/code><\/pre>\n\n<p>I haven't found a way to make the training job to run the <code>train_doc2vec_model.py<\/code>. So how do I create my own custom Framework class\/container?<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1590435558776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590429132650,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1590437261656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62007961",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.5,
        "Challenge_reading_time":49.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":1.785035,
        "Challenge_title":"Where does entry_point script is stored in custom Sagemaker Framework training job container?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":747.0,
        "Challenge_word_count":363,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587648874872,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo, SP, Brasil",
        "Poster_reputation_count":23.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>SageMaker team created a <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">python package <code>sagemaker-training<\/code><\/a> to install in your docker so that your customer container will be able to handle external <code>entry_point<\/code> scripts.\nSee here for an example using Catboost that does what you want to do :)<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.5,
        "Solution_reading_time":7.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1641102333407,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":67.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":1.1414036111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I tried importing cv2 from opencv but i got error saying<\/p>\n<blockquote>\n<p>import error: libgthread-2.0.so.0: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n<p>Since Sagemaker Studio Labs doesn't support installation of Ubuntu packages i couldn't use <code>apt-get<\/code> or <code>yum<\/code> to install libglib2.0-0.<\/p>",
        "Challenge_closed_time":1641103613576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641099504523,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1641107240987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70553701",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.1414036111,
        "Challenge_title":"Couldn't import cv2 in SageMaker Studio Lab",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":534.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589906719620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":63.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>With this line, you can install the glib dependency for Amazon Sagemaker Studio Lab. Just run it on your notebook cell.<\/p>\n<pre><code>! conda install glib=2.51.0 -y\n<\/code><\/pre>\n<p>You also can create another virtual environment for your session that contains glib:<\/p>\n<pre><code>! conda create -n glib-test -c defaults -c conda-forge python=3 glib=2.51.0` -y\n<\/code><\/pre>\n<p>After that maybe you need albumentations to import cv2:<\/p>\n<pre><code>! pip install albumentations\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.4081172222,
        "Challenge_answer_count":1,
        "Challenge_body":"The CloudWatch logs show following error\n```\n    Traceback (most recent call last):\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_scheduler\/handlers.py\", line 194, in post\n        job_id = await ensure_async(self.scheduler.create_job(CreateJob(**payload)))\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_server\/utils.py\", line 182, in ensure_async\n        result = await obj\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 109, in wrapper\n        raise excep\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 105, in wrapper\n        return await func(*args, **kwargs)\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 210, in create_job\n        s3_file_uploader = await self._prepare_job_artifacts(\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 168, in _prepare_job_artifacts\n        input_uri = S3URI(runtime_environment_parameters.s3_input)\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/runtime_environment_parameters.py\", line 40, in s3_input\n        return self.parameters.get(RuntimeEnvironmentParameterName.S3_INPUT.value)\n    AttributeError: 'NoneType' object has no attribute 'get'\n[E 2023-03-16 15:36:01.070 SchedulerApp] 'NoneType' object has no attribute 'get' Traceback (most recent call last): File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_scheduler\/handlers.py\", line 194, in post job_id = await ensure_async(self.scheduler.create_job(CreateJob(**payload))) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_server\/utils.py\", line 182, in ensure_async result = await obj File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 109, in wrapper raise excep File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 105, in wrapper return await func(*args, **kwargs) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 210, in create_job s3_file_uploader = await self._prepare_job_artifacts( File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 168, in _prepare_job_artifacts input_uri = S3URI(runtime_environment_parameters.s3_input) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/runtime_environment_parameters.py\", line 40, in s3_input return self.parameters.get(RuntimeEnvironmentParameterName.S3_INPUT.value) AttributeError: 'NoneType' object has no attribute 'get'\n```\nAlso all the \"advanced options\" are missing in the \"create notebook job\" dialogue.\n\nThe setting is isolated VPC with permissions updated accordingly: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/scheduled-notebook-policies.html\n\nThe VPC endpoints for S3, SageMaker API and Runtime, SSM, STS, Metrics, Logs, ECR API and ECR DKR are deployed in the VPC. Notebooks are working fine.\n\nAny idea what could be wrong?",
        "Challenge_closed_time":1678987295300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678982226078,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1679330199392,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbLVvWJbeS42j0e1nymPYcg\/sagemaker-studio-in-vpc-only-fails-to-create-notebook-job-with-unexpected-error-occurred-during-creation-of-job",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.2,
        "Challenge_reading_time":42.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":1.4081172222,
        "Challenge_title":"Sagemaker Studio in VPC Only fails to create Notebook job with \"Unexpected error occurred during creation of job.\"",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":156.0,
        "Challenge_word_count":235,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for reporting the issue. \n\nCan you try also configuring following two vpc endpoints? \n1. Amazon EC2\n2. Amazon EventBridge\nAlso, if you have s3 vpc gateway endpoint policy with fine control of allowed s3 bucet, please allow s3 access for sagemakerheadlessexecution-prod-* like below. \nFYI - you can find more reference link from https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-notebook-auto-execution-advanced.html\n\n```\n{\n   \"Action\":[\n      \"s3:*\"\n   ],\n   \"Resource\":[\n      \"arn:aws:s3:::sagemakerheadlessexecution-prod-*\",\n      \"arn:aws:s3:::sagemakerheadlessexecution-prod-*\/*\"\n   ],\n   \"Effect\":\"Allow\",\n   \"Sid\":\"SCTASK14554266\"\n}\n```",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1678987295300,
        "Solution_link_count":1.0,
        "Solution_readability":15.5,
        "Solution_reading_time":8.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1124.7847222222,
        "Challenge_answer_count":0,
        "Challenge_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Challenge_closed_time":1623230615000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619181390000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1124.7847222222,
        "Challenge_title":"All MLflow experiments are visible for any user",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":80,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":0.8399936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am attempting to submit an experiment to the Azure Machine Learning Service using a custom docker image.  Everything works ok when I provide the docker image, but fails if I choose to provide a dockerfile.<\/p>\n\n<p>The use of a base_dockerfile in the DockerSection object is documented <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.dockersection?view=azure-ml-py\" rel=\"nofollow noreferrer\">here<\/a> and was added in v1.0.53 of the sdk (as noted <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/service\/azure-machine-learning-release-notes.md\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n\n<p>Example code:<\/p>\n\n<pre><code>ds = DockerSection()\nds.enabled = True\nds.base_dockerfile = \"FROM ubuntu:latest RUN echo 'Hello world!'\"\nds.base_image = None\n<\/code><\/pre>\n\n<p>The rest of the code is the same as when running with a predefined image from the registry (e.g. setting base_image in the above code).<\/p>\n\n<p>Example error from ML service is:<\/p>\n\n<blockquote>\n  <p>raise ActivityFailedException(error_details=json.dumps(error,\n  indent=4))\n  azureml.exceptions._azureml_exception.ActivityFailedException:\n  ActivityFailedException:\n          Message: Activity Failed: {\n      \"error\": {\n          \"code\": \"ServiceError\",\n          \"message\": \"InternalServerError\",\n          \"details\": []\n      },\n      \"correlation\": {\n          \"operation\": null,\n          \"request\": \"K\/C4FSnEz74=\"\n      },\n      \"environment\": \"southcentralus\",\n      \"location\": \"southcentralus\",\n      \"time\": \"2019-08-20T16:33:17.130928Z\" }\n          InnerException None\n          ErrorResponse {\"error\": {\"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"ServiceError\\\",\\n<br>\n  \\\"message\\\": \\\"InternalServerError\\\",\\n        \\\"details\\\": []\\n<br>\n  },\\n    \\\"correlation\\\": {\\n        \\\"operation\\\": null,\\n<br>\n  \\\"request\\\": \\\"K\/C4FSnEz74=\\\"\\n  },\\n    \\\"environment\\\":\n  \\\"southcentralus\\\",\\n    \\\"location\\\": \\\"southcentralus\\\",\\n<br>\n  \\\"time\\\": \\\"2019-08-20T16:33:17.130928Z\\\"\\n}\"}}<\/p>\n<\/blockquote>\n\n<p>I've used an example dockerfile in the code above (taken from the SDK documentation) but get the same error if I use the dockerfile that created the base image that works ok from the registry.<\/p>\n\n<p>Any ideas - or pointers to samples where this actually works - appreciated!<\/p>",
        "Challenge_closed_time":1566323058620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566320034643,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57578332",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":29.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.8399936111,
        "Challenge_title":"Failure of experiment when using base_dockerfile instead of base_image",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":307.0,
        "Challenge_word_count":222,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355436865376,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Thanks for reporting this issue! This appears to be a bug that our team is investigating.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":1.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":142.3802777778,
        "Challenge_answer_count":3,
        "Challenge_body":"This [notebook](https:\/\/github.com\/Microsoft\/Recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to Azure ML SDK preview private index. \r\n\r\n    # Required packages for AzureML execution, history, and data preparation.\r\n    - --extra-index-url https:\/\/azuremlsdktestpypi.azureedge.net\/sdk-release\/Preview\/E7501C02541B433786111FE8E140CAA1\r\n\r\nGiven that Azure ML SDK is now available though regular PyPi as a GA product, and preview versions are unsupported, the extra-index-url should be removed.\r\n",
        "Challenge_closed_time":1548948415000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548435846000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/451",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.3,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":142.3802777778,
        "Challenge_title":"Remove azureml sdk preview private PyPi index from operationalize notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"hey @rastala thanks for the pointer, we are working on updating that notebook to a newer version of databricks and spark. @jreynolds01 is looking at this based on this issue https:\/\/github.com\/Microsoft\/Recommenders\/issues\/427 yes, this should be fixed with my PR. fixed with #438 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.2,
        "Solution_reading_time":3.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":219.2780555556,
        "Challenge_answer_count":3,
        "Challenge_body":"**Describe the bug**\r\nService Workbench appears to be unable to launch SageMaker notebook instances at all, due to a missing permission for `sagemaker:AddTags`. This seems to also be the case when custom tags aren't included in the workspace configuration.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Install Service Workbench from the latest version.\r\n2. Create a workspace configuration for a SageMaker notebook.\r\n3. Launch a workspace using the new configuration.\r\n4. Wait a few minutes and observe the error.\r\n\r\n**Expected behavior**\r\nExpected the notebook to launch :)\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png)\r\n```\r\nError provisioning environment TestNotebook1. Reason: Errors from CloudFormation: [{LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : The following resource(s) failed to create: [BasicNotebookInstance]. Rollback requested by user.}, {LogicalResourceId : BasicNotebookInstance, ResourceType : AWS::SageMaker::NotebookInstance, StatusReason : User: arn:aws:sts::XXXXXXXXXXXX:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:XXXXXXXXXXXX:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: adee97b7-1c89-47e2-8ca7-5aa374a80004; Proxy: null)}, {LogicalResourceId : IAMRole, ResourceType : AWS::IAM::Role, StatusReason : Resource creation Initiated}, {LogicalResourceId : SecurityGroup, ResourceType : AWS::EC2::SecurityGroup, StatusReason : Resource creation Initiated}, {LogicalResourceId : InstanceRolePermissionBoundary, ResourceType : AWS::IAM::ManagedPolicy, StatusReason : Resource creation Initiated}, {LogicalResourceId : BasicNotebookInstanceLifecycleConfig, ResourceType : AWS::SageMaker::NotebookInstanceLifecycleConfig, StatusReason : Resource creation Initiated}, {LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : User Initiated}]\r\n```\r\n\r\n**Versions (please complete the following information):**\r\n5.2.0\r\n(also replicated on an older 5.0.0 install)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1659686697000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658897296000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1018",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":19.7,
        "Challenge_reading_time":33.11,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":219.2780555556,
        "Challenge_title":"[Bug] SageMaker instances can't be launched due to missing tags permission",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":223,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Further context - this only started happening approx. 32 hours ago. If I had to guess... maybe it should have never worked and something just happened to get 'fixed' in the IAM API yesterday? \ud83d\ude01  Hi @tdmalone is this still an issue? Hi @kcadette, it is, yes:\r\n\r\n```\r\nUser: arn:aws:sts::xxxxxxxxxxxx:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:xxxxxxxxxxxx:notebook-instance\/basicnotebookinstance-lqerepcrnmaw because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: 4f72193d-aa17-41b9-8ed0-ee381686cb5b; Proxy: null)}\r\n```\r\n\r\nIt should be a one-line fix, so I've submitted a PR: https:\/\/github.com\/awslabs\/service-workbench-on-aws\/pull\/1021",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":11.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":1803.6491758334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1659992618003,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653499480970,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1803.6491758334,
        "Challenge_title":"Remotely execute ClearML task using local-only repo",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":25.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Poster_created_time":1653498830776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":10.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":162.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":426.9047222222,
        "Challenge_answer_count":0,
        "Challenge_body":"When you have a global variable in the mlflow.yml file (e.g `mlruns: ${USER}\/mlruns`), the global variable is not replaced by its value even if the user has [registered a TemplatedConfigLoader](https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html) in his project. This is due to `get_mlflow_config()` to manually recreate the default ConfigLoader.\r\n\r\nThis is part of the numerous issues that will  be fixed by #66.\r\n\r\n",
        "Challenge_closed_time":1602948810000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601411953000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":426.9047222222,
        "Challenge_title":"mlflow.yml is not parsed properly when using TemplatedConfigLoader",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":64,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3013536111,
        "Challenge_answer_count":3,
        "Challenge_body":"So the notebook is  still running, name of the notebook is  **Studio-Notebook:ml.t3.medium** ,please help me i am about to exceed the free tier limit . Please tell me how to fix this so that i wont be billed as i am not even using sagemaker rightnow.",
        "Challenge_closed_time":1679951160328,
        "Challenge_comment_count":1,
        "Challenge_created_time":1679950075455,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1680297146784,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUoN4KlhnCRp-n47teJGVlWQ\/i-deleted-a-sagemaker-notebook-without-stopping-it",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.0,
        "Challenge_reading_time":3.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.3013536111,
        "Challenge_title":"I deleted a sagemaker notebook without stopping it",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":93.0,
        "Challenge_word_count":52,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"When you delete a notebook instance, SageMaker removes the ML compute instance, and deletes the ML storage volume as well as the network interface associated with the notebook instance.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1679951160328,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":2.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.6887980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What kind of collaboration do we need among the data scientists or developers who need to share these notebooks? What kind of compute does these notebooks require? Is it all single node? <\/p>",
        "Challenge_closed_time":1591962208856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591956129183,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/35432\/azure-ml-notebooks-sharing-and-compute-selection",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.6887980556,
        "Challenge_title":"Azure ml notebooks sharing and compute selection.",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@azureml056-5112<\/a> Please follow the below for managing compute instances. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#managing-a-compute-instance\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#managing-a-compute-instance<\/a> All data scientists or developers need is access to the AzureML Workspace and they will have access to a shared file share where everyone\u2019s notebooks can be accessed.<\/p>\n<p>All notebook require a Compute Instance(CI). CI is a managed VM that exists in AzureML.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":7.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":0.4295236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently trying to open parquet files using Azure Jupyter Notebooks. I have tried both Python kernels (2 and 3).\nAfter the installation of <em>pyarrow<\/em> I can import the module only if the Python kernel is 2 (not working with Python 3)<\/p>\n\n<p>Here is what I've done so far (for clarity, I am not mentioning all my various attempts, such as using <em>conda<\/em> instead of <em>pip<\/em>, as it also failed):<\/p>\n\n<pre><code>!pip install --upgrade pip\n!pip install -I Cython==0.28.5\n!pip install pyarrow\n\nimport pandas  \nimport pyarrow\nimport pyarrow.parquet\n\n#so far, so good\n\nfilePath_parquet = \"foo.parquet\"\ntable_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n<\/code><\/pre>\n\n<p>This works well if I'm doing that off-line (using Spyder, Python v.3.7.0). But it fails using an Azure Notebook.<\/p>\n\n<pre><code> AttributeErrorTraceback (most recent call last)\n&lt;ipython-input-54-2739da3f2d20&gt; in &lt;module&gt;()\n      6 \n      7 #table_parquet_raw = pd.read_parquet(filePath_parquet, engine='pyarrow')\n----&gt; 8 table_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n\nAttributeError: 'module' object has no attribute 'read_parquet'\n<\/code><\/pre>\n\n<p>Any idea please?<\/p>\n\n<p>Thank you in advance !<\/p>\n\n<p>EDIT:<\/p>\n\n<p>Thank you very much for your reply Peter Pan !\nI have typed these  statements, here is what I got:<\/p>\n\n<p>1.<\/p>\n\n<pre><code>    print(pandas.__dict__)\n<\/code><\/pre>\n\n<p>=> read_parquet does not appear<\/p>\n\n<p>2.<\/p>\n\n<pre><code>    print(pandas.__file__)\n<\/code><\/pre>\n\n<p>=> I get:<\/p>\n\n<pre><code>    \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/pandas\/__init__.py\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li><p>import sys; print(sys.path) => I get:<\/p>\n\n<pre><code>['', '\/home\/nbuser\/anaconda3_23\/lib\/python34.zip',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/plat-linux',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/lib-dynload',\n'\/home\/nbuser\/.local\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/Sphinx-1.3.1-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/IPython\/extensions',\n'\/home\/nbuser\/.ipython']\n<\/code><\/pre><\/li>\n<\/ol>\n\n<p>Do you have any idea please ?<\/p>\n\n<p>EDIT 2:<\/p>\n\n<p>Dear @PeterPan, I have typed both <code>!conda update conda<\/code> and  <code>!conda update pandas<\/code> : when checking the Pandas version (<code>pandas.__version__<\/code>), it is still <code>0.19.2<\/code>.<\/p>\n\n<p>I have also tried with <code>!conda update pandas -y -f<\/code>, it returns:\n`Fetching package metadata ...........\nSolving package specifications: .<\/p>\n\n<p>Package plan for installation in environment \/home\/nbuser\/anaconda3_23:<\/p>\n\n<p>The following NEW packages will be INSTALLED:<\/p>\n\n<pre><code>pandas: 0.19.2-np111py34_1`\n<\/code><\/pre>\n\n<p>When typing:\n<code>!pip install --upgrade pandas<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Requirement already up-to-date: pandas in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\nRequirement already up-to-date: pytz&gt;=2011k in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: numpy&gt;=1.9.0 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: python-dateutil&gt;=2 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: six&gt;=1.5 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from python-dateutil&gt;=2-&gt;pandas)<\/code><\/p>\n\n<p>Finally, when typing:<\/p>\n\n<p><code>!pip install --upgrade pandas==0.24.0<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Collecting pandas==0.24.0\n  Could not find a version that satisfies the requirement pandas==0.24.0 (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0rc1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0rc2, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0rc1, 0.19.0, 0.19.1, 0.19.2, 0.20.0rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0rc1, 0.21.0, 0.21.1, 0.22.0)\nNo matching distribution found for pandas==0.24.0<\/code><\/p>\n\n<p>Therefore, my guess is that the problem comes from the way the packages are managed in Azure. Updating a package (here Pandas), should lead to an update to the latest version available, shouldn't it?<\/p>",
        "Challenge_closed_time":1545730275287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1545322944093,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1547188516027,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53872444",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":60.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":113.1475538889,
        "Challenge_title":"Cannot read \".parquet\" files in Azure Jupyter Notebook (Python 2 and 3)",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1879.0,
        "Challenge_word_count":455,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545322329030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I tried to reproduce your issue on my Azure Jupyter Notebook, but failed. There was no any issue for me without doing your two steps <code>!pip install --upgrade pip<\/code> &amp; <code>!pip install -I Cython==0.28.5<\/code> which I think not matter.<\/p>\n\n<p>Please run some codes below to check your import package <code>pandas<\/code> whether be correct.<\/p>\n\n<ol>\n<li>Run <code>print(pandas.__dict__)<\/code> to check whether has the description of <code>read_parquet<\/code> function in the output.<\/li>\n<li>Run <code>print(pandas.__file__)<\/code> to check whether you imported a different <code>pandas<\/code> package.<\/li>\n<li>Run <code>import sys; print(sys.path)<\/code> to check the order of paths whether there is a same named file or directory under these paths.<\/li>\n<\/ol>\n\n<p>If there is a same file or directory named <code>pandas<\/code>, you just need to rename it and restart your <code>ipynb<\/code> to re-run. It's a common issue which you can refer to these SO threads <a href=\"https:\/\/stackoverflow.com\/questions\/35341363\/attributeerror-module-object-has-no-attribute-reader\">AttributeError: &#39;module&#39; object has no attribute &#39;reader&#39;<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/36250353\/importing-installed-package-from-script-raises-attributeerror-module-has-no-at\">Importing installed package from script raises &quot;AttributeError: module has no attribute&quot; or &quot;ImportError: cannot import name&quot;<\/a>.<\/p>\n\n<p>In Other cases, please update your post for more details to let me know.<\/p>\n\n<hr>\n\n<p>The latest <code>pandas<\/code> version should be <code>0.23.4<\/code>, not <code>0.24.0<\/code>.<\/p>\n\n<p>I tried to find out the earliest version of <code>pandas<\/code> which support the <code>read_parquet<\/code> feature via search the function name <code>read_parquet<\/code> in the documents of different version from <code>0.19.2<\/code> to <code>0.23.3<\/code>. Then, I found <code>pandas<\/code> supports <code>read_parquet<\/code> feature after the version <code>0.21.1<\/code>, as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The new features shown in the <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/whatsnew.html\" rel=\"nofollow noreferrer\"><code>What's New<\/code><\/a> of version <code>0.21.1<\/code>\n<a href=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>According to your <code>EDIT 2<\/code> description, it seems that you are using Python 3.4 in Azure Jupyter Notebook. Not all <code>pandas<\/code> versions support Python 3.4 version.<\/p>\n\n<p>The versions <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.21.1<\/code><\/a> &amp; <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.22\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.22.0<\/code><\/a> offically support Python 2.7,3.5, and 3.6, as below.\n<a href=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the <a href=\"https:\/\/pypi.org\/project\/pandas\/\" rel=\"nofollow noreferrer\">PyPI page for <code>pandas<\/code><\/a> also requires the Python version as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6613J.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6613J.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So you can try to install the <code>pandas<\/code> versions <code>0.21.1<\/code> &amp; <code>0.22.0<\/code> in the current notebook of Python 3.4. if failed, please create a new notebook in Python <code>2.7<\/code> or <code>&gt;=3.5<\/code> to install <code>pandas<\/code> version <code>&gt;= 0.21.1<\/code> to use the function <code>read_parquet<\/code>.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1547190062312,
        "Solution_link_count":14.0,
        "Solution_readability":10.9,
        "Solution_reading_time":52.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":51.0,
        "Solution_word_count":383.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":81.3314522222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an issue where vscode when connected to a VM on GCP cannot see packages installed in <code>\/opt\/conda\/lib\/python3.7\/site-packages.<\/code> I have created the VM using Vertex AI. When I open the jupyter notebook through the UI in a the browser I can see all the packages via <code>pip3 list<\/code>. But when I am connected to the VM through SSH in vscode I cannot see the packages installed such as nltk, spacy etc. and when I try to load it gives me <code>ModuleNotFoundError<\/code>. This error does not show up when I use the jupyter notebook from the Vertex AI UI. The site-packages folder is in my system path and the python that I am using is <code>\/opt\/conda\/bin\/python3<\/code>.<\/p>\n<p>Any help is appreciated. Please do let me know if my question is clear.<\/p>\n<p>EDIT: I figured out that my packages are running on a container in the VM. Is there a way for me to access those packages via jupyter notebook in vscode?<\/p>",
        "Challenge_closed_time":1638843453688,
        "Challenge_comment_count":3,
        "Challenge_created_time":1638473538253,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1638550660460,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70205432",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.4,
        "Challenge_reading_time":11.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":102.7542875,
        "Challenge_title":"VSCode cannot see packages on a GCP VM",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":230.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580840045043,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":85.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Posting the answer as community wiki. As confirmed by @Abhishek, he was able to make it work by installing a docker extension on the VM then attach VS code to the container.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":2.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.6766972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Started up a STANDARD_D11_V2 cluster to run some notebooks on.<\/p>\n<p>Wanted to use json_normalize from pandas: <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.json_normalize.html\">https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.json_normalize.html<\/a> and I get the below error:<\/p>\n<pre><code>AttributeError: module 'pandas' has no attribute 'json_normalize'\n<\/code><\/pre>\n<p>Pandas seems to be out of date. Checked the installed version of pandas:<\/p>\n<pre><code>$ python\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import pandas\n\n&gt;&gt;&gt; pandas.__version__\n\n'0.23.4\n<\/code><\/pre>\n<p>Pandas is indeed out of date, the latest version is v1.1.1. Fired up a terminal to run:<\/p>\n<pre><code>conda update --all\n<\/code><\/pre>\n<p>On the azureml_py36 environment which I had selected to run the notebook on. It hangs on:<\/p>\n<pre><code>Solving environment: \/\n<\/code><\/pre>\n<p>Went to update conda to see if that would help:<\/p>\n<pre><code>conda update conda\n<\/code><\/pre>\n<p>But I get this error:<\/p>\n<pre><code>PackageNotInstalledError: Package is not installed in prefix.\n  prefix: \/anaconda\/envs\/azureml_py36\n  package name: conda\n<\/code><\/pre>\n<p>Which leads me to think this is not a typical installation of conda.<\/p>\n<p>Would like to run the most up to date packages on Azure Machine Learning to replicate the local environment I have setup. Does anyone know how to do this?<\/p>",
        "Challenge_closed_time":1598622188187,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598608952077,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/80212\/update-out-of-date-packages-on-azure-machine-learn",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":21.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":3.6766972222,
        "Challenge_title":"Update out of date packages on Azure Machine Learning Compute instance",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":200,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@philmariusnew-9791 I have tried the above steps and the installation completed successfully for conda. But when we upgrade pandas azureml package has a dependency  so it cannot use version v1.1.1     <\/p>\n<p>I have went ahead and upgrade the pandas version but there is a warning as seen below:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/21216-image.png?platform=QnA\" alt=\"21216-image.png\" \/>    <\/p>\n<p>We would recommend to use the package that is compatible with azureml so your environment setup works as expected.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.8,
        "Solution_reading_time":6.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2127.9833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\nWe fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nThe new version of azure-cli is not compatible with the old azureml package and throws an error when creating AzureML workspace:\r\n\r\n```\r\nUnable to create the workspace. \r\n Azure Error: InvalidRequestContent\r\nMessage: The request content was invalid and could not be deserialized: 'Could not find member 'template' on object of type 'DeploymentDefinition'. Path 'template', line 1, position 12.'.\r\n```\r\n\r\nThere is an open issue at Azure cli about the similar error: https:\/\/github.com\/Azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### In which platform does it happen?\r\nLinux Ubuntu\r\n(Haven't tested on other platforms)\r\n\r\n### How do we replicate the issue?\r\nInstall reco_pyspark and run operationalization notebook.\r\n\r\n### Expected behavior (i.e. solution)\r\nFix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### Other Comments\r\nI'm working on #1158 and #900.\r\nIf fixing the azure-cli-core version is okay, then I will address this issue together.\r\n",
        "Challenge_closed_time":1603980914000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596320174000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1171",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2127.9833333333,
        "Challenge_title":"[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Seems we need to fix `azure-mgmt-cosmosdb` version as well... \r\n```\r\nAttributeError: module 'azure.mgmt.cosmosdb' has no attribute 'CosmosDB'\r\n```\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":1.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":3.7412430556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>in tutorials like <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/fine-tuning-a-pytorch-bert-model-and-deploying-it-with-amazon-elastic-inference-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Fine-tuning a pytorch bert model and deploying it with sagemaker<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/fine-tune-and-host-hugging-face-bert-models-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">fine-tune and host huggingface models on sagemaker<\/a>, a hugging face estimator is used to call a training script. What would be the difference if I just directly ran the script's code in the notebook itself? is it because the estimator makes it easier to deploy the model?<\/p>",
        "Challenge_closed_time":1646330906792,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646317438317,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71338750",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":10.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.7412430556,
        "Challenge_title":"what is the difference between using a hugging face estimator with training script and directly using a notebook in AWS sagemaker?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597997723910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You could run the script in the notebook itself but it would not deploy with SageMaker provided capabilities then. The estimator that you are seeing is what specifies to SageMaker what framework you are using and the training script that you are passing in. If you ran the script code in the notebook that would be like training in your local environment. By passing in the script to the Estimator you are running a SageMaker training job. The estimator is meant to encapsulate training on SageMaker.<\/p>\n<p>SageMaker Estimator Overview: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":8.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":32.65,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello, I have an Databricks account on Azure, and the goal is to compare different image tagging services from GCP and other providers via corresponding API calls, with Python notebook. I have problems with GCP vision API calls, specifically with credentials: as far as I understand, the one necessary step is to set 'GOOGLE_APPLICATION_CREDENTIALS' environment variable in my databricks notebook with something like\u00a0\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='\/folder1\/credentials.json'\u00a0\n\nwhere '\/folder1\/credentials.json' is the place my notebook looks for json file with credentials (notebook is in the same folder,\u00a0\/folder1\/notebook_api_test).\n\nI am getting this path by looking into\u00a0Workspace->\u00a0Copy file path\u00a0in the Databricks web page.\n\nBut this approach doesn't work, when cell is executed, I am getting this error:\u00a0\n\nDefaultCredentialsError: File \/folder1\/credentials.json was not found.\u00a0\n\nWhat is the right way to deal with credentials to access google vision API from Databricks notebook?",
        "Challenge_closed_time":1682007420000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681889880000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-is-the-best-way-to-use-credentials-for-API-calls-from\/td-p\/545303\/jump-to\/first-unread-message",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":32.65,
        "Challenge_title":"What is the best way to use credentials for API calls from databricks notebook?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Ok, here is a trick: in my case, the file with GCP credentials is stored in notebook workspace storage, which is not visible to os.environ() command. So solution is to read a content of this file, and save it to the cluster storage attached to the notebook, which is created with the cluster and is erased when cluster is gone (so we need to repeat this procedure every time the cluster is re-created). According to this doc, we can read the content of the credentials json file stored in notebook workspace with\n\n\u00a0 \u00a0 \u00a0 \u00a0 with open('\/Workspace\/folder1\/cred.json'): #note that I need a full path here, for some reason\n\u00a0 \u00a0 \u00a0 \u00a0 content = f.read()\n\nand then according to this doc, we need to save it on another place in a new file (with the same name in my case, cred.json), namely on cluster storage attached to the notebook (which is visible to os-related functions, like os.environ()), with\n\n\u00a0 \u00a0 \u00a0 \u00a0 fd = os.open(\"cred.json\", os.O_RDWR|os.O_CREAT)\n\u00a0 \u00a0 \u00a0 \u00a0 ret = os.write(fd,content.encode())\n\u00a0 \u00a0 \u00a0 \u00a0 #need to add .encode(), or will get TypeError: a bytes-like object is required, not 'str'\n\u00a0 \u00a0 \u00a0 \u00a0 os.close(fd)\n\nOnly after that we can continue with setting an environment variable, required for GCP authentication:\n\n\u00a0 \u00a0 \u00a0 \u00a0 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='.\/cred.json'\n\nand then API calls should work fine, without DefaultCredentialsError.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":16.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":202.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1500385674112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":44.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":1.1039755556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I've created a Sagemaker notebook to dev AWS Glue jobs, but when running through the provided example (\"Joining, Filtering, and Loading Relational Data with AWS Glue\") I get the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does anyone know what I've setup wrong\/haven't setup to cause the import to not work?<\/p>",
        "Challenge_closed_time":1570201013272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570197038960,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58237749",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":12.9,
        "Challenge_reading_time":6.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.1039755556,
        "Challenge_title":"AWS Glue Sagemaker Notebook \"No module named awsglue.transforms\"",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":3551.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384795490587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1012.0,
        "Poster_view_count":102.0,
        "Solution_body":"<p>You'll need to download the library files from <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 0.9 or <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl-1.0\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 1.0 (Check your Glue jobs for the version). <\/p>\n\n<p>Put the zip in S3 and reference it in the \"Python library path\" on your Dev Endpoint.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":6.27,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":320.9530555556,
        "Challenge_answer_count":5,
        "Challenge_body":"Hi, I am trying to install some libraries in Studio Lab which requires root privileges. \r\n\r\nBelow I have run `whoami` to check if I am root user. (I am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nBelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\nI followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\nOn running `su -` , It asks for the password, but we don't have any password for Studio Lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\nCan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nPlease let me know if my query is not clear. ",
        "Challenge_closed_time":1655933766000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654778335000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/118",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.06,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":320.9530555556,
        "Challenge_title":"How to get root access in SageMaker Studio Lab",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":122,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you for trying Studio Lab. Now Studio Lab does not allow `sudo` (similar issue: https:\/\/github.com\/aws\/studio-lab-examples\/issues\/40#issuecomment-1005305538). We can use `pip` and `conda` instead. Please refer the following issue.\r\n\r\nWhat software do you try to install? Some libraries will be available in `conda-forge` . Here is the sample of search.\r\n\r\nhttps:\/\/anaconda.org\/search?q=gym Thanks for the reply, I will try to explain the issue.\r\n I am trying to run bipedal robot from Open-ai gym. \r\n```\r\n!pip install gym\r\n!apt-get update\r\n!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> \/dev\/null\r\n!apt-get install xvfb\r\n!pip install pyvirtualdisplay \r\n!pip -q install pyglet\r\n!pip -q install pyopengl\r\n!apt-get install swig\r\n!pip install box2d box2d-kengz\r\n!pip install pybullet\r\n```\r\nThese are the libraries which I need to install for the code to work. \r\nIt works fine on google-colab: (screenshot below)\r\n![colab_gym_ss](https:\/\/user-images.githubusercontent.com\/91401599\/173034039-1973ee00-6c0b-4f49-adad-9bb324c59b8c.png)\r\n\r\nBut it throws error when I run it on SageMaker Studio Lab: (screenshot below)\r\n![sagemaker1](https:\/\/user-images.githubusercontent.com\/91401599\/173034679-f49340dc-de46-42bb-be1b-7c7e3616dac4.png)\r\n\r\nError Log (I have made gym_install.sh file which installs everything described above, I am running it below.): \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ sh gym_install.sh\r\nRequirement already satisfied: gym in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (0.24.1)\r\nRequirement already satisfied: gym-notices>=0.0.4 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (0.0.7)\r\nRequirement already satisfied: numpy>=1.18.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (1.22.4)\r\nRequirement already satisfied: cloudpickle>=1.2.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (2.1.0)\r\nReading package lists... Done\r\nE: List directory \/var\/lib\/apt\/lists\/partial is missing. - Acquire (13: Permission denied)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nRequirement already satisfied: pyvirtualdisplay in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.0)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nCollecting box2d\r\n  Using cached Box2D-2.3.2.tar.gz (427 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting box2d-kengz\r\n  Using cached Box2D-kengz-2.3.3.tar.gz (425 kB)\r\n  Preparing metadata (setup.py) ... done\r\nBuilding wheels for collected packages: box2d, box2d-kengz\r\n  Building wheel for box2d (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d\r\n  Running setup.py clean for box2d\r\n  Building wheel for box2d-kengz (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d-kengz\r\n  Running setup.py clean for box2d-kengz\r\nFailed to build box2d box2d-kengz\r\nInstalling collected packages: box2d-kengz, box2d\r\n  Running setup.py install for box2d-kengz ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Running setup.py install for box2d-kengz did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [18 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running install\r\n      \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n        warnings.warn(\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: legacy-install-failure\r\n\r\n\u00d7 Encountered error while trying to install package.\r\n\u2570\u2500> box2d-kengz\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for output from the failure.\r\nRequirement already satisfied: pybullet in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.2.5)\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\n### To be specific I am getting error in this line:\r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ apt-get install xvfb\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\n```\r\nSo as mentioned by you I tried to find xvfb in conda-forge, but couldn't find it. \r\nThere are some wrapper xvfb on conda-forge but installing them didn't help with the error. \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ python check.py\r\nTraceback (most recent call last):\r\n  File \"\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/GM_\/ARS_src\/check.py\", line 9, in <module>\r\n    display = Display(visible=0, size=(1024, 768))\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/display.py\", line 54, in __init__\r\n    self._obj = cls(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/xvfb.py\", line 44, in __init__\r\n    AbstractDisplay.__init__(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/abstractdisplay.py\", line 85, in __init__\r\n    helptext = get_helptext(program)\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/util.py\", line 13, in get_helptext\r\n    p = subprocess.Popen(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 966, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 1842, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\nTo reproduce above error run below code (check.py) :->\r\n```\r\nimport os\r\nimport numpy as np\r\nimport gym\r\nfrom gym import wrappers\r\nimport pyvirtualdisplay\r\nfrom pyvirtualdisplay import Display\r\n\r\nif __name__ == \"__main__\":\r\n    display = Display(visible=0, size=(1024, 768))\r\n```\r\n Thank you for sharing the error message. Studio Lab does not allow `apt install` so that the `gym_install.sh` does not work straightly. We have to prepare the environment by `conda` and `pip`.\r\n\r\nAt first we can install `swig` from `conda`. We can not install `xvfb` from `conda`, is this necessary to run the code?  Basically I have to capture the video output using `Display` of `pyvirtualdisplay` library. Everything else works. \r\nAs you can see `display = Display(visible=0, size=(1024, 768))` this line throws error  `FileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'` , so I am trying to install `Xvfb`.\r\n\r\nThanks, @ar8372 Sorry for the late reply. I raised the #124 to work OpenAI Gym in Studio Lab. Please comment to #124 if you have additional information. We need your insight to solve the issue. If you do not mind, please close this issue to suppress the duplication of issues.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.2,
        "Solution_reading_time":134.31,
        "Solution_score_count":2.0,
        "Solution_sentence_count":129.0,
        "Solution_word_count":1049.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1243547542743,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":6438.0,
        "Answerer_view_count":922.0,
        "Challenge_adjusted_solved_time":193.0081575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the \"FinelineNumber\" column. For context, <code>df.shape<\/code> returns <code>(647054, 7)<\/code>. I am trying to make a dummy column for <code>df['FinelineNumber']<\/code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)<\/code>, which I then plan to <code>concat<\/code> to the original dataframe. <\/p>\n\n<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')<\/code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.<\/code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.<\/p>\n\n<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.<\/p>",
        "Challenge_closed_time":1449768300887,
        "Challenge_comment_count":0,
        "Challenge_created_time":1449073471520,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1454300528203,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34047782",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.7,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":193.0081575,
        "Challenge_title":"Jupyter notebook kernel dies when creating dummy variables with pandas",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":5063.0,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Poster_created_time":1373475615300,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New York, United States",
        "Poster_reputation_count":2037.0,
        "Poster_view_count":193.0,
        "Solution_body":"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.<\/p>\n\n<p>You might try doing <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/sparse.html\" rel=\"noreferrer\">.to_sparse()<\/a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":8.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":64.6766666667,
        "Challenge_answer_count":0,
        "Challenge_body":"In every run you can see:\r\n```\r\n               2020-07-03 23:24:19,549 DEBUG: Trying to spawn '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\r\n\/home\/efiop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               2020-07-03 23:24:19,550 DEBUG: Spawned '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\/home\/ef\r\niop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               Unknown mode daemon\r\n```\r\nwe clearly need to take more care on dvc-side, but a good enough workaround is to set CI or DVC_TEST env var to make dvc skip launching the updater.",
        "Challenge_closed_time":1594041670000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593808834000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/149",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.9,
        "Challenge_reading_time":9.95,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":64.6766666667,
        "Challenge_title":"dvc tries to launch updater using asv script",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":66,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":191.5183333333,
        "Challenge_answer_count":5,
        "Challenge_body":"**Describe the bug**\r\n\r\nIdle Sagemaker Notebook instances do not stop after specified time.\r\n\r\nSWB runs autostop.py script to automatically stop Sagemaker Notebook instance. The script is used by `on-start` lifecycle rule of the instance CFN template. According to LifecycleConfigOnStart logs, some packages are missing and autostop script doesn\u2019t work.\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Make sure AutoStopIdleTimeInMinutes parameter in workspace type config is set to a required time (30 minutes in our case)\r\n2. Create a new workspace with Sagemaker notebook instance\r\n3. Leave the instance idle for the time specified (AutoStopIdleTimeInMinutes )\r\n4. After the specified time see that the instance is not stopped\r\n\r\n**Expected behavior**\r\nIdle Sagemaker Notebook instance automatically stops after specified time.\r\n\r\n**Screenshots**\r\n<img width=\"1308\" alt=\"Screen Shot 2022-12-07 at 10 43 09 am\" src=\"https:\/\/user-images.githubusercontent.com\/47466926\/206049662-5ff12457-8bd4-42bd-b12f-ce68fdfacaf6.png\">\r\n\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed v5.0.0\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671059684000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670370218000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":10.0,
        "Challenge_reading_time":15.67,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":191.5183333333,
        "Challenge_title":"[Bug] Sagemaker instance does not stop automatically",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you. We are aware of this issue and have a backlog item to resolve this! See https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065 for more information. Hi, please check the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5) for the fix to this issue.  Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! Hi Marianna,\nThank you. I am going to migrate to v5.2.5 tomorrow. If everything goes\nwell, I'll close the ticket soon.\n\n\nOn Tue, Dec 13, 2022 at 7:55 AM Marianna Ghirardelli <\n***@***.***> wrote:\n\n> Hi! I also want to note that you may need to stop and start any affected\n> instances after upgrade and deploying SWB v5.2.5.\n>\n> If this fixes your issue, please go ahead and close this issue. I am going\n> to mark as closing-soon-if-no-response so we will close in about 7 days if\n> we do not hear that this did not resolve the issue.\n>\n> Thank you for the report!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076#issuecomment-1347314897>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ALKETLSEQCFIMAF5HX4CAT3WM6GN3ANCNFSM6AAAAAASWEAJP4>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n Closing this issue since the fix was merged, please feel free to reopen the issue if it persists on your end.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":6.9,
        "Solution_reading_time":21.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.1608638889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I am not able to use my Jupyterlab and Jupyter Notebook. It cannot connect to a Kernel.  <\/p>\n<p>Will this problem be solved when I uninstall Anaconda and install Microsoft Machine Learning Server.  <\/p>\n<p>Thanks,  <\/p>\n<p>Naveen<\/p>",
        "Challenge_closed_time":1610581957883,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610574178773,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/229665\/machine-learning-server",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.1,
        "Challenge_reading_time":3.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.1608638889,
        "Challenge_title":"machine learning server",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, I'm assuming you're using your own development environment. I found some <a href=\"https:\/\/jupyter-notebook.readthedocs.io\/en\/stable\/troubleshooting.html\">troubleshooting steps<\/a> that may be helpful. Furthermore, <a href=\"https:\/\/learn.microsoft.com\/en-us\/machine-learning-server\/what-is-machine-learning-server\">Azure Machine Learning Server<\/a> is an enterprise software that provides the tools for performing data science tasks. You can review Azure ML Server and other <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment#local\">options<\/a> to determine which option best suits your data science scenario. Hope this helps.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":62.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":63.7779516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Challenge_closed_time":1547708377156,
        "Challenge_comment_count":1,
        "Challenge_created_time":1547478776530,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":63.7779516667,
        "Challenge_title":"AWS Sagemaker does not update the package",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1546.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527781503483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Metz, France",
        "Poster_reputation_count":352.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4417.9619444444,
        "Challenge_answer_count":15,
        "Challenge_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Challenge_closed_time":1599677796000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583773133000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Challenge_link_count":0,
        "Challenge_participation_count":15,
        "Challenge_readability":18.6,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4417.9619444444,
        "Challenge_title":"unable to kick off the sagemaker job",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":193,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@charlesa101  Thanks for trying out. I am assuming you have replaced input, output buckets and role Arn. \r\n\r\nWould you please run the following command provide the output ?\r\n\r\n```\r\nkubectl  get trainingjobs xgboost-mnist\r\nkubectl describe trainingjob xgboost-mnist\r\n``` @gautamkmr, here you go thank you! yeah i have my own bucket and sagemaker executor role\r\n\r\n```kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nxgboost-mnist                               2020-03-09T16:51:08Z ```\r\n\r\n```kubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400``` @charlesa101  Thanks for providing the output. It appears that operator is not running successfully on your k8s cluster.  you can verify that \r\n\r\n```\r\n kubectl get pods -A | grep -i sagemaker\r\n```\r\n\r\nYou can follow steps from [here](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#setup-and-operator-deployment) to install the operator, let us know if you face any issue. yeah that's what i noticed as well now\r\n\r\n```kubectl get pods -n sagemaker-k8s-operator-system\r\nNAME                                                         READY   STATUS    RESTARTS   AGE\r\nsagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8   0\/2     Pending   0          24h``` ```kubectl describe pod  -n sagemaker-k8s-operator-system                                             \r\nName:               sagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8\r\nNamespace:          sagemaker-k8s-operator-system\r\nPriority:           0\r\nPriorityClassName:  <none>\r\nNode:               <none>\r\nLabels:             control-plane=controller-manager\r\n                    pod-template-hash=5858fd7b8d\r\nAnnotations:        kubernetes.io\/psp: eks.privileged\r\nStatus:             Pending\r\nIP:                 \r\nControlled By:      ReplicaSet\/sagemaker-k8s-operator-controller-manager-5858fd7b8d\r\nContainers:\r\n  kube-rbac-proxy:\r\n    Image:      gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Port:       8443\/TCP\r\n    Host Port:  0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    Environment:\r\n      AWS_ROLE_ARN:                 arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\n  manager:\r\n    Image:      957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:  \r\n      AWS_ROLE_ARN:                    arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\nConditions:\r\n  Type           Status\r\n  PodScheduled   False \r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  sagemaker-k8s-operator-default-token-rwdkn:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  sagemaker-k8s-operator-default-token-rwdkn\r\n    Optional:    false\r\nQoS Class:       Burstable\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type     Reason            Age                   From               Message\r\n  ----     ------            ----                  ----               -------\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n my eks\/ecr is on us-east2, but it seems all the crd artifacts are coming from us-east1 could that be the issue?\r\n EKS can pull the image from other region too. I think in your case it seems that you don't have any worker node associated to cluster?  At least thats what below message says.\r\n```\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n```\r\n\r\nCan you run ?  \r\n```\r\nkubectl get node\r\n``` @charlesa101  did you get chance to review it again? ``` kubectl get nodes\r\nNAME                                           STATUS   ROLES    AGE     VERSION\r\nip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n yeah i did, recreated the cluster again but still the same issue\r\n @charlesa101   In previous describe output of `pod` it appears that cluster did not have any worker nodes available `(no nodes available to schedule pods)`.\r\n\r\nBut based on recent output it appears that you have three worker nodes available. \r\n\r\n> NAME                                           STATUS   ROLES    AGE     VERSION\r\n> ip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n\r\n\r\nCould you please describe each of these nodes and operator pod ?\r\n\r\n```\r\n# Describe nodes , assuming the names of nodes are same as you mentioned in previous comment.\r\nkubectl describe node ip-172-16-116-51.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-121-255.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-137-197.us-east-2.compute.internal \r\n```\r\n\r\n\r\n```\r\n#Get the operator pod name \r\nkubectl get pods -A | grep -i sagemaker\r\nkubectl describe pod <put the pod name here>  -n sagemaker-k8s-operator-system\r\n```\r\n\r\n\r\nIf operator has been deployed successfully and if trainingjob is still not yet running please attach the out put of describe trainingjob as well ? \r\n```\r\nkubectl describe trainingjob xgboost-mnist\r\n\r\n```\r\n\r\n i tried to look checked the operator pod, here is  the log @gautamkmr \r\n\r\n```\r\nkubectl logs -f sagemaker-k8s-operator-controller-manager-5858fd7b8d-2dk5c  -n sagemaker-k8s-operator-system manager\r\n2020-03-15T18:09:13.864Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    setup   starting manager\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"trainingjob\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"model\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"batchtransformjob\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hostingdeployment\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"endpointconfig\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hyperparametertuningjob\"}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"trainingjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"model\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Job status is empty, setting to intermediate status     {\"trainingjob\": \"default\/xgboost-mnist\", \"status\": \"SynchronizingK8sJobWithSageMaker\"}\r\n2020-03-15T19:09:19.963Z        INFO    controllers.TrainingJob Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"new-status\": {\"trainingJobStatus\":\"SynchronizingK8sJobWithSageMaker\",\"lastCheckTime\":\"2020-03-15T19:09:19Z\"}}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Adding generated name to spec   {\"trainingjob\": \"default\/xgboost-mnist\", \"new-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}\r\n2020-03-15T19:09:19.982Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:20.916Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:09:20.916Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\",\"lastCheckTime\":\"2020-03-15T19:09:20Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:09:20.924Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:42.150Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:11:42.150Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\",\"lastCheckTime\":\"2020-03-15T19:11:42Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:11:42.159Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n```\r\n @charlesa101  Thanks for sharing the log. You are on right track. I think the issue now is operator pod is unable to retrieve credentials from IAM service to talk to sagemaker. \r\n\r\n`\"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n`\r\n\r\nCould you please check your [trust.json](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#create-an-iam-role) basically **trust policy have three places to update cluster region and OIDC ID and one place to add your AWS account number.** Hi @charlesa101\r\n\r\nClosing this issue since there has been no activity in 90 days. Please re-open if you still need help\r\n\r\nThanks Hi, I'm having the exact same issue except that my pod is running fine. I setup my k8s cluster using terraform with 1 master node and 1 worker node. When I submit the trainingjob, there is no status or job name or anything else. I tried all the commands above and it looks like the scheduler was able to assign the pods to the worker node. Any help would be appreciated! Please see outputs for commands below:\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get pods -A                                                                                                                                                                                                                                                    \r\nNAMESPACE        NAME                                                         READY   STATUS    RESTARTS   AGE                                                                                                                                                                                                                \r\nkube-system      aws-node-67tgx                                               1\/1     Running   0          2d18h\r\nkube-system      aws-node-k2q7z                                               1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-cwfvj                                     1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-x5ld9                                     1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-54vm5                                             1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-r8j7j                                             1\/1     Running   0          2d18h\r\nkube-system      metrics-server-64cf6869bd-6nppx                              1\/1     Running   0          2d18h\r\nsagemaker-jobs   sagemaker-k8s-operator-controller-manager-855f498957-fhkvv   2\/2     Running   0          2d18h\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe pod sagemaker-k8s-operator-controller-manager-855f498957-fhkvv -n sagemaker-jobs\r\nName:         sagemaker-k8s-operator-controller-manager-855f498957-fhkvv\r\nNamespace:    sagemaker-jobs\r\nPriority:     0\r\nNode:         ip-10-0-1-245.us-west-2.compute.internal\/10.0.1.245\r\nStart Time:   Fri, 24 Jun 2022 22:26:03 +0000\r\nLabels:       control-plane=controller-manager\r\n              pod-template-hash=855f498957\r\nAnnotations:  kubernetes.io\/psp: eks.privileged\r\nStatus:       Running\r\nIP:           10.0.1.144\r\nIPs:\r\n  IP:           10.0.1.144\r\nControlled By:  ReplicaSet\/sagemaker-k8s-operator-controller-manager-855f498957\r\nContainers:\r\n  manager:\r\n    Container ID:  docker:\/\/d8fc52b3e20a050999d3f24ab914f1d865a84a168a8b038f3fa81ce59cccbced\r\n    Image:         957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Image ID:      docker-pullable:\/\/957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s@sha256:94ffbba68954249b1724fdb43f1e8ab13547114555b4a217849687d566191e23\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n      --namespace=sagemaker-jobs\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:09 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:\r\n      AWS_DEFAULT_REGION:              us-west-2\r\n      AWS_REGION:                      us-west-2\r\n      AWS_ROLE_ARN:                    arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nkube-rbac-proxy:\r\n    Container ID:  docker:\/\/4ecdaa395fdc70d5cead609465dbf21f6e11771a80ad5db0a6125053ab08b9d3\r\n    Image:         gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Image ID:      docker-pullable:\/\/gcr.io\/kubebuilder\/kube-rbac-proxy@sha256:297896d96b827bbcb1abd696da1b2d81cab88359ac34cce0e8281f266b4e08de\r\n    Port:          8443\/TCP\r\n    Host Port:     0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:11 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:\r\n      AWS_DEFAULT_REGION:           us-west-2\r\n      AWS_REGION:                   us-west-2\r\n      AWS_ROLE_ARN:                 arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             True\r\n  ContainersReady   True\r\n  PodScheduled      True\r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  kube-api-access-6j8rt:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io\/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io\/unreachable:NoExecute op=Exists for 300s\r\nEvents:                      <none>\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl logs sagemaker-k8s-operator-controller-manager-855f498957-fhkvv manager -n sagemaker-jobs\r\nI0624 22:26:11.339445       1 request.go:621] Throttling request took 1.046981399s, request: GET:https:\/\/172.20.0.1:443\/apis\/extensions\/v1beta1?timeout=32s\r\n2022-06-24T22:26:12.443Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2022-06-24T22:26:12.443Z        INFO    Starting manager in the namespace:      sagemaker-jobs\r\n2022-06-24T22:26:12.443Z        INFO    setup   starting manager\r\n2022-06-24T22:26:12.444Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.665Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\"}\r\n2022-06-24T22:26:12.746Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\"}\r\n2022-06-24T22:26:12.747Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nosic-test-run                               2022-06-24T22:38:13Z  \r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe trainingjob osic-test-run                                                                                                                                                                                                                             \r\nName:         osic-test-run                                                                                                                                                                                                                                                                                                   \r\nNamespace:    default                                                                                                                                                                                                                                                                                                         \r\nLabels:       <none>                                                                                                                                                                                                                                                                                                          \r\nAnnotations:  <none>                                                                                                                                                                                                                                                                                                          \r\nAPI Version:  sagemaker.aws.amazon.com\/v1                                                                                                                                                                                                                                                                                     \r\nKind:         TrainingJob                                                                                                                                                                                                                                                                                                     \r\nMetadata:                                                                                                                                                                                                                                                                                                                     \r\n  Creation Timestamp:  2022-06-24T22:38:13Z                                                                                                                                                                                                                                                                                   \r\n  Generation:          1                                                                                                                                                                                                                                                                                                      \r\n  Managed Fields:\r\n    API Version:  sagemaker.aws.amazon.com\/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:kubectl.kubernetes.io\/last-applied-configuration:\r\n      f:spec:\r\n        .:\r\n        f:algorithmSpecification:\r\n          .:\r\n          f:trainingImage:\r\n          f:trainingInputMode:\r\n        f:inputDataConfig:\r\n        f:outputDataConfig:\r\n          .:\r\n          f:s3OutputPath:\r\n        f:region:\r\n        f:resourceConfig:\r\n          .:\r\n          f:instanceCount:\r\n          f:instanceType:\r\n          f:volumeSizeInGB:\r\n        f:roleArn:\r\n        f:stoppingCondition:\r\n          .:\r\n          f:maxRuntimeInSeconds:\r\n        f:trainingJobName:\r\n    Manager:         kubectl-client-side-apply\r\n    Operation:       Update\r\n    Time:            2022-06-24T22:38:13Z\r\n  Resource Version:  3182\r\n  UID:               0a0880c0-baf9-4f1a-8aa3-37480520c3e2\r\nSpec:\r\n  Algorithm Specification:\r\nTraining Image:       438029713005.dkr.ecr.us-west-2.amazonaws.com\/model-training:latest\r\n    Training Input Mode:  File\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Data Source:\r\n      s3DataSource:\r\n        s3DataDistributionType:  FullyReplicated\r\n        s3DataType:              S3Prefix\r\n        s3Uri:                   s3:\/\/osic-full-including-override\r\n  Output Data Config:\r\n    s3OutputPath:  s3:\/\/osic-full-including-override\/experiments\r\n  Region:          us-west-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.p3.2xlarge\r\n    Volume Size In GB:  500\r\n  Role Arn:             arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  900\r\n  Training Job Name:         osic-test-run\r\nEvents:                      <none>\r\n```\r\n\r\nplease let me know if you need to see anything else!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":18.8,
        "Solution_reading_time":403.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":227.0,
        "Solution_word_count":2188.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":124.1602897223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pkl package saved in my azure devops repository<\/p>\n<p>using below code it searches for package in workspace.\nHow to provide package saved in repository<\/p>\n<pre><code> ws = Workspace.get(\n         name=workspace_name,\n         subscription_id=subscription_id,\n        resource_group=resource_group,\n        auth=cli_auth)\n\nimage_config = ContainerImage.image_configuration(\n    execution_script=&quot;score.py&quot;,\n    runtime=&quot;python-slim&quot;,\n    conda_file=&quot;conda.yml&quot;,\n    description=&quot;Image with ridge regression model&quot;,\n    tags={&quot;area&quot;: &quot;ml&quot;, &quot;type&quot;: &quot;dev&quot;},\n)\n\nimage = Image.create(\n    name=image_name,  models=[model], image_config=image_config, workspace=ws\n)\n\nimage.wait_for_creation(show_output=True)\n\nif image.creation_state != &quot;Succeeded&quot;:\n    raise Exception(&quot;Image creation status: {image.creation_state}&quot;)\n\nprint(\n    &quot;{}(v.{} [{}]) stored at {} with build log {}&quot;.format(\n        image.name,\n        image.version,\n        image.creation_state,\n        image.image_location,\n        image.image_build_log_uri,\n    )\n)\n\n# Writing the image details to \/aml_config\/image.json\nimage_json = {}\nimage_json[&quot;image_name&quot;] = image.name\nimage_json[&quot;image_version&quot;] = image.version\nimage_json[&quot;image_location&quot;] = image.image_location\nwith open(&quot;aml_config\/image.json&quot;, &quot;w&quot;) as outfile:\n    json.dump(image_json, outfile)\n<\/code><\/pre>\n<p>I tried to provide path to models but its fails saying package not found<\/p>\n<p>models = $(System.DefaultWorkingDirectory)\/package_model.pkl<\/p>",
        "Challenge_closed_time":1627276170736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626831896507,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68463080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.9,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":123.4095080556,
        "Challenge_title":"how create azure machine learning scoring image using local package",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567209656790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":417.0,
        "Poster_view_count":233.0,
        "Solution_body":"<p>Register model:\nRegister a file or folder as a model by calling <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-\" rel=\"nofollow noreferrer\">Model.register()<\/a>.<\/p>\n<p>In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace.<\/p>\n<pre><code>model = Model.register(workspace=ws,\n                       model_name='',                # Name of the registered model in your workspace.\n                       model_path='',  # Local file to upload and register as a model.\n                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n                       sample_input_dataset=input_dataset,\n                       sample_output_dataset=output_dataset,\n                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n                       description='Ridge regression model to predict diabetes progression.',\n                       tags={'area': 'diabetes', 'type': 'regression'})\n\nprint('Name:', model.name)\nprint('Version:', model.version)\n<\/code><\/pre>\n<p>Deploy machine learning models to Azure: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a><\/p>\n<p>To Troubleshooting remote model deployment Please follow the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?tabs=azcli#function-fails-get_model_path\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1627278873550,
        "Solution_link_count":6.0,
        "Solution_readability":19.4,
        "Solution_reading_time":29.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":159.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1493927732160,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":586.0,
        "Answerer_view_count":158.0,
        "Challenge_adjusted_solved_time":6310.4455736111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>whenever I'm running - <code>from sklearn.ensemble import RandomForestClassifier<\/code><\/p>\n\n<p>I'm getting an error - <code>ImportError: cannot import name 'parallel_helper'<\/code>\nthe stack trace is - <\/p>\n\n<pre><code>--------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-135-d80da5c856d8&gt; in &lt;module&gt;()\n      1 # feature removal using ROC-AUC score\n----&gt; 2 from sklearn.ensemble import RandomForestClassifier\n      3 roc_values = []\n      4 for feature in diabetes_MICE_X.columns:\n      5     clf = RandomForestClassifier()\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sklearn\/ensemble\/__init__.py in &lt;module&gt;()\n      5 \n      6 from .base import BaseEnsemble\n----&gt; 7 from .forest import RandomForestClassifier\n      8 from .forest import RandomForestRegressor\n      9 from .forest import RandomTreesEmbedding\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sklearn\/ensemble\/forest.py in &lt;module&gt;()\n     59 from ..exceptions import DataConversionWarning, NotFittedError\n     60 from .base import BaseEnsemble, _partition_estimators\n---&gt; 61 from ..utils.fixes import parallel_helper, _joblib_parallel_args\n     62 from ..utils.multiclass import check_classification_targets\n     63 from ..utils.validation import check_is_fitted\n\nImportError: cannot import name 'parallel_helper'\n\n\nNote - I'm using jupyter notebook (conda_python3) in sagemaker.\nscipy version = 1.3.1\nnumpy version = 1.17.2\nscikit version = 0.21.3 \n\n\none strange thing that i'm unable to figure out is - whenever i do \n\nimport sklearn\nsklearn.__version__\n<\/code><\/pre>\n\n<p>its gives me output as 0.22<\/p>\n\n<p>can someone help me on this issue ? <\/p>",
        "Challenge_closed_time":1600076529168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1577358925103,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59487643",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":22.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":6310.4455736111,
        "Challenge_title":"ImportError: cannot import name 'parallel_helper'",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2241.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493927732160,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":586.0,
        "Poster_view_count":158.0,
        "Solution_body":"<p>The best way to overcome this problem in sagemaker is to use lifecycle configuration.\nRather than doing pip install inside the notebook, write all your requirements.txt installs inside the lifecycle configurations. The notebook will take more time to spawn but the code but the libraries will be pre-installed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":3.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":168.6647222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Challenge_closed_time":1657139268000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656532075000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.77,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":168.6647222222,
        "Challenge_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1574932278796,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":18454.0,
        "Answerer_view_count":3762.0,
        "Challenge_adjusted_solved_time":12.6983352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to run a Python script in a DevOps pipeline upon check in. A basic 'hellow world' script works, but when I import azureml.core, it errors out with ModuleNotFoundError: No module named 'azureml'.<\/p>\n<p>That makes sense, since I don't know how it was going to find azureml.core. My question is: how do I get the Python script to find the module? Do I need to check it in as part of my code base in DevOps? Or is there some way to reference it via a hyperlink?<\/p>\n<p>Here is my YML file:<\/p>\n<pre><code>trigger:\n- master\n\npool:\n  vmImage: ubuntu-latest\n\nsteps:\n- task: PythonScript@0\n  inputs:\n    scriptSource: 'filepath'\n    scriptPath: test.py\n<\/code><\/pre>\n<p>And here is my python script:<\/p>\n<pre><code>print('hello world')\n\nimport azureml.core\nfrom azureml.core import Workspace\n\n# Load the workspace from the saved config file\nws = Workspace.from_config()\nprint('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n<\/code><\/pre>",
        "Challenge_closed_time":1623837729747,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623792015740,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67993641",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.4,
        "Challenge_reading_time":12.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":12.6983352778,
        "Challenge_title":"DevOps pipeline running python script error on import azureml.core",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1320273319470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Calgary, Canada",
        "Poster_reputation_count":78.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You have to install the lost package into your python, the default python does not have that. Please use the below yml:<\/p>\n<pre><code>trigger:\n- master\n\npool:\n  vmImage: 'ubuntu-latest'\n\nvariables:\n  solution: '**\/*.sln'\n  buildPlatform: 'Any CPU'\n  buildConfiguration: 'Release'\n\nsteps:\n- task: UsePythonVersion@0\n  displayName: 'Use Python 3.8'\n  inputs:\n    versionSpec: 3.8\n\n- script: python3 -m pip install --upgrade pip\n  displayName: 'upgrade pip'\n\n- script: python3 -m pip install azureml.core\n  displayName: 'Install azureml.core'\n\n\n\n- task: PythonScript@0\n  inputs: \n    scriptSource: 'filepath'\n    scriptPath: test.py\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":7.82,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":160.3563522222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running custom training jobs in google's Vertex AI. A simple <code>gcloud<\/code> command to execute a custom job would use something like the following syntax (complete documentation for the command can be seen <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/beta\/ai\/custom-jobs\/create#--config\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n<pre><code>gcloud beta ai custom-jobs create --region=us-central1 \\\n--display-name=test \\\n--config=config.yaml\n<\/code><\/pre>\n<p>In the <code>config.yaml<\/code> file, it is possible to specify the machine and accelerator (GPU) types, etc., and in my case, point to a custom container living in the Google Artifact Registry that executes the training code (specified in the <code>imageUri<\/code> part of the <code>containerSpec<\/code>). An example config file may look like this:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n<\/code><\/pre>\n<p>The code we're running needs some runtime environment variables (that need to be secure) passed to the container. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">API documentation<\/a> for the <code>containerSpec<\/code>, it says it is possible to set environment variables as follows:<\/p>\n<pre><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n    acceleratorType: NVIDIA_TESLA_P100\n    acceleratorCount: 2\n  replicaCount: 1\n  containerSpec:\n    imageUri: {URI_FOR_CUSTOM_CONATINER}\n    args:\n    - {ARGS TO PASS TO CONTAINER ENTRYPOINT COMMAND}\n    env:\n    - name: SECRET_ONE\n      value: $SECRET_ONE\n    - name: SECRET_TWO\n      value: $SECRET_TWO\n<\/code><\/pre>\n<p>When I try and add the <code>env<\/code> flag to the <code>containerSpec<\/code>, I get an error saying it's not part of the container spec:<\/p>\n<pre><code>ERROR: (gcloud.beta.ai.custom-jobs.create) INVALID_ARGUMENT: Invalid JSON payload received. Unknown name &quot;env&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec': Cannot find field.\n- '@type': type.googleapis.com\/google.rpc.BadRequest\n  fieldViolations:\n  - description: &quot;Invalid JSON payload received. Unknown name \\&quot;env\\&quot; at 'custom_job.job_spec.worker_pool_specs[0].container_spec':\\\n      \\ Cannot find field.&quot;\n    field: custom_job.job_spec.worker_pool_specs[0].container_spec\n<\/code><\/pre>\n<p>Any idea how to securely set runtime environment variables in Vertex AI custom jobs using custom containers?<\/p>",
        "Challenge_closed_time":1632987814700,
        "Challenge_comment_count":7,
        "Challenge_created_time":1632409414673,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1632410531832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69302528",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":14.7,
        "Challenge_reading_time":35.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":160.6666741667,
        "Challenge_title":"How to pass environment variables to gcloud beta ai custom-jobs create with custom container (Vertex AI)",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1033.0,
        "Challenge_word_count":283,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464106929608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":457.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>There are two versions of the REST API - \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1<\/a>\u201d and \u201c<a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1beta1\/CustomJobSpec#containerspec\" rel=\"nofollow noreferrer\">v1beta1<\/a>\u201d where &quot;v1beta1&quot; does not have the <code>env<\/code> option in <code>ContainerSpec<\/code> but &quot;v1&quot; does. The <code>gcloud ai custom-jobs create<\/code> command without the <code>beta<\/code> parameter doesn\u2019t throw the error as it uses version \u201cv1\u201d to make the API calls.<\/p>\n<p>The environment variables from the yaml file can be passed to the custom container in the following way:<\/p>\n<p>This is the docker file of the sample custom training application I used to test the requirement. Please refer to this <a href=\"https:\/\/codelabs.developers.google.com\/vertex_custom_training_prediction\" rel=\"nofollow noreferrer\">codelab<\/a> for more information about the training application.<\/p>\n<pre class=\"lang-docker prettyprint-override\"><code>FROM gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3\nWORKDIR \/root\n\nWORKDIR \/\n\n# Copies the trainer code to the docker image.\nCOPY trainer \/trainer\n\n\n# Copies the bash script to the docker image.\nCOPY commands.sh \/scripts\/commands.sh\n\n# Bash command to make the script file an executable\nRUN [&quot;chmod&quot;, &quot;+x&quot;, &quot;\/scripts\/commands.sh&quot;]\n\n\n# Command to execute the file\nENTRYPOINT [&quot;\/scripts\/commands.sh&quot;]\n\n# Sets up the entry point to invoke the trainer.\n# ENTRYPOINT &quot;python&quot; &quot;-m&quot; $SECRET_TWO \u21d2 To use the environment variable  \n# directly in the docker ENTRYPOINT. In case you are not using a bash script, \n# the trainer can be invoked directly from the docker ENTRYPOINT.\n<\/code><\/pre>\n<br \/>\n<p>Below is the <code>commands.sh<\/code> file used in the docker container to test whether the environment variables are passed to the container.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\nmkdir \/root\/.ssh\necho $SECRET_ONE\npython -m $SECRET_TWO\n<\/code><\/pre>\n<br \/>\n<p>The example <code>config.yaml<\/code> file<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code># config.yaml\nworkerPoolSpecs:\n  machineSpec:\n    machineType: n1-highmem-2\n  replicaCount: 1\n  containerSpec:\n    imageUri: gcr.io\/infosys-kabilan\/mpg:v1\n    env:\n    - name: SECRET_ONE\n      value: &quot;Passing the environment variables&quot;\n    - name: SECRET_TWO\n      value: &quot;trainer.train&quot;\n<\/code><\/pre>\n<p>As the next step, I built and pushed the container to Google Container Repository. Now, the <code>gcloud ai custom-jobs create --region=us-central1  --display-name=test --config=config.yaml<\/code> can be run to create the custom training job and the output of the <code>commands.sh<\/code> file can be seen in the job logs as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qRHV7.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":12.1,
        "Solution_reading_time":39.15,
        "Solution_score_count":4.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":319.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":224.3239358333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to start an EMR cluster every time a sagemaker notebook is started.\nHowever, I have find out that Lifecycle configuration scripts cannot run for longer than 5 minutes. Sadly my EMR cluster take more then 5 minutes to be up. This is a problem as I need to wait for the cluster to be up in order to retrieve the master ip address (that ip address is then used to configure the connection between sagemaker notebbok and the cluster).<\/p>\n\n<p>Below an extract of the code which is run into the lifecycle configuration script. <\/p>\n\n<p>Is there someone out there which have faced a similar problem and found a solution ? <\/p>\n\n<pre><code>job_flow_id = client.run_job_flow(**CLUSTER_CONFIG)['JobFlowId']\n\n...\n...\n\n# Retrieve private Ip of master node for later use\nmaster_instance = client.list_instances(ClusterId=job_flow_id, InstanceGroupTypes=['MASTER'])['Instances'][0]\nmaster_private_ip = master_instance['PrivateIpAddress']\n\n# Send to sagemaker the config file in order to tell him how to communicate with spark\ns3 = boto3.client('s3')\nfile_object = s3.get_object(Bucket='dataengine', Key='emr\/example_config.json')\ndata = json.loads(file_object['Body'].read().decode('utf-8'))\ndata['kernel_python_credentials']['url'] = 'http:\/\/{}:8998'.format(master_private_ip)\ndata['kernel_scala_credentials']['url'] = 'http:\/\/{}:8998'.format(master_private_ip)\ndata['kernel_r_credentials']['url'] = 'http:\/\/{}:8998'.format(master_private_ip)\n\nwith open('.\/sparkmagic\/config.json', 'w') as outfile:\n    json.dump(data, outfile)```\n<\/code><\/pre>",
        "Challenge_closed_time":1550792434116,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549984867947,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54653359",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.8,
        "Challenge_reading_time":21.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":224.3239358333,
        "Challenge_title":"Is there a way to use sagemaker lifecycle configuration to run an EMR cluster on notebook start",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1652.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408609820407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":111.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>You can also consider using <a href=\"https:\/\/en.wikipedia.org\/wiki\/Nohup\" rel=\"nofollow noreferrer\">nohup<\/a> in Lifecycle config to execute your script in background, so you don't get blocked by 5-min limit. <\/p>\n\n<p>Let us know if there's anything else you need assistance of.<\/p>\n\n<p>Thanks,<\/p>\n\n<p>Han<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.2,
        "Solution_reading_time":3.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":890.5602777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to deploy ml model using az ml model deploy command with additional files.\r\n\r\nEg:-\r\n\r\naz ml model deploy --ds  docker-additional-steps.txt \r\n```\r\ndocker-additional-steps.txt\r\n\r\nCOPY *.txt \/var\/azureml-app\/\r\n```\r\n\r\nbut it gives an error as below\r\n```\r\nFailed\r\nERROR: {'Azure-cli-ml Version': '1.29.0', 'Error': WebserviceException:\r\n\tMessage: Image creation polling reached non-successful terminal state, current state: Failed\r\nError response from server:\r\nStatusCode: 400\r\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Image creation polling reached non-successful terminal state, current state: Failed\\nError response from server:\\nStatusCode: 400\\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\"\r\n    }\r\n}}\r\n\r\n```",
        "Challenge_closed_time":1626798489000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623592472000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1509",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":12.91,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":890.5602777778,
        "Challenge_title":"How to copy files into  docker image while deploying ml model using azure ml model deploy command",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Extra docker steps is no longer supported. Please create an environment instead where you can inject files as you wish and use that environment for deployment. Here is a sample.\r\n\r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":4.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1540309037063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":404.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":72.6951308334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Want to have a copy of my Google Colab python notebook in my AWS Sagemaker Jupyter notebook(newbie to AWS Sagemaker)<\/p>\n\n<p>I tried selecting all cells in my Colab notebook and pasting in my sagemaker Jupyter notebook using copy paste icons and via cmd+C and cmd+V<\/p>\n\n<p>Cannot copy paste all selected cells at once between Colab and Sagemaker Jupyter notebooks<\/p>",
        "Challenge_closed_time":1561274854928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561274387650,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1561274899232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56721821",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":5.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1297994444,
        "Challenge_title":"Can I copy or upload a Google Colab notebook onto a AWS Sagemaker instance?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1621.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540309037063,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":404.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>While doing the drudgery work of copy pasting each cell between the notebooks(my bad), I realized that we could just <strong>download the notebook as .ipynb file on Colab<\/strong> and <strong>upload on the Sagemaker notebook instance using the <code>Upload button<\/code><\/strong>.<a href=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1561536601703,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":5.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1608747030727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":105.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":15.1535161111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I'm a beginner in docker and containers and I've been getting this error for days now.\nI get this error when my lambda function runs a sagemaker processing job.\nMy core python file resides in an s3 bucket.\nMy docker image resides in ECR.\nBut I dont understand why I dont get a similar error when I run the same processing job with a python docker image.\nPFB the python docker file that didnt throw any errors.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM python:latest\n#installing dependencies\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\nRUN pip3 install matplotlib\n<\/code><\/pre>\n<p>I only get this error when i run this with a an ubunutu docker image with python3 installed.\nPFB the dockerfile which throws the error mentioned.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM ubuntu:20.04\n\nRUN apt-get update -y\nRUN apt-get install -y python3\nRUN apt-get install -y python3-pip\n\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy==1.19.1\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\n\nENTRYPOINT [ &quot;python3&quot; ]\n<\/code><\/pre>\n<p>How do I fix this?<\/p>",
        "Challenge_closed_time":1622294378528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622233871527,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1622239825870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67745141",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":17.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":16.8075002778,
        "Challenge_title":"Facing this error : container_linux.go:367: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1123.0,
        "Challenge_word_count":205,
        "Platform":"Stack Overflow",
        "Poster_created_time":1608747030727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Fixed this error by changing the entry point to<\/p>\n<p><strong>ENTRYPOINT [ &quot;\/usr\/bin\/python3.8&quot;]<\/strong><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":1.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1430.2352777778,
        "Challenge_answer_count":1,
        "Challenge_body":"In a fresh conda environment, I get several warnings that halt the script execution:\r\n```\r\n...\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), Requirement.parse('docker<5.0.0'), {'azureml-core'}).\r\n...\r\n```\r\n\r\nMy environment is specified by:\r\n```yaml\r\nname: xxx\r\nchannels:\r\n  - anaconda\r\n  - pytorch-lts\r\ndependencies:\r\n  - python=3.6\r\n  - pandas=1.1.3\r\n  - numpy=1.19.2\r\n  - scikit-learn=0.23.2\r\n  - matplotlib\r\n  - mkl=2020.2\r\n  - pytorch=1.8.1\r\n  - cpuonly=1.0\r\n  - pip\r\n  - pip:\r\n      - azureml-sdk==1.31.0\r\n      - azureml-defaults==1.31.0\r\n      - azure-storage-blob==12.8.1\r\n      - mlflow==1.18.0\r\n      - azureml-mlflow==1.31.0\r\n      - pytorch-lightning==1.3.8\r\n      - onnxruntime==1.8.0\r\n      - docker<5.0.0 # this is the fix needed\r\n```\r\nThe fix is to specify `docker<5.0.0`. Perhaps, there are some wrong deps checks somewhere.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Challenge_closed_time":1630367426000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625218579000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1537",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":21.68,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1430.2352777778,
        "Challenge_title":"Bug: Failure while loading azureml_run_type_providers",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":129,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for the report! azureml-sdk==1.13.0 does specify docker<5.0.0, while mlflow==1.18.0 requires 5.0.0. \r\n\r\nI'm going to close this issue as there is no action for azureml-sdk.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.7888888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nCloning a single notebook using the \"Open In in Sagemaker Studio Lab\" fails.  Cloning the whole repo works.  \r\n\r\nUsing sagemaker's sample, https:\/\/github.com\/aws\/studio-lab-examples\/tree\/main\/open-in-studio-lab, I get this error:\r\n```\r\nUnable to copy notebook to project.\r\nThe link to this notebook is broken or blocked. If this is a private GitHub notebook, sign in to GitHub before copying the notebook.aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb\r\n```\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create MD cell with `[![Open in SageMaker Studio Lab](https:\/\/studiolab.sagemaker.aws\/studiolab.svg)](https:\/\/studiolab.sagemaker.aws\/import\/github\/aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb)`  and run it\r\n2. Click on the button that appears once you run the cell.  Will open new tab in browser\r\n3. In the new pop up tab, click \"Copy to Project\".  Will open new tab in browser\r\n4. In the new pop up tab's modal, select \"Copy Notebook Only\"\r\n5. Error will now appear\r\n\r\n**Expected behavior**\r\nMy notebook will open and appear, just as it would with cloning a directory\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/46935140\/151415892-7d033f97-f98c-4ac8-9c50-99223253b1ee.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [Windows 11]\r\n - Browser [Chrome]\r\n - Version [97.0.4692.71]\r\n\r\n",
        "Challenge_closed_time":1643319424000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643305784000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/54",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":19.98,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":3.7888888889,
        "Challenge_title":"[BUG] \"Open In in Sagemaker Studio Lab\" button process fails when attempting to \"Copy Notebooks Only\"",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":177,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Interesting - this works fine on my end, but I'm using a Mac. I'll create a ticket for you. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.843735,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I am trying to create an Azure ML Environment using a Dockerfile but it contains the 'COPY' instruction.    <\/p>\n<p>From the documentation of Environment.from_dockerfile ( <a href=\"https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-\">https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-<\/a> ), I can not find a way to give it some files along with the Dockerfile itself.    <\/p>\n<p>So, how to pass context to enable using COPY in the Dockerfile ?    <\/p>\n<p>Thank you for your time !<\/p>",
        "Challenge_closed_time":1634774742763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634739305317,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/597612\/(azure)(ml)(python-sdk)(environment)(docker)-docke",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.843735,
        "Challenge_title":"[Azure][ML][Python SDK][Environment][Docker] Docker copy missing context",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Docker context is not supported with AzureML Python SDK at the moment. Context support will added later this year<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":0.6922330556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Trying to understand <a href=\"https:\/\/dvc.org\/doc\/start\" rel=\"nofollow noreferrer\">dvc<\/a>, most tutorials mention generation of dvc.yaml by running <code>dvc run<\/code> command.<\/p>\n<p>But at the same time, dvc.yaml which defines the DAG is also <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files\" rel=\"nofollow noreferrer\">well documented<\/a>. Also the fact that it is a yaml format and human readable\/writable would point to the fact that it is meant to be a DSL for specifying your data pipeline.<\/p>\n<p>Can somebody clarify which is the better practice?\nWriting the dvc.yaml or let it be generated by <code>dvc run<\/code> command?\nOr is it left to user's choice and there is no technical difference?<\/p>",
        "Challenge_closed_time":1623859205940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623853195940,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1623873485368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68004538",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.6694444444,
        "Challenge_title":"Is dvc.yaml supposed to be written or generated by dvc run command?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1101.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1243446134992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gothenburg, Sweden",
        "Poster_reputation_count":1547.0,
        "Poster_view_count":212.0,
        "Solution_body":"<p>I'd recommend manual editing as the main route! (I believe that's officially recommended since <a href=\"https:\/\/dvc.org\/blog\/dvc-2-0-release\" rel=\"nofollow noreferrer\">DVC 2.0<\/a>)<\/p>\n<p><code>dvc stage add<\/code> can still be very helpful for programmatic generation of pipelines files, but it doesn't support all the features of <code>dvc.yaml<\/code>, for example setting <code>vars<\/code> values or defining <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#foreach-stages\" rel=\"nofollow noreferrer\"><code>foreach<\/code> stages<\/a>.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1623875977407,
        "Solution_link_count":2.0,
        "Solution_readability":15.1,
        "Solution_reading_time":7.55,
        "Solution_score_count":4.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":54.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1032.455,
        "Challenge_answer_count":2,
        "Challenge_body":"Since we changed the vscode-azureml-remote extension to be of UI type it is not supported anymore in the web or codespaces.\r\n\r\nGiven that main extension depends on vscode-azureml-remote, main is also unavailable in the web or codespaces.\r\n\r\nChanging the dependency should enable the main extension in the web context again.",
        "Challenge_closed_time":1665527881000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661811043000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1655",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.0,
        "Challenge_reading_time":4.74,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1032.455,
        "Challenge_title":"Can't use Azure ML features when remotely connected to a compute",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Seems to work when remotely connected via VS Code desktop, but it definitely doesn't work when connected via vscode.dev. The azure ml remote extension is disabled in this case. Seems like we should be able to support this. Yes, this is only on web platforms like vscode.dev or codespaces.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":3.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1551701850312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Poland, Cracow",
        "Answerer_reputation_count":11273.0,
        "Answerer_view_count":1888.0,
        "Challenge_adjusted_solved_time":340.1792388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create a python utility that will take dataset from vertex ai datasets and will generate statistics for that dataset. But I am unable to check the dataset using jupyter notebook. Is there any way out for this?<\/p>",
        "Challenge_closed_time":1631634873007,
        "Challenge_comment_count":3,
        "Challenge_created_time":1630410227747,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68998065",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":340.1792388889,
        "Challenge_title":"Read vertex ai datasets in jupyter notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":737.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1616070829583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>If I understand correctly, you want to use <a href=\"https:\/\/cloud.google.com\/vertex-ai\" rel=\"nofollow noreferrer\">Vertex AI<\/a> dataset inside <code>Jupyter Notebook<\/code>. I don't think that this is currently possible. You are able to export <code>Vertex AI<\/code> datasets to <code>Google Cloud Storage<\/code> in JSONL format:<\/p>\n<blockquote>\n<p>Your dataset will be exported as a list of text items in JSONL format. Each row contains a Cloud Storage path, any label(s) assigned to that item, and a flag that indicates whether that item is in the training, validation, or test set.<\/p>\n<\/blockquote>\n<p>At this moment, you can use <code>BigQuery<\/code> data inside <code>Notebook<\/code> using <code>%%bigquery<\/code> like it's mentioned in <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/visualize-jupyter\" rel=\"nofollow noreferrer\">Visualizing BigQuery data in a Jupyter notebook.<\/a> or use <code>csv_read()<\/code> from machine directory or <code>GCS<\/code> like it's showed in the <a href=\"https:\/\/stackoverflow.com\/questions\/61956470\/\">How to read csv file in Google Cloud Platform jupyter notebook<\/a> thread.<\/p>\n<p>However, you can fill a <code>Feature Request<\/code> in <a href=\"https:\/\/developers.google.com\/issue-tracker\" rel=\"nofollow noreferrer\">Google Issue Tracker<\/a> to add the possibility to use <code>VertexAI<\/code> dataset directly in the <code>Jupyter Notebook<\/code> which will be considered by the <code>Google Vertex AI Team<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.8,
        "Solution_reading_time":19.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":174.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.5253416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Anaconda announced that commercial users should purchase the licenses on APR, 20, 2020.  <br \/>\nHowever, Azure Machine Learning heavily depends on this anaconda packages; developing models on computing instance and deploy container environment.  <\/p>\n<p>Do commercial developers have to pay to anaconda to continue usage of Azure Machine Learning with anaconda?  <\/p>",
        "Challenge_closed_time":1605602646227,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605597154997,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/165312\/anaconda-commercial-use-on-azure-machine-learning",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":5.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.5253416667,
        "Challenge_title":"Anaconda commercial use on Azure Machine Learning",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=55ef7a9f-c4d6-4e64-b536-02ebed405538\">@Eisuke Yonezawa  <\/a> If you are have the commercial version of Anaconda you can configure the same for your experiments to deploy packages that are available under license but in most cases you can simply use conda to install available python packages without paying for the commercial license. There is no restriction to have a commercial license to continue using Azure Machine Learning with anaconda. I hope this helps!!<\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.4,
        "Solution_reading_time":6.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1538134316367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Ahmedabad, Gujarat, India",
        "Answerer_reputation_count":1009.0,
        "Answerer_view_count":102.0,
        "Challenge_adjusted_solved_time":0.7625188889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to dockerize <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">mlflow<\/a> with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP).<\/p>\n<p>Before deploying anything to GCP however, I wanted to get a local deployment working. I found <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">this<\/a> guide that details the process of setting up the environment. Having followed the guide (excluding the SQL part), I can see the  mlflow UI on <code>localhost:80<\/code> as nginx redirects traffic on port 80 to 5000. To add authentication, I found <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-set-up-password-authentication-with-nginx-on-ubuntu-14-04\" rel=\"nofollow noreferrer\">here<\/a> that I can do it using <code>sudo htpasswd -c .htpasswd &lt;username&gt;<\/code> in the <code>etc\/nginx\/<\/code> directory and then adding<\/p>\n<pre><code>location \\ {\n   auth_basic &quot;Private Property&quot;;\n   auth_basic_user_file .htpasswd;\n}\n<\/code><\/pre>\n<p>to the <code>nginx.conf<\/code> (or <code>mlflow.conf<\/code> in this case) to make it appear online. Trouble is, when I go to <code>localhost:80<\/code> <em>now<\/em> and enter in my username\/password, I continue to see<\/p>\n<pre><code>[error] 6#6: *1 open() &quot;\/etc\/nginx\/.htpasswd&quot; failed (2: No such file or directory)\n<\/code><\/pre>\n<p>in the <code>docker-compose up<\/code> logs as they are printed to the terminal, and as such <em>I'm not able to see the mlflow UI<\/em> on <code>localhost:80<\/code> (either a blank screen or nginx 403 error).<\/p>\n<p>Now, I've looked at several other posts (such as <a href=\"https:\/\/stackoverflow.com\/questions\/2010677\/nginx-and-auth-basic\">this one<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/16510374\/403-forbidden-nginx-using-correct-credentials\">this one<\/a>) and it seems to me that nginx doesn't have the right permissions to read the <code>.htpasswd<\/code> in the <code>etc\/nginx\/<\/code> directory file or that the path of the file isn't correct, i.e. the path has to be in reference to the <code>nginx.conf<\/code> file.<\/p>\n<p>Even though I made these corrections to the above towards-data-science files, the problem still persists.  I've been stuck for a while on this. Any particular reasons why this may be happening?<\/p>\n<p>Edit:\nHere is my directory structure in case it may help:<\/p>\n<pre><code>mlflow-docker\/:\n  mlflow\/:\n    Dockerfile\n  nginx\/:\n    Dockerfile\n    mlflow.conf\n    nginx.conf\n  docker-compose.yml\n<\/code><\/pre>",
        "Challenge_closed_time":1601618748112,
        "Challenge_comment_count":2,
        "Challenge_created_time":1601594496277,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1601616621328,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64164367",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":34.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":6.7366208333,
        "Challenge_title":"Nginx authentication issues when building mlflow through docker-compose",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":841.0,
        "Challenge_word_count":305,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524443788580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>You need to add the .htpasswd file inside your container's file system.<\/p>\n<p>Generate the password file in your project's nginx folder.<\/p>\n<pre><code>sudo htpasswd -c .htpasswd sammy\n<\/code><\/pre>\n<p>Copy the password file to the nginx container's directory. Add following line in nginx dockerfile.<\/p>\n<pre><code>COPY .htpasswd \/etc\/nginx\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1601619366396,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":4.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":46.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.4228347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Challenge_closed_time":1618568305232,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618566783027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1618585069787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":25.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.4228347222,
        "Challenge_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1216.0,
        "Challenge_word_count":251,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369257942212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":70285.0,
        "Poster_view_count":13121.0,
        "Solution_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":10.19,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1432655047272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":463.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":581.8209875,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have to do large scale feature engineering on some data. My current approach is to spin up an instance using <code>SKLearnProcessor<\/code> and then scale the job by choosing a larger instance size or increasing the number of instances. I require using some packages that are not installed on Sagemaker instances by default and so I want to install the packages using .whl files.<\/p>\n<p>Another hurdle is that the Sagemaker role does not have internet access.<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nsess = sagemaker.Session()\nsess.default_bucket()        \n\nregion = boto3.session.Session().region_name\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session = sess,\n                                     instance_type=&quot;ml.t3.medium&quot;,\n                                     instance_count=1)\n\nsklearn_processor.run(code='script.py')\n<\/code><\/pre>\n<p><strong>Attempted resolutions:<\/strong><\/p>\n<ol>\n<li>Upload the packages to a CodeCommit repository and clone the repo into the SKLearnProcessor instances. Failed with error <code>fatal: could not read Username for 'https:\/\/git-codecommit.eu-west-1.amazonaws.com': No such device or address<\/code>. I tried cloning the repo into a sagemaker notebook instance and it works, so its not a problem with my script.<\/li>\n<li>Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off <a href=\"https:\/\/medium.com\/@shadidc\/installing-custom-python-package-to-sagemaker-notebook-b7b897f4f655\" rel=\"nofollow noreferrer\">this post<\/a>. But the packages never get copied, and an error message is not thrown.<\/li>\n<li>Also looked into using the package <code>s3fs<\/code> but it didn't seem suitable to copy the wheel files.<\/li>\n<\/ol>\n<p><strong>Alternatives<\/strong><\/p>\n<p>My client is hesitant to spin up containers from custom docker images. Any alternatives?<\/p>",
        "Challenge_closed_time":1604006575952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601912020397,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64211755",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":25.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":581.8209875,
        "Challenge_title":"How to upload packages to an instance in a Processing step in Sagemaker?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1071.0,
        "Challenge_word_count":242,
        "Platform":"Stack Overflow",
        "Poster_created_time":1535553190827,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Johannesburg",
        "Poster_reputation_count":438.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p><code>2. Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off this post. But the packages never get copied, and an error message is not thrown.<\/code><\/p>\n<p>This approach seems sound.<\/p>\n<p>You may be better off overriding the <code>command<\/code> field on the <code>SKLearnProcessor<\/code> to <code>\/bin\/bash<\/code>, run a bash script like <code>install_and_run_my_python_code.sh<\/code> that installs the wheel containing your python dependencies, then runs your main python entry point script.<\/p>\n<p>Additionally, instead of using AWS S3 calls to download your code in a script, you could use a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ProcessingInput\" rel=\"nofollow noreferrer\">ProcessingInput<\/a> to download your code rather than doing this with AWS CLI calls in a bash script, which is what the <code>SKLearnProcessor<\/code> does to download your entry point <code>script.py<\/code> code across all the instances.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":13.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":96.0740822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Challenge_closed_time":1613774515323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613428648627,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.1,
        "Challenge_reading_time":6.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":96.0740822222,
        "Challenge_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":276.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585870038407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Christchurch, \u041d\u043e\u0432\u0430\u044f \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u044f",
        "Poster_reputation_count":45.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.5,
        "Solution_reading_time":13.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1554060427012,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2433.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":68.9593552778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been using a jupyter notebook instance to spin up a training job (on separate instance) and deploy the endpoint (on another instance). I am using sagemaker tensorflow APIs for this as shown below:<\/p>\n<pre><code># create Tensorflow object and provide and entry point script\ntf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',\n                      train_instance_count=1, train_instance_type='ml.p2.xlarge',\n                      framework_version='1.12', py_version='py3')\n\n# train model on data on s3 and save model artifacts to s3\ntf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n\n# deploy model on another instance using checkpoints saved on S3\npredictor = estimator.deploy(initial_instance_count=1,\n                         instance_type='ml.c5.xlarge',\n                         endpoint_type='tensorflow-serving')\n<\/code><\/pre>\n<p>I have been doing all of these steps through a jupyter notebook instance. What AWS services I can use to get rid off the dependency of jupyter notebook instance and automate these tasks of training and deploying the model in serverless fashion?<\/p>",
        "Challenge_closed_time":1595624761856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595377179513,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63024900",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":14.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":68.7728730556,
        "Challenge_title":"How to train and deploy model in script mode on Sagemaker without using jupyter notebook instance (serverless)?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":758.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378268842847,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, India",
        "Poster_reputation_count":4616.0,
        "Poster_view_count":592.0,
        "Solution_body":"<p>I recommend <code>AWS Step Functions<\/code>.  Been using it to schedule <code>SageMaker Batch Transform<\/code> and preprocessing jobs since it integrates with <code>CloudWatch<\/code> event rules.  It can also train models, perform hpo tuning, and integrates with <code>lambda<\/code>.  There is a SageMaker\/Step Functions SDK as well as you can use Step Functions directly by creating state machines. Some examples and documentation:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/<\/a><\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1595625433192,
        "Solution_link_count":4.0,
        "Solution_readability":21.9,
        "Solution_reading_time":12.5,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1489644560420,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Planet Earth",
        "Answerer_reputation_count":791.0,
        "Answerer_view_count":253.0,
        "Challenge_adjusted_solved_time":3.8482858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to install python package cassandra driver in Azure Machine Learning studio. I am following this answer from <a href=\"https:\/\/stackoverflow.com\/questions\/44371692\/install-python-packages-in-azure-ml\">here<\/a>. Unfortunately i don't see any wheel file for cassandra-driver <a href=\"https:\/\/pypi.python.org\/pypi\/cassandra-driver\/\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/cassandra-driver\/<\/a> so i downloaded the .tar file and converted to zip.<\/p>\n<p>I included this .zip file as dataset and connected to python script<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" alt=\"jpg1\" \/><\/a><\/p>\n<p>But when i run it, it says No module named cassandra\n<a href=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" alt=\"jpg2\" \/><\/a><\/p>\n<p>Does this work only with wheel file? Any solution is much appreciated.<\/p>\n<p>I am using Python Version :  Anoconda 4.0\/Python 3.5<\/p>",
        "Challenge_closed_time":1519124227916,
        "Challenge_comment_count":2,
        "Challenge_created_time":1519110374087,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48879595",
        "Challenge_link_count":7,
        "Challenge_participation_count":3,
        "Challenge_readability":10.9,
        "Challenge_reading_time":14.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":3.8482858333,
        "Challenge_title":"ImportError: No module named cassandra in Azure Machine Learning Studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":500.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489644560420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Planet Earth",
        "Poster_reputation_count":791.0,
        "Poster_view_count":253.0,
        "Solution_body":"<p>I got it working. Changed the folder inside .zip file to <code>\"cassandra\"<\/code> (just like cassandra package). <\/p>\n\n<p>And in the Python script, i added <\/p>\n\n<pre><code>from cassandra import *\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":29.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1331550180963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Spain",
        "Answerer_reputation_count":1697.0,
        "Answerer_view_count":97.0,
        "Challenge_adjusted_solved_time":0.7546861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When creating a notebook instance via GCP console, I can check a box &quot;Turn on Secure Boot&quot;. Is it possible to turn on Secure Boot when creating the notebook using gcloud command?<\/p>",
        "Challenge_closed_time":1664037589950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664034657440,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1664034873080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73838593",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":2.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.8145861111,
        "Challenge_title":"CGP Vertex AI notebook - turn on Secure Boot",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":13.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1592856629630,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>It's a little involved, but <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/shielded-vm\" rel=\"nofollow noreferrer\">yes you can<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.2,
        "Solution_reading_time":2.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1317052342823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Franklin, TN",
        "Answerer_reputation_count":8183.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":0.9585202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When trying to upload a custom R module to Azure Machine Learning Studio what causes the following error.<\/p>\n\n<blockquote>\n  <p>[ModuleOutput]<\/p>\n<\/blockquote>\n\n<pre><code>\"ErrorId\":\"BuildCustomModuleFailed\",\"ErrorCode\":\"0114\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 0114: Custom module build failed with error(s): An item with the same key has already been added.\"}} [ModuleOutput] Error: Error 0114: Custom module build failed with error(s): An item with the same key has already been added. \n<\/code><\/pre>\n\n<p>I have tried renaming the module so a name that does not exists.<\/p>",
        "Challenge_closed_time":1496847821796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496847821797,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44416344",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":9.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio Custom Module Upload Error 0114 : An item with the same key has already been added",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":75.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1317052342823,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Franklin, TN",
        "Poster_reputation_count":8183.0,
        "Poster_view_count":727.0,
        "Solution_body":"<p>The duplicate key exception is a red herring. <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn962112.aspx\" rel=\"nofollow noreferrer\" title=\"MSDN Module Error Code 0114\">Build error 0114<\/a> is a general error that occurs if there is a system exception while building the custom module. The real issue my module was compressed using the built in compress folder option in the Mac Finder. To fix this compress the file using the command line interface for <code>zip<\/code> in Terminal in the following very specific manner.<\/p>\n\n<blockquote>\n  <p>The following example:<\/p>\n<\/blockquote>\n\n<pre><code>cd ScoredDatasetMetadata\/\nzip ScoredDatasetMetadata *\nmv ScoredDatasetMetadata.zip ..\/\n<\/code><\/pre>\n\n<p>Builds a zip file with the correct file structure.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1496851272470,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1546431264350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Norway",
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":55.8339541667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are trying to develop an MLflow pipeline. We have our developing environment in a series of dockers (no local python environment &quot;whatsoever&quot;). This means that we have set up a docker container with MLflow and all requirements necessary to run pipelines. The issue we have is that when we write our MLflow project file we need to use &quot;docker_env&quot; to specify the environment. This figure illustrates what we want to achieve:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tLuw.jpg\" rel=\"nofollow noreferrer\">MLflow run dind<\/a><\/p>\n<p>MLflow inside the docker needs to access the docker daemon\/service so that it can either use the &quot;docker-image&quot; in the MLflow project file or pull it from docker hub. We are aware of the possibility of using &quot;conda_env&quot; in the MLflow project file but wish to avoid this.<\/p>\n<p>Our question is,<\/p>\n<p>Do we need to set some sort of &quot;docker in docker&quot; solution to achieve our goal?<\/p>\n<p>Is it possible to set up the docker container in which MLflow is running so that it can access the &quot;host machine&quot; docker daemon?<\/p>\n<p>I have been all over Google and MLflow's documentation but I can seem to find anything that can guide us. Thanks a lot in advance for any help or pointers!<\/p>",
        "Challenge_closed_time":1640385689187,
        "Challenge_comment_count":4,
        "Challenge_created_time":1640160788793,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1640184686952,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70445997",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":9.0,
        "Challenge_reading_time":17.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":62.4723316667,
        "Challenge_title":"MLflow run within a docker container - Running with \"docker_env\" in MLflow project file",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":779.0,
        "Challenge_word_count":211,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546431264350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I managed to create my pipeline using docker and docker_env in MLflow. It is not necessary to run d-in-d, the &quot;sibling approach&quot; is sufficient. This approach is described here:<\/p>\n<p><a href=\"https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/\" rel=\"nofollow noreferrer\">https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/<\/a><\/p>\n<p>and it is the preferred method to avoid d-in-d.<\/p>\n<p>One needs to be very careful when mounting volumes within the primary and secondary docker environments: all volume mounts happen in the host machine.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.87,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1587281590603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":473.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":84.1130386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<h2>The problem<\/h2>\n<p>I have a jupyter notebook with an <code>ipydatagrid<\/code> widget that displays a dataframe. This notebook works correctly when run locally, but not when run in AWS SageMaker Studio. When run in SageMaker Studio, instead of showing the widget it simply shows the text <code>Loading widget...<\/code><\/p>\n<p>How does one use a <code>ipydatagrid<\/code> widget in the SageMaker Studio environment?<\/p>\n<h2>Details<\/h2>\n<p>Python version:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ python --version\nPython 3.7.10\n<\/code><\/pre>\n<p>Run at start:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip install -r requirements.txt\n$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n$ jupyter nbextension install --py --symlink --sys-prefix ipydatagrid\n$ jupyter nbextension enable --py --sys-prefix ipydatagrid\n<\/code><\/pre>\n<p>File <code>requirements.txt<\/code>:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>ipydatagrid==1.1.11\npandas==1.0.1\n<\/code><\/pre>\n<p>Notebook contents:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># %%\nimport pandas as pd\nfrom ipydatagrid import DataGrid\nfrom IPython.display import display\nimport ipywidgets\n\n# %%\ndata = [\n    (&quot;potato&quot;, 1.2, True),\n    (&quot;sweet potato&quot;, 0.8, False),\n    (&quot;french fries&quot;, 4.5, True),\n    (&quot;waffle fries&quot;, 4.9, True)\n]\ndf = pd.DataFrame(\n    data,\n    columns=[&quot;food&quot;, &quot;stars&quot;, &quot;is_available&quot;]\n)\n\n# %%\ngrid = DataGrid(df)\n\n# %%\ndisplay(grid)\n\n# %%\n# SANITY CHECK:\nbutton = ipywidgets.Button(\n    description=&quot;Button&quot;,\n    disabled=False\n)\ndef on_click(b):\n    print(&quot;CLICK&quot;)\nbutton.on_click(on_click)\ndisplay(button)\n<\/code><\/pre>\n<h3>Error messages<\/h3>\n<p>If I use the Google Chrome developer tools, I can see more logs in the browser that give some error messages, most of which are repeated:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>manager.js:305 Uncaught (in promise) Error: Module ipydatagrid, semver range ^1.1.11 is not registered as a widget module\n    at C.loadClass (manager.js:305:19)\n    at C.&lt;anonymous&gt; (manager-base.js:263:46)\n    at l (manager-base.js:44:23)\n    at Object.next (manager-base.js:25:53)\n    at manager-base.js:19:71\n    at new Promise (&lt;anonymous&gt;)\n    at Rtm6.k (manager-base.js:15:12)\n    at C.e._make_model (manager-base.js:257:16)\n    at C.&lt;anonymous&gt; (manager-base.js:246:45)\n    at l (manager-base.js:44:23)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>utils.js:119 Error: Could not create a model.\n    at n (utils.js:119:27)\n    at async C._handleCommOpen (manager.js:61:51)\n    at async v._handleCommOpen (default.js:994:100)\n    at async v._handleMessage (default.js:1100:43)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>manager-base.js:273 Could not instantiate widget\n<\/code><\/pre>\n<p>However, there is no overt error message that's immediately obvious to the user, including in the log where <code>print<\/code> statements send their output.<\/p>",
        "Challenge_closed_time":1648549758096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648246951157,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71623732",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":39.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":84.1130386111,
        "Challenge_title":"ipydatagrid widget does not display in SageMaker Studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1560606498248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Provo, UT, USA",
        "Poster_reputation_count":528.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab<\/em>), and per the <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid#Installation\" rel=\"nofollow noreferrer\">ipydatagrid installation instructions<\/a>, current\/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.<\/p>\n<p>I had a quick look at the past releases of <code>ipydatagrid<\/code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/0.2.16\" rel=\"nofollow noreferrer\">v0.2.16<\/a> and <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/1.0.1\" rel=\"nofollow noreferrer\">v1.0.1<\/a> (which are adjacent on GitHub).<\/p>\n<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package<\/code> and also note that versions &lt;1.0 don't seem to be present <a href=\"https:\/\/libraries.io\/pypi\/ipydatagrid\/versions\" rel=\"nofollow noreferrer\">on PyPI<\/a>.<\/p>\n<p>So unfortunately I think (unless\/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.3,
        "Solution_reading_time":18.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7837736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Would like to upload Jupyter notebooks from different sources like GitHub into my workspace either directly or through my local machine (download locally first and then upload) but I would like to do it programmatically. Either with the AzureML SDK or azure cli  <\/p>",
        "Challenge_closed_time":1651104442392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651101620807,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/829311\/how-could-i-upload-notebooks-to-my-azureml-workspa",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.7837736111,
        "Challenge_title":"How could I upload notebooks to my AzureML workspace programatically",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. You can use compute instance <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal\">terminal<\/a> in AML notebooks to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/samples-notebooks#get-samples-on-azure-machine-learning-compute-instance\">clone<\/a> the GitHub repo. There's currently no option to upload notebooks to your workspace programmatically using sdk or cli.<\/p>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":7.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1375058329287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":0.1746386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a project with DVC (Data Version Control), when I push files in my remote storage, the name of the files are changed. How I can conserve the names?<\/p>",
        "Challenge_closed_time":1620171651067,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620170453717,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1620171022368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67393339",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.9,
        "Challenge_reading_time":2.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.3325972222,
        "Challenge_title":"dvc push, change the names of files on the remote storage",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":447.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1379685720448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>Short answer: there is no way to do that.<\/p>\n<p>Long answer:\nDvc remote is a content-based storage, so names are not preserved. Dvc creates metafiles (*.dvc files) in your workspace that contain names and those files are usually tracked by git, so you need to use git remote and dvc remote together to have both filenames and their contents. Here is a more detailed explanation about the format of local and remote storage <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/internal-files#structure-of-the-cache-directory<\/a> . Also, checkout <a href=\"https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/sharing-data-and-model-files<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.5,
        "Solution_reading_time":10.87,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":81.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":0.3643536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to AWS and I'm trying out AWS Sagemaker. I'm currently doing my project which involves quite a long time to finish and I don't think I can finish it in a day. I'm worried if I close my JupyterLab of my notebook instance in SageMaker, my code will be gone. How do I save my code and cell run progress when using Sagemaker?<\/p>",
        "Challenge_closed_time":1619085494700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619084183027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67210677",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.8,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3643536111,
        "Challenge_title":"If I close my JupyterLab from notebook instance, would my code be gone?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605938672327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Poster_reputation_count":97.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>If you are training directly in the notebook the answer is yes.\nHowever the best practice is not to train directly with the notebook.\nUse instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!\nAs a quick example.. in my notebook I have this cell:<\/p>\n<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :\n0.01}\n\nmetrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n\ninputs = {'train': train_data, 'test': validation_data}\n\nmyJobName = 'myname'\n\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n<\/code><\/pre>\n<p>My training script training.py is something like this:<\/p>\n<pre><code>if __name__ =='__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # input data and model directories\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--learning-rate', type=float, default=0.0001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--epochs', type=int, default=1)\n....\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":32.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":227.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":193.1908333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Execution get stuck if this case happens. It is necessary to manage this exception properly.",
        "Challenge_closed_time":1639743100000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639047613000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ugr-sail\/sinergym\/issues\/101",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":255.0,
        "Challenge_repo_star_count":40.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":193.1908333333,
        "Challenge_title":"Experiments with mlflow don't work if MLFLOW_TRACKING_URI container variable specify an incorrect ip address",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":25.7031369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I attempt to Upgrade <strong>Pandas<\/strong> above version <strong>1.1.5<\/strong> on my <strong>AWS Sagemaker<\/strong> provided <strong>JupyterLab<\/strong> notebook I receive the error <strong>No Matching Distribution Found<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\n!{sys.executable} -m pip install --pre --upgrade pandas==1.3.5\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>ERROR: Could not find a version that satisfies the requirement pandas==1.3.5 (from versions: 0.1, 0.2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.25.2, 0.25.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5)\nERROR: No matching distribution found for pandas==1.3.5\n<\/code><\/pre>\n<h2>Background<\/h2>\n<p>I created a Notebook instance from the AWS Console via <strong>AWS Sagemaker -&gt; Notebook instances -&gt; Create Notebook instance<\/strong>.<\/p>\n<p>I then selected the Kernel <strong>conda_Python3<\/strong>.<\/p>\n<p>I use <strong>sys.executable<\/strong> to show the Kernel's Python, Pip and Pandas version.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -version\nPython 3.6.13\n\n!{sys.executable} -m pip show pip\nName: pip\nVersion: 21.3.1\nSummary: The PyPA recommended tool for installing Python packages.\nHome-page: https:\/\/pip.pypa.io\/\nAuthor: The pip developers\nAuthor-email: distutils-sig@python.org\nLicense: MIT\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: \nRequired-by: \n\n!{sys.executable} -m pip show pandas\nName: pandas\nVersion: 1.1.5\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: https:\/\/pandas.pydata.org\nAuthor: \nAuthor-email: \nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: numpy, python-dateutil, pytz\nRequired-by: autovizwidget, awswrangler, hdijupyterutils, odo, sagemaker, seaborn, shap, smclarify, sparkmagic, statsmodels\n<\/code><\/pre>\n<p>I cannot upgrade <strong>Pandas<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -m pip install --pre --upgrade pandas\nRequirement already satisfied: pandas in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (1.1.5)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy&gt;=1.15.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (1.18.5)\nRequirement already satisfied: six&gt;=1.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0)\n<\/code><\/pre>",
        "Challenge_closed_time":1654275686200,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654183154907,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72478572",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":43.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":25.7031369444,
        "Challenge_title":"Amazon Sagemaker JupiterLab Notebook - No matching distribution found for Pandas",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":315,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504724308867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington D.C., DC, United States",
        "Poster_reputation_count":97.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>see this - <a href=\"https:\/\/stackoverflow.com\/questions\/68750375\/no-matching-distribution-found-for-pandas-1-3-1\">No matching distribution found for pandas==1.3.1<\/a><\/p>\n<p>The latest version to support python 3.6 is 1.1.5.<\/p>\n<p>You can create a new conda environment with python version &gt;= 3.7 in your existing notebook, or move to notebooks with Amazon Linux 2 (see <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebook-instance-now-supports-amazon-linux-2\/\" rel=\"nofollow noreferrer\">blog post<\/a>). In the AL2 notebooks, <code>conda_python3<\/code> kernels come with Python 3.8.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":8.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1326780552283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":17.3923927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a prediction web service to Azure using ML Workbench process using cluster mode in this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally<\/a>)<\/p>\n\n<p>The model gets sent to the manifest, the scoring script and schema <\/p>\n\n<blockquote>\n  <p>Creating\n  service..........................................................Error\n  occurred: {'Error': {'Code': 'KubernetesDeploymentFailed', 'Details':\n  [{'Message': 'Back-off 40s restarting failed container=...pod=...',\n  'Code': 'CrashLoopBackOff'}], 'StatusCode': 400, 'Message':\n  'Kubernetes Deployment failed'}, 'OperationType': 'Service',\n  'State':'Failed', 'Id': '...', 'ResourceLocation':\n  '\/api\/subscriptions\/...', 'CreatedTime':\n  '2017-10-26T20:30:49.77362Z','EndTime': '2017-10-26T20:36:40.186369Z'}<\/p>\n<\/blockquote>\n\n<p>Here is the result of checking the ml service realtime logs <\/p>\n\n<pre><code>C:\\Users\\userguy\\Documents\\azure_ml_workbench\\projecto&gt;az ml service logs realtime -i projecto\n2017-10-26 20:47:16,118 CRIT Supervisor running as root (no user in config file)\n2017-10-26 20:47:16,120 INFO supervisord started with pid 1\n2017-10-26 20:47:17,123 INFO spawned: 'rsyslog' with pid 9\n2017-10-26 20:47:17,124 INFO spawned: 'program_exit' with pid 10\n2017-10-26 20:47:17,124 INFO spawned: 'nginx' with pid 11\n2017-10-26 20:47:17,125 INFO spawned: 'gunicorn' with pid 12\n2017-10-26 20:47:18,160 INFO success: rsyslog entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:18,160 INFO success: program_exit entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:22,164 INFO success: nginx entered RUNNING state, process has stayed up for &gt; than 5 seconds (startsecs)\n2017-10-26T20:47:22.519159Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting gunicorn 19.6.0\n2017-10-26T20:47:22.520097Z, INFO, 00000000-0000-0000-0000-000000000000, , Listening at: http:\/\/127.0.0.1:9090 (12)\n2017-10-26T20:47:22.520375Z, INFO, 00000000-0000-0000-0000-000000000000, , Using worker: sync\n2017-10-26T20:47:22.521757Z, INFO, 00000000-0000-0000-0000-000000000000, , worker timeout is set to 300\n2017-10-26T20:47:22.522646Z, INFO, 00000000-0000-0000-0000-000000000000, , Booting worker with pid: 22\n2017-10-26 20:47:27,669 WARN received SIGTERM indicating exit request\n2017-10-26 20:47:27,669 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26T20:47:27.669556Z, INFO, 00000000-0000-0000-0000-000000000000, , Handling signal: term\n2017-10-26 20:47:30,673 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:33,675 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\nInitializing logger\n2017-10-26T20:47:36.564469Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insights client\n2017-10-26T20:47:36.564991Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up request id generator\n2017-10-26T20:47:36.565316Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insight hooks\n2017-10-26T20:47:36.565642Z, INFO, 00000000-0000-0000-0000-000000000000, , Invoking user's init function\n2017-10-26 20:47:36.715933: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36,716 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:36.716376: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716542: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructio\nns, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716703: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructi\nons, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716860: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructio\nns, but these are available on your machine and could speed up CPU computations.\nthis is the init\n2017-10-26T20:47:37.551940Z, INFO, 00000000-0000-0000-0000-000000000000, , Users's init has completed successfully\nUsing TensorFlow backend.\n2017-10-26T20:47:37.553751Z, INFO, 00000000-0000-0000-0000-000000000000, , Worker exiting (pid: 22)\n2017-10-26T20:47:37.885303Z, INFO, 00000000-0000-0000-0000-000000000000, , Shutting down: Master\n2017-10-26 20:47:37,885 WARN killing 'gunicorn' (12) with SIGKILL\n2017-10-26 20:47:37,886 INFO stopped: gunicorn (terminated by SIGKILL)\n2017-10-26 20:47:37,889 INFO stopped: nginx (exit status 0)\n2017-10-26 20:47:37,890 INFO stopped: program_exit (terminated by SIGTERM)\n2017-10-26 20:47:37,891 INFO stopped: rsyslog (exit status 0)\n\nReceived 41 lines of log\n<\/code><\/pre>\n\n<p>My best guess is theres something silent happening to cause \"WARN received SIGTERM indicating exit request\". The rest of the scoring.py script seems to kick off - see tensorflow get initiated and the \"this is the init\" print statement.<\/p>\n\n<p><a href=\"http:\/\/127.0.0.1:63437\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:63437<\/a> is accessible from my local machine, but the ui endpoint is blank.<\/p>\n\n<p>Any ideas on how to get this up and running in an Azure cluster? I'm not very familiar with how Kubernetes works, so any basic debugging guidance would be appreciated.<\/p>",
        "Challenge_closed_time":1509114740967,
        "Challenge_comment_count":2,
        "Challenge_created_time":1509052128353,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1511310430992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46963846",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":77.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":17.3923927778,
        "Challenge_title":"Azure ML Workbench Kubernetes Deployment Failed",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":447.0,
        "Challenge_word_count":620,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421081882987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":588.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>We discovered a bug in our system that could have caused this. The fix was deployed last night. Can you please try again and let us know if you still encounter this issue?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.14,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1545360696800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Earth",
        "Answerer_reputation_count":1011.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":6002.4414652778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I started with SageMaker recently, and I'm loving it. However, I've been installing the same libraries over and over again to one of the in-built conda environments, and I want to create a life cycle configuration to do that automatically on startup. based on <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">the bottom of this<\/a>:<\/p>\n<blockquote>\n<p>notebook instance lifecycle configurations are available when you create a new notebook instance.<\/p>\n<\/blockquote>\n<p>the trouble is, I already have a notebook I've been working in for a while. Is there any way to apply a life cycle configuration on startup to an already existing notebook?<\/p>",
        "Challenge_closed_time":1597108074440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597107545867,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597107925232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63350039",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.1468258333,
        "Challenge_title":"Add lifecycle configuration to existing notebook in SageMaker?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":395.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>You need to shut the instance down, then you can edit it. Then, if you use your eyes (which I neglected to do) you can see the &quot;Additional Configurations&quot; section contains lifecycle configurations<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1618716714507,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1576086206363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Buffalo, NY, USA",
        "Answerer_reputation_count":458.0,
        "Answerer_view_count":120.0,
        "Challenge_adjusted_solved_time":2.1269572222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I accidentally deleted a jupyter notebook file on my AWS Sagemaker instance. I wonder if there's anyway to restore\/recover the file?<\/p>",
        "Challenge_closed_time":1602399891896,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602392234850,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64300737",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":9.0,
        "Challenge_reading_time":2.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2.1269572222,
        "Challenge_title":"Recover deleted notebook on AWS Sagemaker",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":3501.0,
        "Challenge_word_count":26,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403038629316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Well, first check <code>{notebook_directory}\/.ipynb_checkpoints\/<\/code> for a snapshot.. the directory should contain copies of your notebooks from the point in time when they were last saved.<\/p>\n<p>If that's gone, the IPython kernel saves commands issued to it in a sqlite db. found in <code>~\/.ipython\/profile_default<\/code>. There you should find <em><strong>history.sqlite<\/strong><\/em>.<\/p>\n<p>history.sqlite should contain all commands issued to the ipython kernel. Tables of interest are <strong>sessions<\/strong> and <strong>history<\/strong>.<\/p>\n<p>You need to query the sessions table by session start or end time to determine which sessions are related to the ones you're looking for:<\/p>\n<pre><code>select \n    sessions.session, sessions.start, sessions.end, history.source\nfrom sessions \njoin history using (session)\nwhere\n    start &gt; 'your_start_date' and start &lt; 'your_end_date';\n<\/code><\/pre>\n<p>once you find your session:<\/p>\n<pre><code>select '# @@ Cell '|| line || char(10) || source || char(10) from history where session = your_session_number;\n<\/code><\/pre>\n<p>Output your query to .py then use jupytext to convert to ipynb, then you have your notebook back:<\/p>\n<pre><code>sqlite3 history.sqlite \\\n    &quot;select '# @@ Cell '|| line || char(10) || source || char(10)    from history where session = your_session_number;&quot; \\\n&gt; output.py\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":17.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":161.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":8283.79234,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?<\/p>\n\n<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"noreferrer\">SageMaker containers<\/a>, so my understanding is that I need to build my own.<\/p>\n\n<p>What is the best Base image to use? Example Dockerfile?<\/p>",
        "Challenge_closed_time":1567635751827,
        "Challenge_comment_count":1,
        "Challenge_created_time":1565186451297,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1565186790803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57396212",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":13.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":680.3612583334,
        "Challenge_title":"SageMaker and TensorFlow 2.0",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":4136.0,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446859510543,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":3259.0,
        "Poster_view_count":233.0,
        "Solution_body":"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.<\/strong><\/p>\n<ul>\n<li>SageMaker + TensorFlow docs: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html<\/a><\/li>\n<li>Supported Tensorflow versions (and Docker URIs): <a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images<\/a><\/li>\n<\/ul>\n<hr \/>\n<p><em>Original answer<\/em><\/p>\n<p>Here is an example Dockerfile that uses <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">the underlying SageMaker Containers library<\/a> (this is what is used in the official pre-built Docker images):<\/p>\n<pre><code>FROM tensorflow\/tensorflow:2.0.0b1\n\nRUN pip install sagemaker-containers\n\n# Copies the training code inside the container\nCOPY train.py \/opt\/ml\/code\/train.py\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>For more information on this approach, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html<\/a><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1595008443227,
        "Solution_link_count":7.0,
        "Solution_readability":25.0,
        "Solution_reading_time":19.1,
        "Solution_score_count":10.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":94.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1401187183867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sweden",
        "Answerer_reputation_count":1709.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":4.5600138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm setting up a conda environment on Windows 10 Pro x64 using Miniconda 4.5.12 and have done a pip install of azureml-sdk inside the environment but get a ModuleNotFoundError when attempting to execute the following code:<\/p>\n\n<pre><code>import azureml.core\nazureml.core.VERSION\n<\/code><\/pre>\n\n<p>This is the output:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\n  File \"D:\\Projects\\style-transfer\\azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\nModuleNotFoundError: No module named 'azureml.core'; 'azureml' is not a package\n<\/code><\/pre>\n\n<p>The code above has been run from the conda prompt, with the test environment active as well as in vscode with the same environment selected.<\/p>\n\n<p>I setup the conda environment as per the following:<\/p>\n\n<ol>\n<li>Created the conda environment <code>conda create -n test<\/code>.<\/li>\n<li>Activated the environment <code>activate test<\/code>.<\/li>\n<li>Installed pip <code>conda install pip<\/code>.<\/li>\n<li>Installed azureml-sdk <code>pip install azureml-sdk<\/code>.<\/li>\n<\/ol>\n\n<p>This results in the following packages being installed in the environment as per <code>conda list<\/code>:<\/p>\n\n<pre><code>adal                      1.2.0                     &lt;pip&gt;\nantlr4-python3-runtime    4.7.2                     &lt;pip&gt;\napplicationinsights       0.11.7                    &lt;pip&gt;\nargcomplete               1.9.4                     &lt;pip&gt;\nasn1crypto                0.24.0                    &lt;pip&gt;\nazure-cli-command-modules-nspkg 2.0.2                     &lt;pip&gt;\nazure-cli-core            2.0.54                    &lt;pip&gt;\nazure-cli-nspkg           3.0.3                     &lt;pip&gt;\nazure-cli-profile         2.1.2                     &lt;pip&gt;\nazure-cli-telemetry       1.0.0                     &lt;pip&gt;\nazure-common              1.1.16                    &lt;pip&gt;\nazure-graphrbac           0.53.0                    &lt;pip&gt;\nazure-mgmt-authorization  0.51.1                    &lt;pip&gt;\nazure-mgmt-containerregistry 2.5.0                     &lt;pip&gt;\nazure-mgmt-keyvault       1.1.0                     &lt;pip&gt;\nazure-mgmt-nspkg          3.0.2                     &lt;pip&gt;\nazure-mgmt-resource       2.0.0                     &lt;pip&gt;\nazure-mgmt-storage        3.1.0                     &lt;pip&gt;\nazure-nspkg               3.0.2                     &lt;pip&gt;\nazure-storage-blob        1.4.0                     &lt;pip&gt;\nazure-storage-common      1.4.0                     &lt;pip&gt;\nazure-storage-nspkg       3.1.0                     &lt;pip&gt;\nazureml-core              1.0.6                     &lt;pip&gt;\nazureml-pipeline          1.0.6                     &lt;pip&gt;\nazureml-pipeline-core     1.0.6                     &lt;pip&gt;\nazureml-pipeline-steps    1.0.6                     &lt;pip&gt;\nazureml-sdk               1.0.6                     &lt;pip&gt;\nazureml-telemetry         1.0.6                     &lt;pip&gt;\nazureml-train             1.0.6                     &lt;pip&gt;\nazureml-train-core        1.0.6                     &lt;pip&gt;\nazureml-train-restclients-hyperdrive 1.0.6                     &lt;pip&gt;\nbackports.tempfile        1.0                       &lt;pip&gt;\nbackports.weakref         1.0.post1                 &lt;pip&gt;\nbcrypt                    3.1.5                     &lt;pip&gt;\nca-certificates           2018.03.07                    0\ncertifi                   2018.11.29               py37_0\ncffi                      1.11.5                    &lt;pip&gt;\nchardet                   3.0.4                     &lt;pip&gt;\ncolorama                  0.4.1                     &lt;pip&gt;\ncontextlib2               0.5.5                     &lt;pip&gt;\ncryptography              2.4.2                     &lt;pip&gt;\ndocker                    3.6.0                     &lt;pip&gt;\ndocker-pycreds            0.4.0                     &lt;pip&gt;\nfutures                   3.1.1                     &lt;pip&gt;\nhumanfriendly             4.17                      &lt;pip&gt;\nidna                      2.8                       &lt;pip&gt;\nisodate                   0.6.0                     &lt;pip&gt;\njmespath                  0.9.3                     &lt;pip&gt;\njsonpickle                1.0                       &lt;pip&gt;\nknack                     0.5.1                     &lt;pip&gt;\nmsrest                    0.6.2                     &lt;pip&gt;\nmsrestazure               0.6.0                     &lt;pip&gt;\nndg-httpsclient           0.5.1                     &lt;pip&gt;\noauthlib                  2.1.0                     &lt;pip&gt;\nopenssl                   1.1.1a               he774522_0\nparamiko                  2.4.2                     &lt;pip&gt;\npathspec                  0.5.9                     &lt;pip&gt;\npip                       18.1                     py37_0\nportalocker               1.2.1                     &lt;pip&gt;\npyasn1                    0.4.4                     &lt;pip&gt;\npycparser                 2.19                      &lt;pip&gt;\nPygments                  2.3.1                     &lt;pip&gt;\nPyJWT                     1.7.1                     &lt;pip&gt;\nPyNaCl                    1.3.0                     &lt;pip&gt;\npyOpenSSL                 18.0.0                    &lt;pip&gt;\npypiwin32                 223                       &lt;pip&gt;\npyreadline                2.1                       &lt;pip&gt;\npython                    3.7.1                h8c8aaf0_6\npython-dateutil           2.7.5                     &lt;pip&gt;\npytz                      2018.7                    &lt;pip&gt;\npywin32                   224                       &lt;pip&gt;\nPyYAML                    3.13                      &lt;pip&gt;\nrequests                  2.21.0                    &lt;pip&gt;\nrequests-oauthlib         1.0.0                     &lt;pip&gt;\nruamel.yaml               0.15.51                   &lt;pip&gt;\nSecretStorage             2.3.1                     &lt;pip&gt;\nsetuptools                40.6.3                   py37_0\nsix                       1.12.0                    &lt;pip&gt;\nsqlite                    3.26.0               he774522_0\ntabulate                  0.8.2                     &lt;pip&gt;\nurllib3                   1.23                      &lt;pip&gt;\nvc                        14.1                 h0510ff6_4\nvs2015_runtime            14.15.26706          h3a45250_0\nwebsocket-client          0.54.0                    &lt;pip&gt;\nwheel                     0.32.3                   py37_0\nwheel                     0.30.0                    &lt;pip&gt;\nwincertstore              0.2                      py37_0\n<\/code><\/pre>\n\n<p>If I run <code>which pip<\/code>, I get the following output, which confirms that I used the pip inside the environment to install azureml-sdk, I think:<\/p>\n\n<pre><code>\/c\/Users\/allan\/Miniconda3\/envs\/test\/Scripts\/pip\n<\/code><\/pre>\n\n<p>I can also see that the azureml packages do in fact exist within the environment folder structure.<\/p>",
        "Challenge_closed_time":1545633498167,
        "Challenge_comment_count":3,
        "Challenge_created_time":1545617082117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53908529",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.5,
        "Challenge_reading_time":56.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":99,
        "Challenge_solved_time":4.5600138889,
        "Challenge_title":"How to fix ModuleNotFoundError in azureml-sdk when installed inside conda environment",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":6343.0,
        "Challenge_word_count":466,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460456204196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Australia",
        "Poster_reputation_count":140.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>It's probably because the name if your python file is the same as a module name you are trying import. In this case, rename the file to something other than <code>azureml.py<\/code>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.8,
        "Solution_reading_time":2.32,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.724685,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I need some help trying to understand why I can't see any GIT options (left panel and top selection drop down menu) in my Azure machine learning JupyterLab.<\/p>\n<p>I did the following steps:<\/p>\n<pre><code> jupyter labextension install @jupyterlab\/git\n pip install --upgrade jupyterlab-git\n jupyter serverextension enable --py jupyterlab_git\n jupyter lab build\n<\/code><\/pre>\n<p>I've restarted my jupyterLab a couple of times, if I check the command:<\/p>\n<pre><code> jupyter labextension list\n<\/code><\/pre>\n<p>I get that @jupyterlab\/git v0,20,0 is enabled and ok.  <br \/>\nWhat am I doing wrong?<\/p>\n<p>Thank you in advance,  <br \/>\nCarla<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/12015-issue1.png?platform=QnA\" alt=\"12015-issue1.png\" \/><\/p>",
        "Challenge_closed_time":1594762360103,
        "Challenge_comment_count":5,
        "Challenge_created_time":1594716551237,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/46614\/azure-machine-learning-and-jupyterlab-git-extensio",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":10.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":12.724685,
        "Challenge_title":"Azure Machine Learning and jupyterlab git extension not working",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a>@ramr-msft<\/a> ,<\/p>\n<p>I did the steps mention in the link you gave me (<a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab-git\">https:\/\/github.com\/jupyterlab\/jupyterlab-git<\/a>) but still I can't open the Git extension from the Git tab on the left panel because it still doesn't exists.<\/p>\n<p>You mentioned we can still manage git repositories using the command line. Do you have any useful documentation on this approach?<\/p>\n<p>Once again, thank you in advance.  <br \/>\nCarla<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":6.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1327302732867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"USA",
        "Answerer_reputation_count":19711.0,
        "Answerer_view_count":1030.0,
        "Challenge_adjusted_solved_time":0.1674261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Challenge_closed_time":1597366468132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597366468133,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.3,
        "Challenge_reading_time":56.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1174.0,
        "Challenge_word_count":293,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327302732867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"USA",
        "Poster_reputation_count":19711.0,
        "Poster_view_count":1030.0,
        "Solution_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1597367070867,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":32.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":235.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1622632545867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":1.2254397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a <strong>Jupyter Lab<\/strong> instance on <strong>AWS SageMaker<\/strong>.<\/p>\n<p>Kernel: <code>conda_mxnet_latest_p37<\/code>.<\/p>\n<p><code>url_lib<\/code> contains some false urls, that I exception handle.<\/p>\n<pre><code>['15', '259', '26', '58', 'https:\/\/imagepool.1und1-drillisch.de\/v2\/download\/nachhaltigkeitsbericht\/1&amp;1Drillisch_Sustainability_Report_EN_2018.pdf', 'https:\/\/imagepool.1und1-drillisch.de\/\/v2\/download\/nachhaltigkeitsbericht\/2018-04-06_1und1-Drillisch_Sustainability_Report_eng.pdf', '6', 'http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf', '80', 'https:\/\/multimedia.3m.com\/mws\/media\/1691941O\/2019-sustainability-report.PDF', 'https:\/\/s3-us-west-2.amazonaws.com\/ungc-production\/attachments\/cop_2020\/483648\/original\/GPIC_Sustainability_Report_2020__-_40_Years_of_Sustainable_Success.pdf?1583154650', 'https:\/\/drive.google.com\/open?id=1_dnBcfXWjexy9QoWRhOk_3gnOkWfYRCw', 'http:\/\/aepsustainability.com\/performance\/docs\/2020AEPGRIReport.pdf']  # sample\n<\/code><\/pre>\n<p>However, ones that are working URLs, throw this error:<\/p>\n<pre><code>[Errno 13] Permission denied: '\/data'\n<\/code><\/pre>\n<p>I don't have the directory opened, nor files since I they're not downloaded.<\/p>\n<p>I ran in <strong>Terminal<\/strong> without luck:<\/p>\n<pre><code>sh-4.2$ chmod 777 data\nsh-4.2$ chmod 777 data\/\nsh-4.2$ chmod 777 data\/gri\nsh-4.2$ chmod 777 data\/gri\/\n<\/code><\/pre>\n<p><strong>Code:<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport opendatasets as od\nimport urllib\nimport zipfile\nimport os\n\ncsr_df = pd.read_excel('data\/Company Sustainability Reports.xlsx', index_col=None)\nurl_list = csr_df['Report PDF Address'].tolist()\n\nfor url in url_list:\n    try:\n        download = od.download(url, '\/data\/gri\/')\n        filename = url.rsplit('\/', 1)[1]\n\n        path_extract = 'data\/gri\/' + filename\n        with zipfile.ZipFile('data\/gri\/' + filename + '.zip', 'r') as zip_ref:\n            zip_ref.extractall(path_extract)\n\n        os.remove(path_extract + 'readme.txt')\n\n        filenames = os.listdir(path_extract)\n        scans = []\n        for f in filenames:\n            with Image.open(path_extract + f) as img:\n                matrix = np.array(img)\n                scans.append(matrix)\n\n        # shutil.rmtree(path_extract)\n        os.remove(path_extract[:-1] + '.zip')\n\n    except (urllib.error.URLError, IOError, RuntimeError) as e:\n        print('Download PDFs', e)\n<\/code><\/pre>\n<p><strong>Output:<\/strong><\/p>\n<pre><code>Download PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs list index out of range\nDownload PDFs [Errno 13] Permission denied: '\/data'\n...\n<\/code><\/pre>\n<p>Please let me know if there is anything else I should clarify.<\/p>",
        "Challenge_closed_time":1639406624843,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639402213260,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1639406637992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70335470",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":41.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":1.2254397222,
        "Challenge_title":"'[Errno 13] Permission denied' - Jupyter Labs on AWS SageMaker",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":684.0,
        "Challenge_word_count":264,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622632545867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p><code>download<\/code> has a forward-slash <code>\/<\/code> as first character of save directory (second parameter). I removed this:<\/p>\n<pre><code>download = od.download(url, 'data\/gri\/')\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>...\nDownloading http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf to data\/gri\/1556248045.pdf\n450560it [00:02, 207848.59it\/s]\n...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":140.5201130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On AzureML Batchendpoint, I'm recently hitting the following error:<\/p>\n<pre><code>Unable to get image details : Environment version Autosave_(date)T(time)Z_******** provided in request doesn't match environ.\n<\/code><\/pre>\n<p>when I setup the batch-endpoint with a <code>yml<\/code> config:<\/p>\n<p><code>environment: azureml:env-name:env-version<\/code><\/p>\n<p>So, AzureML creates and builds the environment with the version I specify <code>env-version<\/code>, which is just a number (in my case = 3).<\/p>\n<p>and then for some weird reason, AzureML creates an extra environment version called <code>Autosave_(date)T(time)Z_********<\/code>, which is not built, but based on the previous one just created, and then it becomes the <code>latest<\/code> version of that environment.<\/p>\n<p>In summary, AzureML instead of looking for the version that I specified as <code>env-name:3<\/code> it seems to be looking for <code>env-name:Autosave_(date)T(time)Z_********<\/code> and then throws the error message mentioned above.<\/p>",
        "Challenge_closed_time":1648808005063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648738692993,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71694816",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":14.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":19.2533527778,
        "Challenge_title":"Unable to get image details : Environment version Autosave_(date)T(time)Z_******** provided in request doesn't match environ",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":92.0,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>I found the problem was that when creating an environment from a YAML specification file, one of my <strong>conda dependencies<\/strong> was <code>cmake<\/code>, which I needed to allow installation of another python module. The docker image is exactly the same as a previously created environment.<\/p>\n<p>Removing the <code>cmake<\/code> dependency from the YAML file, eliminated the issue. So the workaround is to install it using a Dockerfile.<\/p>\n<p>The error message was very misleading to start with, but got there in the end after understanding that AzureML reuses a cached image, based on the hash value, from the environment definition accordingly to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#image-caching-and-reuse\" rel=\"nofollow noreferrer\">this<\/a><\/p>\n<p>So for that reason, the automatically created <code>Autosave<\/code> docker image  references to that same build, which only happens once when the first job is sent.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1649244565400,
        "Solution_link_count":1.0,
        "Solution_readability":15.1,
        "Solution_reading_time":12.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":128.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1569518464147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":432.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":0.1342155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Pip installation is stuck in an infinite loop if there are unresolvable conflicts in dependencies. To reproduce, <code>pip==20.3.0<\/code> and:<\/p>\n<pre><code>pip install pyarrow==2.0.0 azureml-defaults==1.18.0\n<\/code><\/pre>",
        "Challenge_closed_time":1606928528563,
        "Challenge_comment_count":6,
        "Challenge_created_time":1606928045387,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1608739608112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65112585",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":11.3,
        "Challenge_reading_time":4.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":13.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1342155556,
        "Challenge_title":"Pip installation stuck in infinite loop if unresolvable conflicts in dependencies",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2146.0,
        "Challenge_word_count":34,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568658032912,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA, USA",
        "Poster_reputation_count":133.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Workarounds:<\/p>\n<p>Local environment:\nDowngrade pip to &lt; 20.3<\/p>\n<p>Conda environment created from yaml:\nThis will be seen only if conda-forge is highest priority channel, anaconda channel doesn't have pip 20.3 (as of now). To mitigate the issue please explicitly specify pip&lt;20.3 (!=20.3 or =20.2.4 pin to other version) as a conda dependency in the conda specification file<\/p>\n<p>AzureML experimentation:\nFollow the case above to make sure pinned pip resulted as a conda dependency in the environment object, either from yml file or programmatically<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":7.12,
        "Solution_score_count":12.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":4.6192766667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to work with the capacities of the new Azure ML Workspace and I can't find any option to track my notebooks on git. <\/p>\n\n<p>It's this possible as well as you can do with Azure notebooks? If not is possible... how it's suposed to work with this notebooks? Only inside this workspace? <\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1582827641316,
        "Challenge_comment_count":3,
        "Challenge_created_time":1582811011920,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60434642",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":3.8,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.6192766667,
        "Challenge_title":"Version control of azure machine learning workspace notebooks",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":344.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Poster_created_time":1348126905516,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Barcelona",
        "Poster_reputation_count":327.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>AFAIK, Git isn't currently supported by Azure Machine Learning Notebooks. If you're looking for a more fully-featured development environment, I suggest setting one up locally. There's more work up front, but it will give you the ability to version control. Check out this development environment set-up guide. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment<\/a><\/p>\n\n<pre><code>| Environment                                                   | Pros                                                                                                                                                                                                                                    | Cons                                                                                                                                                                                 |\n|---------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud-based Azure Machine Learning compute instance (preview) | Easiest way to get started. The entire SDK is already installed in your workspace VM, and notebook tutorials are pre-cloned and ready to run.                                                                                           | Lack of control over your development environment and dependencies. Additional cost incurred for Linux VM (VM can be stopped when not in use to avoid charges). See pricing details. |\n| Local environment                                             | Full control of your development environment and dependencies. Run with any build tool, environment, or IDE of your choice.                                                                                                             | Takes longer to get started. Necessary SDK packages must be installed, and an environment must also be installed if you don't already have one.                                      |\n| Azure Databricks                                              | Ideal for running large-scale intensive machine learning workflows on the scalable Apache Spark platform.                                                                                                                               | Overkill for experimental machine learning, or smaller-scale experiments and workflows. Additional cost incurred for Azure Databricks. See pricing details.                          |\n| The Data Science Virtual Machine (DSVM)                       | Similar to the cloud-based compute instance (Python and the SDK are pre-installed), but with additional popular data science and machine learning tools pre-installed. Easy to scale and combine with other custom tools and workflows. | A slower getting started experience compared to the cloud-based compute instance.                                                                                                    |\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":31.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":247.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1526004205792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"China",
        "Answerer_reputation_count":28087.0,
        "Answerer_view_count":3298.0,
        "Challenge_adjusted_solved_time":64.41573,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AKS cluster in a VNET\/Subnet. My AKS is linked to AzureML.<\/p>\n<p>I successfully deployed an Azure ML service to that AKS.<\/p>\n<p>However, I see that the azureml-fe service is responding to a public IP and not a private IP from my VNET\/Subnet.<\/p>\n<p>How can I make it so my AzureML inference service is exposed with a private IP?<\/p>",
        "Challenge_closed_time":1615790277368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615558380740,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66601526",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":4.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":64.41573,
        "Challenge_title":"Azure ML Kubernetes - Private IP",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":232.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1274212839807,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":591.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>Maybe you need to use an internal load balancer, then it will use a private IP address. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-kubernetes?tabs=python#create-or-attach-an-aks-cluster-to-use-internal-load-balancer-with-private-ip\" rel=\"nofollow noreferrer\">Here<\/a> is the example code for Python:<\/p>\n<pre><code>from azureml.core.compute.aks import AksUpdateConfiguration\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# When you create an AKS cluster, you can specify Internal Load Balancer to be created with provisioning_config object\nprovisioning_config = AksCompute.provisioning_configuration(load_balancer_type = 'InternalLoadBalancer')\n\n# when you attach an AKS cluster, you can update the cluster to use internal load balancer after attach\naks_target = AksCompute(ws,&quot;myaks&quot;)\n\n# Change to the name of the subnet that contains AKS\nsubnet_name = &quot;default&quot;\n# Update AKS configuration to use an internal load balancer\nupdate_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)\naks_target.update(update_config)\n# Wait for the operation to complete\naks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":20.2,
        "Solution_reading_time":16.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":112.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":265.7839591667,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I recently started using the <code>wandb.Api()<\/code> in order not to manually download all the Charts in <code>.csv<\/code> format.<\/p>\n<p>The problem is that I cannot get consistent results, most of the times that I call the API  in a jupyter-notebook I get different results.<\/p>\n<p>I have made public one of my dashboards to tackle this issue. Here is a screenshot with a reproducible example:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" data-download-href=\"\/uploads\/short-url\/8x7Rm9lNkSyg4pi6edKG0wNOxgE.png?dl=1\" title=\"2022-05-16-165542_647x517_scrot\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" alt=\"2022-05-16-165542_647x517_scrot\" data-base62-sha1=\"8x7Rm9lNkSyg4pi6edKG0wNOxgE\" width=\"625\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">2022-05-16-165542_647x517_scrot<\/span><span class=\"informations\">647\u00d7517 50.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>In order to obtain the <code>csv_val_f1<\/code> variable one just needs to download the <code>Val F1<\/code> chart. Two things can be seen here:<\/p>\n<ol>\n<li>Multiple runs of the same code produce different results<\/li>\n<li>The maximum value obtained by the API differs from the maximum value obtained by manually downloading the <code>.csv<\/code> version of the Chart.<\/li>\n<\/ol>\n<p>Any ideas on what I\u2019m missing?<\/p>",
        "Challenge_closed_time":1653670051375,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652713229122,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/run-history-returns-different-values-on-almost-each-call\/2431",
        "Challenge_link_count":3,
        "Challenge_participation_count":5,
        "Challenge_readability":14.1,
        "Challenge_reading_time":26.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":265.7839591667,
        "Challenge_title":"Run.history() returns different values on almost each call",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":832.0,
        "Challenge_word_count":165,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jaeheelee\">@jaeheelee<\/a> and <a class=\"mention\" href=\"\/u\/carloshernandezp\">@carloshernandezp<\/a>,<br>\nI believe you are seeing this because we sample the data points when you call <code>run.history()<\/code>. You can use <code>run.scan_history()<\/code> if you would like to have the entire history returned. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#sampling\">Here<\/a> is some more information on this.<\/p>\n<p>Let me know if this solves the issue for you.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":7.07,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":59.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":20.4100311111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have setup a mlflow server locally at http:\/\/localhost:5000<\/p>\n<p>I followed the instructions at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker<\/a> and tried to run the example docker with<\/p>\n<pre><code>\/mlflow\/examples\/docker$ mlflow run . -P alpha=0.5\n<\/code><\/pre>\n<p>but I encountered the following error.<\/p>\n<pre><code>2021\/05\/09 17:11:20 INFO mlflow.projects.docker: === Building docker image docker-example:7530274 ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.utils: === Created directory \/tmp\/tmp9wpxyzd_ for downloading remote URIs passed to arguments of type 'path' ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts:\/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts -e MLFLOW_RUN_ID=ae69145133bf49efac22b1d390c354f1 -e MLFLOW_TRACKING_URI=http:\/\/localhost:5000 -e MLFLOW_EXPERIMENT_ID=0 docker-example:7530274 python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'ae69145133bf49efac22b1d390c354f1' === \n\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/__init__.py:55: DeprecationWarning: MLflow support for Python 2 is deprecated and will be dropped in a future release. At that point, existing Python 2 workflows that use MLflow will continue to work without modification, but Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3 - see https:\/\/docs.python.org\/3\/howto\/pyporting.html for a migration guide.\n  &quot;for a migration guide.&quot;, DeprecationWarning)\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 56, in &lt;module&gt;\n    with mlflow.start_run():\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/client.py&quot;, line 96, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 49, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 133, in call_endpoint\n    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 70, in http_request\n    url=url, headers=headers, verify=verify, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 51, in request_with_ratelimit_retries\n    response = requests.request(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/api.py&quot;, line 58, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 508, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 618, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/adapters.py&quot;, line 508, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?run_uuid=ae69145133bf49efac22b1d390c354f1&amp;run_id=ae69145133bf49efac22b1d390c354f1 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f5cbd80d690&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))\n2021\/05\/09 17:11:22 ERROR mlflow.cli: === Run (ID 'ae69145133bf49efac22b1d390c354f1') failed ===\n<\/code><\/pre>\n<p>Any ideas how to fix this? I tried adding the following in MLproject file but it doesn't help<\/p>\n<pre><code>environment: [[&quot;network&quot;, &quot;host&quot;], [&quot;add-host&quot;, &quot;host.docker.internal:host-gateway&quot;]]\n<\/code><\/pre>\n<p>Thanks for your help! =)<\/p>",
        "Challenge_closed_time":1620627546968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620552530280,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1620554070856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67456172",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":61.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":20.8379688889,
        "Challenge_title":"Unable to connect to MLFLOW_TRACKING_URI when running MLflow run in Docker container",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1151.0,
        "Challenge_word_count":357,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316620102630,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1308.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost<\/code>.  Then point the <code>mlflow run<\/code> to that IP instead of <code>http:\/\/localhost:5000<\/code>.   The main reason is that <code>localhost<\/code> of Docker process is its own, not your machine.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":3.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1526489986910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia, PA, USA",
        "Answerer_reputation_count":926.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":9.0913125,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm new to the aws how to set path of my bucket and access file of that bucket?<\/p>\n\n<p>Is there anything i need to change with prefix ?<\/p>\n\n<pre><code>import os\nimport boto3\nimport re\nimport copy\nimport time\nfrom time import gmtime, strftime\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\nregion = boto3.Session().region_name\n\nbucket='ltfs1' # Replace with your s3 bucket name\nprefix = 'sagemaker\/ltfs1' # Used as part of the path in the bucket where you store data\n# bucket_path = 'https:\/\/s3-{}.amazonaws.com\/{}'.format(region,bucket) # The URL to access the bucket\n<\/code><\/pre>\n\n<p>I'm using the above code but it's showing file not found error<\/p>",
        "Challenge_closed_time":1562166882852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562134154127,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56863907",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":9.0913125,
        "Challenge_title":"how to set path of bucket in amazonsagemaker jupyter notebook?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":852.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559907487263,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>If the file you are accessing is in the root directory of your s3 bucket, you can access the file like this:<\/p>\n\n<pre><code>import pandas as pd\n\nbucket='ltfs1'\ndata_key = 'data.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ntraining_data = pd.read_csv(data_location)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":3.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1407449881432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":228.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":732.1807,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Challenge_closed_time":1539818808056,
        "Challenge_comment_count":3,
        "Challenge_created_time":1538879533133,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1539893383687,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Challenge_link_count":3,
        "Challenge_participation_count":7,
        "Challenge_readability":16.6,
        "Challenge_reading_time":96.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":63,
        "Challenge_solved_time":260.9097008333,
        "Challenge_title":"AWS NoCredentials in training",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1374.0,
        "Challenge_word_count":621,
        "Platform":"Stack Overflow",
        "Poster_created_time":1380496984176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gloucester, VA, USA",
        "Poster_reputation_count":544.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1542529234207,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":12.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1388584380832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":396.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":157.3425175,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm researching the use of MLflow as part of our data science initiatives and I wish to set up a minimum working example of remote execution on databricks from windows.<\/p>\n\n<p>However, when I perform the remote execution a path is created locally on windows in the MLflow package which is sent to databricks. This path specifies the upload location of the '.tar.gz' file corresponding to the Github repo containing the MLflow Project. In cmd this has a combination of '\\' and '\/', but on databricks there are no separators at all in this path, which raises the 'rsync: No such file or directory (2)' error.<\/p>\n\n<p>To be more general, I reproduced the error using an MLflow standard example and following this <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/projects.html\" rel=\"nofollow noreferrer\">guide<\/a> from databricks. The MLflow example is the <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">sklearn_elasticnet_wine<\/a>, but I had to add a default value to a parameter so I forked it and the MLproject which can be executed remotely can be found at (<a href=\"https:\/\/github.com\/aestene\/mlflow\/tree\/master\/examples\/sklearn_elasticnet_wine\" rel=\"nofollow noreferrer\">forked repo<\/a>).<\/p>\n\n<p>The Project can be executed remotely by the following command (assuming a databricks instance has been set up)<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine -b databricks -c db-clusterconfig.json --experiment-id &lt;insert-id-here&gt;\n<\/code><\/pre>\n\n<p>where \"db-clusterconfig.json\" correspond to the cluster to set up in databricks and is in this example set to<\/p>\n\n<pre><code>{\n    \"autoscale\": {\n        \"min_workers\": 1,\n        \"max_workers\": 2\n    },\n    \"spark_version\": \"5.5.x-scala2.11\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"driver_node_type_id\": \"Standard_DS3_v2\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {\n        \"PYSPARK_PYTHON\": \"\/databricks\/python3\/bin\/python3\"\n    }\n}\n<\/code><\/pre>\n\n<p>When running the project remotely, this is the output in cmd:<\/p>\n\n<pre><code>2019\/10\/04 10:09:50 INFO mlflow.projects: === Fetching project from https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine into C:\\Users\\ARNTS\\AppData\\Local\\Temp\\tmp2qzdyq9_ ===\n2019\/10\/04 10:10:04 INFO mlflow.projects.databricks: === Uploading project to DBFS path \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Finished uploading project to \/dbfs\\mlflow-experiments\\3947403843428882\\projects-code\\aa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz ===\n2019\/10\/04 10:10:05 INFO mlflow.projects.databricks: === Running entry point main of project https:\/\/github.com\/aestene\/mlflow#examples\/sklearn_elasticnet_wine on Databricks ===\n2019\/10\/04 10:10:06 INFO mlflow.projects.databricks: === Launched MLflow run as Databricks job run with ID 8. Getting run status page URL... ===\n2019\/10\/04 10:10:18 INFO mlflow.projects.databricks: === Check the run's status at https:\/\/&lt;region&gt;.azuredatabricks.net\/?o=&lt;databricks-id&gt;#job\/8\/run\/1 ===\n<\/code><\/pre>\n\n<p>Where the DBFS path has a leading '\/' before the remaining are '\\'. <\/p>\n\n<p>The command spins up a cluster in databricks and is ready to execute the job, but ends up with the following error message on the databricks side:<\/p>\n\n<pre><code>rsync: link_stat \"\/dbfsmlflow-experiments3947403843428882projects-codeaa5fbb4769e27e1be5a983751eb1428fe998c3e65d0e66eb9b4c77355076f524.tar.gz\" failed: No such file or directory (2)\nrsync error: some files\/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.1]\n<\/code><\/pre>\n\n<p>Where we can see the same path but without the '\\' inserted. I narrowed down the creation of this path to this <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/projects\/databricks.py\" rel=\"nofollow noreferrer\">file<\/a> in the MLflow Github repo, where the following code creates the path (line 133):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dbfs_path = os.path.join(DBFS_EXPERIMENT_DIR_BASE, str(experiment_id),\n                                     \"projects-code\", \"%s.tar.gz\" % tarfile_hash)\ndbfs_fuse_uri = os.path.join(\"\/dbfs\", dbfs_path)\n<\/code><\/pre>\n\n<p>My current hypothesis is that <code>os.path.join()<\/code> in the first line joins the string together in a \"windows fashion\" such that they have backslashes. Then the following call to <code>os.path.join()<\/code> adds a '\/'. The databricks file system is then unable to handle this path and something causes the 'tar.gz' file to not be properly uploaded or to be accessed at the wrong path. <\/p>\n\n<p>It should also be mentioned that the project runs fine locally.<\/p>\n\n<p>I'm running the following versions:<\/p>\n\n<p>Windows 10<\/p>\n\n<p>Python 3.6.8<\/p>\n\n<p>MLflow 1.3.0 (also replicated the fault with 1.2.0)<\/p>\n\n<p>Any feedback or suggestions are greatly appreciated!<\/p>",
        "Challenge_closed_time":1570752025483,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570185592420,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58234777",
        "Challenge_link_count":8,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":66.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":157.3425175,
        "Challenge_title":"MLflow remote execution on databricks from windows creates an invalid dbfs path",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":350.0,
        "Challenge_word_count":537,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570173110492,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Thanks for the catch, you're right that using <code>os.path.join<\/code> when working with DBFS paths is incorrect, resulting in a malformed path that breaks project execution. I've filed to <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1926\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1926<\/a> track this, if you're interested in making a bugfix PR (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst\" rel=\"nofollow noreferrer\">see the MLflow contributor guide for info on how to do this<\/a>) to replace <code>os.path.join<\/code> here with <code>os.posixpath.join<\/code> I'd be happy to review :)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":7.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":69.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4194444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I receive the following error when running the following [notebook](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-azureml\/track-and-monitor-experiments\/using-mlflow\/train-local\/train-local.ipynb)\r\n\r\n```python\r\nIn [6]: ws.get_mlflow_tracking_uri()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-6c16e13b21e5> in <module>\r\n----> 1 ws.get_mlflow_tracking_uri()\r\n\r\nAttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'\r\n```",
        "Challenge_closed_time":1581065545000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1581064035000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/776",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":18.1,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4194444444,
        "Challenge_title":"AttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":38,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":13264.2386313889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?<\/p>",
        "Challenge_closed_time":1523213115336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522449441927,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49582307",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":3.8,
        "Challenge_reading_time":2.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":14.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":212.1315025,
        "Challenge_title":"How to schedule tasks on SageMaker",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":17339.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420001102892,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":173.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=\"https:\/\/aws.amazon.com\/tools\/\" rel=\"noreferrer\">SDK<\/a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html<\/a> )<\/p>\n\n<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:<\/p>\n\n<ul>\n<li><strong>Stopping and Starting Notebook Instances<\/strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"noreferrer\">stop-notebook-instance<\/a> API at the end of the working day (8PM, for example), and the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StartNotebookInstance.html\" rel=\"noreferrer\">start-notebook-instance<\/a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).<\/li>\n<li><strong>Refreshing an ML Model<\/strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"noreferrer\">create-training-job<\/a> API from a scheduled Lambda function (or even from a <a href=\"https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/events\/WhatIsCloudWatchEvents.html\" rel=\"noreferrer\">CloudWatch Event<\/a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateModel.html\" rel=\"noreferrer\">create a refreshed model<\/a> that you can now deploy into an <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"noreferrer\">A\/B testing environment<\/a> .<\/li>\n<\/ul>\n\n<p>----- UPDATE (thanks to @snat2100 comment) -----<\/p>\n\n<ul>\n<li><strong>Creating and Deleting Real-time Endpoints<\/strong> - If your realtime endpoints are not needed 24\/7 (for example, serving internal company users working during workdays and hours), you can also <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpoint.html\" rel=\"noreferrer\">create the endpoints<\/a> in the morning and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DeleteEndpoint.html\" rel=\"noreferrer\">delete them<\/a> at night. <\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1570200701000,
        "Solution_link_count":11.0,
        "Solution_readability":15.5,
        "Solution_reading_time":39.08,
        "Solution_score_count":19.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":307.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":1.6038955556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Challenge_closed_time":1611228029327,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611222255303,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65824766",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.6038955556,
        "Challenge_title":"SSH automation in jenkins",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Poster_created_time":1493101921288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pakistan",
        "Poster_reputation_count":133.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":2.12,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":7804.7933702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried connecting through Sagemaker notebook to RDS. However, to connect to RDS, my public IP needs to be allowed for security reasons. I can see when I run this command: &quot;curl ifconfig.me&quot; on Sagemaker Notebook instance that public IP keeps changing from time to time.<\/p>\n<p>What is the correct way to connect to RDS with notebook on sagemaker? Do I need to crawl the RDS with AWS Glue and then use Athena on crawled tables and then take the query results from S3 with Sagemaker notebook?<\/p>",
        "Challenge_closed_time":1627578308223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599481052090,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63777462",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7804.7933702778,
        "Challenge_title":"Sagemaker Jupyter Notebook Cannot Connect to RDS",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":865.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509559597047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.<\/p>\n<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":7.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1291082954112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation_count":20306.0,
        "Answerer_view_count":1991.0,
        "Challenge_adjusted_solved_time":7560.4671713889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a Sagemaker Jupyter notebook instance that I keep leaving online overnight by mistake, unnecessarily costing money... <\/p>\n\n<p>Is there any way to automatically stop the Sagemaker notebook instance when there is no activity for say, 1 hour? Or would I have to make a custom script?<\/p>",
        "Challenge_closed_time":1571132773200,
        "Challenge_comment_count":0,
        "Challenge_created_time":1543915091383,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53609409",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":4.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":19.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7560.4671713889,
        "Challenge_title":"Automatically \"stop\" Sagemaker notebook instance after inactivity?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":12683.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1299752760990,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2563.0,
        "Poster_view_count":167.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/\" rel=\"noreferrer\">Lifecycle configurations<\/a> to set up an automatic job that will stop your instance after inactivity.<\/p>\n\n<p>There's <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\" rel=\"noreferrer\">a GitHub repository<\/a> which has samples that you can use. In the repository, there's a <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh\" rel=\"noreferrer\">auto-stop-idle<\/a> script which will shutdown your instance once it's idle for more than 1 hour.<\/p>\n\n<p>What you need to do is<\/p>\n\n<ol>\n<li>to create a Lifecycle configuration using the script and<\/li>\n<li>associate the configuration with the instance. You can do this when you edit or create a Notebook instance.<\/li>\n<\/ol>\n\n<p>If you think 1 hour is too long you can tweak the script. <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh#L17\" rel=\"noreferrer\">This line<\/a> has the value.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":17.0,
        "Solution_reading_time":17.33,
        "Solution_score_count":26.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":110.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1468845236196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":0.0645166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>so I've been poking around with sacred a little bit and it seems great. unfortunately I did not find any multiple files use-cases examples like I am trying to implement.<\/p>\n\n<p>so i have this file called configuration.py, it is intended to contain different variables which will (using sacred) be plugged in to the rest of the code (laying in different files):<\/p>\n\n<pre><code>from sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.config\ndef configure_analysis_default():\n    \"\"\"Initializes default  \"\"\"\n    generic_name = \"C:\\\\basic_config.cfg\" # configuration filename\n    message = \"This is my generic name: %s!\" % generic_name\n    print(message)\n\n@ex.automain #automain function needs to be at the end of the file. Otherwise everything below it is not defined yet\n#  when the experiment is run.\ndef my_main(message):\n    print(message)\n<\/code><\/pre>\n\n<p>This by itself works great. sacred is working as expected. However, when I'm trying to introduce a second file named Analysis.py:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.capture\ndef what_is_love(generic_name):\n    message = \" I don't know\"\n    print(message)\n    print(generic_name)\n\n@ex.automain\ndef my_main1():\n    what_is_love()\n<\/code><\/pre>\n\n<p>running Analysis.py yields:<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>TypeError: what_is_love is missing value(s) for ['generic_name']<\/p>\n<\/blockquote>\n\n<p>I expected that the 'import configuration' statement to include the configuration.py file, thus importing everything that was configured in there including configure_analysis_default() alongside its decorator @ex.config and then inject it to what_is_love(generic_name).\nWhat am I doing wrong? how can i fix this?<\/p>\n\n<p>Appreciate it!<\/p>",
        "Challenge_closed_time":1513160793190,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513160560930,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1513161714983,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47790619",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":22.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":0.0645166667,
        "Challenge_title":"sacred, python - ex.config in one file and ex",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":1446.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468845236196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>So, pretty dumb, but I'll post it here in favour of whoever will have similar issue...<\/p>\n\n<p>My issue is that I have created a different instance of experiment. I needed simply to import my experiment from the configuration file.<\/p>\n\n<p>replacing this:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n<\/code><\/pre>\n\n<p>with this:<\/p>\n\n<pre><code>import configuration\nex = configuration.ex\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.6,
        "Solution_reading_time":5.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1508797229168,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":1115.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":34.0639138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In our company I use Azure ML and I have the following issue. I specify a <em>conda_requirements.yaml<\/em> file with the PyTorch estimator class, like so (... are placeholders so that I do not have to type everything out):<\/p>\n\n<pre><code>from azureml.train.dnn import PyTorch\nest = PyTorch(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., conda_dependencies_file_path=\u2019conda_requirements.yaml\u2019, environment_variables=..., framework_version=\u20191.1\u2019)\n<\/code><\/pre>\n\n<p>The <em>conda_requirements.yaml<\/em> (shortened version of the pip part) looks like this:<\/p>\n\n<pre><code>dependencies:\n  -  conda=4.5.11\n  -  conda-package-handling=1.3.10\n  -  python=3.6.2\n  -  cython=0.29.10\n  -  scikit-learn==0.21.2\n  -  anaconda::cloudpickle==1.2.1\n  -  anaconda::cffi==1.12.3\n  -  anaconda::mxnet=1.1.0\n  -  anaconda::psutil==5.6.3\n  -  anaconda::pip=19.1.1\n  -  anaconda::six==1.12.0\n  -  anaconda::mkl==2019.4\n  -  conda-forge::openmpi=3.1.2\n  -  conda-forge::pycparser==2.19\n  -  tensorboard==1.13.1\n  -  tensorflow==1.13.1\n  -  pip:\n        - torch==1.1.0\n        - torchvision==0.2.1\n<\/code><\/pre>\n\n<p>This successfully builds on Azure. Now in order to reuse the resulting docker image in that case, I use the <code>custom_docker_image<\/code> parameter to pass to the <\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\nest = Estimator(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., custom_docker_image=\u2019&lt;container registry name&gt;.azurecr.io\/azureml\/azureml_c3a4f...\u2019, environment_variables=...)\n<\/code><\/pre>\n\n<p>But now Azure somehow seems to rebuild the image again and when I run the experiment it cannot install torch. So it seems to only install the conda dependencies and not the pip dependencies, but actually I do not want Azure to rebuild the image. Can I solve this somehow?<\/p>\n\n<p>I attempted to somehow build a docker image from my Docker file and then add to the registry. I can do az login and according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication<\/a> I then should also be able to do an acr login and push. This does not work. \nEven using the credentials from<\/p>\n\n<pre><code>az acr credential show \u2013name &lt;container registry name&gt;\n<\/code><\/pre>\n\n<p>and then doing a <\/p>\n\n<pre><code>docker login &lt;container registry name&gt;.azurecr.io \u2013u &lt;username from credentials above&gt; -p &lt;password from credentials above&gt;\n<\/code><\/pre>\n\n<p>does not work.\nThe error message is <em>authentication required<\/em> even though I used <\/p>\n\n<pre><code>az login\n<\/code><\/pre>\n\n<p>successfully. Would also be happy if someone could explain that to me in addition to how to reuse docker images when using Azure ML.\nThank you!<\/p>",
        "Challenge_closed_time":1565501907287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565379277197,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57436140",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.8,
        "Challenge_reading_time":37.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":34.0639138889,
        "Challenge_title":"How to reuse successfully built docker images in Azure ML?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":604.0,
        "Challenge_word_count":307,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524670343368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Z\u00fcrich, Schweiz",
        "Poster_reputation_count":460.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>AzureML should actually cache your docker image once it was created. The service will hash the base docker info and the contents of the conda.yaml file and will use that as the hash key -- unless you change any of that information, the docker should come from the ACR. <\/p>\n\n<p>As for the custom docker usage, did you set the parameter <code>user_managed=True<\/code>? Otherwise, AzureML will consider your docker to be a base image on top of which it will create the conda environment per your yaml file.<br>\nThere is an example of how to use a custom docker image in this notebook:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.9,
        "Solution_reading_time":13.29,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":105.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1425426748316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":91.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":1787.9299130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am doing the following steps to install R Hash_2.2.6.zip package on to Azure ML<\/p>\n\n<ol>\n<li>Upload the .zip file as a dataset<\/li>\n<li>Create a new experiment and Add \"Execute R Script\" to experiment<\/li>\n<li>Drag and drop .zip file dataset to experiment.<\/li>\n<li>Connect the Dataset in step3 to \"Execute R Script\" of step2<\/li>\n<li>Run the experiment to install the package<\/li>\n<\/ol>\n\n<p>However I am getting this error: <code>zip file src\/hash_2.2.6.zip not found<\/code><\/p>\n\n<p>Just so that its very clear, I am following steps mentioned in this article: <a href=\"http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx\" rel=\"nofollow\">http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx<\/a>.<\/p>\n\n<p>Any help in this regard is greatly appreciated.<\/p>",
        "Challenge_closed_time":1425437860360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1419001312673,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1483481029407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27568624",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1787.9299130556,
        "Challenge_title":"Installing additional R Package on Azure ML",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":2765.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419000630800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":159.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>To install a package this way, you have to create a .zip of a .zip. The outer layer of packaging will get unzipped into the src\/ folder when the dataset is passed in to the module, and you'll be able to install the inner package from there.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1425439161243,
        "Solution_link_count":0.0,
        "Solution_readability":5.9,
        "Solution_reading_time":2.95,
        "Solution_score_count":5.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":22.3896641667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Challenge_closed_time":1573630807523,
        "Challenge_comment_count":1,
        "Challenge_created_time":1573549904240,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1573550204732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":11.6,
        "Challenge_reading_time":49.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":22.4731341667,
        "Challenge_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":4605.0,
        "Challenge_word_count":399,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550756471932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.5,
        "Solution_reading_time":9.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":1.2572055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting started with Kedro, so I created the new kedro project for default iris dataset.<\/p>\n<p>I am able to succesfully run it with <code>kedro run<\/code> command. My question now is how do I run it as a python command? From the documentation I read that the command <code>kedro run<\/code>  runs the <code>src\/project-name\/run.py<\/code>. However, if I run the <code>run.py<\/code> I get <code>ModuleNotFoundError: No module named 'iris_workflow'<\/code>. I get the same error if I run the <code>run<\/code> method from <code>src\/project-name\/cli.py<\/code>.<\/p>\n<p>Everything works fine If I run <code>kedro run<\/code> in terminal.<\/p>\n<p>How do I run <code>kedro run<\/code> from a python script without <code>subprocess.run()<\/code>. If I import the <code>run.py<\/code> or <code>cli.py<\/code> in a script and run it, I get the same error <code>ModuleNotFoundError: No module named 'iris_workflow'<\/code>.<\/p>\n<p>This is the default workflow I created with <code>kedro new --starter=pandas-iris<\/code><\/p>",
        "Challenge_closed_time":1615038763867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615034237927,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66505695",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":13.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1.2572055556,
        "Challenge_title":"kedro run as a python command instead of command line",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":805.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519724643532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":453.0,
        "Poster_view_count":79.0,
        "Solution_body":"<p>The problem is that your <code>src\/<\/code> folder which is where your project python package lives isn't on your Python path, so if you modify your <code>PYTHONPATH<\/code> first, you'll be able to run <code>run.py<\/code>:<\/p>\n<pre><code>~\/code\/kedro\/test-project\ntest-project \u276f PYTHONPATH=$PYTHONPATH:$pwd\/src python3 src\/test_project\/run.py\n<\/code><\/pre>\n<p>To more concretely answer your question, if you wanted to run Kedro from a Python script, you'd do something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\n\nsys.path.append(&quot;&lt;path-to-your-project-src&quot;)\nwith KedroSession.create(package_path.name) as session:\n    session.run()\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.4,
        "Solution_reading_time":9.03,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":71.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":10.5594166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a pytorch model in sagemaker <\/p>\n\n<pre><code>from sagemaker.pytorch import PyTorch\n\nestimator = PyTorch(entry_point='train.py',\n                    role=role,\n                    framework_version='1.0.0',\n                    train_instance_count=1,\n                    train_instance_type='ml.m4.xlarge',\n                    source_dir='source', #the directory where the supporting files are\n\n                    #what is passed in\n                    hyperparameters={\n                        'max_epochs' : 6,\n                        'layer_dim'  : \"2500,500,100,1\",\n                        'batch_size' : 64,\n                        'seed'       : 4524,\n                        'cuda'       : False\n                        }\n                   )\n<\/code><\/pre>\n\n<p>where the entry script train.py includes several imports<\/p>\n\n<pre><code>import argparse\nimport math\nimport os\nfrom shutil import copy\nimport time\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler\n<\/code><\/pre>\n\n<p>But the sklearn call fails:<\/p>\n\n<pre><code>  File \"\/opt\/ml\/code\/train.py\", line 9, in &lt;module&gt;\n    from sklearn.preprocessing import StandardScaler\nModuleNotFoundError: No module named 'sklearn'\n<\/code><\/pre>\n\n<p>Questions:<\/p>\n\n<ol>\n<li>How do I use sklearn functions in this case?<\/li>\n<li>Can you add additional pip installs without going the custom docker route?<\/li>\n<\/ol>",
        "Challenge_closed_time":1559476232443,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559438218543,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56411563",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":14.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":10.5594166667,
        "Challenge_title":"Sagemaker Pytorch : Cant import sklearn from training script",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":571.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294628108596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1748.0,
        "Poster_view_count":393.0,
        "Solution_body":"<p>Yes, you can install dependencies without going the custom dockerfile route (also BYO Container)<\/p>\n\n<p>Use this in your code (where <code>mypackage<\/code> represents a pip package of your choice)<\/p>\n\n<pre><code>import subprocess as sb \nimport sys \n\nsb.call([sys.executable, \"-m\", \"pip\", \"install\", mypackage]) \n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":4.19,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1348171784712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":93.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":105.8507644444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I use PyTorch estimator with SageMaker to train\/fine-tune my Graph Neural Net on multi-GPU machines.<\/p>\n<p>The <code>requirements.txt<\/code> that gets installed into the Estimator container, has lines like:<\/p>\n<pre><code>torch-scatter -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\ntorch-sparse -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\ntorch-cluster -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\ntorch-spline-conv -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\n<\/code><\/pre>\n<p>When SageMaker installs these requirements in the Estimator on the endpoint, it takes ~<strong>2 hrs<\/strong> to build the wheel. It takes only seconds on a local Linux box.<\/p>\n<p>SageMaker Estimator:<\/p>\n<p>PyTorch v1.10\nCUDA 11.x\nPython 3.8\nInstance: ml.p3.16xlarge<\/p>\n<p>I have noticed the same issue with other wheel-based components that require CUDA.<\/p>\n<p>I have also tried building a Docker container on p3.16xlarge and running that on SageMaker, but it was unable to recognize the instance GPUs<\/p>\n<p>Anything I can do to cut down these build times?<\/p>",
        "Challenge_closed_time":1649126605252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648745542500,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71696407",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":7.7,
        "Challenge_reading_time":14.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":105.8507644444,
        "Challenge_title":"Amazon SageMaker ScriptMode Long Python Wheel Build Times for CUDA Components",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":136.0,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1348171784712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":93.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The solution is to augment the stock estimator image with the right components and then it can be run in the SageMaker script mode:<\/p>\n<pre><code>FROM    763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-training:1.10-gpu-py38\n\nCOPY requirements.txt \/tmp\/requirements.txt\nRUN pip install -r \/tmp\/requirements.tx\n<\/code><\/pre>\n<p>The key is to make sure <code>nvidia<\/code> runtime is used at build time, so <code>daemon.json<\/code> needs to be configured accordingly:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;default-runtime&quot;: &quot;nvidia&quot;,\n    &quot;runtimes&quot;: {\n        &quot;nvidia&quot;: {\n            &quot;path&quot;: &quot;nvidia-container-runtime&quot;,\n            &quot;runtimeArgs&quot;: []\n        }\n    }\n}\n<\/code><\/pre>\n<p>This is still not a complete solution, because viability of the build for SageMaker depends on the host where the build is performed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":11.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1418476563283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":676.0,
        "Answerer_view_count":113.0,
        "Challenge_adjusted_solved_time":51185.3566025,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was hoping that someone would have tried to or had success in implementing it and would have knowledge of any pitfalls in using it.<\/p>",
        "Challenge_closed_time":1474621313252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458446579210,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1459352400923,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36110109",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4492.9816783333,
        "Challenge_title":"Has anyone any experience on implementing the R Package XGBoost within the Azure ML Studio environment?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":436.0,
        "Challenge_word_count":40,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428820274636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lexington, KY, United States",
        "Poster_reputation_count":65.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>You need to zip &amp; load the package windows binaries in dataset &amp; import it to the R environment.<\/p>\n<p>You can follow the instructions over <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Using-XGBoost-to-build-Graduation-Admit-Model-1\" rel=\"nofollow noreferrer\">here<\/a>. I couldn't import it for the latest version, so I simply downloaded the xgboost version from this experiment &amp; loaded it to my saved datasets<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/archive\/blogs\/benjguin\/how-to-upload-an-r-package-to-azure-machine-learning\" rel=\"nofollow noreferrer\">This<\/a> is for any generic packages which are not preloaded in the environment<\/p>\n<p>The following is a <a href=\"https:\/\/web.archive.org\/web\/20161020215633\/https:\/\/azure.microsoft.com\/en-in\/documentation\/articles\/machine-learning-r-csharp-web-service-examples\/\" rel=\"nofollow noreferrer\">list of experiments<\/a> to publish R models as a web service<\/p>\n<p>Hope this helps!<\/p>\n<p>Edit: You can also simply change the R version to Microsoft Open R (current version 3.2.2) and you can import xgboost as any common library<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1643619684692,
        "Solution_link_count":3.0,
        "Solution_readability":14.8,
        "Solution_reading_time":14.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":116.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1639907005592,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":323.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":1.51305,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Looking at <a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/r_examples\/r_in_sagemaker_processing\/r_in_sagemaker_processing.html\" rel=\"nofollow noreferrer\">this page<\/a> and this piece of code in particular:<\/p>\n<pre><code>import boto3\n\naccount_id = boto3.client(&quot;sts&quot;).get_caller_identity().get(&quot;Account&quot;)\nregion = boto3.session.Session().region_name\n\necr_repository = &quot;r-in-sagemaker-processing&quot;\ntag = &quot;:latest&quot;\n\nuri_suffix = &quot;amazonaws.com&quot;\nprocessing_repository_uri = &quot;{}.dkr.ecr.{}.{}\/{}&quot;.format(\n    account_id, region, uri_suffix, ecr_repository + tag\n)\n\n# Create ECR repository and push Docker image\n!docker build -t $ecr_repository docker\n!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n!aws ecr create-repository --repository-name $ecr_repository\n!docker tag {ecr_repository + tag} $processing_repository_uri\n!docker push $processing_repository_uri\n<\/code><\/pre>\n<p>This is not pure Python obviously? Are these AWS CLI commands? I have used docker previously but I find this example very confusing. Is anyone aware of an end-2-end example of simply running some R job in AWS using sage maker\/docker? Thanks.<\/p>",
        "Challenge_closed_time":1639923580880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639919443300,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70411715",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":16.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1.1493277778,
        "Challenge_title":"running r script in AWS",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":113,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>This is Python code mixed with shell script magic calls (the <code>!commands<\/code>).<\/p>\n<p>Magic commands aren't unique to this platform, you can use them in <a href=\"https:\/\/ipython.readthedocs.io\/en\/stable\/interactive\/magics.html\" rel=\"nofollow noreferrer\">Jupyter<\/a>, but this particular code is meant to be run on their platform. In what seems like a fairly convoluted way of running R scripts as processing jobs.<\/p>\n<p>However, the only thing you really need to focus on is the R script, and the final two cell blocks. The instruction at the top (don't change this line) creates a file (preprocessing.R) which gets executed later, and then you can see the results.<\/p>\n<p>Just run all the code cells in that order, with your own custom R code in the first cell. Note the line <code>plot_key = &quot;census_plot.png&quot;<\/code> in the last cell. This refers to the image being created in the R code. As for other output types (eg text) you'll have to look up the necessary Python package (PIL is an image manipulation package) and adapt accordingly.<\/p>\n<p>Try this to get the CSV file that the R script is also generating (this code is not validated, so you might need to fix any problems that arise):<\/p>\n<pre><code>import csv\n\ncsv_key = &quot;plot_data.csv&quot;\ncsv_in_s3 = &quot;{}\/{}&quot;.format(preprocessed_csv_data, csv_key)\n!aws s3 cp {csv_in_s3} .\n\nfile = open(csv_key)\ndat = csv.reader(file)\n\ndisplay(dat)\n<\/code><\/pre>\n<p>So now you should have an idea of how two different output types the R script example generates are being handled, and from there you can try and adapt your own R code based on what it outputs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1639924890280,
        "Solution_link_count":1.0,
        "Solution_readability":7.8,
        "Solution_reading_time":20.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":247.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1017638889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When users were creating a new compute in AML environment by default RStudio application was created.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238009-screenshot-2022-08-23-170502.png?platform=QnA\" alt=\"With RStudio\" \/>    <\/p>\n<p>However, from month of July, by default RStudio application is not getting created. Only JupyterLab, Jupyter, VS Code, Terminal, Notebook applications installed not a RStudio.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/237995-4.png?platform=QnA\" alt=\"without RStudio\" \/>    <\/p>\n<p>Is there any way to by default install RStudio application in azure ML compute instance?    <\/p>",
        "Challenge_closed_time":1662454114980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662442948630,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/995175\/rstudio-application-not-installed-in-azure-ml-comp",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":9.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.1017638889,
        "Challenge_title":"RStudio application not installed in Azure ML compute",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8876fdcf-bcee-403a-827a-7cff9a68f8ee\">@SHAIKH, Alif Abdul  <\/a> Yes, this is a recent change in the setup of compute instance that happens during the creation of compute instance. This change does not setup the rstudio community edition by default but you can set it up while creation by adding it as a custom application from advanced settings. This change is done as part of RStudio requirements to allow users to setup their own license key or use open studio version during creation. Please refer this documentation <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#add-custom-applications-such-as-rstudio-preview\">page<\/a> for details to setup licensed and open version of the studio.    <\/p>\n<p>If you add a license key then please use RStudio Workbench (bring your own license) option from the advanced options and enter your own license key.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238075-image.png?platform=QnA\" alt=\"238075-image.png\" \/>    <\/p>\n<p>If you need to add the open version you need to select custom application and enter the docker image and mount point details to setup the rstudio during creation.    <\/p>\n<p>Target\/Published port <code>8787<\/code>    <br \/>\nDocker image set to <code>ghcr.io\/azure\/rocker-rstudio-ml-verse:latest<\/code>    <br \/>\n<code>\/home\/azureuser\/cloudfiles<\/code> for Host path    <br \/>\n<code>\/home\/azureuser\/cloudfiles<\/code> for Container path    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238135-image.png?platform=QnA\" alt=\"238135-image.png\" \/>    <\/p>\n<p>Once the creation and setup is complete the options to use Rstudio for open and licensed version will be visible as links on the compute instances page.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238142-image.png?platform=QnA\" alt=\"238142-image.png\" \/>    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":14.3,
        "Solution_reading_time":30.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":240.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.5002777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I've installed and updated the sdk yet when attempting to import the module for the ExplainationDashboard i keep getting the error that the interpret module does not exist.\r\ni am running build 1.0.72 and following the sample in 'how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb \r\nthe failing line in the sample notebook is:\r\n**from azureml.contrib.interpret.visualize import ExplanationDashboard**\r\n**\"ModuleNotFoundError: No module named 'azureml.contrib.interpret' \"**\r\nthis is the update command i ran:\r\npip install --upgrade azureml-sdk[explain,automl,contrib] \r\n(the install ran fine - no errors)\r\njim",
        "Challenge_closed_time":1573060395000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572788594000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/639",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.78,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":75.5002777778,
        "Challenge_title":"azureml.contrib.interpret - ModuleNotFoundError - build 1.0.72",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"azureml-contrib-interpret is not a part of azureml-sdk contrib extras. Please install it separately",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":1.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1572812561640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3502.0,
        "Answerer_view_count":120.0,
        "Challenge_adjusted_solved_time":0.1742652778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using LightGBM in Azure ML Jupyter notebooks, it works fine and I also installed graphviz.<\/p>\n<p>However this line:<\/p>\n<pre><code>lgb.plot_tree(clf, tree_index = 1, figsize=(20,12))\n<\/code><\/pre>\n<p>throws this error:<\/p>\n<pre><code>ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n<\/code><\/pre>",
        "Challenge_closed_time":1660922530248,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660921902893,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73418843",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":6.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_resolution":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1742652778,
        "Challenge_title":"Azure ML ExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>Common problem (very common).  There are two systems named Graphviz, and you need both!\nsee <a href=\"https:\/\/stackoverflow.com\/questions\/73040021\/im-getting-this-issue-when-trying-to-run-the-code-i-found-on-github-pydot-and\/73041302#73041302\">I&#39;m getting this issue when trying to run the code I found on GitHub. Pydot and graphivz are installed but still getting this error<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":5.13,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":40.0,
        "Tool":"Azure Machine Learning"
    }
]