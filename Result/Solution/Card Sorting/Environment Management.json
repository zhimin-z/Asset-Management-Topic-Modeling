[
    {
        "Answerer_created_time":1589293508567,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been trying to send a train job through azure ml python sdk with:<\/p>\n<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig \n\nif __name__ == &quot;__main__&quot;:\n    ws = Workspace.from_config()\n    experiment = Experiment(workspace=ws, name='ConstructionTopicsModel')\n\n    config = ScriptRunConfig(source_directory='.\/',\n                         script='src\/azureml\/train.py',\n                         arguments=None,\n                         compute_target='ComputeTargetName',\n                         )\n\n    env = ws.environments['test-env']\n    config.run_config.environment = env\n    run = experiment.submit(config)\n    \n    run.wait_for_completion(show_output=True)\n\n    aml_url = run.get_portal_url()\n    print(aml_url)\n<\/code><\/pre>\n<p>But I was getting the <code>ServiceError<\/code> message:<\/p>\n<pre><code>AzureMLCompute job failed. FailedLoginToImageRegistry: Unable to login to docker image repo\nReason: Failed to login to the docker registry\nerror: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nserviceURL: 7ac86b04d6564d36aa80ae2ad090582c.azurecr.io\nReason: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.\n<\/code><\/pre>\n<p>I also tried using the azure cli without success, same error message<\/p>",
        "Challenge_closed_time":1643645330912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643645330913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to send a train job through Azure ML Python SDK. The job failed with a \"FailedLoginToImageRegistry\" error message, indicating that the user was unable to login to the docker image repo. The error message also suggested using \"--password-stdin\" instead of \"--password\" via the CLI. The user also tried using the Azure CLI but encountered the same error message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70929123",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":20.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureMLCompute job failed with `FailedLoginToImageRegistry`",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":202.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>The only way I've found so far to make this work, was to run it on a terminal of the compute-target itself. That's how the docker error goes away. Trying to run the experiment from a terminal of a different compute instance raises the exception.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":3.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3914.1044444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello, receiving the following error in an Azure Notebook VM while trying to import the ML library - \r\n\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\n# error here!!!\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b8d543bb7111> in <module>\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n----> 5 from azureml.train.automl import AutoMLConfig\r\n      6 from sklearn.externals import joblib\r\n      7 from azureml.core.model import Model\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/__init__.py in <module>\r\n     23     # Suppress the warnings at the import phase.\r\n     24     warnings.simplefilter(\"ignore\")\r\n---> 25     from ._automl import fit_pipeline\r\n     26     from .automlconfig import AutoMLConfig\r\n     27     from .automl_step import AutoMLStep, AutoMLStepRun\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_automl.py in <module>\r\n     17 from automl.client.core.runtime.cache_store import CacheStore\r\n     18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities\r\n---> 19 from azureml.automl.core import data_transformation, fit_pipeline as fit_pipeline_helper\r\n     20 from azureml.automl.core.automl_pipeline import AutoMLPipeline\r\n     21 from azureml.automl.core.data_context import RawDataContext, TransformedDataContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/fit_pipeline.py in <module>\r\n     18 from automl.client.core.common.limit_function_call_exceptions import TimeoutException\r\n     19 from automl.client.core.runtime.datasets import DatasetBase\r\n---> 20 from . import package_utilities, pipeline_run_helper, training_utilities\r\n     21 from .automl_base_settings import AutoMLBaseSettings\r\n     22 from .automl_pipeline import AutoMLPipeline\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/pipeline_run_helper.py in <module>\r\n     18 from automl.client.core.common.exceptions import ClientException\r\n     19 from automl.client.core.runtime import metrics\r\n---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module\r\n     21 from automl.client.core.runtime.datasets import DatasetBase\r\n     22 from automl.client.core.runtime.execution_context import ExecutionContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in <module>\r\n     21 \r\n     22 from automl.client.core.common import constants\r\n---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers\r\n     24 from automl.client.core.runtime.nimbus_wrappers import AveragedPerceptronBinaryClassifier, \\\r\n     25     AveragedPerceptronMulticlassClassifier, NimbusMlClassifierMixin, NimbusMlRegressorMixin\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in <module>\r\n     34 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n     35 if tf_found:\r\n---> 36     tf.logging.set_verbosity(tf.logging.ERROR)\r\n     37 \r\n     38     OPTIMIZERS = {\r\n \r\nAttributeError: module 'tensorflow' has no attribute 'logging'\r\n",
        "Challenge_closed_time":1587086020000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1572995244000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when attempting to import the \"ExplainationDashboard\" module from \"azureml.contrib.interpret\" in build 1.0.72, despite having installed and updated the SDK without any errors. The failing line is in a sample notebook for \"how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/644",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":45.44,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":3914.1044444444,
        "Challenge_title":"Error trying to load azureml.train.automl",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":272,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Do you know which version of tensorflow you are using? \r\n\r\nThis SO question may be applicable: https:\/\/stackoverflow.com\/questions\/55318626\/module-tensorflow-has-no-attribute-logging Hello, Not sure about tensorflow.  This is a \"stock\" Notebook VM that was created last week, so no changes were made to the libraries. Hello,\r\n\r\nSorry for the inconvenience. This issue has been fixed since v1.0.72 but, it's related to the fact that tf==2.0. is installed by default on the notebook instance. It broke other things too as TF2.0 has many changes in its API. Your two options are to upgrade to v1.0.72+ or use the following code to downgrade tensorflow.\r\n\r\npip install -U tensorflow-gpu==1.14.0 \r\ntensorflow==estimator==1.14.0 \r\n\r\nThat should fix it for you.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.2,
        "Solution_reading_time":9.24,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1418276189383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":272.0,
        "Answerer_view_count":142.0,
        "Challenge_adjusted_solved_time":1.9387202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have started using SageMaker Studio Lab.<\/p>\n<p>When I run &quot;apt install xvfb&quot; in SageMaker Studio Lab Notebook, I get the following error.<\/p>\n<pre><code>!apt install xvfb\n\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\n<\/code><\/pre>\n<p>Then I tried with sudo, but the sudo command was not installed.<\/p>\n<pre><code>!sudo apt install xvfb\n\n\/usr\/bin\/sh: 1: sudo: not found\n<\/code><\/pre>\n<p>Can you please tell me how to solve this problem?<\/p>",
        "Challenge_closed_time":1638468298390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638461318997,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to install xvfb in SageMaker Studio Lab Notebook using \"apt install\" command but is facing an error related to permission denied. The user also tried using sudo command but it was not installed. The user is seeking help to solve this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70202848",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.9387202778,
        "Challenge_title":"Is it possible to \"apt install\" in SageMaker Studio Lab?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1174.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1481861289276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>To date Studio Lab doesn't support package installs that require root access. It does support packages installable via pip and conda. You can do that either in your notebook with the %, rather than the !, or you can do that via opening a terminal.<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html<\/a><\/li>\n<\/ul>\n<p>If you'd like to open an issue you're welcome to do that on our repository right here:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md<\/a><\/li>\n<\/ul>\n<p>Thanks for trying out Studio Lab!<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.5,
        "Solution_reading_time":11.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":420.1906525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Please see the screenshots below. Once it said terminated but without reason:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/140829-screenshot-2021-10-13-221133.png?platform=QnA\" alt=\"140829-screenshot-2021-10-13-221133.png\" \/>    <br \/>\nThe other time there was nothing just stopped:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/140846-screenshot-2021-10-13-215523.png?platform=QnA\" alt=\"140846-screenshot-2021-10-13-215523.png\" \/>    <\/p>",
        "Challenge_closed_time":1635818701492,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634306015143,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's script is stopping without any explanation or error message, as shown in the attached screenshots. The script has terminated once without any reason and another time it just stopped abruptly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/592153\/my-script-stops-running-without-any-message-explai",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.6,
        "Challenge_reading_time":7.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":420.1906525,
        "Challenge_title":"my script stops running without any message explaining the reason",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>Hope you have solved this issue and we are sorry not seeing your response. Since this issue happened without any error details, support ticket would be the best way to debug that. Please let me know if you still need that. Thanks.  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":3.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1608747030727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":105.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":15.1535161111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I'm a beginner in docker and containers and I've been getting this error for days now.\nI get this error when my lambda function runs a sagemaker processing job.\nMy core python file resides in an s3 bucket.\nMy docker image resides in ECR.\nBut I dont understand why I dont get a similar error when I run the same processing job with a python docker image.\nPFB the python docker file that didnt throw any errors.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM python:latest\n#installing dependencies\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\nRUN pip3 install matplotlib\n<\/code><\/pre>\n<p>I only get this error when i run this with a an ubunutu docker image with python3 installed.\nPFB the dockerfile which throws the error mentioned.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM ubuntu:20.04\n\nRUN apt-get update -y\nRUN apt-get install -y python3\nRUN apt-get install -y python3-pip\n\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy==1.19.1\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\n\nENTRYPOINT [ &quot;python3&quot; ]\n<\/code><\/pre>\n<p>How do I fix this?<\/p>",
        "Challenge_closed_time":1622294378528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622233871527,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error \"container_linux.go:367: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown\" while running a sagemaker processing job in a docker container. The error occurs when using an Ubuntu docker image with Python3 installed, but not with a Python docker image. The user is seeking a solution to fix this issue.",
        "Challenge_last_edit_time":1622239825870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67745141",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":17.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":16.8075002778,
        "Challenge_title":"Facing this error : container_linux.go:367: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1123.0,
        "Challenge_word_count":205,
        "Platform":"Stack Overflow",
        "Poster_created_time":1608747030727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Fixed this error by changing the entry point to<\/p>\n<p><strong>ENTRYPOINT [ &quot;\/usr\/bin\/python3.8&quot;]<\/strong><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":1.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405882600928,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":115.0,
        "Challenge_adjusted_solved_time":0.3938825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an existing project, cloned with <code>git clone<\/code>.<\/p>\n\n<p>After I <code>pip install kedro<\/code> I can run <code>kedro info<\/code> fine but I dont seem to have access to the projects CLI for example if I try to run<code>kedro install<\/code> I get the following error:<\/p>\n\n<pre><code>Usage: kedro [OPTIONS] COMMAND [ARGS]...\nTry 'kedro -h' for help.\n\nError: No such command 'install'.\n<\/code><\/pre>\n\n<p>Any clues on what to do for existing projects are much appreciated.<\/p>\n\n<p>Not sure if this matters but I am working inside a conda environment which is inside a docker container.<\/p>",
        "Challenge_closed_time":1589565622150,
        "Challenge_comment_count":2,
        "Challenge_created_time":1589564204173,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has an existing project cloned with git and has installed Kedro using pip. While the user can run \"kedro info\" without any issues, they are unable to access the project's CLI and receive an error message when attempting to run \"kedro install\". The user is seeking guidance on how to access the CLI for existing projects. The user is working within a conda environment inside a docker container.",
        "Challenge_last_edit_time":1589721006283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61825202",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3938825,
        "Challenge_title":"Accessing Kedro CLI from an existing project",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1175.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1387377887347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Edinburgh, United Kingdom",
        "Poster_reputation_count":1240.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>Project CLIs are available if you run <code>kedro<\/code> at your Kedro project directory. <\/p>\n\n<ol>\n<li><p>Run <code>kedro new<\/code> to create a Kedro project<\/p><\/li>\n<li><p><code>cd &lt;your-kedro-project&gt;<\/code><\/p><\/li>\n<li><p><code>kedro<\/code> at the project directory<\/p><\/li>\n<\/ol>\n\n<p>And you should see the project level CLIs<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Also for your existing project, check if you have <code>kedro_cli.py<\/code> at your project directory.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":8.23,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.3988888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Challenge_closed_time":1606993756000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606945520000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in opening a terminal on an m5.8xlarge instance in SageMaker Studio, as the terminal only shows 2 CPUs instead of the expected 32. They are seeking guidance on how to open a terminal on the correct instance.",
        "Challenge_last_edit_time":1668512453971,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sagemaker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":13.3988888889,
        "Challenge_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1054.0,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Where do you launch the terminal from? If you use the launcher window, it would start on the t2.medium as you are experiencing.\n\nHowever, if you use the **launch terminal button** in the toolbar that is displayed at the top of your notebook, it will launch the image terminal on the new instance the notebook's kernel is running on (your m5.8xlarge instance).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1610011918846,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":4.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1538275960603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":5.2167880555,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I'm following the guidelines (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a>) to use a custom docker file on Azure. My script to create the environment looks like this:<\/p>\n<pre><code>from azureml.core.environment import Environment\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n<\/code><\/pre>\n<p>Upon execution, this is totally ignored and libgl1 is not installed. Any ideas why?<\/p>\n<p>EDIT: Here's the rest of my code:<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n    script_params = script_params,\n    use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n    pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    )\n\nrun = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a><\/p>",
        "Challenge_closed_time":1594750426867,
        "Challenge_comment_count":2,
        "Challenge_created_time":1594687513407,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use a custom docker file on Azure ML by following the guidelines provided by Microsoft. However, upon execution, the installation of libgl1 is ignored and not installed. The user is seeking help to understand why this is happening.",
        "Challenge_last_edit_time":1594731646430,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62886435",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":16.1,
        "Challenge_reading_time":19.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":17.4759611111,
        "Challenge_title":"Using a custom docker with Azure ML",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1819.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366237908703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>This should work :<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.environment import Environment\nfrom azureml.train.estimator import Estimator\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core import Experiment\n\nws = Workspace (...)\nexp = Experiment(ws, 'test-so-exp')\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n\n## You need to instead put your packages in the Environment definition instead... \n## see below for some changes too\n\nmyenv.python.conda_dependencies = CondaDependencies.create(pip_packages = ['scipy==1.1.0', 'torch==1.5.1'])\n<\/code><\/pre>\n<p>Finally you can build your estimator a bit differently :<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n#     script_params = script_params,\n#     use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n#     pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    environment_definition=myenv\n    )\n<\/code><\/pre>\n<p>And submit it :<\/p>\n<pre><code>run = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>Let us know if that works.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":18.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":121.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424063473423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":334.0,
        "Answerer_view_count":347.0,
        "Challenge_adjusted_solved_time":52.7002555556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>On Amazon SageMaker, it's possible to edit most properties of a notebook instance when the instance is not active, but it does not seem possible to change its name.<\/p>\n\n<p>Is there any way to rename an existing SageMaker notebook instance?<\/p>",
        "Challenge_closed_time":1556304752800,
        "Challenge_comment_count":0,
        "Challenge_created_time":1556108497957,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in renaming an existing SageMaker notebook instance as it does not seem possible to change its name even though most properties of the instance can be edited when it is not active.",
        "Challenge_last_edit_time":1556115031880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55829940",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":3.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":54.5152341667,
        "Challenge_title":"How to rename a SageMaker notebook instance?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1311.0,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1283441031492,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lausanne, Switzerland",
        "Poster_reputation_count":4841.0,
        "Poster_view_count":261.0,
        "Solution_body":"<p>Thank you for using Amazon SageMaker. <\/p>\n\n<p>SageMaker Notebook Instance's name cannot be edited. <\/p>\n\n<p>Thanks,<br>\nNeelam<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":1.7,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":8.0431913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Azure ML, I'm trying to execute a Python module that needs to import the module pyxdameraulevenshtein (<a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>).<\/p>\n\n<p>I followed the usual way, which is to create a zip file and then import it; however for this specific module, it seems to never be able to find it. The error message is as usual:<\/p>\n\n<p><em>ImportError: No module named 'pyxdameraulevenshtein'<\/em><\/p>\n\n<p>Has anyone included this pyxdameraulevenshtein module in Azure ML with success ?<\/p>\n\n<p>(I took the package from <a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>.)<\/p>\n\n<p>Thanks for any help you can provide,<\/p>\n\n<p>PH<\/p>",
        "Challenge_closed_time":1496307870776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496235911927,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to execute a Python module that needs to import the module pyxdameraulevenshtein in Azure ML. Despite following the usual way of creating a zip file and importing it, the module seems to never be found, resulting in an ImportError. The user is seeking help to include this module in Azure ML successfully.",
        "Challenge_last_edit_time":1496278915287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44285641",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":11.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":19.9885691667,
        "Challenge_title":"Azure ML Python with Script Bundle cannot import module",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2395.0,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1496233557192,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I viewed the <code>pyxdameraulevenshtein<\/code> module page, there are two packages you can download which include a wheel file for MacOS and a source code tar file. I don't think you can directly use the both on Azure ML, because the MacOS one is just a share library <code>.so<\/code> file for darwin which is not compatible with Azure ML, and the other you need to first compile it.<\/p>\n\n<p>So my suggestion is as below for using <code>pyxdameraulevenshtein<\/code>.<\/p>\n\n<ol>\n<li>First, compile the source code of <code>pyxdameraulevenshtein<\/code> to a DLL file on Windows, please refer to the document for Python <a href=\"https:\/\/docs.python.org\/2\/extending\/windows.html\" rel=\"nofollow noreferrer\">2<\/a>\/<a href=\"https:\/\/docs.python.org\/3\/extending\/windows.html\" rel=\"nofollow noreferrer\">3<\/a> or search for doing this.<\/li>\n<li>Write a Python script using the DLL you compiled to implement your needs, please refer to the SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/252417\/how-can-i-use-a-dll-file-from-python\">How can I use a DLL file from Python?<\/a> for how to use DLL from Python and refer to the Azure offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts\" rel=\"nofollow noreferrer\">tutorial<\/a> to write your Python script<\/li>\n<li>Package your Python script and DLL file as a zip file, then to upload the zip file to use it in <code>Execute Python script<\/code> model of Azure ML.<\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.5,
        "Solution_reading_time":19.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":192.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.9785680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Since the latest Azure ML release, we have been unable to submit any job using a private docker registry. Same jobs were working before the new release.  <br \/>\nWe configure the job as follows (all of this is automated and the code has not changed):<\/p>\n<p>base_image_name = 'REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_1601582281'<\/p>\n<pre><code># Set the container registry information  \n\nmyenv = Environment(name=&amp;#34;lb&amp;#34;)  \n\nmyenv.docker.enabled = True  \nmyenv.docker.base_image = base_image_name  \nmyenv.docker.base_image_registry.address = &amp;#39;REDACTED.azurecr.io\/lb\/&amp;#39;  \nmyenv.docker.base_image_registry.username, myenv.docker.base_image_registry.password = get_docker_secrets()  \nmyenv.python.user_managed_dependencies = True  \n\nmyenv.python.interpreter_path = &amp;#34;\/opt\/miniconda\/bin\/python&amp;#34;  \n<\/code><\/pre>\n<p>Instead of successful job submission, we are instead getting:  <br \/>\n{  <br \/>\n&quot;error&quot;: {  <br \/>\n&quot;message&quot;: &quot;Activity Failed:\\n{\\n \\&quot;error\\&quot;: {\\n \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n \\&quot;message\\&quot;: \\&quot;Unable to get image details : Specified base docker image REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_16\\&quot;,\\n \\&quot;details\\&quot;: []\\n },\\n \\&quot;correlation\\&quot;: {\\n \\&quot;operation\\&quot;: null,\\n \\&quot;request\\&quot;: \\&quot;c41448d429f9c80b\\&quot;\\n },\\n \\&quot;environment\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;location\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;time\\&quot;: \\&quot;2020-11-09T21:40:39.699533Z\\&quot;,\\n \\&quot;componentName\\&quot;: \\&quot;execution-worker\\&quot;\\n}&quot;  <br \/>\n}  <br \/>\n}  <br \/>\nThe image has not changed (we tried a few different ones from prior successful jobs) and the use of the SDK has not changed.  <br \/>\nHas anybody else encountered a similar problem since the Nov 5 upgrade (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes<\/a>)?  <br \/>\nThis is a major block as we cannot proceed with any project that depend on Azure ML at this time.<\/p>",
        "Challenge_closed_time":1605016029612,
        "Challenge_comment_count":4,
        "Challenge_created_time":1604958506767,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to submit any job using a private docker registry since the latest Azure ML release. The error message received is \"Unable to get image details\". The image and the use of the SDK have not changed. The user is seeking help from others who may have encountered a similar problem since the Nov 5 upgrade.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/157021\/unable-to-use-private-docker-registry-with-latest",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":14.5,
        "Challenge_reading_time":29.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15.9785680556,
        "Challenge_title":"Unable to use private docker registry with latest Azure ML release",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":191,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d02d6aeb-d51f-460e-9be5-e8da649952cc\">@Fabien Campagne  <\/a>  Thanks for the details, with fully qualified base image name you do not need to specify container registry address. container registry address itself should be just a host name.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":3.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1444758849803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11962.0,
        "Answerer_view_count":960.0,
        "Challenge_adjusted_solved_time":4.5755286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What is the command to check the version of Gremlin Python client running on a AWS Sagemaker jupyter notebook? I would like to run the command on the jupyter notebook cell.<\/p>",
        "Challenge_closed_time":1610839775480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610823303577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to check the version of Gremlin Python client running on an AWS Sagemaker Jupyter notebook and wants to know the command to run on the Jupyter notebook cell.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65753455",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":4.5755286111,
        "Challenge_title":"How to get the version of gremlin python client on AWS SageMaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":198.0,
        "Challenge_word_count":42,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475704783950,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":473.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>From a notebook cell you should be able to just ask Pip which version is being used<\/p>\n<pre><code>!pip list | grep gremlinpython\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":1.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1370286859500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16981.0,
        "Answerer_view_count":1733.0,
        "Challenge_adjusted_solved_time":21792.3181611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need the sacred package for a new code base I downloaded. It requires sacred. \n<a href=\"https:\/\/pypi.python.org\/pypi\/sacred\" rel=\"noreferrer\">https:\/\/pypi.python.org\/pypi\/sacred<\/a><\/p>\n\n<p>conda install sacred fails with \nPackageNotFoundError: Package missing in current osx-64 channels: \n  - sacred<\/p>\n\n<p>The instruction on the package site only explains how to install with pip. What do you do in this case?<\/p>",
        "Challenge_closed_time":1501767830263,
        "Challenge_comment_count":2,
        "Challenge_created_time":1501710168027,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to install the 'sacred' package using conda and is receiving a 'PackageNotFoundError'. The user is seeking guidance on how to install the package in this scenario as the package's website only provides instructions for installation using pip.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45471477",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.2,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.0172877778,
        "Challenge_title":"python package can be installed by pip but not conda",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":7845.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1321905325720,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California, United States",
        "Poster_reputation_count":3278.0,
        "Poster_view_count":399.0,
        "Solution_body":"<p>That package is not available as a conda package at all. You can search for packages on anaconda.org: <a href=\"https:\/\/anaconda.org\/search?q=sacred\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=sacred<\/a> You can see the type of package in the 4th column. Other Python packages may be available as conda packages, for instance, NumPy: <a href=\"https:\/\/anaconda.org\/search?q=numpy\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=numpy<\/a><\/p>\n\n<p>As you can see, the conda package numpy is available from a number of different channels (the channel is the name before the slash). If you wanted to install a package from a different channel, you can add the option to the install\/create command with the <code>-c<\/code>\/<code>--channel<\/code> option, or you can add the channel to your configuration <code>conda config --add channels channel-name<\/code>.<\/p>\n\n<p>If no conda package exists for a Python package, you can either install via pip (if available) or <a href=\"https:\/\/docs.conda.io\/projects\/conda-build\/en\/latest\/user-guide\/tutorials\/building-conda-packages.html\" rel=\"nofollow noreferrer\">build your own conda package<\/a>. This isn't usually too difficult to do for pure Python packages, especially if one can use <code>skeleton<\/code> to build a recipe from a package on PyPI.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1580162513407,
        "Solution_link_count":5.0,
        "Solution_readability":11.0,
        "Solution_reading_time":16.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":163.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1317676233236,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":2348.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":71.3002738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?<\/p>",
        "Challenge_closed_time":1555007316016,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554750635030,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know how to update their SageMaker notebook's Jupyter environment to the latest alpha release and restart the process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55580232",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":2.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":71.3002738889,
        "Challenge_title":"Update SageMaker Jupyterlab environment",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1720.0,
        "Challenge_word_count":22,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>Hi and thank you for using SageMaker!<\/p>\n\n<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait<\/code>.<\/p>\n\n<p>Best,\nKevin<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":2.95,
        "Solution_score_count":8.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1537462795807,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":99.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":2937.8422519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to load containers stored in docker hub registry in Amazon Sagemaker.\nAccording to some documentation, it should be possible, but I have not been able to find any relevan example or guide for it.<\/p>",
        "Challenge_closed_time":1582242007587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571665775480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in loading containers stored in Docker Hub registry in Amazon Sagemaker and is unable to find relevant examples or guides for the same.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58487710",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":3.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2937.8422519445,
        "Challenge_title":"User Docker Hub registry containers in AWS Sagemaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":135.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1382876091092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Valencia, Espa\u00f1a",
        "Poster_reputation_count":149.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>While you can use any registry when working with Docker on a SageMaker notebook, as of this writing other SageMaker components presently only support images from Amazon ECR repositories.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.8,
        "Solution_reading_time":2.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5544444444,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation.  I have some initialization to do at creation time (clone a github repo, etc.).  That all works fine.  The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a `Stopped` state.  A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\n```\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n```\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige.  I would have thought it would let me do the same in the lifecycle config.  Unfortunately, SageMaker still has the notebook in the `Pending` state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Challenge_closed_time":1539777191000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539775195000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to create a notebook instance using CloudFormation with SageMaker LifeCycleConfig, but there is no way to leave the newly-created instance in a `Stopped` state. The user tried using the AWS CLI to stop the instance from the lifecycle create script, but it failed. The user is looking for other hooks or creative options to solve this issue.",
        "Challenge_last_edit_time":1668613505334,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloudformation-with-sagemaker-lifecycleconfig-without-leaving-the-instance-running",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.5544444444,
        "Challenge_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":479.0,
        "Challenge_word_count":208,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"One solutions will be to create a [CFN custom resource](https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/template-custom-resources-lambda.html) backed by lambda.\nYou can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925548143,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":4.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1348171784712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":93.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":105.8507644444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I use PyTorch estimator with SageMaker to train\/fine-tune my Graph Neural Net on multi-GPU machines.<\/p>\n<p>The <code>requirements.txt<\/code> that gets installed into the Estimator container, has lines like:<\/p>\n<pre><code>torch-scatter -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\ntorch-sparse -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\ntorch-cluster -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\ntorch-spline-conv -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\n<\/code><\/pre>\n<p>When SageMaker installs these requirements in the Estimator on the endpoint, it takes ~<strong>2 hrs<\/strong> to build the wheel. It takes only seconds on a local Linux box.<\/p>\n<p>SageMaker Estimator:<\/p>\n<p>PyTorch v1.10\nCUDA 11.x\nPython 3.8\nInstance: ml.p3.16xlarge<\/p>\n<p>I have noticed the same issue with other wheel-based components that require CUDA.<\/p>\n<p>I have also tried building a Docker container on p3.16xlarge and running that on SageMaker, but it was unable to recognize the instance GPUs<\/p>\n<p>Anything I can do to cut down these build times?<\/p>",
        "Challenge_closed_time":1649126605252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648745542500,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing long build times for CUDA components when using Amazon SageMaker ScriptMode with PyTorch estimator. The requirements.txt file takes around 2 hours to build the wheel on the SageMaker Estimator, while it takes only seconds on a local Linux box. The user has tried building a Docker container on p3.16xlarge and running it on SageMaker, but it was unable to recognize the instance GPUs. The user is seeking advice on how to reduce the build times.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71696407",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":7.7,
        "Challenge_reading_time":14.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":105.8507644444,
        "Challenge_title":"Amazon SageMaker ScriptMode Long Python Wheel Build Times for CUDA Components",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":136.0,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1348171784712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":93.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The solution is to augment the stock estimator image with the right components and then it can be run in the SageMaker script mode:<\/p>\n<pre><code>FROM    763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-training:1.10-gpu-py38\n\nCOPY requirements.txt \/tmp\/requirements.txt\nRUN pip install -r \/tmp\/requirements.tx\n<\/code><\/pre>\n<p>The key is to make sure <code>nvidia<\/code> runtime is used at build time, so <code>daemon.json<\/code> needs to be configured accordingly:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;default-runtime&quot;: &quot;nvidia&quot;,\n    &quot;runtimes&quot;: {\n        &quot;nvidia&quot;: {\n            &quot;path&quot;: &quot;nvidia-container-runtime&quot;,\n            &quot;runtimeArgs&quot;: []\n        }\n    }\n}\n<\/code><\/pre>\n<p>This is still not a complete solution, because viability of the build for SageMaker depends on the host where the build is performed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":11.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1370085456943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":344.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":27.1951413889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have some python code that trains a Neural Network using tensorflow. <\/p>\n\n<p>I've created a docker image based on a tensorflow\/tensorflow:latest-gpu-py3 image that runs my python script.\nWhen I start an EC2 p2.xlarge instance I can run my docker container using the command<\/p>\n\n<pre><code>docker run --runtime=nvidia cnn-userpattern train\n<\/code><\/pre>\n\n<p>and the container with my code runs with no errors and uses the host GPU. <\/p>\n\n<p>The problem is, when I try to run the same container in an AWS Sagemaker training job with instance ml.p2.xlarge (I also tried with ml.p3.2xlarge), the algorithm fails with error code:<\/p>\n\n<blockquote>\n  <p>ImportError: libcuda.so.1: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n\n<p>Now I know what that error code means. It means that the runtime environment of the docker host is not set to \"nvidia\". The AWS documentation says that the command used to run the docker image is always<\/p>\n\n<pre><code>docker run image train\n<\/code><\/pre>\n\n<p>which would work if the default runtime is set to \"nvidia\" in the docker\/deamon.json. Is there any way to edit the host deamon.json or tell docker in the Dockerfile to use \"--runtime=nvidia\"?<\/p>",
        "Challenge_closed_time":1551964655196,
        "Challenge_comment_count":0,
        "Challenge_created_time":1551866752687,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to start an AWS Sagemaker training job with GPU access in their docker container. They have created a docker image based on a tensorflow\/tensorflow:latest-gpu-py3 image that runs their python script. While running the container in an AWS Sagemaker training job with instance ml.p2.xlarge, the algorithm fails with an error code \"ImportError: libcuda.so.1: cannot open shared object file: No such file or directory\". The user suspects that the runtime environment of the docker host is not set to \"nvidia\" and is looking for a way to edit the host deamon.json or tell docker in the Dockerfile to use \"--runtime=nvidia\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55020390",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":16.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":27.1951413889,
        "Challenge_title":"How do I start an AWS Sagemaker training job with GPU access in my docker container?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1858.0,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370085456943,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":344.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>With some help of the AWS support service we were able to find the problem.\nThe docker image I used to run my code on was, as I said tensorflow\/tensorflow:latest-gpu-py3 (available on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a>)<\/p>\n\n<p>the \"latest\" tag refers to version 1.12.0 at this time. The problem was not my own, but with this version of the docker image. <\/p>\n\n<p>If I base my docker image on tensorflow\/tensorflow:1.10.1-gpu-py3, it runs as it should and uses the GPU fully. <\/p>\n\n<p>Apparently the default runtime is set to \"nvidia\" in the docker\/deamon.json on all GPU instances of AWS sagemaker.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.3,
        "Solution_reading_time":9.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":98.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":145.0984213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What-If and Individual Conditional Expectation (ICE) plots are not supported in Azure Machine Learning studio under the Explanations tab since the uploaded explanation needs an active compute to recalculate predictions and probabilities of perturbed features. It is currently supported in Jupyter notebooks when run as a widget using the SDK. How can I open the automated ML explanation in Jupyter notebooks?<\/p>",
        "Challenge_closed_time":1614614318550,
        "Challenge_comment_count":5,
        "Challenge_created_time":1614091964233,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in opening the automated ML explanation in Jupyter notebooks. What-If and Individual Conditional Expectation (ICE) plots are not supported in Azure Machine Learning studio under the Explanations tab, but can be run as a widget using the SDK in Jupyter notebooks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/285089\/how-can-i-open-the-automated-ml-explanation-in-jup",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":13.8,
        "Challenge_reading_time":6.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":145.0984213889,
        "Challenge_title":"How can I open the automated ML explanation in Jupyter notebooks?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello Cagatay,<\/p>\n<p>In jupyter notebook for AutoML models, you can download the trained model, then compute explanations locally and visualize the explanation results using ExplanationDashboard from interpret-community. Sample code below:-<\/p>\n<pre><code>best_run, fitted_model = remote_run.get_output()\n\nfrom azureml.train.automl.runtime.automl_explain_utilities import AutoMLExplainerSetupClass, automl_setup_model_explanations\nautoml_explainer_setup_obj = automl_setup_model_explanations(fitted_model, X=X_train,\n                                                                                                                         X_test=X_test, y=y_train,\n                                                                                                                         task='regression')\n\nfrom interpret.ext.glassbox import LGBMExplainableModel\nfrom azureml.interpret.mimic_wrapper import MimicWrapper\nexplainer = MimicWrapper(ws, automl_explainer_setup_obj.automl_estimator, LGBMExplainableModel,\n                         init_dataset=automl_explainer_setup_obj.X_transform, run=best_run,\n                         features=automl_explainer_setup_obj.engineered_feature_names,\n                         feature_maps=[automl_explainer_setup_obj.feature_map],\n                         classes=automl_explainer_setup_obj.classes)\n\npip install interpret-community[visualization]\n\nengineered_explanations = explainer.explain(['local', 'global'], eval_dataset=automl_explainer_setup_obj.X_test_transform)\nprint(engineered_explanations.get_feature_importance_dict()),\nfrom interpret_community.widget import ExplanationDashboard\nExplanationDashboard(engineered_explanations, automl_explainer_setup_obj.automl_estimator, datasetX=automl_explainer_setup_obj.X_test_transform)\n\nraw_explanations = explainer.explain(['local', 'global'], get_raw=True, \n                                     raw_feature_names=automl_explainer_setup_obj.raw_feature_names,\n                                     eval_dataset=automl_explainer_setup_obj.X_test_transform)\nprint(raw_explanations.get_feature_importance_dict()),\nfrom interpret_community.widget import ExplanationDashboard\nExplanationDashboard(raw_explanations, automl_explainer_setup_obj.automl_pipeline, datasetX=automl_explainer_setup_obj.X_test_raw)\n<\/code><\/pre>\n<p>The code sample repo please refer to: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/explain-model\/azure-integration\/scoring-time\/train-explain-model-locally-and-deploy.ipynb\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/explain-model\/azure-integration\/scoring-time\/train-explain-model-locally-and-deploy.ipynb<\/a><\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":48.0,
        "Solution_reading_time":32.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":103.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1329.0686111111,
        "Challenge_answer_count":0,
        "Challenge_body":"I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\r\nThe name of the inputs are:\r\n1. `input_image` \r\n2. `input_image_meta` \r\n3. `input_anchors` \r\n\r\n\r\nand the name of outputs are:\r\n1.  `output_detections`\r\n2.  `output_mrcnn_class`\r\n3.  `output_mrcnn_bbox`\r\n4.  `output_mrcnn_mask`\r\n5.  `output_rois`\r\n\r\nI have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting `{'error': \"Missing 'inputs' or 'instances' key\"}` in return.\r\n \r\nI have made a model.tar.gz file which has the following structure:\r\n\r\n    mymodel\r\n        |__1\r\n            |__variables\r\n            |__saved_model.pb\r\n\r\n    code\r\n        |__inference.py\r\n        |__requirements.txt\r\n\r\nAs specified in the documentation, inference.py has input_handler and output handler functions. From the client-side, I pass the S3 link of the image which then transforms to the three inputs for the model. \r\n\r\nThe structure of input_handler is as follows:\r\n\r\n```\r\ndef input_handler(data, context):\r\n     input_data = json.loads(data.read().decode('utf-8'))\r\n\r\n    obj = bucket.Object(input_data['img_link'])\r\n    tmp = tempfile.NamedTemporaryFile()\r\n    \r\n    # download image from AWS S3\r\n    with open(tmp.name, 'wb') as f:\r\n        obj.download_fileobj(f)\r\n        image=mpimg.imread(tmp.name)\r\n    \r\n    # make preprocessing\r\n    image = Image.fromarray(image)\r\n     \r\n     ...... # some more transformations \r\n     return = {\"input_image\": Python list for image,\r\n                    \"input_image_meta: Python list for input image meta,\r\n                    \"input_anchors\": Python list for input anchors}\r\n\r\n```\r\nThe deifinition of output_handler is as follows:\r\n\r\n```\r\ndef output_handler(data, context):\r\n      output_string = data.content.decode('unicode-escape')\r\n      return output_string, context.accept_header\r\n```\r\n\r\nThe sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\r\nOn the client side, I call the predictor using follwoing code:\r\n\r\n```\r\nrequest = {}\r\nrequest[\"img_link\"] = \"image.jpg\"\r\nresult = predictor.predict(request)\r\n```\r\n\r\nBut when I print the result the following gets printed out, `{'error': \"Missing 'inputs' or 'instances' key\"}`\r\nAll the bucket connections for loading the image are in inference.py\r\n      ",
        "Challenge_closed_time":1571957138000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1567172491000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error message stating that no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/73",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":28.7,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":104.0,
        "Challenge_repo_issue_count":229.0,
        "Challenge_repo_star_count":160.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1329.0686111111,
        "Challenge_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":283,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello @janismdhanbad,\r\n\r\nI believe your inference requests will have to follow the TensorFlow serving REST API specifications defined here: https:\/\/www.tensorflow.org\/tfx\/serving\/api_rest#request_format_2\r\n\r\n```\r\nThe request body for predict API must be JSON object formatted as follows:\r\n\r\n{\r\n  \/\/ (Optional) Serving signature to use.\r\n  \/\/ If unspecifed default serving signature is used.\r\n  \"signature_name\": <string>,\r\n\r\n  \/\/ Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\r\n  \/\/ A request can have either of them but NOT both.\r\n  \"instances\": <value>|<(nested)list>|<list-of-objects>\r\n  \"inputs\": <value>|<(nested)list>|<object>\r\n}\r\n``` closing due to inactivity. feel free to reopen if necessary.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.3,
        "Solution_reading_time":8.78,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1264671735676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA",
        "Answerer_reputation_count":8619.0,
        "Answerer_view_count":1286.0,
        "Challenge_adjusted_solved_time":90.8566333334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to use a post-startup script to create a Vertex AI User Managed Notebook whose Jupyter Lab has a dedicated virtual environment and corresponding computing kernel when first launched. I have had success creating the instance and then, as a second manual step from within the Jupyter Lab &gt; Terminal, running a bash script like so:<\/p>\n<pre><code>#!\/bin\/bash\ncd \/home\/jupyter\nmkdir -p env\ncd env\npython3 -m venv envName --system-site-packages\nsource envName\/bin\/activate\nenvName\/bin\/python3 -m pip install --upgrade pip\npython -m ipykernel install --user --name=envName\npip3 install geemap --user \npip3 install earthengine-api --user \npip3 install ipyleaflet --user \npip3 install folium --user \npip3 install voila --user \npip3 install jupyterlab_widgets\ndeactivate\njupyter labextension install --no-build @jupyter-widgets\/jupyterlab-manager jupyter-leaflet\njupyter lab build --dev-build=False --minimize=False\njupyter labextension enable @jupyter-widgets\/jupyterlab-manager\n<\/code><\/pre>\n<p>However, I have not had luck using this code as a post-startup script (being supplied through the console creation tools, as opposed to command line, thus far). When I open Jupyter Lab and look at the relevant structures, I find that there is no environment or kernel. Could someone please provide a working example that accomplishes my aim, or otherwise describe the order of build steps that one would follow?<\/p>",
        "Challenge_closed_time":1662967888203,
        "Challenge_comment_count":2,
        "Challenge_created_time":1662640804323,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is attempting to create a custom kernel via a post-startup script in Vertex AI User Managed notebook. They have been successful in creating the instance and running a bash script manually, but have not been able to use the same code as a post-startup script. They are seeking assistance in creating a working example or understanding the order of build steps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73649262",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":17.8,
        "Challenge_reading_time":18.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":90.8566333334,
        "Challenge_title":"Create custom kernel via post-startup script in Vertex AI User Managed notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":206,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459350905808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Post startup scripts run as root.\nWhen you run:<\/p>\n<pre><code>python -m ipykernel install --user --name=envName\n<\/code><\/pre>\n<p>Notebook is using current user which is <code>root<\/code> vs when you use Terminal, which is running as <code>jupyter<\/code> user.<\/p>\n<p>Option 1) Have 2 scripts:<\/p>\n<ul>\n<li>Script A. Contents specified in original post. Example: <code>gs:\/\/newsml-us-central1\/so73649262.sh<\/code><\/li>\n<li>Script B. Downloads script and execute it as <code>jupyter<\/code>. Example: <code>gs:\/\/newsml-us-central1\/so1.sh<\/code> and use it as post-startup script.<\/li>\n<\/ul>\n<pre><code>#!\/bin\/bash\n\nset -x\n\ngsutil cp gs:\/\/newsml-us-central1\/so73649262.sh \/home\/jupyter\nchown jupyter \/home\/jupyter\/so73649262.sh\nchmod a+x \/home\/jupyter\/so73649262.sh\nsu -c '\/home\/jupyter\/so73649262.sh' jupyter\n<\/code><\/pre>\n<p>Option 2) Create a file in bash using EOF. Write the contents into a single file and execute it as mentioned above.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.5,
        "Solution_reading_time":12.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":108.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1511190340208,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":385.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":1001.86839,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I noticed my Sagemaker (Amazon aws) jupyter notebook has an outdated version of the sklearn library.<\/p>\n<p>when I run <code>! pip freeze<\/code> I get:<\/p>\n<pre><code>sklearn==0.0\n<\/code><\/pre>\n<p>and when I run (with python) <code>print(sklearn.__version__)<\/code> I get<\/p>\n<pre><code>0.24.1\n<\/code><\/pre>\n<p>I'm not sure which one is my real version but I need 1.0.0 in order to use the <code>from_predictions()<\/code> method.<\/p>\n<p>But when I am trying to run <code>! \/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/bin\/python -m pip install --upgrade sklearn<\/code> I am getting the following output:<\/p>\n<blockquote>\n<p>Requirement already satisfied: sklearn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(0.0) Requirement already satisfied: scikit-learn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from sklearn) (0.24.1) Requirement already satisfied: scipy&gt;=0.19.1\nin\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.5.3) Requirement already satisfied:\njoblib&gt;=0.11 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.0.1) Requirement already satisfied:\nthreadpoolctl&gt;=2.0.0 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (2.1.0) Requirement already satisfied:\nnumpy&gt;=1.13.3 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.19.5)<\/p>\n<\/blockquote>\n<p>This is a very pupular library so it's weird if sagemaker cant upgrade it. Anyone has an idea what am I doing wrong?<\/p>",
        "Challenge_closed_time":1641036273827,
        "Challenge_comment_count":2,
        "Challenge_created_time":1637426688750,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to upgrade the sklearn library in their Sagemaker Jupyter notebook to version 1.0.0 in order to use the `from_predictions()` method. However, when they run the upgrade command, they receive a message indicating that the requirement is already satisfied with version 0.24.1. The user is seeking assistance in resolving this issue.",
        "Challenge_last_edit_time":1637429547623,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70047920",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.1,
        "Challenge_reading_time":23.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1002.6625213889,
        "Challenge_title":"How to upgrade the sklearn library in sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":634.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1381413304940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":593.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>I managed to update sklearn to version 0.24.2 via the following command:<\/p>\n<pre><code>!conda update scikit-learn --yes\n<\/code><\/pre>\n<p>To further update it, you probably have to also update Python, which is version 3.6 in the current conda_python3 kernel on Sagemaker.<\/p>\n<p>It also looks promising to create your custom conda environment, as explained here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":7.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.3733694445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pip install pyenchant, but It doesn't seem to be working.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/169225-image.png?platform=QnA\" alt=\"169225-image.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/169252-image.png?platform=QnA\" alt=\"169252-image.png\" \/>    <\/p>\n<p>Is there any other way?    <br \/>\n<a href=\"https:\/\/stackoverflow.com\/questions\/21083059\/enchant-c-library-not-found-while-installing-pyenchant-using-pip-on-osx\">https:\/\/stackoverflow.com\/questions\/21083059\/enchant-c-library-not-found-while-installing-pyenchant-using-pip-on-osx<\/a>    <br \/>\nI looked it up but do not know where to put it    <br \/>\nThanks!<\/p>",
        "Challenge_closed_time":1643357543967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643330999837,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has installed pyenchant using pip but it is not working. They are looking for an alternative solution and have found a Stack Overflow post suggesting to install the enchant C library, but they are unsure where to put it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/713217\/machine-learning-conda-env-package(pyenchant)",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":20.1,
        "Challenge_reading_time":9.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7.3733694445,
        "Challenge_title":"machine learning conda env package(pyenchant)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c427e306-40a4-4da4-b489-b3f6aae251d7\">@Yongchao Liu (Neusoft America Inc)  <\/a> Based on the error it looks like you also need to ensure the enchant C library is available to use for the package. Based on the pip install <a href=\"https:\/\/pyenchant.github.io\/pyenchant\/install.html#installation\">page<\/a> of pyenchant, the package will not work directly out of the box using pip.    <\/p>\n<blockquote>\n<p>In general, PyEnchant will not work out of the box after having been installed with pip. See the Installation section for more details.    <\/p>\n<\/blockquote>\n<p>Since you are using Linux, this is the guidance on the installation page.    <\/p>\n<blockquote>\n<p>The quickest way is to install libenchant using the package manager of your current distribution. PyEnchant tries to be compatible with a large number of libenchant versions. If you find an incompatibility with your libenchant installation, feel free to open a bug report.    <\/p>\n<p>To detect the libenchant binaries, PyEnchant uses ctypes.util.find_library(), which requires ldconfig, gcc, objdump or ld to be installed. This is the case on most major distributions, however statically linked distributions (like Alpine Linux) might not bring along binutils by default.    <\/p>\n<\/blockquote>\n<p>I believe you are using the ubuntu flavor of the azureml base image, In this case I think adding <a href=\"https:\/\/ubuntu.pkgs.org\/20.04\/ubuntu-main-amd64\/libenchant-2-dev_2.2.8-1_amd64.deb.html\">libenchant-2-dev<\/a> as dependency in your YAML should work.     <\/p>\n<pre><code>-libenchant-2-dev=2.2.8  \n<\/code><\/pre>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.1,
        "Solution_reading_time":25.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":236.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":168.6647222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Challenge_closed_time":1657139268000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656532075000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to initialize the kedro-mlflow project as the CLI commands are not available. The user has tried to create a Kedro project using the starter `pandas-iris` and installing kedro-mlflow, but the `mlflow` command is unknown to Kedro inside the project folder. The user is seeking advice on how to fix this issue. The bug also happens with the last version on master.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.77,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":168.6647222222,
        "Challenge_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":0.6604344445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to overwrite properties taken from the parameters.yaml file within a Kedro notebook?<\/p>\n\n<p>I am trying to dynamically change parameter values within a notebook. I would like to be able to give users the ability to run a standard pipeline but with customizable parameters. I don't want to change the YAML file, I just want to change the parameter for the life of the notebook.<\/p>\n\n<p>I have tried editing the params within the context but this has no affect.<\/p>\n\n<pre><code>context.params.update({\"test_param\": 2})\n<\/code><\/pre>\n\n<p>Am I missing something or is this not an intended use case?<\/p>",
        "Challenge_closed_time":1582114523347,
        "Challenge_comment_count":1,
        "Challenge_created_time":1582112145783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to dynamically change parameter values within a Kedro notebook without changing the YAML file. They have attempted to edit the parameters within the context but it did not work. They are seeking clarification on whether this is an intended use case or if they are missing something.",
        "Challenge_last_edit_time":1583420995670,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60299478",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":8.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.6604344445,
        "Challenge_title":"Setting parameters in Kedro Notebook",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":586.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582111403048,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Kedro supports specifying <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/03_configuration.html#specifying-extra-parameters\" rel=\"nofollow noreferrer\">extra parameters<\/a> from the command line by running<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>kedro run --params \"key1:value1,key2:value2\"\n<\/code><\/pre>\n\n<p>which solves your second use case.<\/p>\n\n<p>As for the notebook use case, updating <code>context.params<\/code> does not have any effect since the context does not store the parameters on <code>self<\/code> but rather pulls them from the config every time the property is being called.<\/p>\n\n<p>However you can still add extra parameters to the context object after it being instantiated:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>extra_params = context._extra_params or {}\nextra_params.update({\"test_param\": 2})\ncontext._extra_params = extra_params\n<\/code><\/pre>\n\n<p>This will update extra parameters that are applied on top of regular parameters coming from the config.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.4,
        "Solution_reading_time":13.31,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":106.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":17.6446433334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I made a classifier in Python that uses a lot of libraries. I have uploaded the model to Amazon S3 as a pickle (my_model.pkl). Ideally, every time someone uploads a file to a specific S3 bucket, it should trigger an AWS Lambda that would load the classifier, return predictions and save a few files on an Amazon S3 bucket.<\/p>\n<p>I want to know if it is possible to use a Lambda to execute a Jupyter Notebook in AWS SageMaker. This way I would not have to worry about the dependencies and would generally make the classification more straight forward.<\/p>\n<p>So, is there a way to use an AWS Lambda to execute a Jupyter Notebook?<\/p>",
        "Challenge_closed_time":1603033131243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602969610527,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to use AWS Lambda to trigger a Jupyter Notebook on AWS Sagemaker for executing a classifier that uses multiple libraries. The user has uploaded the model to Amazon S3 as a pickle and wants to trigger the Lambda every time someone uploads a file to a specific S3 bucket. The user is seeking a solution to avoid worrying about dependencies and make the classification more straightforward.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64407452",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.0,
        "Challenge_reading_time":8.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":17.6446433334,
        "Challenge_title":"Use AWS Lambda to execute a jupyter notebook on AWS Sagemaker",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2656.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526405779360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mexico City, CDMX, Mexico",
        "Poster_reputation_count":4713.0,
        "Poster_view_count":507.0,
        "Solution_body":"<p>Scheduling notebook execution is a bit of a SageMaker anti-pattern, because (1) you would need to manage data I\/O (training set, trained model) yourself, (2) you would need to manage metadata tracking yourself, (3) you cannot run on distributed hardware and (4) you cannot use Spot. Instead, it is recommended for scheduled task to leverage the various SageMaker long-running, background job APIs: SageMaker Training, SageMaker Processing or SageMaker Batch Transform (in the case of a batch inference).<\/p>\n<p>That being said, if you still want to schedule a notebook to run, you can do it in a variety of ways:<\/p>\n<ul>\n<li>in the <a href=\"https:\/\/www.youtube.com\/watch?v=6EYEoAqihPg\" rel=\"nofollow noreferrer\">SageMaker CICD Reinvent 2018 Video<\/a>, Notebooks are launched as Cloudformation templates, and their execution is automated via a SageMaker lifecycle configuration.<\/li>\n<li>AWS released <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/scheduling-jupyter-notebooks-on-sagemaker-ephemeral-instances\/\" rel=\"nofollow noreferrer\">this blog post<\/a> to document how to launch Notebooks from within Processing jobs<\/li>\n<\/ul>\n<p>But again, my recommendation for scheduled tasks would be to remove them from Jupyter, turn them into scripts and run them in SageMaker Training<\/p>\n<p>No matter your choices, all those tasks can be launched as API calls from within a Lambda function, as long as the function role has appropriate permissions<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.2,
        "Solution_reading_time":18.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1433746746023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":22140.0,
        "Answerer_view_count":1710.0,
        "Challenge_adjusted_solved_time":6.9395733333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been trying to install a machine learning package that I can use in my R script.<\/p>\n\n<p>I have done placed the tarball of the installer inside a zip file and am doing <\/p>\n\n<pre><code>install.packages(\"src\/packagename_2.0-3.tar.gz\", repos = NULL, type=\"source\") \n<\/code><\/pre>\n\n<p>from within the R script. However, the progress indicator just circles indefinitely, and it's not installed in environment.<\/p>\n\n<p>How can I install this package?<\/p>\n\n<p><code>ada<\/code> is the package I'm trying to install and <code>ada_2.0-3.tar.gz<\/code> is the file I'm using.<\/p>",
        "Challenge_closed_time":1443095996316,
        "Challenge_comment_count":0,
        "Challenge_created_time":1443064053390,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in installing an R library in Azure ML. They have tried to install the package using a tarball of the installer inside a zip file, but the progress indicator just circles indefinitely, and the package is not installed in the environment. The package they are trying to install is \"ada\" using the file \"ada_2.0-3.tar.gz\".",
        "Challenge_last_edit_time":1443071013852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32752659",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":7.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":8.873035,
        "Challenge_title":"unable to install R library in azure ml",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":982.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426899441168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2024.0,
        "Poster_view_count":298.0,
        "Solution_body":"<p>You cannot use the tarball packages. If you are on windows you need to do the following:<\/p>\n\n<p>Once you install a package (+ it's dependencies) it will download the packages in a directory <\/p>\n\n<blockquote>\n  <p>C:\\Users\\xxxxx\\AppData\\Local\\Temp\\some directory\n  name\\downloaded_packages<\/p>\n<\/blockquote>\n\n<p>These will be in a zip format. These are the packages you need. <\/p>\n\n<p>Or download the windows binaries from cran.<\/p>\n\n<p>Next you need to put all the needed packages in one total zip-file and upload this to AzureML as a new dataset.<\/p>\n\n<p>in AzureML load the data package connected to a r-script<\/p>\n\n<pre><code>install.packages(\"src\/ada.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(ada, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>Be sure to check that all dependent packages are available in Azure. Rpart is available.<\/p>\n\n<p>For a complete overview, look at this <a href=\"http:\/\/blogs.msdn.com\/b\/benjguin\/archive\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning.aspx\" rel=\"nofollow\">msdn blog<\/a> explaining it a bit better with some visuals.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":13.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":135.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1318047784840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":6665.0,
        "Answerer_view_count":926.0,
        "Challenge_adjusted_solved_time":0.5016727778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to install the following library in my Azure ML instance:<\/p>\n<p><a href=\"https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI<\/a><\/p>\n<p>My Dockerfile looks like this:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04:20210615.v1\n\nENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.7\n\n# Create conda environment\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n    python=3.7 \\\n    pip=20.2.4 \\\n    pytorch=1.7.1 \\\n    torchvision=0.8.2 \\\n    torchaudio=0.7.2 \\\n    cudatoolkit=11.0 \\\n    nvidia-apex=0.1.0 \\\n    -c anaconda -c pytorch -c conda-forge\n\n# Prepend path to AzureML conda environment\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                'psutil&gt;=5.8,&lt;5.9' \\\n                'tqdm&gt;=4.59,&lt;4.60' \\\n                'pandas&gt;=1.1,&lt;1.2' \\\n                'scipy&gt;=1.5,&lt;1.6' \\\n                'numpy&gt;=1.10,&lt;1.20' \\\n                'azureml-core==1.31.0' \\\n                'azureml-defaults==1.31.0' \\\n                'azureml-mlflow==1.31.0' \\\n                'azureml-telemetry==1.31.0' \\\n                'tensorboard==2.4.0' \\\n                'tensorflow-gpu==2.4.1' \\\n                'onnxruntime-gpu&gt;=1.7,&lt;1.8' \\\n                'horovod[pytorch]==0.21.3' \\\n                'future==0.17.1' \\\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n\n# This is needed for mpi to locate libpython\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n<\/code><\/pre>\n<p>An error is thrown when the library is being installed:<\/p>\n<pre><code>  Cloning https:\/\/github.com\/philferriere\/cocoapi.git to \/tmp\/pip-install-_i3sjryy\/pycocotools\n[91m    ERROR: Command errored out with exit status 1:\n     command: \/azureml-envs\/pytorch-1.7\/bin\/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base \/tmp\/pip-pip-egg-info-o68by1_q\n         cwd: \/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\n    Complete output (5 lines):\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py&quot;, line 2, in &lt;module&gt;\n        from Cython.Build import cythonize\n    ModuleNotFoundError: No module named 'Cython'\n<\/code><\/pre>\n<p>I've tried adding Cython as a dependecy in both the pip section and as part of the conda environment but the error is still thrown.<\/p>",
        "Challenge_closed_time":1625324240192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625322434170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to install a library in their Azure ML instance using a Dockerfile, but encounters an error stating that the 'Cython' module is not found. The user has attempted to add Cython as a dependency in both the pip section and as part of the conda environment, but the error persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68237132",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":20.7,
        "Challenge_reading_time":39.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.5016727778,
        "Challenge_title":"No module named 'Cython' setting up Azure ML docker instance",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":209.0,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_created_time":1318047784840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":6665.0,
        "Poster_view_count":926.0,
        "Solution_body":"<p>Solution was to add the following to the Dockerfile:<\/p>\n<pre><code># Install Cython\nRUN pip3 install Cython\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                ...\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.9,
        "Solution_reading_time":4.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":15.1942297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Vertex AI managed notebook in <code>europe-west1-d<\/code>. I try both (locally and in the notebook):<\/p>\n<pre><code>gcloud notebooks instances list --location=europe-west1-d\ngcloud compute instances list --filter=&quot;my-notebook-name&quot; --format &quot;[box]&quot;\n<\/code><\/pre>\n<p>and both return nothing. What am I missing?<\/p>",
        "Challenge_closed_time":1653610592520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653555893293,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a managed notebook in Vertex AI located in europe-west1-d, but when using gcloud to list instances, nothing is returned. The user is seeking assistance in identifying what they may be missing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72389357",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":5.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":15.1942297222,
        "Challenge_title":"Why does gcloud not see my managed notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>When creating a Vertex AI managed notebook, it is expected that the instance won't appear on your project. By design, the notebook instance is created in a Google managed project and is not visible to the end user.<\/p>\n<p>But if you want to see instance details of your managed notebooks, you can use the Notebooks API to send a request to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/reference\/rest\/v1\/projects.locations.runtimes\/list\" rel=\"nofollow noreferrer\">runtimes.list<\/a>. See example request:<\/p>\n<pre><code>project=your-project-here\nlocation=us-central1 #adjust based on your location\n\ncurl -X GET -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/notebooks.googleapis.com\/v1\/projects\/${project}\/locations\/${location}\/runtimes&quot;\n<\/code><\/pre>\n<p>Response output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.5,
        "Solution_reading_time":14.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":102.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.6138888889,
        "Challenge_answer_count":0,
        "Challenge_body":"as the title suggests\r\n",
        "Challenge_closed_time":1641229646000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1641220236000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to configure their profile with AWS CLI for using AWS Built-in sagemaker algorithms. They are encountering a ValueError that requires them to set up local AWS configuration with a region supported by SageMaker. The user is unsure if it is possible to link access AWS resources in Studiolab.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/38",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.0,
        "Challenge_reading_time":0.98,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.6138888889,
        "Challenge_title":"I just wonder if i can initialize my sagemaker studio lab? ",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":15,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello - if you have any specific technical issues please use our issue template here to describe it in detail. \r\n\r\nhttps:\/\/github.com\/aws\/studio-lab-examples\/issues\/new?assignees=&labels=bug&template=bug-report-for-sagemaker-studio-lab.md&title=\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.4,
        "Solution_reading_time":3.25,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":745.5772222222,
        "Challenge_answer_count":0,
        "Challenge_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Challenge_closed_time":1650643135000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1647959057000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"When pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard. This happens only with [full] installation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.23,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":745.5772222222,
        "Challenge_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":227,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It appears that the bug has been fixed by https:\/\/github.com\/triton-inference-server\/server\/pull\/3828 and I am not able to reproduce it using the model example for the plugin. Can you try the plugin from the latest codebase?\r\n```\r\npython `pwd`\/mlflow-triton-plugin\/scripts\/publish_model_to_mlflow.py \\\r\n    --model_name onnx_float32_int32_int32 \\\r\n    --model_directory `pwd`\/mlflow-triton-plugin\/examples\/onnx_float32_int32_int32\/ \\\r\n    --flavor triton\r\n```\r\nreturns:\r\n```\r\nRegistered model 'onnx_float32_int32_int32' already exists. Creating a new version of this model...\r\n2022\/04\/07 23:03:53 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: onnx_float32_int32_int32, version 3\r\nCreated version '3' of model 'onnx_float32_int32_int32'.\r\n.\/mlruns\/0\/945d5c5d6806470d889248cfc7f10b69\/artifacts\r\n``` Closing due to in-activity.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.9,
        "Solution_reading_time":11.49,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":86.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1524630627116,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":154.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":47.0271861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to install a package <code>aclocal<\/code> on <code>Amazon SageMmaker<\/code> (which is a requirement for <code>teseract<\/code>) using the command <code>sudo yum install aclocal<\/code><\/p>\n<p>But it gives me the following error.<\/p>\n<p><code>No package aclocal available<\/code><\/p>",
        "Challenge_closed_time":1663134824987,
        "Challenge_comment_count":2,
        "Challenge_created_time":1662965527117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to install the package 'aclocal' on Amazon SageMaker using the command 'sudo yum install aclocal', which is a requirement for 'tesseract', but is receiving an error message stating that the package is not available.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73685446",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":47.0271861111,
        "Challenge_title":"No package aclocal available",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524630627116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":154.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>To install <code>tesseract<\/code> in SageMaker you can simply follow the instructions here: <a href=\"https:\/\/tesseract-ocr.github.io\/tessdoc\/Compiling.html#linux\" rel=\"nofollow noreferrer\">https:\/\/tesseract-ocr.github.io\/tessdoc\/Compiling.html#linux<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":24.1,
        "Solution_reading_time":3.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.2260158334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.    <\/p>\n<p>Features in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python<\/a> are neat, but that of having API docs\/swagger for the webservice seems missing.    <\/p>\n<p>Having some documentation is useful especially if the model takes in input several features of different type.    <\/p>\n<p>To overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.     <\/p>\n<p>Can I do something equivalent to this with AKS in AzureML instead? If so, how?<\/p>",
        "Challenge_closed_time":1600930445547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600897231890,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to deploy a machine learning service using AzureML on AKS and add OpenAPI specification for it. The user finds that the API docs\/swagger feature is missing in AzureML on AKS and currently uses Docker containers with FastAPI library to build the API and OpenAPI\/Swagger specs. The user is seeking guidance on how to add OpenAPI specification to a webservice deployed with AzureML in AKS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":12.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":9.2260158334,
        "Challenge_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=9ced4628-b03a-4169-99b4-e42b0955c045\">@Davide Fiocco  <\/a> The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.akswebservice?view=azure-ml-py\">here<\/a>. You can print your <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service\">swagger_uri<\/a> of the web service and check if it confirms with the specifications you are creating currently.     <\/p>\n<p>If the above response helps, please accept the response as answer. Thanks!!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":8.84,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1548341556520,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mumbai, Maharashtra, India",
        "Answerer_reputation_count":2907.0,
        "Answerer_view_count":238.0,
        "Challenge_adjusted_solved_time":701.3743433333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm not able to upload statsmodels 0.9rc1 python package in Azure ML studio for Time series analysis.<\/p>\n\n<p>I have downloaded <a href=\"https:\/\/files.pythonhosted.org\/packages\/df\/6f\/df6cf5faecd8082ee23916ff45d396dfee5a1f17aa275da7bab4f5c8926a\/statsmodels-0.9.0rc1-cp36-cp36m-win_amd64.whl\" rel=\"nofollow noreferrer\">statsmodels 0.9rc1<\/a>, unzipped contents and added statsmodels folder and model.pkl file to zip folder.<\/p>\n\n<p>But, while uploading to Microsoft Azure ML studio it says <strong>failed to build schema and visualization<\/strong><\/p>\n\n<p>I'm using this external package in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts\" rel=\"nofollow noreferrer\">Execute Python script<\/a><\/p>\n\n<p>PS: I have succesfully uploaded packages like Adal, dateutils etc.<\/p>",
        "Challenge_closed_time":1573144655863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570616046857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to upload the statsmodels 0.9rc1 python package in Azure ML studio for time series analysis. They have downloaded and unzipped the package, added it to a zip folder, but while uploading to Azure ML studio, it fails to build schema and visualization. The user has successfully uploaded other packages like Adal and dateutils.",
        "Challenge_last_edit_time":1570619708227,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58301879",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":11.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":702.3913905556,
        "Challenge_title":"Unable to upload statsmodels 0.9rc1 python package in Azure ML studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":141.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548341556520,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mumbai, Maharashtra, India",
        "Poster_reputation_count":2907.0,
        "Poster_view_count":238.0,
        "Solution_body":"<p>I have switched to Azure Jupyter Notebook where I installed package using pip<\/p>\n\n<pre><code>!pip install statsmodels==0.9.0rc1\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5089333333,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>When I finished the training with the offline mode, I use  the following command to upload the trained results to the cloud service.<\/p>\n<pre><code class=\"lang-auto\">wandb  sync   MY_RUN_DIRECTORY\n<\/code><\/pre>\n<p>But I got the KeyError: \u2018run_url\u2019<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044.jpeg\" data-download-href=\"\/uploads\/short-url\/oQmDb7gGEFmjb7DrMnTKtSiKweM.jpeg?dl=1\" title=\"Screenshot 2023-04-24 202643\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_690x240.jpeg\" alt=\"Screenshot 2023-04-24 202643\" data-base62-sha1=\"oQmDb7gGEFmjb7DrMnTKtSiKweM\" width=\"690\" height=\"240\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_690x240.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_1035x360.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_1380x480.jpeg 2x\" data-dominant-color=\"181818\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2023-04-24 202643<\/span><span class=\"informations\">1396\u00d7487 192 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>How to solve this question?<\/p>",
        "Challenge_closed_time":1682341145608,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682339313448,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a KeyError: 'run_url' while trying to upload trained results to a cloud service using the 'wandb sync' command after finishing training in offline mode. The user is seeking a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sync-error\/4267",
        "Challenge_link_count":5,
        "Challenge_participation_count":7,
        "Challenge_readability":27.6,
        "Challenge_reading_time":23.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.5089333333,
        "Challenge_title":"Sync error",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lee086824\">@lee086824<\/a> thanks for reporting this issue. There was a regression in wandb <code>v0.14.1<\/code> that would throw this <code>KeyError: 'run_url'<\/code>. Is this your current version, and if so could you please upgrade to our most recent client\/SDK version and try to sync your runs again? Would it work for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":4.6,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":63.7779516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Challenge_closed_time":1547708377156,
        "Challenge_comment_count":1,
        "Challenge_created_time":1547478776530,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS Sagemaker notebook where the package Scikit-Learn version is not updating to the desired version. The user tried using lifecycle configurations to update the package but it did not work. The correct version of the package is found in the terminal but not in the notebook. The user is seeking help to fix the notebook lifecycle configuration and wondering if there is any virtual environment on the SageMaker instances where the package should be installed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":63.7779516667,
        "Challenge_title":"AWS Sagemaker does not update the package",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1546.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527781503483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Metz, France",
        "Poster_reputation_count":352.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.9663888889,
        "Challenge_answer_count":1,
        "Challenge_body":"What is value and use case for Deep Learning AMI (DLAMI)?\n\nIt seems that customers often pack ML dependencies at the docker level (themselves, or with DL containers or with SageMaker containers), instead of the AMI level. So what is the value and use-case of DL AMI ?",
        "Challenge_closed_time":1594216705000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594209626000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning the value and use case of Deep Learning AMI (DLAMI) as customers tend to pack ML dependencies at the docker level instead of the AMI level.",
        "Challenge_last_edit_time":1668530670747,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUQInSlgeCS6mIe4DJv3KwnQ\/what-is-value-and-use-case-for-deep-learning-ami-dlami",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":3.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.9663888889,
        "Challenge_title":"What is value and use case for Deep Learning AMI (DLAMI)?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The value of the DLAMI (https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/what-is-dlami.html) is ease of use and saving time to get up to speed in a development environment.  If you are developing code for ML there is a huge variety of frameworks and software that you might need to install. The DLAMI includes the more popular ones, so you may quickly deploy a machine complete with common dependencies. This results in a reduction of the time needed for installing and configuring things. It speeds up experimentation and evaluation. If you want to try a new framework, it is already there.\n\nThe second reason is that AWS keeps the AMI up to date, so you may just deploy a new AMI periodically rather than having to patch.  Again, this saves you time and lets you concentrate on the underlying development and business activities.\n\nAll that said, for running in production and at volume you might want to use a different tool, I would imagine that for most cases creating docker images to your specific requirements would make a lot of sense. No need to go over the good and bad points of containers here.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612481183580,
        "Solution_link_count":1.0,
        "Solution_readability":8.7,
        "Solution_reading_time":13.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1469978721363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":73.1672544445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We can now publish Docker images to AWS ECR directly from <strong>SageMaker Studio<\/strong> using this code <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli<\/a>\nI did follow the easy installation instructions:<\/p>\n<pre><code>!pip install sagemaker-studio-image-build\nsm-docker build .\n<\/code><\/pre>\n<p>Also Trust policy and permissions have been set as described in the instructions.\nBut I'm getting the error &quot;<strong>Command did not exit successfully docker push<\/strong>&quot; at the stage where it is pushing the Docker image to AWS ECR. Any idea why? Here are the details print as output:<\/p>\n<pre><code>[Container] 2021\/05\/04 06:57:20 Running command echo Pushing the Docker image...\nPushing the Docker image...\n\n[Container] 2021\/05\/04 06:57:20 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG\nThe push refers to repository [752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml]\nAn image does not exist locally with the tag: 752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml\n\n[Container] 2021\/05\/04 06:57:20 Command did not exit successfully docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG exit status 1\n[Container] 2021\/05\/04 06:57:20 Phase complete: POST_BUILD State: FAILED\n[Container] 2021\/05\/04 06:57:20 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG. Reason: exit status 1\n<\/code><\/pre>",
        "Challenge_closed_time":1620378926343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620115524227,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while pushing a Docker image to AWS ECR from SageMaker Studio using AWS CLI. The error message states that the command did not exit successfully while pushing the Docker image and an image does not exist locally with the tag. The user has followed the installation instructions and set trust policy and permissions as described.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67380942",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":23.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":73.1672544445,
        "Challenge_title":"Pushing docker image in AWS ECR from SageMaker Studio using AWS CLI",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1046.0,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469978721363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":415.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>In the Dockerfile, there was a reference to another file that was not present in the directory from where the command <code>sm-docker build .<\/code> was launched.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":2.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1502815666600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Memphis, TN, USA",
        "Answerer_reputation_count":5028.0,
        "Answerer_view_count":957.0,
        "Challenge_adjusted_solved_time":0.08215,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.<\/p>\n\n<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\nmlflow from https:\/\/files.pythonhosted.org\/packages\/01\/ec\/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270\/mlflow-1.0.0-py3-none-any.whl#sha256=0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4:\n    Expected sha256 0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4\n         Got        eb34ea16ecfe02d474ce50fd1f88aba82d56dcce9e8fdd30193ab39edf32ac9e\n<\/code><\/pre>",
        "Challenge_closed_time":1560545869140,
        "Challenge_comment_count":1,
        "Challenge_created_time":1560545573400,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install mlflow using pip install on a Windows 10 machine. The error message indicates that the packages do not match the hashes from the requirements file, and suggests that someone may have tampered with them.",
        "Challenge_last_edit_time":1560799785056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56604989",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":10.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.08215,
        "Challenge_title":"How to install mlflow using pip install",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355343131932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5655.0,
        "Poster_view_count":629.0,
        "Solution_body":"<p>It is trying to check cache for packages. They were likely compiled in linux or some other OS and you are trying to install them in Windows.<\/p>\n\n<p>This should fix your issue:<\/p>\n\n<pre><code>pip install --no-cache-dir mlflow\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":3.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1569518464147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":432.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":204.5016991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Python 3.10, Pip install azureml-sdk 1.39.0.<br \/>\nEnvironments: Win10 PS, VS2022, and a docker image- all same results . Pip show shows the azureml-core package.<\/p>\n<p>Simple (I thought) script, but it can't find &quot;azureml.core&quot;   No module named azureml is the error.\nHow do I make it &quot;find&quot; it? I'm new at python so it could be syntax.<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Experiment, Environment, Model,Dataset,Datastore,ScriptRunConfig\n     \n    # check core SDK version number\n    print(&quot;Azure ML SDK Version: &quot;, azureml.core.VERSION)\n<\/code><\/pre>",
        "Challenge_closed_time":1648177993987,
        "Challenge_comment_count":2,
        "Challenge_created_time":1647441787870,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to run a Python script that uses the AzureML SDK. Despite installing the required package and checking its presence, the script is unable to find the \"azureml.core\" module, resulting in a \"No module named azureml\" error. The user is seeking guidance on how to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71499094",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.5,
        "Challenge_reading_time":8.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":204.5016991667,
        "Challenge_title":"Python AzureML Hello world - Can't find module azureml",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":310.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1263312456836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>azureml python sdk does not support py3.10 yet, AutoML sdk supports py&lt;=3.8.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":1.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.184865,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there an option to export the Azure ML Designer to code so we can copy between workspaces?<\/p>",
        "Challenge_closed_time":1646202005287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646150939773,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for an option to export Azure ML Designer to code for copying between workspaces.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/755142\/azure-ml-designer-export-code",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":1.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":14.184865,
        "Challenge_title":"Azure ML Designer: Export Code",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":22,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, this feature is currently not supported as mentioned on this <a href=\"https:\/\/stackoverflow.com\/questions\/60306240\/export-azure-ml-studio-designer-project-as-jupyter-notebook\">thread<\/a>. However, it's on the roadmap.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.4,
        "Solution_reading_time":3.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.8151808334,
        "Challenge_answer_count":2,
        "Challenge_body":"I am running a Sagemaker Notebook instance. How can I tell if my Notebook is frozen or just taking a long time? I am using a 24xlarge and querying from Athena in parallel and it seems to be stuck on the same query for a long time. How can I tell if I need more Memory or more VCPUs?",
        "Challenge_closed_time":1683362949783,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683306015132,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running a Sagemaker Notebook instance and is unsure if it is frozen or just taking a long time. They are using a 24xlarge and querying from Athena in parallel, but it seems to be stuck on the same query for a long time. The user is unsure if they need more Memory or more VCPUs.",
        "Challenge_last_edit_time":1683644479959,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUK_hTODBBTeWX_c-lcO34mg\/how-can-i-tell-if-my-notebook-instance-is-frozen",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.9,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":15.8151808334,
        "Challenge_title":"How can I tell if my Notebook instance is frozen?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there,\n\nGreetings for the day!\n\nI understand that you wanted to know how can you determine if you need more VCPU or memory when your SageMaker Notebook Instance is frozen or just taking a long time?\n\nI\u2019d like to inform you that If your Sagemaker Notebook instance is taking a long time, you can check if it is frozen or still running by monitoring the CPU and memory usage. If the CPU usage is low or zero, it may be frozen. \n\nIf the CPU usage is high but the memory usage is low, you may need more VCPUs and If the memory usage is high, you may need more memory. \n\nYou can check it via SageMaker Notebook Instance terminal:\n\nTo see the memory and CPU information in detail , kindly follow the below instructions:-\n\n[1] Start Your Notebook Instance\n[2] Go to the  Jupyter Home Page \n[3] Right hand side ,Click on DropDown Option \u201cNew\u201d \n[4] Select \u201cTerminal\u201d.\n\nIn the Jupyter terminal, Run the below commands to see the information of Memory and CPUs.\n\n[+] To see the memory information:\n\n$ free -h\n\n=> output of \u201cfree -h\u201d will provide the information of total memory, used memory, free memory, shared memory etc in human readable form.\n\n[+] To see the CPU information, you can run any of the commands:\n\n$ mpstat -u\n\n=> Output of \u201cmpstat -u\u201d consists of different fields like %guest, %gnice, %steal etc.\n\n%steal\nShow the percentage of time spent in involuntary wait by the virtual CPU or CPUs while the hyper\u2010\nvisor was servicing another virtual processor.\n\n%guest\nShow the percentage of time spent by the CPU or CPUs to run a virtual processor.\n\n%idle\nShow the percentage of time that the CPU or CPUs were idle and the system did not have an out\u2010\nstanding disk I\/O request.\n\nmany more.. You can find more detail about the each field of mpstat command by visiting the manual page of it. To see the manual page of mpstat command, use \u201c$ man mpstst\u201d.\n\nAlong with \u201cmpstat -u\u201d, you can also try the below listed commands to get information about the cpu:\n\n$ lscpu\n$ cat \/proc\/cpuinfo\n$ top\n\nAdditionally, You can also check the cloudwatch logs for any errors or warnings that may indicate the cause of the issue. Most of the time, Cloud watch logs helps to find out the root cause of the issue.\n\nYou can find the CloudWatch logs under CloudWatch \u2192 Log Groups \u2192 \/aws\/sagemaker\/NotebookInstances -> Notebook Instance Name\n\nBased on the analysis , you can select different Notebook Instance type, You can find more detail of SageMaker Instance Here [1].\n\nI request you to kindly follow the above suggested workarounds. \n\nIf you have any difficulty or run into any issue, Please reach out to AWS Support[+] (Sagemaker), along with your issue\/use case in details and we would be happy to assist you further.\n\n[+] Creating support cases and case management - https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-casehttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-case \n\nI hope this information would be useful to you.\n\nThank you!\n\nREFERENCES:\n\n[1] https:\/\/aws.amazon.com\/sagemaker\/pricing\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1683362949783,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":37.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":488.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1407449881432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":228.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":2.7172083334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Iam new with SageMaker and I try to use my own sickit-learn algorithm . For this I use Docker.\nI try to do the same task as described here in this github account : <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a><\/p>\n\n<p>My question is should I create manually the repository <strong><code>\/opt\/ml<\/code><\/strong>  (I work with windows OS) ?<\/p>\n\n<p>Can you explain me please?<\/p>\n\n<p>thank you<\/p>",
        "Challenge_closed_time":1533928824743,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533919042793,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to AWS SageMaker and is trying to use their own sickit-learn algorithm using Docker. They are following a guide on GitHub and are unsure if they need to manually create the repository \"\/opt\/ml\" since they are using Windows OS. They are seeking clarification on this matter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51790720",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.8,
        "Challenge_reading_time":9.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2.7172083334,
        "Challenge_title":"Brewing up custom ML models on AWS SageMaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518617852856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":495.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>You don't need to create <code>\/opt\/ml<\/code>, SageMaker will do it for you when it launches your training job.<\/p>\n\n<p>The contents of the <code>\/opt\/ml<\/code> directory are determined by the parameters you pass to the CreateTrainingJob API call. The scikit example notebook you linked to describes this (look at the <strong>Running your container<\/strong> sections). You can find more info about this in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">Create a Training Job<\/a> section of the main SageMaker documentation.<\/p>\n\n<hr>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":7.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341161196310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo",
        "Answerer_reputation_count":1057.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When running <code>kedro install<\/code> I get the following error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>Attempting uninstall: terminado\n    Found existing installation: terminado 0.8.3\nERROR: Cannot uninstall 'terminado'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n<\/code><\/pre>\n<p>This github <a href=\"https:\/\/github.com\/jupyter\/notebook\/issues\/4543\" rel=\"nofollow noreferrer\">issue<\/a> suggests the following fix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install terminado --user --ignore-installed\n<\/code><\/pre>\n<p>But it does not work for me as I keep having the same error.<\/p>\n<p><strong>Note:<\/strong>\nThis question is similar to <a href=\"https:\/\/stackoverflow.com\/questions\/61770369\/docker-ubuntu-20-04-cannot-uninstall-terminado-and-problems-with-pip\">this<\/a> but different enough that I think it is worth asking separately.<\/p>",
        "Challenge_closed_time":1605936149736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605936149737,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running 'kedro install' due to the inability to uninstall 'terminado', which is a distutils installed project. The suggested fix of 'pip install terminado --user --ignore-installed' did not work for the user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64940102",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.3,
        "Challenge_reading_time":13.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro install - Cannot uninstall `terminado`",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2331.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1341161196310,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tokyo",
        "Poster_reputation_count":1057.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>The problem is the version that the kedro template project requires see <code>src\/requiremetns.txt<\/code><\/p>\n<p>In my project it is <code>terminado==0.9.1<\/code>, hence the following solves the problem:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install terminado==0.9.1  --user --ignore-installed\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":4.33,
        "Solution_score_count":10.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1317052342823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Franklin, TN",
        "Answerer_reputation_count":8183.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":0.9585202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When trying to upload a custom R module to Azure Machine Learning Studio what causes the following error.<\/p>\n\n<blockquote>\n  <p>[ModuleOutput]<\/p>\n<\/blockquote>\n\n<pre><code>\"ErrorId\":\"BuildCustomModuleFailed\",\"ErrorCode\":\"0114\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 0114: Custom module build failed with error(s): An item with the same key has already been added.\"}} [ModuleOutput] Error: Error 0114: Custom module build failed with error(s): An item with the same key has already been added. \n<\/code><\/pre>\n\n<p>I have tried renaming the module so a name that does not exists.<\/p>",
        "Challenge_closed_time":1496847821796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496847821797,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error (0114) while trying to upload a custom R module to Azure Machine Learning Studio, which states that the custom module build has failed due to an item with the same key already being added. The user has attempted to rename the module to a non-existing name but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44416344",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":9.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio Custom Module Upload Error 0114 : An item with the same key has already been added",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":75.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1317052342823,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Franklin, TN",
        "Poster_reputation_count":8183.0,
        "Poster_view_count":727.0,
        "Solution_body":"<p>The duplicate key exception is a red herring. <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn962112.aspx\" rel=\"nofollow noreferrer\" title=\"MSDN Module Error Code 0114\">Build error 0114<\/a> is a general error that occurs if there is a system exception while building the custom module. The real issue my module was compressed using the built in compress folder option in the Mac Finder. To fix this compress the file using the command line interface for <code>zip<\/code> in Terminal in the following very specific manner.<\/p>\n\n<blockquote>\n  <p>The following example:<\/p>\n<\/blockquote>\n\n<pre><code>cd ScoredDatasetMetadata\/\nzip ScoredDatasetMetadata *\nmv ScoredDatasetMetadata.zip ..\/\n<\/code><\/pre>\n\n<p>Builds a zip file with the correct file structure.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1496851272470,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":66.2517636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>Is there a way to specify the disk storage type for Compute instances?     <br \/>\nBoth the Azure portal and ARM templates do not have an option to define the disk storage type, which defaults to the P10 disks (Premium SSD).     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/148883-azureml-compute.png?platform=QnA\" alt=\"148883-azureml-compute.png\" \/>Thanks    <\/p>",
        "Challenge_closed_time":1636952401092,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636713894743,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in specifying the disk storage type for Compute instances in Azure Machine Learning. The default disk storage type is P10 disks (Premium SSD) and there is no option to define it in both the Azure portal and ARM templates.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/625035\/azure-machine-learning-specify-disk-storage-type",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":66.2517636111,
        "Challenge_title":"Azure Machine Learning - Specify disk storage type",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=41b4924d-c8f5-4ca4-9844-0c0af46eb5d5\">@Simon Magrin  <\/a>  Thanks, Currently There's no way to change the disk storage type for CIs or compute clusters. We have added this to our product backlog item to support in the near future.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":13.8,
        "Solution_reading_time":17.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":16.1130741667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Similar question as <a href=\"https:\/\/stackoverflow.com\/questions\/43176442\/install-r-packages-in-azure-ml\">here<\/a> but now on Python packages. Currently, the CVXPY is missing in Azure ML. I am also trying to get other solvers such as GLPK, CLP and COINMP working in Azure ML.<\/p>\n<p><strong>How can I install Python packages in Azure ML?<\/strong><\/p>\n<hr \/>\n<p><em>Update about trying to install the Python packages not found in Azure ML.<\/em><\/p>\n<blockquote>\n<p>I did as instructed by Peter Pan but I think the 32bits CVXPY files are wrong for the Anaconda 4 and Python 3.5 in Azure ML, logs and errors are <a href=\"https:\/\/pastebin.com\/zN5QrPtL\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>[Information]         Running with Python 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)]\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9glSm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9glSm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/blockquote>\n<p><em>Update 2 with win_amd64 files (paste <a href=\"https:\/\/pastebin.com\/tisWuP5C\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code>[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         cvxopt-1.1.9-cp35-cp35m-win_amd64.whl          2017-06-07 01:03:34      1972074\n[Information]         __MACOSX\/                                      2017-06-07 01:26:28            0\n[Information]         __MACOSX\/._cvxopt-1.1.9-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:34          452\n[Information]         cvxpy-0.4.10-py3-none-any.whl                  2017-06-07 00:25:36       300880\n[Information]         __MACOSX\/._cvxpy-0.4.10-py3-none-any.whl       2017-06-07 00:25:36          444\n[Information]         ecos-2.0.4-cp35-cp35m-win_amd64.whl            2017-06-07 01:03:40        56522\n[Information]         __MACOSX\/._ecos-2.0.4-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:40          450\n[Information]         numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl   2017-06-07 01:25:02    127909457\n[Information]         __MACOSX\/._numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl 2017-06-07 01:25:02          459\n[Information]         scipy-0.19.0-cp35-cp35m-win_amd64.whl          2017-06-07 01:05:12     12178932\n[Information]         __MACOSX\/._scipy-0.19.0-cp35-cp35m-win_amd64.whl 2017-06-07 01:05:12          452\n[Information]         scs-1.2.6-cp35-cp35m-win_amd64.whl             2017-06-07 01:03:34        78653\n[Information]         __MACOSX\/._scs-1.2.6-cp35-cp35m-win_amd64.whl  2017-06-07 01:03:34          449\n[Information]         [ READING ] 0:00:00\n[Information]         Input pandas.DataFrame #1:\n[Information]         Empty DataFrame\n[Information]         Columns: [1]\n[Information]         Index: []\n[Information]         [ EXECUTING ] 0:00:00\n[Information]         [ WRITING ] 0:00:00\n<\/code><\/pre>\n<p>where <code>import cvxpy<\/code>, <code>import cvxpy-0.4.10-py3-none-any.whl<\/code> or <code>cvxpy-0.4.10-py3-none-any<\/code> do not work so<\/p>\n<p><strong>How can I use the following wheel files downloaded from <a href=\"http:\/\/www.lfd.uci.edu\/%7Egohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\">here<\/a> to use the external Python packages not found in Azure ML?<\/strong><\/p>\n<\/blockquote>\n<p><em>Update about permission problem about importing cvxpy (paste <a href=\"https:\/\/pastebin.com\/3kTKgLfc\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code> [Error]         ImportError: No module named 'canonInterface'\n<\/code><\/pre>\n<p>where the ZIP Bundle is organised a bit differently, the content of each wheel downloaded to a folder and the content having all zipped as a ZIP Bundle.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1496732346280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496674339213,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is trying to install Python packages such as CVXPY, GLPK, CLP, and COINMP in Azure ML but is encountering errors. They have tried installing 32-bit CVXPY files but it did not work. They have also downloaded win_amd64 files but are unsure how to use them. The user is seeking guidance on how to use the downloaded wheel files to install external Python packages not found in Azure ML. Additionally, they are facing a permission problem while importing cvxpy.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44371692",
        "Challenge_link_count":11,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":49.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":16.1130741667,
        "Challenge_title":"Install Python Packages in Azure ML?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":12625.0,
        "Challenge_word_count":346,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts#limitations\" rel=\"nofollow noreferrer\"><code>Limitations<\/code><\/a> and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of <code>Execute Python Script<\/code> tutorial, the only way to add custom Python modules is via the zip file mechanism to package the modules and all dependencies.<\/p>\n\n<p>For example to install <code>CVXPY<\/code>, as below.<\/p>\n\n<ol>\n<li>Download the wheel file of <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\"><code>CVXPY<\/code><\/a> and its dependencies like <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxopt\" rel=\"nofollow noreferrer\"><code>CVXOPT<\/code><\/a>.<\/li>\n<li>Decompress these wheel files, and package these files in the path <code>cvxpy<\/code> and <code>cvxopt<\/code>, etc as a zipped file with your script.<\/li>\n<li>Upload the zip file as a dataset and use it as the script bundle.<\/li>\n<\/ol>\n\n<p>If you were using IPython, you also can try to install the Python Package via the code <code>!pip install cvxpy<\/code>.<\/p>\n\n<p>And there are some similar SO threads which may be helpful for you, as below.<\/p>\n\n<ol>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/44285641\/azure-ml-python-with-script-bundle-cannot-import-module\">Azure ML Python with Script Bundle cannot import module<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/8663046\/how-to-install-a-python-package-from-within-ipython\">How to install a Python package from within IPython?<\/a><\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>\n\n<hr>\n\n<p>Update:<\/p>\n\n<p>For IPython interface of Azure ML, you move to the <code>NOTEBOOKS<\/code> tab to create a notebook via <code>ADD TO PROJECT<\/code> button at the bottom of the page, as the figure below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Or you can directly login to the website <code>https:\/\/notebooks.azure.com<\/code> to use it.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1496760284030,
        "Solution_link_count":9.0,
        "Solution_readability":10.8,
        "Solution_reading_time":28.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":215.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1655773889523,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":390.0,
        "Answerer_view_count":240.0,
        "Challenge_adjusted_solved_time":7.7833411111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am porting custom job training from gcp AI Platform to Vertex AI.\nI am able to start a job, but can't find how to to get the status and how to stream the logs to my local client.<\/p>\n<p>For AI Platform I was using this to get the state:<\/p>\n<pre><code>from google.oauth2 import service_account\nfrom googleapiclient import discovery\nscopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform']\ncredentials = service_account.Credentials.from_service_account_file(keyFile, scopes=scopes)\nml_apis = discovery.build(&quot;ml&quot;,&quot;v1&quot;, credentials=credentials, cache_discovery=False)\nx = ml_apis.projects().jobs().get(name=&quot;projects\/%myproject%\/jobs\/&quot;+job_id).execute()  # execute http request\nreturn x['state']\n<\/code><\/pre>\n<p>And this to stream the logs:<\/p>\n<pre><code>cmd = 'gcloud ai-platform jobs stream-logs ' + job_id\n<\/code><\/pre>\n<p>This does not work for Vertex AI job. What is the replacement code?<\/p>",
        "Challenge_closed_time":1657854766996,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657835239540,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while porting a custom job from GCP AI Platform to Vertex AI. They are unable to find a way to get the status and stream logs to their local client. The code they were using for AI Platform is not working for Vertex AI, and they are looking for a replacement code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72986981",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.1,
        "Challenge_reading_time":13.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":5.4242933333,
        "Challenge_title":"Porting custom job from GCP AI Platform to Vertex AI - how to get state and logs of job?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254829817772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2595.0,
        "Poster_view_count":357.0,
        "Solution_body":"<p>Can you try this command for streaming logs :<\/p>\n<pre><code>gcloud ai custom-jobs stream-logs 123 --region=europe-west4\n<\/code><\/pre>\n<p>123 is the <strong>ID<\/strong> of the custom job for this case, you can add glcoud wide flags such as --format as well.<\/p>\n<p>You can visit this <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/custom-jobs\/stream-logs\" rel=\"nofollow noreferrer\">link<\/a> for more details about this command and additional flags available.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1657863259568,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":6.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":56.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":204.9339161111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When attempting to select the Dependencies file in the AML GUI - the file selector window that opens has the wrong file extension selected (*.py) - it should be either <em>.<\/em> or potentially <em>.yml\/<\/em>.yaml - thanks    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/272512-capture3.png?platform=QnA\" alt=\"272512-capture3.png\" \/>    <\/p>",
        "Challenge_closed_time":1672281348808,
        "Challenge_comment_count":2,
        "Challenge_created_time":1671543586710,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with selecting the Dependencies file in the AML GUI for deploying to an endpoint. The file selector window that opens has the wrong file extension selected (*.py) instead of either \".\" or potentially \".yml\/\".yaml.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1136021\/selecting-the-dependencies-file-in-the-studio-gui",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":204.9339161111,
        "Challenge_title":"Selecting the Dependencies file in the Studio GUI for deploying to an endpoint is bugged - wrong file format specified",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=0ec06fb6-513e-4f5c-9aff-281bc5e44e22\">@Neil McAlister  <\/a>     <\/p>\n<p>Thanks for your waiting due to the holiday season and sorry for the confusion. This buttion is actually for adding <strong>code dependencies<\/strong>, i.e., my scoring script requires dependency.py. For package dependencies, those need to be included in the environment to use for the deployment.    <\/p>\n<p>The button name &quot;Add Dependencies&quot; is confused here, and product team is working on changing the name or add more explanation for this button.     <\/p>\n<p>In the UI here, you must have already created an environment to do a deployment. Providing a conda.yml file in the endpoints create dialog will not register a new environment for you.  The environment must be selected in the bottom section of the page as below -    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/274589-microsoftteams-image-12.png?platform=QnA\" alt=\"274589-microsoftteams-image-12.png\" \/>    <\/p>\n<p>If your environment doesn't exist in this list, or in the custom environments, then you'll need to go to the environments tab on the left side of the portal and create an enviornment.    <\/p>\n<p>I am sorry for the confusion and product team will fix it in the near future.    <\/p>\n<p>I hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly acceept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":18.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":204.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1622632545867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":1.2254397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a <strong>Jupyter Lab<\/strong> instance on <strong>AWS SageMaker<\/strong>.<\/p>\n<p>Kernel: <code>conda_mxnet_latest_p37<\/code>.<\/p>\n<p><code>url_lib<\/code> contains some false urls, that I exception handle.<\/p>\n<pre><code>['15', '259', '26', '58', 'https:\/\/imagepool.1und1-drillisch.de\/v2\/download\/nachhaltigkeitsbericht\/1&amp;1Drillisch_Sustainability_Report_EN_2018.pdf', 'https:\/\/imagepool.1und1-drillisch.de\/\/v2\/download\/nachhaltigkeitsbericht\/2018-04-06_1und1-Drillisch_Sustainability_Report_eng.pdf', '6', 'http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf', '80', 'https:\/\/multimedia.3m.com\/mws\/media\/1691941O\/2019-sustainability-report.PDF', 'https:\/\/s3-us-west-2.amazonaws.com\/ungc-production\/attachments\/cop_2020\/483648\/original\/GPIC_Sustainability_Report_2020__-_40_Years_of_Sustainable_Success.pdf?1583154650', 'https:\/\/drive.google.com\/open?id=1_dnBcfXWjexy9QoWRhOk_3gnOkWfYRCw', 'http:\/\/aepsustainability.com\/performance\/docs\/2020AEPGRIReport.pdf']  # sample\n<\/code><\/pre>\n<p>However, ones that are working URLs, throw this error:<\/p>\n<pre><code>[Errno 13] Permission denied: '\/data'\n<\/code><\/pre>\n<p>I don't have the directory opened, nor files since I they're not downloaded.<\/p>\n<p>I ran in <strong>Terminal<\/strong> without luck:<\/p>\n<pre><code>sh-4.2$ chmod 777 data\nsh-4.2$ chmod 777 data\/\nsh-4.2$ chmod 777 data\/gri\nsh-4.2$ chmod 777 data\/gri\/\n<\/code><\/pre>\n<p><strong>Code:<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport opendatasets as od\nimport urllib\nimport zipfile\nimport os\n\ncsr_df = pd.read_excel('data\/Company Sustainability Reports.xlsx', index_col=None)\nurl_list = csr_df['Report PDF Address'].tolist()\n\nfor url in url_list:\n    try:\n        download = od.download(url, '\/data\/gri\/')\n        filename = url.rsplit('\/', 1)[1]\n\n        path_extract = 'data\/gri\/' + filename\n        with zipfile.ZipFile('data\/gri\/' + filename + '.zip', 'r') as zip_ref:\n            zip_ref.extractall(path_extract)\n\n        os.remove(path_extract + 'readme.txt')\n\n        filenames = os.listdir(path_extract)\n        scans = []\n        for f in filenames:\n            with Image.open(path_extract + f) as img:\n                matrix = np.array(img)\n                scans.append(matrix)\n\n        # shutil.rmtree(path_extract)\n        os.remove(path_extract[:-1] + '.zip')\n\n    except (urllib.error.URLError, IOError, RuntimeError) as e:\n        print('Download PDFs', e)\n<\/code><\/pre>\n<p><strong>Output:<\/strong><\/p>\n<pre><code>Download PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs list index out of range\nDownload PDFs [Errno 13] Permission denied: '\/data'\n...\n<\/code><\/pre>\n<p>Please let me know if there is anything else I should clarify.<\/p>",
        "Challenge_closed_time":1639406624843,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639402213260,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a permission denied error while trying to download files using Jupyter Lab on AWS SageMaker. The error occurs when trying to download working URLs, and the user has attempted to change permissions using Terminal without success. The user has provided code and output for reference.",
        "Challenge_last_edit_time":1639406637992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70335470",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":41.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":1.2254397222,
        "Challenge_title":"'[Errno 13] Permission denied' - Jupyter Labs on AWS SageMaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":684.0,
        "Challenge_word_count":264,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622632545867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p><code>download<\/code> has a forward-slash <code>\/<\/code> as first character of save directory (second parameter). I removed this:<\/p>\n<pre><code>download = od.download(url, 'data\/gri\/')\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>...\nDownloading http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf to data\/gri\/1556248045.pdf\n450560it [00:02, 207848.59it\/s]\n...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1326780552283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":17.3923927778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a prediction web service to Azure using ML Workbench process using cluster mode in this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally<\/a>)<\/p>\n\n<p>The model gets sent to the manifest, the scoring script and schema <\/p>\n\n<blockquote>\n  <p>Creating\n  service..........................................................Error\n  occurred: {'Error': {'Code': 'KubernetesDeploymentFailed', 'Details':\n  [{'Message': 'Back-off 40s restarting failed container=...pod=...',\n  'Code': 'CrashLoopBackOff'}], 'StatusCode': 400, 'Message':\n  'Kubernetes Deployment failed'}, 'OperationType': 'Service',\n  'State':'Failed', 'Id': '...', 'ResourceLocation':\n  '\/api\/subscriptions\/...', 'CreatedTime':\n  '2017-10-26T20:30:49.77362Z','EndTime': '2017-10-26T20:36:40.186369Z'}<\/p>\n<\/blockquote>\n\n<p>Here is the result of checking the ml service realtime logs <\/p>\n\n<pre><code>C:\\Users\\userguy\\Documents\\azure_ml_workbench\\projecto&gt;az ml service logs realtime -i projecto\n2017-10-26 20:47:16,118 CRIT Supervisor running as root (no user in config file)\n2017-10-26 20:47:16,120 INFO supervisord started with pid 1\n2017-10-26 20:47:17,123 INFO spawned: 'rsyslog' with pid 9\n2017-10-26 20:47:17,124 INFO spawned: 'program_exit' with pid 10\n2017-10-26 20:47:17,124 INFO spawned: 'nginx' with pid 11\n2017-10-26 20:47:17,125 INFO spawned: 'gunicorn' with pid 12\n2017-10-26 20:47:18,160 INFO success: rsyslog entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:18,160 INFO success: program_exit entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:22,164 INFO success: nginx entered RUNNING state, process has stayed up for &gt; than 5 seconds (startsecs)\n2017-10-26T20:47:22.519159Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting gunicorn 19.6.0\n2017-10-26T20:47:22.520097Z, INFO, 00000000-0000-0000-0000-000000000000, , Listening at: http:\/\/127.0.0.1:9090 (12)\n2017-10-26T20:47:22.520375Z, INFO, 00000000-0000-0000-0000-000000000000, , Using worker: sync\n2017-10-26T20:47:22.521757Z, INFO, 00000000-0000-0000-0000-000000000000, , worker timeout is set to 300\n2017-10-26T20:47:22.522646Z, INFO, 00000000-0000-0000-0000-000000000000, , Booting worker with pid: 22\n2017-10-26 20:47:27,669 WARN received SIGTERM indicating exit request\n2017-10-26 20:47:27,669 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26T20:47:27.669556Z, INFO, 00000000-0000-0000-0000-000000000000, , Handling signal: term\n2017-10-26 20:47:30,673 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:33,675 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\nInitializing logger\n2017-10-26T20:47:36.564469Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insights client\n2017-10-26T20:47:36.564991Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up request id generator\n2017-10-26T20:47:36.565316Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insight hooks\n2017-10-26T20:47:36.565642Z, INFO, 00000000-0000-0000-0000-000000000000, , Invoking user's init function\n2017-10-26 20:47:36.715933: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36,716 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:36.716376: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716542: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructio\nns, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716703: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructi\nons, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716860: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructio\nns, but these are available on your machine and could speed up CPU computations.\nthis is the init\n2017-10-26T20:47:37.551940Z, INFO, 00000000-0000-0000-0000-000000000000, , Users's init has completed successfully\nUsing TensorFlow backend.\n2017-10-26T20:47:37.553751Z, INFO, 00000000-0000-0000-0000-000000000000, , Worker exiting (pid: 22)\n2017-10-26T20:47:37.885303Z, INFO, 00000000-0000-0000-0000-000000000000, , Shutting down: Master\n2017-10-26 20:47:37,885 WARN killing 'gunicorn' (12) with SIGKILL\n2017-10-26 20:47:37,886 INFO stopped: gunicorn (terminated by SIGKILL)\n2017-10-26 20:47:37,889 INFO stopped: nginx (exit status 0)\n2017-10-26 20:47:37,890 INFO stopped: program_exit (terminated by SIGTERM)\n2017-10-26 20:47:37,891 INFO stopped: rsyslog (exit status 0)\n\nReceived 41 lines of log\n<\/code><\/pre>\n\n<p>My best guess is theres something silent happening to cause \"WARN received SIGTERM indicating exit request\". The rest of the scoring.py script seems to kick off - see tensorflow get initiated and the \"this is the init\" print statement.<\/p>\n\n<p><a href=\"http:\/\/127.0.0.1:63437\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:63437<\/a> is accessible from my local machine, but the ui endpoint is blank.<\/p>\n\n<p>Any ideas on how to get this up and running in an Azure cluster? I'm not very familiar with how Kubernetes works, so any basic debugging guidance would be appreciated.<\/p>",
        "Challenge_closed_time":1509114740967,
        "Challenge_comment_count":2,
        "Challenge_created_time":1509052128353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a prediction web service to Azure using ML Workbench process using cluster mode. The error message indicates that the Kubernetes deployment has failed. The user has checked the ml service realtime logs and found that there is a warning message indicating an exit request. The user is seeking guidance on how to debug and resolve the issue.",
        "Challenge_last_edit_time":1511310430992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46963846",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":77.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":17.3923927778,
        "Challenge_title":"Azure ML Workbench Kubernetes Deployment Failed",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":447.0,
        "Challenge_word_count":620,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421081882987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":588.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>We discovered a bug in our system that could have caused this. The fix was deployed last night. Can you please try again and let us know if you still encounter this issue?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.14,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":5.2137747222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My current project have the following structure:<\/p>\n<p>Starts with a script in jupyter notebook which dowloads data from a CRM API to put in a local PostgressSql database I run with PgAdmin. After that it runs cluster analysis, return some scoring values, creates a table in database with the results and updates this values in the CRM with another API call. This process will take between 10 to 20 hours (the API only allows 400 requests per minute).<\/p>\n<p>The second notebook reads the database, detects last update, runs api call to update database since the last call, runs kmeans analysis to cluster the data, compare results with the previous call, updates the new ones and the CRM via API. This second process takes less than 2 hours in my estimation and I want this script to run every 24 hours.<\/p>\n<p>After testing, this works fine. Now I'm evaluating how to put this in production in AWS. I understand for the notebooks I need Sagemaker and from I have seen is not that complicated, my only doubt here is if I can call the API without implementing aditional code or need some configuration. My second problem is database. I don't understand the difference between RDS which is the one I think I have to use for this and Aurora or S3. My goal is to write the less code as possible, but a have try some tutorial of RDS  like this one: [1]: <a href=\"https:\/\/www.youtube.com\/watch?v=6fDTre5gikg&amp;t=10s\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=6fDTre5gikg&amp;t=10s<\/a>, and I understand this connect my local postgress to AWS but I can't find the data in the amazon page, only creates an instance?? and how to connect to it to analysis this data from SageMaker. My final goal is to run the notebooks in the cloud and connect to my postgres in the cloud. Just some orientation about how to use this tools would be appreciated.<\/p>",
        "Challenge_closed_time":1609812237472,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609793467883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a Jupyter notebook project that involves downloading data from a CRM API, running cluster analysis, and updating the CRM with the results. The first notebook takes 10-20 hours to run, while the second takes less than 2 hours and needs to run every 24 hours. The user wants to put this project in production in AWS using Sagemaker, but is unsure about how to call the API and which database to use (RDS, Aurora, or S3). The user has tried a tutorial on RDS but is unable to find the data in the Amazon page and needs guidance on how to connect to the database from SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65569634",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":23.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":5.2137747222,
        "Challenge_title":"Best way to set up jupyter notebook project in AWS",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":322,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586658390352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lima, Per\u00fa",
        "Poster_reputation_count":89.0,
        "Poster_view_count":27.0,
        "Solution_body":"<blockquote>\n<p>I don't understand the difference between RDS which is the one I think I have to use for this and Aurora or S3<\/p>\n<\/blockquote>\n<p>RDS and Aurora are <a href=\"https:\/\/en.wikipedia.org\/wiki\/Relational_database\" rel=\"nofollow noreferrer\">relational databases<\/a> fully managed by AWS. &quot;Regular&quot; RDS allows you to launch the existing popular databases such as MySQL, PostgreSQSL and other which you can launch at home\/work as well.<\/p>\n<p>Aurora is <strong>in-house, cloud-native<\/strong> implementation databases compatible with MySQL and PosrgreSQL. It can store the same data as RDS MySQL or PosrgreSQL, but provides a number of features not available for RDS, such as more read replicas, distributed storage, global databases and more.<\/p>\n<p>S3 is <strong>not a database<\/strong>, but an <a href=\"https:\/\/en.wikipedia.org\/wiki\/Object_storage\" rel=\"nofollow noreferrer\">object storage<\/a>, where you can store your files, such as images, csv, excels, similarly like you would store them on your computer.<\/p>\n<blockquote>\n<p>I understand this connect my local postgress to AWS but I can't find the data in the amazon page, only creates an instance??<\/p>\n<\/blockquote>\n<p>You can migrate your data from your local postgress to RDS or Aurora if you wish. But RDS nor Aurora will not connect to your existing local database, as they are databases themselfs.<\/p>\n<blockquote>\n<p>My final goal is to run the notebooks in the cloud and connect to my postgres in the cloud.<\/p>\n<\/blockquote>\n<p>I don't see a reason why you wouldn't be able to connect to the database. You can try to make it work, and if you encounter difficulties you can make new question on SO with RDS\/Aurora setup details.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.9,
        "Solution_reading_time":21.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":254.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7837736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Would like to upload Jupyter notebooks from different sources like GitHub into my workspace either directly or through my local machine (download locally first and then upload) but I would like to do it programmatically. Either with the AzureML SDK or azure cli  <\/p>",
        "Challenge_closed_time":1651104442392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651101620807,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to upload Jupyter notebooks from various sources like GitHub to their AzureML workspace programmatically using either AzureML SDK or Azure CLI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/829311\/how-could-i-upload-notebooks-to-my-azureml-workspa",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.7837736111,
        "Challenge_title":"How could I upload notebooks to my AzureML workspace programatically",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. You can use compute instance <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal\">terminal<\/a> in AML notebooks to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/samples-notebooks#get-samples-on-azure-machine-learning-compute-instance\">clone<\/a> the GitHub repo. There's currently no option to upload notebooks to your workspace programmatically using sdk or cli.<\/p>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":7.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.6736786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>In my MLStudio my notebook files window has disappeared so I can not access any of my data (as seen on the image) and I do not know what to do.    <\/p>\n<p>Please your help to solve this as soon as poosible.    <\/p>\n<p>Thank you.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/203167-image.png?platform=QnA\" alt=\"203167-image.png\" \/>    <\/p>",
        "Challenge_closed_time":1652928385600,
        "Challenge_comment_count":1,
        "Challenge_created_time":1652875560357,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's notebook files window has disappeared in MLStudio, making it impossible to access any data. They are seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/854288\/notebook-files-have-disaperred",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.6,
        "Challenge_reading_time":5.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":14.6736786111,
        "Challenge_title":"Notebook files have disaperred",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,    <\/p>\n<p>Thanks for reaching out to us. Could you please check the access of Storage?  <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/storage\/blobs\/assign-azure-role-data-access?tabs=portal#assign-an-azure-role\">https:\/\/learn.microsoft.com\/en-us\/azure\/storage\/blobs\/assign-azure-role-data-access?tabs=portal#assign-an-azure-role<\/a>    <\/p>\n<p>To access these storage services, you must have at least Storage Blob Data Reader access to the storage account. Only storage account owners can change your access level via the Azure portal.    <\/p>\n<p>Or, your admin put the data storage behind V-Net and you can not get access to it- <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-identity-based-data-access#work-with-virtual-networks\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-identity-based-data-access#work-with-virtual-networks<\/a>    <br \/>\nIn this situation, you need to ask permission from your admin.    <\/p>\n<p>Could you please share which situation you are in?     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.0,
        "Solution_reading_time":13.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":98.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1985.0091666667,
        "Challenge_answer_count":0,
        "Challenge_body":"If you run any command that uses azureml (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message:\r\n\r\n```\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), Requirement.parse('flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"')).\r\n```\r\n\r\n**Expected Behavior**\r\nNo warning message should be printed.\r\n\r\n**Steps to Reproduce the Issue**\r\n1. From latest master branch in a fresh virtualenv run: `make build install`\r\n2. `cd \/path\/to\/azure\/a2ml-project`\r\n3. `a2ml experiment leaderboard`\r\n4. Observe the warning message above.\r\n\r\n\r\n**Environment Details:**\r\n - OS: macOS 10.15\r\n - A2ML Version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34\r\n - Python Version: 3.7.7\r\n",
        "Challenge_closed_time":1597072927000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1589926894000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue where installing azureml-sdk downgrades pyarrow to 3.0.0, which breaks cudf. The error message shows that the module 'pyarrow.lib' has no attribute 'MonthDayNanoIntervalArray'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/173",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":12.25,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":614.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1985.0091666667,
        "Challenge_title":"Warning message about hyperdrive loading with azureml_run_type_providers",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":99,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"try again pls, I cannot reproduce it with latest azure ml Not able to reproduce now.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.01,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1425426748316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":91.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":1787.9299130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am doing the following steps to install R Hash_2.2.6.zip package on to Azure ML<\/p>\n\n<ol>\n<li>Upload the .zip file as a dataset<\/li>\n<li>Create a new experiment and Add \"Execute R Script\" to experiment<\/li>\n<li>Drag and drop .zip file dataset to experiment.<\/li>\n<li>Connect the Dataset in step3 to \"Execute R Script\" of step2<\/li>\n<li>Run the experiment to install the package<\/li>\n<\/ol>\n\n<p>However I am getting this error: <code>zip file src\/hash_2.2.6.zip not found<\/code><\/p>\n\n<p>Just so that its very clear, I am following steps mentioned in this article: <a href=\"http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx\" rel=\"nofollow\">http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx<\/a>.<\/p>\n\n<p>Any help in this regard is greatly appreciated.<\/p>",
        "Challenge_closed_time":1425437860360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1419001312673,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install the R Hash_2.2.6.zip package on Azure ML. The user has followed the steps mentioned in a blog post but is still getting an error stating that the zip file is not found. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1483481029407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27568624",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1787.9299130556,
        "Challenge_title":"Installing additional R Package on Azure ML",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2765.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419000630800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":159.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>To install a package this way, you have to create a .zip of a .zip. The outer layer of packaging will get unzipped into the src\/ folder when the dataset is passed in to the module, and you'll be able to install the inner package from there.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1425439161243,
        "Solution_link_count":0.0,
        "Solution_readability":5.9,
        "Solution_reading_time":2.95,
        "Solution_score_count":5.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":22.3896641667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Challenge_closed_time":1573630807523,
        "Challenge_comment_count":1,
        "Challenge_created_time":1573549904240,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a custom Keras model in AWS Sagemaker using a Docker image and pushing it to the ECR repository. While the training is done correctly, the user encounters an error while deploying the model. The error message suggests that the primary container for production variant AllTraffic did not pass the ping health check. The user checks the logs and finds that the error is related to the serve file code. The user has also provided the code for the serve file and the deployment command used.",
        "Challenge_last_edit_time":1573550204732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":11.6,
        "Challenge_reading_time":49.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":22.4731341667,
        "Challenge_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":4605.0,
        "Challenge_word_count":399,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550756471932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.5,
        "Solution_reading_time":9.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":71.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3654761111,
        "Challenge_answer_count":1,
        "Challenge_body":"I work in SM Studio, and I do not understand why CPU and memory usage do not appear in the notebook toolbar. These metrics should be there, at least given this description:\n\nhttps:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/notebooks-menu.html\n\nWhen I open a notebook in SM Studio, I see the same toolbar but without CPU and memory usage listed. Moreover, I see 'cluster' before the kernel's name in my toolbar.\n\nHas anyone experienced sth similar? I assume an alternative for me would be to use CloudWatch.",
        "Challenge_closed_time":1652798353412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652797037698,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue in SM Studio where CPU and memory usage are missing from the notebook toolbar, despite being listed in the documentation. The user also notes that 'cluster' appears before the kernel's name in the toolbar. They are seeking advice from others who may have experienced a similar issue and suggest using CloudWatch as an alternative.",
        "Challenge_last_edit_time":1668563929559,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGQfGnTgqQcyNbWVb3U9V8Q\/cpu-memory-usage-missing-from-sm-studio-notebook-toolbar",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3654761111,
        "Challenge_title":"CPU + memory usage missing from SM Studio notebook toolbar",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":614.0,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, you should be able to see your CPU and Memory on the bottom toolbar, looks like `Kernel: Idle | Instance MEM`. You can click on that text to show the kernel and instance usage metrics.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1652798353412,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":2.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":35.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":20.4100311111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have setup a mlflow server locally at http:\/\/localhost:5000<\/p>\n<p>I followed the instructions at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker<\/a> and tried to run the example docker with<\/p>\n<pre><code>\/mlflow\/examples\/docker$ mlflow run . -P alpha=0.5\n<\/code><\/pre>\n<p>but I encountered the following error.<\/p>\n<pre><code>2021\/05\/09 17:11:20 INFO mlflow.projects.docker: === Building docker image docker-example:7530274 ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.utils: === Created directory \/tmp\/tmp9wpxyzd_ for downloading remote URIs passed to arguments of type 'path' ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts:\/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts -e MLFLOW_RUN_ID=ae69145133bf49efac22b1d390c354f1 -e MLFLOW_TRACKING_URI=http:\/\/localhost:5000 -e MLFLOW_EXPERIMENT_ID=0 docker-example:7530274 python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'ae69145133bf49efac22b1d390c354f1' === \n\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/__init__.py:55: DeprecationWarning: MLflow support for Python 2 is deprecated and will be dropped in a future release. At that point, existing Python 2 workflows that use MLflow will continue to work without modification, but Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3 - see https:\/\/docs.python.org\/3\/howto\/pyporting.html for a migration guide.\n  &quot;for a migration guide.&quot;, DeprecationWarning)\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 56, in &lt;module&gt;\n    with mlflow.start_run():\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/client.py&quot;, line 96, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 49, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 133, in call_endpoint\n    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 70, in http_request\n    url=url, headers=headers, verify=verify, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 51, in request_with_ratelimit_retries\n    response = requests.request(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/api.py&quot;, line 58, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 508, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 618, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/adapters.py&quot;, line 508, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?run_uuid=ae69145133bf49efac22b1d390c354f1&amp;run_id=ae69145133bf49efac22b1d390c354f1 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f5cbd80d690&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))\n2021\/05\/09 17:11:22 ERROR mlflow.cli: === Run (ID 'ae69145133bf49efac22b1d390c354f1') failed ===\n<\/code><\/pre>\n<p>Any ideas how to fix this? I tried adding the following in MLproject file but it doesn't help<\/p>\n<pre><code>environment: [[&quot;network&quot;, &quot;host&quot;], [&quot;add-host&quot;, &quot;host.docker.internal:host-gateway&quot;]]\n<\/code><\/pre>\n<p>Thanks for your help! =)<\/p>",
        "Challenge_closed_time":1620627546968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620552530280,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to connect to MLFLOW_TRACKING_URI when running MLflow run in a Docker container. The error message suggests that the connection to the server is being refused, and the user has tried adding network and host configurations to the MLproject file but it did not help.",
        "Challenge_last_edit_time":1620554070856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67456172",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":61.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":20.8379688889,
        "Challenge_title":"Unable to connect to MLFLOW_TRACKING_URI when running MLflow run in Docker container",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1151.0,
        "Challenge_word_count":357,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316620102630,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1308.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost<\/code>.  Then point the <code>mlflow run<\/code> to that IP instead of <code>http:\/\/localhost:5000<\/code>.   The main reason is that <code>localhost<\/code> of Docker process is its own, not your machine.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":3.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":947.3408333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### What steps did you take:\r\n[A clear and concise description of what the bug is.]\r\n\r\nI am use the re usable Sagemaker Components for building kubeflow pipelines.\r\n\r\nsagemaker_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/train\/component.yaml')\r\nsagemaker_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/model\/component.yaml')\r\nsagemaker_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/deploy\/component.yaml')\r\n\r\nWhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = sagemaker_deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='Endpoint-price-prediction-model',\r\n        endpoint_config_name='EndpointConfig-price-prediction-model',\r\n        update_endpoint=True,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### What happened:\r\nI am getting this error \r\nTypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\nI think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\nTraceback (most recent call last):\r\n--\r\n414 | File \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | File \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = sagemaker_deploy_op(\r\n424 | TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### What did you expect to happen:\r\nto update the endpoint without any issue\r\n### Environment:\r\n<!-- Please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\nsagemaker 2.1.0\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\n<!-- If you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nKFP version: <!-- If you are not sure, build commit shows on bottom of KFP UI left sidenav. -->\r\n\r\nKFP SDK version: <!-- Please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### Anything else you would like to add:\r\n[Miscellaneous information that will assist in solving the issue.]\r\n\r\nPlease help me out \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1611093472000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1607683045000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to create a custom model and perform batch transform in Amazon SageMaker without HPO and training jobs. They have removed HPO and training jobs but are facing issues while compiling kfp. They are getting the desired output in Kubeflow but want to see the custom model output in SageMaker without HPO and batch job. The user also requests for any open source loan data model using KF.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":14.3,
        "Challenge_reading_time":43.94,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":947.3408333333,
        "Challenge_title":"TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":304,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"\/assign @mameshini \r\n\/assign @PatrickXYS \r\n\r\nDo you mind taking a look? Thanks @numerology Thanks!\r\n\r\n@akartsky @RedbackThomson Can you take a look?  Hi @jchaudari, \r\nThanks for reporting the issue, we are taking a look at it. \r\n\r\nThanks,\r\nMeghna Hi @jchaudari, \r\nAre you certain you are using the latest version of the components ? The attached yaml files show that you are using version 0.3.0 of the image which is very old. This feature was added more recently in version 0.9.0 - \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/Changelog.md. \r\n\r\nCould you please try with the newer version and let us know if that fixes your issue ?\r\nThanks,\r\nMeghna Baijal If there aren't any further issues, we'll close this by the end of the week. Otherwise, let us know. \/close @akartsky: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888#issuecomment-763167821):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.7,
        "Solution_reading_time":16.34,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.9680911111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello, I'd like to deploy SageMaker's built-in algorithm, BlazingText model on Fargate instead of Sagemaker endpoint. So, I tried to make an ECS task using BlazingText docker path. Here is my CDK code for it.\n\nconst loadBalancedFargateService = new ecsPatterns.ApplicationLoadBalancedFargateService(this, 'Service', {\n            memoryLimitMiB: 1024,\n            desiredCount: 1,\n            cpu: 512,\n            taskImageOptions: {\n              image: ecs.ContainerImage.fromRegistry(\"811284229777.dkr.ecr.us-east-1.amazonaws.com\/blazingtext:1\"),\n            },\n          });\n\nHowever, I got an error: \nCannotPullContainerError: inspect image has been retried 1 time(s): failed to resolve ref \"811284229777.dkr.ecr.us-east-1.amazonaws.com\/blazingtext:1\": pulling from host 811284229777.dkr.ecr.us-east-1.amazonaws.com failed with status code [manifests 1]...\n\nIs it impossible to pull docker container of sagemaker built-in algorithm from ECS?",
        "Challenge_closed_time":1651328900608,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651321815480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is attempting to deploy a SageMaker built-in algorithm, BlazingText model on Fargate instead of Sagemaker endpoint by making an ECS task using BlazingText docker path. However, the user encountered an error \"CannotPullContainerError\" while trying to pull the docker container of the SageMaker built-in algorithm from ECS.",
        "Challenge_last_edit_time":1668180679166,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCbp7XzQSSPSH200r45m4Uw\/ecs-load-container-image-from-sagemaker-built-in-algorithm-docker-path",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":12.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.9680911111,
        "Challenge_title":"ECS load container image from sagemaker built-in algorithm docker path",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To my knowledge, no - it's not generally possible to pull the [built-in algorithm](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html) containers outside SageMaker: Your easiest route would probably just be to deploy the model on SageMaker and integrate your other containerized tasks to call the SageMaker endpoint.\n\nIt's maybe worth mentioning that the [framework containers](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers-prebuilt.html) for custom\/script-mode modelling (e.g. the [AWS DLCs](https:\/\/github.com\/aws\/deep-learning-containers) for PyTorch\/HuggingFace\/etc) are not subject to this restriction (can check you should even be able to pull them locally): So if you were to use those to implement a customized text processing model I think you should be able to deploy it on ECS if needed. Of course this'd mean a more initial build and later maintenance effort though.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1651328900611,
        "Solution_link_count":3.0,
        "Solution_readability":13.2,
        "Solution_reading_time":11.62,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.5513888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nThe conda environment for python3.6 in notebooks cannot find `pandas.CSVDataSet`\r\n\r\n## Context\r\nI'm wanting to use sagemaker as my development environment. However, I cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## Steps to Reproduce\r\n\r\n0. Startup a Sagemaker instance with defaults\r\n\r\nTerminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => Success!\r\n\r\nNotebook fail:\r\n1. Create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> The environments for the terminal and notebooks are separate by design in Sagemaker\r\n2. Load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> Note that I've started to use the code below; Without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `Path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # Check it exists first. For some reason this is not an idempotent operation?\r\n    current_dir = Path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. Run `context.catalog.list()`\r\n\r\n## Expected Result\r\nThe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## Actual Result\r\n```\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\nFull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not None)\r\n    418         except StopIteration:\r\n\r\nStopIteration: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except StopIteration:\r\n--> 419             raise DataSetError(\"Class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\nDataSetError: Class `pandas.CSVDataSet` not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = AbstractDataSet.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise DataSetError(\r\n    153                 \"An exception occurred when parsing config \"\r\n--> 154                 \"for DataSet `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\nDataSetError: An exception occurred when parsing config for DataSet `example_iris_data`:\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\n## Investigations so far\r\n\r\n### `CSVLocalDataSet`\r\nUpon changing the yaml type for iris.csv from `pandas.CSVDataSet` to `CSVLocalDataSet`, we get success on both the terminal and the notebook. However, this is not my desired outcome; The transition to using `pandas.CSVDataSet` makes it easier, for me at least, to use both S3 and local datasets.\r\n\r\n### `pip install kedro` output from notebook\r\n```\r\nCollecting kedro\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/6f\/4faaa0e58728a318aeabc490271a636f87f6b9165245ce1d3adc764240cf\/kedro-0.15.8-py3-none-any.whl (12.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.5MB 4.1MB\/s eta 0:00:01\r\nRequirement already satisfied: xlsxwriter<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.0.4)\r\nCollecting azure-storage-file<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c9\/33\/6c611563412ffc409b2413ac50e3a063133ea235b86c137759774c77f3ad\/azure_storage_file-1.4.0-py2.py3-none-any.whl\r\nCollecting fsspec<1.0,>=0.5.1 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6e\/2b\/63420d49d5e5f885451429e9e0f40ad1787eed0d32b1aedd6b10f9c2719a\/fsspec-0.7.1-py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 33.5MB\/s ta 0:00:01\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (0.24.2)\r\nCollecting s3fs<1.0,>=0.3.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b8\/e4\/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675\/s3fs-0.4.2-py3-none-any.whl\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nCollecting tables<3.6,>=3.4.4 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/f7\/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a\/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 10.2MB\/s ta 0:00:01\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (2.20.0)\r\nCollecting toposort<2.0,>=1.5 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e9\/8a\/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4\/toposort-1.5-py2.py3-none-any.whl\r\nRequirement already satisfied: click<8.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (6.7)\r\nCollecting azure-storage-queue<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/72\/94\/4db044f1c155b40c5ebc037bfd9d1c24562845692c06798fbe869fe160e6\/azure_storage_queue-1.4.0-py2.py3-none-any.whl\r\nCollecting cookiecutter<2.0,>=1.6.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/86\/c9\/7184edfb0e89abedc37211743d1420810f6b49ae4fa695dfc443c273470d\/cookiecutter-1.7.0-py2.py3-none-any.whl (40kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 24.6MB\/s ta 0:00:01\r\nCollecting pandas-gbq<1.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c3\/74\/126408f6bdb7b2cb1dcb8c6e4bd69a511a7f85792d686d1237d9825e6194\/pandas_gbq-0.13.1-py3-none-any.whl\r\nCollecting pip-tools<5.0.0,>=4.0.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/94\/8f\/59495d651f3ced9b06b69545756a27296861a6edd6c5709fbe1265ed9032\/pip_tools-4.5.1-py2.py3-none-any.whl (41kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 27.5MB\/s ta 0:00:01\r\nCollecting azure-storage-blob<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/25\/f4\/a307ed89014e9abb5c5cfc8ca7f8f797d12f619f17a6059a6fd4b153b5d0\/azure_storage_blob-1.5.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 35.2MB\/s ta 0:00:01\r\nCollecting pyarrow<1.0.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/10\/93fad5849418eade4a4cd581f8cd27be1bbe51e18968ba1492140c887f3f\/pyarrow-0.16.0-cp36-cp36m-manylinux1_x86_64.whl (62.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.9MB 779kB\/s eta 0:00:01    40% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 25.7MB 56.1MB\/s eta 0:00:01\r\nRequirement already satisfied: SQLAlchemy<2.0,>=1.2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.2.11)\r\nRequirement already satisfied: xlrd<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.1.0)\r\nCollecting python-json-logger<1.0,>=0.1.9 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/80\/9d\/1c3393a6067716e04e6fcef95104c8426d262b4adaf18d7aa2470eab028d\/python-json-logger-0.1.11.tar.gz\r\nCollecting anyconfig<1.0,>=0.9.7 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4c\/00\/cc525eb0240b6ef196b98300d505114339bbb7ddd68e3155483f1eb32050\/anyconfig-0.9.10.tar.gz (103kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 34.4MB\/s ta 0:00:01\r\nCollecting azure-storage-common~=1.4 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/6c\/b2285bf3687768dbf61b6bc085b0c1be2893b6e2757a9d023263764177f3\/azure_storage_common-1.4.2-py2.py3-none-any.whl (47kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 25.9MB\/s ta 0:00:01\r\nCollecting azure-common>=1.1.5 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e5\/4d\/d000fc3c5af601d00d55750b71da5c231fcb128f42ac95b208ed1091c2c1\/azure_common-1.1.25-py2.py3-none-any.whl\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.7.3)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.14.3)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2018.4)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (4.0.1)\r\nRequirement already satisfied: numexpr>=2.6.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (2.6.5)\r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.11.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.23)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.6)\r\nCollecting whichcraft>=0.4.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b5\/a2\/81887a0dae2e4d2adc70d9a3557fdda969f863ced51cd3c47b587d25bce5\/whichcraft-0.6.1-py2.py3-none-any.whl\r\nCollecting future>=0.15.2 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/45\/0b\/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9\/future-0.18.2.tar.gz (829kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 27.8MB\/s ta 0:00:01\r\nCollecting poyo>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/42\/50\/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc\/poyo-0.5.0-py2.py3-none-any.whl\r\nCollecting binaryornot>=0.2.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/7e\/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2\/binaryornot-0.4.4-py2.py3-none-any.whl\r\nCollecting jinja2-time>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6a\/a1\/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4\/jinja2_time-0.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.10)\r\nCollecting google-auth-oauthlib (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7b\/b8\/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b\/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\r\nCollecting google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/b0\/cc391ebf8ebf7855cdcfe0a9a4cdc8dcd90287c90e1ac22651d104ac6481\/google_auth-1.12.0-py2.py3-none-any.whl (83kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 35.5MB\/s ta 0:00:01\r\nCollecting pydata-google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/ed\/9c9f410c032645632de787b8c285a78496bd89590c777385b921eb89433d\/pydata_google_auth-0.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (39.1.0)\r\nCollecting google-cloud-bigquery>=1.11.1 (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/8f\/f7\/b6f55e144da37f38a79552a06103f2df4a9569e2dfc6d741a7e2a63d3592\/google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 39.2MB\/s ta 0:00:01\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.14)\r\nCollecting arrow (from jinja2-time>=0.1.0->cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/fa\/f84896dede5decf284e6922134bf03fd26c90870bbf8015f4e8ee2a07bcc\/arrow-0.15.5-py2.py3-none-any.whl (46kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 26.3MB\/s ta 0:00:01\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.0)\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a3\/12\/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379\/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/95\/de\/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d\/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 32.5MB\/s ta 0:00:01\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/08\/6a\/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425\/cachetools-4.0.0-py3-none-any.whl\r\nCollecting google-api-core<2.0dev,>=1.15.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/63\/7e\/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77\/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 29.9MB\/s ta 0:00:01\r\nCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/89\/3c\/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97\/google_cloud_core-1.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.6.1)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/35\/9e\/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098\/google_resumable_media-0.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.11.5)\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/57\/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704\/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 42.0MB\/s ta 0:00:01\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/46\/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf\/googleapis-common-protos-1.51.0.tar.gz\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.18)\r\nBuilding wheels for collected packages: python-json-logger, anyconfig, future, googleapis-common-protos\r\n  Running setup.py bdist_wheel for python-json-logger ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\r\n  Running setup.py bdist_wheel for anyconfig ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\r\n  Running setup.py bdist_wheel for future ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\r\n  Running setup.py bdist_wheel for googleapis-common-protos ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\r\nSuccessfully built python-json-logger anyconfig future googleapis-common-protos\r\ncookiecutter 1.7.0 has requirement click>=7.0, but you'll have click 6.7 which is incompatible.\r\ngoogle-auth 1.12.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\r\ngoogle-cloud-bigquery 1.24.0 has requirement six<2.0.0dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\r\npip-tools 4.5.1 has requirement click>=7, but you'll have click 6.7 which is incompatible.\r\nInstalling collected packages: azure-common, azure-storage-common, azure-storage-file, fsspec, s3fs, tables, toposort, azure-storage-queue, whichcraft, future, poyo, binaryornot, arrow, jinja2-time, cookiecutter, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, pandas-gbq, pip-tools, azure-storage-blob, pyarrow, python-json-logger, anyconfig, kedro\r\n  Found existing installation: s3fs 0.1.5\r\n    Uninstalling s3fs-0.1.5:\r\n      Successfully uninstalled s3fs-0.1.5\r\n  Found existing installation: tables 3.4.3\r\n    Uninstalling tables-3.4.3:\r\n      Successfully uninstalled tables-3.4.3\r\nSuccessfully installed anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 cookiecutter-1.7.0 fsspec-0.7.1 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 oauthlib-3.1.0 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 s3fs-0.4.2 tables-3.5.2 toposort-1.5 whichcraft-0.6.1\r\n```\r\n\r\n### `pip install kedro` output from terminal\r\n```\r\nCollecting kedro\r\n  Using cached kedro-0.15.8-py3-none-any.whl (12.5 MB)\r\nCollecting pandas<1.0,>=0.24.0\r\n  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4 MB 9.6 MB\/s \r\nCollecting azure-storage-file<2.0,>=1.1.0\r\n  Using cached azure_storage_file-1.4.0-py2.py3-none-any.whl (30 kB)\r\nCollecting click<8.0\r\n  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 1.7 MB\/s \r\nCollecting cookiecutter<2.0,>=1.6.0\r\n  Using cached cookiecutter-1.7.0-py2.py3-none-any.whl (40 kB)\r\nCollecting SQLAlchemy<2.0,>=1.2.0\r\n  Downloading SQLAlchemy-1.3.15.tar.gz (6.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.1 MB 49.2 MB\/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting tables<3.6,>=3.4.4\r\n  Using cached tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\/python_json_logger-0.1.11-py2.py3-none-any.whl\r\nCollecting azure-storage-blob<2.0,>=1.1.0\r\n  Using cached azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\r\nCollecting pandas-gbq<1.0,>=0.12.0\r\n  Using cached pandas_gbq-0.13.1-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: fsspec<1.0,>=0.5.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.6.3)\r\nCollecting xlsxwriter<2.0,>=1.0.0\r\n  Downloading XlsxWriter-1.2.8-py2.py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 65.9 MB\/s \r\nCollecting pip-tools<5.0.0,>=4.0.0\r\n  Using cached pip_tools-4.5.1-py2.py3-none-any.whl (41 kB)\r\nCollecting pyarrow<1.0.0,>=0.12.0\r\n  Downloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63.1 MB 25 kB\/s \r\nCollecting xlrd<2.0,>=1.0.0\r\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 66.5 MB\/s \r\nRequirement already satisfied: s3fs<1.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.4.0)\r\nCollecting azure-storage-queue<2.0,>=1.1.0\r\n  Using cached azure_storage_queue-1.4.0-py2.py3-none-any.whl (23 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\/anyconfig-0.9.10-py2.py3-none-any.whl\r\nCollecting toposort<2.0,>=1.5\r\n  Using cached toposort-1.5-py2.py3-none-any.whl (7.6 kB)\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (2.23.0)\r\nRequirement already satisfied: pytz>=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2019.3)\r\nRequirement already satisfied: numpy>=1.13.3 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.18.1)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.8.1)\r\nCollecting azure-common>=1.1.5\r\n  Using cached azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\r\nCollecting azure-storage-common~=1.4\r\n  Using cached azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\r\nCollecting poyo>=0.1.0\r\n  Using cached poyo-0.5.0-py2.py3-none-any.whl (10 kB)\r\nCollecting jinja2-time>=0.1.0\r\n  Using cached jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\r\nCollecting whichcraft>=0.4.0\r\n  Using cached whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\r\nCollecting binaryornot>=0.2.0\r\n  Using cached binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.11.1)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\/future-0.18.2-cp36-none-any.whl\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (3.0.5)\r\nCollecting numexpr>=2.6.2\r\n  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 66.7 MB\/s \r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.14.0)\r\nCollecting pydata-google-auth\r\n  Using cached pydata_google_auth-0.3.0-py2.py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting google-cloud-bigquery>=1.11.1\r\n  Using cached google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\r\nCollecting google-auth\r\n  Using cached google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (46.1.1.post20200323)\r\nRequirement already satisfied: boto3>=1.9.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.12.27)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: idna<3,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nCollecting arrow\r\n  Using cached arrow-0.15.5-py2.py3-none-any.whl (46 kB)\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.1.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0\r\n  Using cached google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\r\nCollecting google-cloud-core<2.0dev,>=1.1.0\r\n  Using cached google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.11.3)\r\nCollecting google-api-core<2.0dev,>=1.15.0\r\n  Using cached google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.3.3)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.15.2)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.14.0)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\/googleapis_common_protos-1.51.0-cp36-none-any.whl\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.20)\r\nBuilding wheels for collected packages: SQLAlchemy\r\n  Building wheel for SQLAlchemy (PEP 517) ... done\r\n  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.15-cp36-cp36m-linux_x86_64.whl size=1215829 sha256=112167e02a19acada7f367d8aca55bbd1e0c655de9edfabebae5e9d055d9a9a6\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/4a\/1b\/3a\/c73044d7be48baeb47cbee343334f7803726ca1e9ba7b29095\r\nSuccessfully built SQLAlchemy\r\nInstalling collected packages: pandas, azure-common, azure-storage-common, azure-storage-file, click, poyo, arrow, jinja2-time, whichcraft, binaryornot, future, cookiecutter, SQLAlchemy, numexpr, tables, python-json-logger, azure-storage-blob, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery, pandas-gbq, xlsxwriter, pip-tools, pyarrow, xlrd, azure-storage-queue, anyconfig, toposort, kedro\r\n  Attempting uninstall: pandas\r\n    Found existing installation: pandas 0.22.0\r\n    Uninstalling pandas-0.22.0:\r\n      Successfully uninstalled pandas-0.22.0\r\nSuccessfully installed SQLAlchemy-1.3.15 anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 click-7.1.1 cookiecutter-1.7.0 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 numexpr-2.7.1 oauthlib-3.1.0 pandas-0.25.3 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 tables-3.5.2 toposort-1.5 whichcraft-0.6.1 xlrd-1.2.0 xlsxwriter-1.2.8\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n|environment | terminal | notebook|\r\n|----|----|----|\r\n|`kedro -V` | kedro, version 0.15.8 | kedro, version 0.15.8|\r\n|`python -V` | Python 3.6.10 :: Anaconda, Inc. | Python 3.6.5 :: Anaconda, Inc.|\r\n|os |  `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"` | `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"`|\r\n|`pip freeze` | anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==1.3.0<br>attrs==19.3.0<br>autovizwidget==0.12.9<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>backcall==0.1.0<br>bcrypt==3.1.7<br>binaryornot==0.4.4<br>bleach==3.1.0<br>boto3==1.12.27<br>botocore==1.15.27<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.14.0<br>chardet==3.0.4<br>click==7.1.1<br>colorama==0.4.3<br>cookiecutter==1.7.0<br>cryptography==2.8<br>decorator==4.4.2<br>defusedxml==0.6.0<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.15.2<br>entrypoints==0.3<br>environment-kernels==1.1.1<br>fsspec==0.6.3<br>future==0.18.2<br>gitdb==4.0.2<br>GitPython==3.1.0<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>hdijupyterutils==0.12.9<br>idna==2.9<br>importlib-metadata==1.5.0<br>ipykernel==5.1.4<br>ipython==7.13.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.5.1<br>jedi==0.16.0<br>Jinja2==2.11.1<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>json5==0.9.3<br>jsonschema==3.2.0<br>jupyter==1.0.0<br>jupyter-client==6.0.0<br>jupyter-console==6.1.0<br>jupyter-core==4.6.1<br>jupyterlab==1.2.7<br>jupyterlab-git==0.9.0<br>jupyterlab-server==1.0.7<br>kedro==0.15.8<br>MarkupSafe==1.1.1<br>mistune==0.8.4<br>mock==3.0.5<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.3<br>nbconvert==5.6.1<br>nbdime==2.0.0<br>nbexamples==0.0.0<br>nbformat==5.0.4<br>nbserverproxy==0.3.2<br>nose==1.3.7<br>notebook==5.7.8<br>numexpr==2.7.1<br>numpy==1.18.1<br>oauthlib==3.1.0<br>packaging==20.3<br>pandas==0.25.3<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.6.2<br>pexpect==4.8.0<br>pickleshare==0.7.5<br>pid==3.0.0<br>pip-tools==4.5.1<br>plotly==4.5.4<br>poyo==0.5.0<br>prometheus-client==0.7.1<br>prompt-toolkit==3.0.3<br>protobuf==3.11.3<br>protobuf3-to-dict==0.1.5<br>psutil==5.7.0<br>psycopg2==2.8.4<br>ptyprocess==0.6.0<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycparser==2.20<br>pydata-google-auth==0.3.0<br>pygal==2.4.0<br>Pygments==2.6.1<br>pykerberos==1.1.14<br>PyNaCl==1.3.0<br>pyOpenSSL==19.1.0<br>pyparsing==2.4.6<br>pyrsistent==0.15.7<br>PySocks==1.7.1<br>pyspark==2.3.2<br>python-dateutil==2.8.1<br>python-json-logger==0.1.11<br>pytz==2019.3<br>PyYAML==5.3.1<br>pyzmq==18.1.1<br>qtconsole==4.7.1<br>QtPy==1.9.0<br>requests==2.23.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rsa==3.4.2<br>s3fs==0.4.0<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-experiments==0.1.10<br>sagemaker-nbi-agent==1.0<br>sagemaker-pyspark==1.2.8<br>scipy==1.4.1<br>Send2Trash==1.5.0<br>six==1.14.0<br>smdebug-rulesconfig==0.1.2<br>smmap==3.0.1<br>sparkmagic==0.15.0<br>SQLAlchemy==1.3.15<br>tables==3.5.2<br>terminado==0.8.3<br>testpath==0.4.4<br>texttable==1.6.2<br>toposort==1.5<br>tornado==6.0.4<br>traitlets==4.3.3<br>urllib3==1.22<br>wcwidth==0.1.8<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>whichcraft==0.6.1<br>widgetsnbextension==3.5.1<br>xlrd==1.2.0<br>XlsxWriter==1.2.8<br>zipp==2.2.0 | alabaster==0.7.10<br>anaconda-client==1.6.14<br>anaconda-project==0.8.2<br>anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==0.24.0<br>astroid==1.6.3<br>astropy==3.0.2<br>attrs==18.1.0<br>Automat==0.3.0<br>autovizwidget==0.15.0<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>Babel==2.5.3<br>backcall==0.1.0<br>backports.shutil-get-terminal-size==1.0.0<br>bcrypt==3.1.7<br>beautifulsoup4==4.6.0<br>binaryornot==0.4.4<br>bitarray==0.8.1<br>bkcharts==0.2<br>blaze==0.11.3<br>bleach==2.1.3<br>bokeh==1.0.4<br>boto==2.48.0<br>boto3==1.12.27<br>botocore==1.15.27<br>Bottleneck==1.2.1<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.11.5<br>characteristic==14.3.0<br>chardet==3.0.4<br>click==6.7<br>cloudpickle==0.5.3<br>clyent==1.2.2<br>colorama==0.3.9<br>contextlib2==0.5.5<br>cookiecutter==1.7.0<br>cryptography==2.8<br>cycler==0.10.0<br>Cython==0.28.4<br>cytoolz==0.9.0.1<br>dask==0.17.5<br>datashape==0.5.4<br>decorator==4.3.0<br>defusedxml==0.6.0<br>distributed==1.21.8<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.14<br>entrypoints==0.2.3<br>enum34==1.1.9<br>environment-kernels==1.1.1<br>et-xmlfile==1.0.1<br>fastcache==1.0.2<br>filelock==3.0.4<br>Flask==1.0.2<br>Flask-Cors==3.0.4<br>fsspec==0.7.1<br>future==0.18.2<br>gevent==1.3.0<br>glob2==0.6<br>gmpy2==2.0.8<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>greenlet==0.4.13<br>h5py==2.8.0<br>hdijupyterutils==0.15.0<br>heapdict==1.0.0<br>html5lib==1.0.1<br>idna==2.6<br>imageio==2.3.0<br>imagesize==1.0.0<br>importlib-metadata==1.5.0<br>ipykernel==4.8.2<br>ipyparallel==6.2.2<br>ipython==6.4.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.4.0<br>isort==4.3.4<br>itsdangerous==0.24<br>jdcal==1.4<br>jedi==0.12.0<br>Jinja2==2.10<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>jsonschema==2.6.0<br>jupyter==1.0.0<br>jupyter-client==5.2.3<br>jupyter-console==5.2.0<br>jupyter-core==4.4.0<br>jupyterlab==0.32.1<br>jupyterlab-launcher==0.10.5<br>kedro==0.15.8<br>kiwisolver==1.0.1<br>lazy-object-proxy==1.3.1<br>llvmlite==0.23.1<br>locket==0.2.0<br>lxml==4.2.1<br>MarkupSafe==1.0<br>matplotlib==3.0.3<br>mccabe==0.6.1<br>mistune==0.8.3<br>mkl-fft==1.0.0<br>mkl-random==1.0.1<br>mock==4.0.1<br>more-itertools==4.1.0<br>mpmath==1.0.0<br>msgpack==0.6.0<br>msgpack-python==0.5.6<br>multipledispatch==0.5.0<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.2<br>nbconvert==5.4.1<br>nbformat==4.4.0<br>networkx==2.1<br>nltk==3.3<br>nose==1.3.7<br>notebook==5.5.0<br>numba==0.38.0<br>numexpr==2.6.5<br>numpy==1.14.3<br>numpydoc==0.8.0<br>oauthlib==3.1.0<br>odo==0.5.1<br>olefile==0.45.1<br>opencv-python==3.4.2.17<br>openpyxl==2.5.3<br>packaging==20.1<br>pandas==0.24.2<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.2.0<br>partd==0.3.8<br>path.py==11.0.1<br>pathlib2==2.3.2<br>patsy==0.5.0<br>pep8==1.7.1<br>pexpect==4.5.0<br>pickleshare==0.7.4<br>Pillow==5.1.0<br>pip-tools==4.5.1<br>pkginfo==1.4.2<br>plotly==4.5.2<br>pluggy==0.6.0<br>ply==3.11<br>poyo==0.5.0<br>prompt-toolkit==1.0.15<br>protobuf==3.6.1<br>protobuf3-to-dict==0.1.5<br>psutil==5.4.5<br>psycopg2==2.7.5<br>ptyprocess==0.5.2<br>py==1.5.3<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycodestyle==2.4.0<br>pycosat==0.6.3<br>pycparser==2.18<br>pycrypto==2.6.1<br>pycurl==7.43.0.1<br>pydata-google-auth==0.3.0<br>pyflakes==1.6.0<br>pygal==2.4.0<br>Pygments==2.2.0<br>pykerberos==1.2.1<br>pylint==1.8.4<br>PyNaCl==1.3.0<br>pyodbc==4.0.23<br>pyOpenSSL==18.0.0<br>pyparsing==2.2.0<br>PySocks==1.6.8<br>pyspark==2.3.2<br>pytest==3.5.1<br>pytest-arraydiff==0.2<br>pytest-astropy==0.3.0<br>pytest-doctestplus==0.1.3<br>pytest-openfiles==0.3.0<br>pytest-remotedata==0.2.1<br>python-dateutil==2.7.3<br>python-json-logger==0.1.11<br>pytz==2018.4<br>PyWavelets==0.5.2<br>PyYAML==5.3.1<br>pyzmq==17.0.0<br>QtAwesome==0.4.4<br>qtconsole==4.3.1<br>QtPy==1.4.1<br>requests==2.20.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rope==0.10.7<br>rsa==3.4.2<br>ruamel-yaml==0.15.35<br>s3fs==0.4.2<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-pyspark==1.2.8<br>scikit-image==0.13.1<br>scikit-learn==0.20.3<br>scipy==1.1.0<br>seaborn==0.8.1<br>Send2Trash==1.5.0<br>simplegeneric==0.8.1<br>singledispatch==3.4.0.3<br>six==1.11.0<br>smdebug-rulesconfig==0.1.2<br>snowballstemmer==1.2.1<br>sortedcollections==0.6.1<br>sortedcontainers==1.5.10<br>sparkmagic==0.12.5<br>Sphinx==1.7.4<br>sphinxcontrib-websupport==1.0.1<br>spyder==3.2.8<br>SQLAlchemy==1.2.11<br>statsmodels==0.9.0<br>sympy==1.1.1<br>tables==3.5.2<br>TBB==0.1<br>tblib==1.3.2<br>terminado==0.8.1<br>testpath==0.3.1<br>texttable==1.6.2<br>toolz==0.9.0<br>toposort==1.5<br>tornado==5.0.2<br>traitlets==4.3.2<br>typing==3.6.4<br>unicodecsv==0.14.1<br>urllib3==1.23<br>wcwidth==0.1.7<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>Werkzeug==0.14.1<br>whichcraft==0.6.1<br>widgetsnbextension==3.4.2<br>wrapt==1.10.11<br>xlrd==1.1.0<br>XlsxWriter==1.0.4<br>xlwt==1.3.0<br>zict==0.1.3<br>zipp==3.0.0|\r\n",
        "Challenge_closed_time":1585791347000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1585713762000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running a query on the `aws_sagemaker_notebook_instance` table in Steampipe. The error message indicates that the `hydrate call listAwsSageMakerNotebookInstanceTags` failed with a panic interface conversion. The user is using Steampipe version v0.4.1 and aws plugin version v0.15.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro\/issues\/308",
        "Challenge_link_count":35,
        "Challenge_participation_count":5,
        "Challenge_readability":22.7,
        "Challenge_reading_time":580.3,
        "Challenge_repo_contributor_count":164.0,
        "Challenge_repo_fork_count":740.0,
        "Challenge_repo_issue_count":1942.0,
        "Challenge_repo_star_count":7884.0,
        "Challenge_repo_watch_count":102.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":434,
        "Challenge_solved_time":21.5513888889,
        "Challenge_title":"Sagemaker notebooks raise error for `pandas.CSVDataSet`",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1973,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Kedro aside there are a couple of things that you can do to ensure that your environments match from the terminal vs notebook.  I am not familiar with the new `pandas.CSVDataSet` as I am just now starting with my first `0.15.8` myself.  We have struggled to get package installs correct through our notebooks, I make sure my team is all using their own environment, created from the terminal.\r\n\r\n## activate python3 from the terminal before install\r\n\r\nNote that the file browser on the left hand side of a SageMaker notebook is really mounted at `~\/SageMaker`.\r\n\r\n``` bash\r\nsource activate python3\r\n# may also be - conda activate python3\r\n# unrelated on windows it was - activate python 3\r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n## install ipykernel in your terminal env\r\n\r\nFor conda environments to show up in the notebook dropdown selection you will need `ipykernel` installed. see [docs](https:\/\/ipython.readthedocs.io\/en\/stable\/install\/kernel_install.html)\r\n\r\n```\r\nconda create -n testing python=3.6\r\npip install ipykernel\r\n# I typically don't have to go this far, but installing ipykernel is recommended by the docs\r\nipykernel install --user \r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n\r\n\r\nDo note that if you shut down your SageMaker notebook you will loose your packages and environments by default.\r\n\r\nI also noticed that you have a difference between pandas.  I have no idea if that changes things, but might be a simple fix. Your second idea worked @WaylonWalker. I slightly adapted it as it didn't work straight up:\r\n```\r\nconda create --yes --name kedroenv python=3.6 ipykernel\r\nsource activate kedroenv\r\npython -m ipykernel install --user --name kedroenv --display-name \"Kedro py3.6\"\r\n\r\ncd ~\/Sagemaker\r\nkedro new # Name testing and example pipeline\r\ncd testing\/\r\nkedro run\r\n```\r\nWith a reasonable solution, I'll call this issue closed. Massive thank you @WaylonWalker for pointing me in the right direction.\r\n\r\nCheers,\r\nTom @tjcuddihy We're working with the AWS team to produce a knowledge document on using Kedro and Sagemaker. Would we be able to talk to you about how you used them together? I'd be keen on learning more about how to make Sagemaker play nicely with kedro so I can still access everything I need from my kedro context. @yetudada I have an alpha version of a kedro plugin that plays nicely with sagemaker and allows you to run processing jobs. @uwaisiqbal then you might be interested in this knowledge article that was just published on AWS: https:\/\/aws.amazon.com\/blogs\/opensource\/using-kedro-pipelines-to-train-amazon-sagemaker-models\/ \ud83d\ude80 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":32.51,
        "Solution_score_count":null,
        "Solution_sentence_count":20.0,
        "Solution_word_count":397.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":4.6262580556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a function I can use to get the instance type of my SageMaker instance.<\/p>\n<p>I basically want to do something like this<\/p>\n<pre><code>region = boto3.Session().region_name\n<\/code><\/pre>\n<p>but for the instance type.<\/p>\n<p>I know I can find it manually, but I want to automate it so that my script can work on any instance.<\/p>",
        "Challenge_closed_time":1658271130892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658254476363,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a function to retrieve the instance type of their SageMaker instance automatically, instead of manually searching for it. They want to automate their script to work on any instance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73041737",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4.6262580556,
        "Challenge_title":"Print SageMaker instance type",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1657050754840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeNotebookInstance.html\" rel=\"nofollow noreferrer\">DescribeNotebookInstance<\/a> API to get the instance size.<\/p>\n<pre><code>sm_client = boto3.client(&quot;sagemaker&quot;)\nsm.describe_notebook_instance(\n    NotebookInstanceName=&lt;nb-name&gt;\n)['InstanceType']\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":24.0,
        "Solution_reading_time":5.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":15.8759286111,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am new with <code>Jupyter<\/code>, and I use Amazon SageMaker so that everything is cloud based and not local.  I cannot use any resources locally, nor can I install <code>Jupyter<\/code> on this local computer that I want to do this on, so I cannot use the command line to put :<\/p>\n\n<pre><code>jupyter nbconvert Jupyter\\ Slides.ipynb --to slides --post serve\n<\/code><\/pre>\n\n<p>So, I am struggling to find a way to convert my notebook to a slideshow NOT using command line. Thanks in advance!<\/p>",
        "Challenge_closed_time":1532732192903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532649972710,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is new to Jupyter and is using Amazon SageMaker, which is cloud-based. They are unable to use the command line to convert their Jupyter notebook to a slideshow and are seeking alternative methods to do so.",
        "Challenge_last_edit_time":1532675039560,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51549048",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":8.2,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":22.8389425,
        "Challenge_title":"How to convert jupyter notebook (ipython) to slideshow NOT using command line",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":4610.0,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528567336707,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kelowna, BC, Canada",
        "Poster_reputation_count":165.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>You can follow below steps to convert your notebook to slides on AWS Sagemaker (tried on sagemaker notebook instance) without installing any extensions.<\/p>\n\n<p><strong>Step 1:<\/strong> Follow this <a href=\"https:\/\/medium.com\/@mjspeck\/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67\" rel=\"nofollow noreferrer\">article<\/a> to chose which cells in your notebook can be presented or skipped.\n  - Go to View \u2192 Cell Toolbar \u2192 Slideshow\n  - A light gray bar will appear above each cell with a scroll down window on the top right\n  - Select type of slide each cell should be - regular slide, sub-slide, skip, notes<\/p>\n\n<p><strong>Step 2:<\/strong> Go to Sagemaker notebook home page and open terminal<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 3:<\/strong> Change directory in the instance where your notebook exists<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 4:<\/strong> Clone <code>reveal.js<\/code> in the directory where notebook exists from <a href=\"https:\/\/github.com\/hakimel\/reveal.js\" rel=\"nofollow noreferrer\">github<\/a>. <code>reveal.js<\/code> is used for rendering HTML file as presentation.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dillF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dillF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 5:<\/strong> Run the below command (same as in your question) to convert the notebook to slides without serving them (since there is no browser on the Sagemaker instance). This will just convert notebook to slides html.<\/p>\n\n<pre><code>jupyter nbconvert Image-classification-fulltraining.ipynb --to slides\n[NbConvertApp] Converting notebook Image-classification-fulltraining.ipynb to slides\n[NbConvertApp] Writing 346423 bytes to Image-classification-fulltraining.slides.html\n<\/code><\/pre>\n\n<p><strong>Step 6:<\/strong> Now open the html file from Sagemaker notebook file browser <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fykyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fykyl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now you can see the notebook rendered as slides based on how setup each cell in your notebook in Step 1<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":12.0,
        "Solution_readability":12.5,
        "Solution_reading_time":34.51,
        "Solution_score_count":2.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":263.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1.1966977778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We developed a Jupyter Notebook in a local machine to train models with the Python (V3) libraries <code>sklearn<\/code> and <code>gensim<\/code>.\nAs we set the <code>random_state<\/code> variable to a fixed integer, the results were always the same.<\/p>\n\n<p>After this, we tried moving the notebook to a workspace in Azure Machine Learning Studio (classic), but the results differ even if we leave the <code>random_state<\/code> the same.<\/p>\n\n<p>As suggested in the following links, we installed the same libraries versions and checked the <code>MKL<\/code> version was the same and the <code>MKL_CBWR<\/code> variable was set to <code>AUTO<\/code>.<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/46766714\/t-sne-generates-different-results-on-different-machines\">t-SNE generates different results on different machines<\/a><\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/38228088\/same-python-code-same-data-different-results-on-different-machines\">Same Python code, same data, different results on different machines<\/a><\/p>\n\n<p>Still, we are not able to get the same results.<\/p>\n\n<p>What else should we check or why is this happening?<\/p>\n\n<p><strong>Update<\/strong><\/p>\n\n<p>If we generate a <code>pkl<\/code> file in the local machine and import it in AML, the results are the same (as the intention of the pkl file is).<\/p>\n\n<p>Still, we are looking to get the same results (if possible) without importing the pkl file.<\/p>\n\n<p><strong>Library versions<\/strong><\/p>\n\n<pre><code>gensim 3.8.3.\nsklearn 0.19.2.\nmatplotlib 2.2.3.\nnumpy 1.17.2.\nscipy 1.1.0.\n<\/code><\/pre>\n\n<p><strong>Code<\/strong><\/p>\n\n<p>Full code can be found <a href=\"https:\/\/t.ly\/YlCi\" rel=\"nofollow noreferrer\">here<\/a>, sample data link inside.<\/p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nfrom gensim.models import KeyedVectors\n%matplotlib inline\n\nimport time\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\nwordvectors_file_vec = '..\/libraries\/embeddings-new_large-general_3B_fasttext.vec'\nwordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)\n\nmath_quests = # some transformations using wordvectors\n\ndf_subset = pd.DataFrame()\n\npca = PCA(n_components=3, random_state = 42)\npca_result = pca.fit_transform(mat_quests)\ndf_subset['pca-one'] = pca_result[:,0]\ndf_subset['pca-two'] = pca_result[:,1] \n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state = 42)\ntsne_results = tsne.fit_transform(mat_quests)\n\ndf_subset['tsne-2d-one'] = tsne_results[:,0]\ndf_subset['tsne-2d-two'] = tsne_results[:,1]\n\npca_50 = PCA(n_components=50, random_state = 42)\npca_result_50 = pca_50.fit_transform(mat_quests)\nprint('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))\n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, random_state = 42)\ntsne_pca_results = tsne.fit_transform(pca_result_50)\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n<\/code><\/pre>",
        "Challenge_closed_time":1591493823768,
        "Challenge_comment_count":5,
        "Challenge_created_time":1591464199347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user developed a Jupyter Notebook in a local machine to train models with the Python libraries sklearn and gensim. They set the random_state variable to a fixed integer, and the results were always the same. However, when they moved the notebook to a workspace in Azure Machine Learning Studio (classic), the results differ even if they leave the random_state the same. They installed the same libraries versions and checked the MKL version was the same and the MKL_CBWR variable was set to AUTO, but they are still not able to get the same results. If they generate a pkl file in the local machine and import it in AML, the results are the same.",
        "Challenge_last_edit_time":1591489515656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62235365",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":12.2,
        "Challenge_reading_time":42.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":8.2290058333,
        "Challenge_title":"Models generate different results when moving to Azure Machine Learning Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":201.0,
        "Challenge_word_count":320,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585590244876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Definitely empathize with the issue you're having. Every data scientist has struggled with this at some point.<\/p>\n\n<p>The hard truth I have for you is that Azure ML Studio (classic) isn't really capable of  solving this \"works on my machine\" problem. However, the good news is that Azure ML Service is incredible at it. Studio classic doesn't let you define custom environments deterministically, only add and remove packages (and not so well even at that) <\/p>\n\n<p>Because ML Service's execution is built on top of <code>Docker<\/code> containers and <code>conda<\/code> environments, you can feel more confident in repeated results. I highly recommend you take the time to learn it (and I'm also happy to debug any issues that come up). Azure's <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\" rel=\"nofollow noreferrer\">MachineLearningNotebooks repo<\/a> has a lot of great tutorials for getting started.<\/p>\n\n<p>I spent two hours making <a href=\"https:\/\/github.com\/swanderz\/MachineLearningNotebooks\/blob\/SO_CPR\/how-to-use-azureml\/training\/train-on-amlcompute\/train-on-amlcompute.ipynb\" rel=\"nofollow noreferrer\">a proof of concept<\/a> that demonstrate how ML Service solves the problem you're having by synthesizing:<\/p>\n\n<ul>\n<li>your code sample (before you shared your notebook),<\/li>\n<li><a href=\"https:\/\/scikit-learn.org\/stable\/auto_examples\/manifold\/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py\" rel=\"nofollow noreferrer\">Jake Vanderplas's sklearn example<\/a>, and<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-amlcompute\/train-on-amlcompute.ipynb\" rel=\"nofollow noreferrer\">this Azure ML tutorial<\/a> on remote training.<\/li>\n<\/ul>\n\n<p>I'm no T-SNE expert, but from the screenshot below, you can see that the t-sne outputs are the same when I run the script locally and remotely. This might be possible with Studio classic, but it would be hard to guarantee that it will always work.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mhlg6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mhlg6.png\" alt=\"Azure ML Experiment Results Page\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.5,
        "Solution_reading_time":28.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":242.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":529.4461111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nA SageMaker Notebook-v3 workspace that was working fine on Friday today appears with the status as \"Unknown\". \r\nWhen clicking on connect the new window pop up but is empty, and when going back to the SWB page, we see the message, \"We have a problem! Something went wrong\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to 'Workspaces'\r\n2. Look for the workspace that was expected to be \"Stoped\"\r\n2. Click on 'connect'\r\n4. See error\r\n\r\n**Expected behavior**\r\nThat the workspace was \"Stopped\" and when clicking on Connect we can access to the workspace. \r\n\r\n**Screenshots**\r\n![Screen Shot 2021-09-13 at 1 27 57 PM](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**Versions (please complete the following information):**\r\nRelease Version installed: 3.3.1\r\n\r\n**Additional context**\r\nThe workspace was working fine all previous week, autostop and connect without any issue. Unknown status found today.",
        "Challenge_closed_time":1633460282000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1631554276000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error message \"null is not an object\" while trying to connect to Sagemaker notebook. The error is intermittent and occurs after the workspace has been open for a while. The notebook window is not opened after clicking on 'Connect'. The expected behavior is a new window should open with a Jupyter\/Sagemaker notebook in a new window.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/708",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":529.4461111111,
        "Challenge_title":"[Bug] SageMaker Notebook-v3 Workspace changed to \"Unknown\" status and cannot connect anymore",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":148,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think there's a good chance this instance was autostopped, but that information was not propagated to DDB correctly.\r\n\r\nCan you log onto the hosting account for that Sagemaker instance and check if it's currently in the `Stopped` state. If yes, the latest code fixes that issue.\r\nhttps:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/8cb199b8093f5e799d2d87c228930a4929ebebb7 Hi @nguyen102 yes, I can confirm that the Sagemaker instance is  in the Stopped sate. So then, the latest code that you mention should fix the issue. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":6.64,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":129.76,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nAttempting to deploy the docker image build yaml file available under https:\/\/github.com\/polyaxon\/polyaxon-examples\/blob\/master\/in_cluster\/build_image\/build-ml.yaml using the polyaxon cli returns the following error:\n\n>polyaxon run -f build_docker_image.yaml -l\nPolyaxonfile is not valid.\nError message: The Polyaxonfile's version specified is not supported by your current CLI.Your CLI support Polyaxonfile versions between: 1.1 <= v <= 1.1.You can run `polyaxon upgrade` and check documentation for the specification..\n\nTo reproduce\n\nThe contents of build_docker_image.yaml\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: python:3.8.8-buster\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nExpected behavior\n\nA build operation should begin that results in an image being saved to the docker registry connected under destination: connection:\n\nEnvironment\n\nPolyaxon 1.8.0\nPolyaxon CLI 1.8.0",
        "Challenge_closed_time":1618580258000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618113122000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while attempting to deploy a docker image build yaml file using the Polyaxon CLI. The error message indicates that the Polyaxonfile's version specified is not supported by the current CLI version. The user is using Polyaxon 1.8.0 and Polyaxon CLI 1.8.0. The expected behavior is to begin a build operation resulting in an image being saved to the docker registry.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1303",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.9,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":129.76,
        "Challenge_title":"Issue building docker image using example build config",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":140,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Very strange, and I could not reproduce. Did you try other files in the same examples folder https:\/\/github.com\/polyaxon\/polyaxon-examples\/tree\/master\/in_cluster\/build_image ?\nNot sure if quoting all values would work on your system:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: \"polyaxon-examples:ml\"\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"python:3.8.8-buster\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - 'pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nFinally you may try with polyaxon -v ... to trigger some additional debug information.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":8.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":78.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":23.6360758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm creating my own Docker container for use with SageMaker and I'm wondering why the serve command creates a Flask app to serve predictions on data when I want to do a batch transform job. Wouldn't it be simpler to just unpickle the model and run the model's predict method on the dataset I want predictions for? I don't need a web api\/endpoint. I just need to automatically generate predictions once a day.<\/p>",
        "Challenge_closed_time":1574462969596,
        "Challenge_comment_count":1,
        "Challenge_created_time":1574377879723,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is creating a Docker container for use with AWS SageMaker and is questioning why the serve command creates a Flask app to serve predictions on data when they want to do a batch transform job. They are wondering if it would be simpler to just unpickle the model and run the model's predict method on the dataset they want predictions for, as they do not need a web API\/endpoint and just need to automatically generate predictions once a day.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58985124",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":5.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":23.6360758333,
        "Challenge_title":"Why does AWS SageMaker run a web server for batch transform?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":470.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446895388123,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":473.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Good question :) using the exact same code for batch inference and online inference reduces development overhead - the exact same stack can be used for both use-cases - and also reduces risks of having different results between something done in Batch and something done online. That being said, SageMaker is very flexible and what you describe can easily be done using the Training API. There is nothing in the Training API forcing you to use it for ML training, it is actually a very versatile docker orchestrator with advanced logging, metadata persistance, and built for fast and distributed data ingestion.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.0,
        "Solution_reading_time":7.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":98.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":47.865,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nGetting error \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" while running \"mlflow ui\".\r\n\r\n**To Reproduce**\r\nRun \"mlflow ui\"\r\n\r\n\r\n**Expected behavior**\r\nIt should run without any issues\r\n\r\n\r\n**Versions**\r\n2.3.10\r\n\r\nNot sure if this is the right forum to post this issue. If it is not, please ignore.\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1650449956000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1650277642000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered challenges with pycaret's clustering module, such as not saving model artifacts and some plots, and the status always failing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2425",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":5.9,
        "Challenge_reading_time":4.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":47.865,
        "Challenge_title":"[BUG] mlflow ui never runs",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":59,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@maverick-scientist there is not enough information here to recreate or debug the issue. Please provide a complete reproducible example so we can debug. Also, note that if the data is proprietary, you can create a fake dataset yourself to recreate the issue. Refer to the following for more details:\r\n\r\n\ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\r\n\ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\r\n\r\n- The do's and dont's have an example of how to create the fake data - minimal to reproduce the problem.\r\n- Alternately, you could try to reproduce the issue with a publicly available dataset or one available in pycaret itself.\r\n\r\nThanks!\r\n Hi Nikhil,\nCan we please discuss this over teams call? I can share a python file with\nyou for this issue but I feel if we can discuss this on a call it would\nsave me some time.\n\nWhat do you think?\n\nPlease advise.\n\nOn Mon, 18 Apr 2022 at 22:55, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> there is not\n> enough information here to recreate or debug the issue. Please provide a\n> complete reproducible example so we can debug. Also, note that if the data\n> is proprietary, you can create a fake dataset yourself to recreate the\n> issue. Refer to the following for more details:\n>\n> \ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\n> \ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\n>\n>    - The do's and dont's have an example of how to create the fake data -\n>    minimal to reproduce the problem.\n>    - Alternately, you could try to reproduce the issue with a publicly\n>    available dataset or one available in pycaret itself.\n>\n> Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1101586641>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRACMIA55LOVAGIZZKPLVFWLKHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @maverick-scientist Honestly, I would prefer not to do that since it sets the wrong precedent for the open-source community and is not sustainable in the long run. The globally accepted best practice is to provide a minimal reproducible example and I would encourage you to do that.\r\n\r\nThanks! Hi Nikhil,\r\nAttaching the code file to reproduce this issue. Could you please check and tell me what's wrong with it or my machine?\r\n\r\nThanks & Regards,\r\nAbhinav\r\n\r\n[Simple MLflow.zip](https:\/\/github.com\/pycaret\/pycaret\/files\/8511837\/Simple.MLflow.zip)\r\n\r\n @maverick-scientist The example that you posted has no reference to pycaret. It is a generic MLFlow example. How is it related to pycaret and this repo? Hi Nikhil,\nThank you for your reply. However, if you read the issue carefully, I\u2019d\nclearly mentioned my dilemma whether it was the right forum to post this\nissue.\n\nAnyways, thanks for your support. I\u2019d try and see what\u2019s preventing the\nMLFlow to run properly on my machine.\n\nOn Wed, 20 Apr 2022 at 15:21, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> The example\n> that you posted has no reference to pycaret. It is a generic MLFlow\n> example. How is it related to pycaret and this repo?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1103727978>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRADHBVLOBH4SACXZHNLVF7HTHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_readability":7.7,
        "Solution_reading_time":43.39,
        "Solution_score_count":null,
        "Solution_sentence_count":42.0,
        "Solution_word_count":477.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":15.915975,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I tried to import causalImpact library from github using \"devtools\" in AzureML studio for one of my projects.\ncode used was:<\/p>\n\n<pre><code>library(devtools)\ndevtools::install_github(\"google\/CausalImpact\")\n<\/code><\/pre>\n\n<p>Unfortunately, Azure doesn't support this.So tried importing it following the procedure in this <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow\">blog<\/a>.It is giving multiple errors on the name of dependent packages of casualImpact(i.e. BOOM, BH etc.). Can anyone help me out in importing this package on Azure?<\/p>\n\n<p>This is the R-script I used following the link given above:<\/p>\n\n<pre><code>library(assertthat)\nlibrary(dplyr)\nlibrary(hflights)\nlibrary(Lahman)\nlibrary(magrittr)\nlibrary(LGPL)\ninstall.packages(\"src\/BH_1.55.0-3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BH \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\nlibrary(BH)\ninstall.packages(\"src\/Boom_0.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"Boom \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/BoomSpikeSlab.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BoomSpikeSlab\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/bsts_0.5.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"bsts\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\nlibrary(zoo)\nlibrary(xts)\ninstall.packages(\"src\/CausalImpact.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"CausalImpact\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n<\/code><\/pre>",
        "Challenge_closed_time":1459314427723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459257130213,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to import the \"causalImpact\" library from GitHub using \"devtools\" in AzureML studio, but Azure does not support it. The user tried to import it following a blog post, but it gave multiple errors on the name of dependent packages of casualImpact (i.e. BOOM, BH, etc.). The user is seeking help in importing this package on Azure.",
        "Challenge_last_edit_time":1459402444996,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36285329",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":23.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":15.915975,
        "Challenge_title":"How to import a third party library \"causalImpact\" using R script in AzureML studio?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":326.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423214734843,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore",
        "Poster_reputation_count":55.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>You will have to upload all dependent packages of casualImpact as a zip file - see sample <a href=\"http:\/\/gallery.azureml.net\/Details\/7507f907deb845d9b9b193b455a8615d\" rel=\"nofollow\">here<\/a> which shows uploading two packages required for xgboost<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":3.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":265.7839591667,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I recently started using the <code>wandb.Api()<\/code> in order not to manually download all the Charts in <code>.csv<\/code> format.<\/p>\n<p>The problem is that I cannot get consistent results, most of the times that I call the API  in a jupyter-notebook I get different results.<\/p>\n<p>I have made public one of my dashboards to tackle this issue. Here is a screenshot with a reproducible example:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" data-download-href=\"\/uploads\/short-url\/8x7Rm9lNkSyg4pi6edKG0wNOxgE.png?dl=1\" title=\"2022-05-16-165542_647x517_scrot\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" alt=\"2022-05-16-165542_647x517_scrot\" data-base62-sha1=\"8x7Rm9lNkSyg4pi6edKG0wNOxgE\" width=\"625\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">2022-05-16-165542_647x517_scrot<\/span><span class=\"informations\">647\u00d7517 50.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>In order to obtain the <code>csv_val_f1<\/code> variable one just needs to download the <code>Val F1<\/code> chart. Two things can be seen here:<\/p>\n<ol>\n<li>Multiple runs of the same code produce different results<\/li>\n<li>The maximum value obtained by the API differs from the maximum value obtained by manually downloading the <code>.csv<\/code> version of the Chart.<\/li>\n<\/ol>\n<p>Any ideas on what I\u2019m missing?<\/p>",
        "Challenge_closed_time":1653670051375,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652713229122,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering inconsistent results when using the wandb.Api() in a jupyter-notebook, with the run.history() function returning different values on almost each call. The user has made a dashboard public to demonstrate the issue, with multiple runs of the same code producing different results and the maximum value obtained by the API differing from the maximum value obtained by manually downloading the .csv version of the Chart. The user is seeking advice on what they may be missing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/run-history-returns-different-values-on-almost-each-call\/2431",
        "Challenge_link_count":3,
        "Challenge_participation_count":5,
        "Challenge_readability":14.1,
        "Challenge_reading_time":26.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":265.7839591667,
        "Challenge_title":"Run.history() returns different values on almost each call",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":866.0,
        "Challenge_word_count":165,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jaeheelee\">@jaeheelee<\/a> and <a class=\"mention\" href=\"\/u\/carloshernandezp\">@carloshernandezp<\/a>,<br>\nI believe you are seeing this because we sample the data points when you call <code>run.history()<\/code>. You can use <code>run.scan_history()<\/code> if you would like to have the entire history returned. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#sampling\">Here<\/a> is some more information on this.<\/p>\n<p>Let me know if this solves the issue for you.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":7.07,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":59.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1181.1,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello,\n\nI am trying to run a Custom Training Job in the Vertex AI Training service.\n\nThe job is based on a tutorial for that fine-tuning a pre-trained BERT model (from HuggingFace).\n\nWhen I use the `gcloud` CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:\n\n\n$BASE_GPU_IMAGE=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\"\n$BUCKET_NAME = \"my-bucket\"\n\ngcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=\"--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier\" `\n--worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task\"\n\n\n\n... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.\n\nGranted the base image is around 6.5GB but where do the additional >10GB come from? Is there a way for me to avoid incurring the added size increase?\n\nPlease note that my job loads the training data using the `datasets` Python package at run time and AFAIK does not include it in the auto-packaged docker image.\n\n\u00a0\n\nThanks,\nurig",
        "Challenge_closed_time":1650182580000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645930620000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges while running a Custom Training Job in the Vertex AI Training service. The Docker image created by the auto-packaging process is very large, around 18GB, which takes a long time to upload to the GCP registry. The user is unsure where the additional >10GB come from and is looking for a way to avoid incurring the added size increase.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Training-Auto-packaged-Custom-Training-Job-Yields-Very\/m-p\/397685#M214",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":16.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1181.1,
        "Challenge_title":"Vertex AI Training: Auto-packaged Custom Training Job Yields Very Large Docker Image",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":620.0,
        "Challenge_word_count":162,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":5.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":73.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.0890694444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using SageMaker JupyterLab, but I found pandas is out of date, what's the process of updating it?<\/p>\n<p>I tried this:\nIn terminal:<\/p>\n<pre><code>cd SageMaker\nconda update pandas\n<\/code><\/pre>\n<p>The package has been updated to 1.0.5\nbut when I use this command in SageMaker instance:<\/p>\n<pre><code>import pandas\nprint(pandas,__version__)\n\nreturn:\n0.24.2\n<\/code><\/pre>\n<p>It didn't work at all, can someone help me? Thanks.<\/p>",
        "Challenge_closed_time":1593895412640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593894159730,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with updating the pandas version in SageMaker JupyterLab. They tried updating the package using the terminal command 'conda update pandas', which was successful, but when they checked the version using the 'import pandas' command, it still showed the old version. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1593895091990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62734059",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":6.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3480305556,
        "Challenge_title":"How to update pandas version in SageMaker notebook terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1158.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>If you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.4,
        "Solution_reading_time":3.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1594906911350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41528.0,
        "Answerer_view_count":6164.0,
        "Challenge_adjusted_solved_time":138.9138230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are using AWS Sagemaker feature, bring your own docker, where we have inference model written in R. As I understood, batch transform job runs container in a following way:<\/p>\n<pre><code>docker run image serve\n<\/code><\/pre>\n<p>Also, on docker we have a logic to determine which function to invoke:<\/p>\n<pre><code>args &lt;- commandArgs()\nif (any(grepl('train', args))) {\n    train()}\nif (any(grepl('serve', args))) {\n    serve()}\n<\/code><\/pre>\n<p>Is there a way, to override default container invocation so we can pass some additional parameters?<\/p>",
        "Challenge_closed_time":1599691026140,
        "Challenge_comment_count":6,
        "Challenge_created_time":1599220820873,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is using AWS Sagemaker with a docker container running an inference model in R. They want to know if there is a way to override the default container invocation to pass additional parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63740792",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":16.6,
        "Challenge_reading_time":7.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":130.6125741667,
        "Challenge_title":"Provide additional input to docker container running inference model",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":702.0,
        "Challenge_word_count":84,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473837223712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgrade",
        "Poster_reputation_count":353.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>As you said, and is indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, Sagemaker will run your container with the following command:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>docker run image serve\n<\/code><\/pre>\n<p>By issuing this command Sagemaker will overwrite any <code>CMD<\/code> that you provide in your container Dockerfile, so you cannot use <code>CMD<\/code> to provide dynamic arguments to your program.<\/p>\n<p>We can think in use the Dockerfile <code>ENTRYPOINT<\/code> to consume some environment variables, but the documentation of AWS dictates that it is preferable use the <code>exec<\/code> form of the <code>ENTRYPOINT<\/code>. Somethink like:<\/p>\n<pre><code>ENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;]\n<\/code><\/pre>\n<p>I think that, for analogy with <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">model training<\/a>, they need this kind of container execution to enable the container to receive termination signals:<\/p>\n<blockquote>\n<p>The exec form of the <code>ENTRYPOINT<\/code> instruction starts the executable directly, not as a child of <code>\/bin\/sh<\/code>. This enables it to receive signals like <code>SIGTERM<\/code> and <code>SIGKILL<\/code> from SageMaker APIs.<\/p>\n<\/blockquote>\n<p>To allow variable expansion, we need to use the <code>ENTRYPOINT<\/code> <code>shell<\/code> form. Imagine:<\/p>\n<pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;, &quot;$ENV_VAR1&quot;]\n<\/code><\/pre>\n<p>If you try to do the same with the <code>exec<\/code> form the variables provided will be treated as a literal and will not be sustituited for their actual values.<\/p>\n<p>Please, see the approved answer of <a href=\"https:\/\/stackoverflow.com\/questions\/37904682\/how-do-i-use-docker-environment-variable-in-entrypoint-array\">this<\/a> stackoverflow question for a great explanation of this subject.<\/p>\n<p>But, one thing you can do is obtain the value of these variables in your R code, similar as when you process <code>commandArgs<\/code>:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>ENV_VAR1 &lt;- Sys.getenv(&quot;ENV_VAR1&quot;)\n<\/code><\/pre>\n<p>To pass environment variables to the container, as indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\"><code>CreateModel<\/code><\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\"><code>CreateTransformJob<\/code><\/a> requests on your container.<\/p>\n<p>You probably will need to include in your Dockerfile <code>ENV<\/code> definitions for every required environment variable on your container, and provide for these definitions default values with <code>ARG<\/code>:<\/p>\n<pre><code>ARG ENV_VAR1_DEFAULT_VALUE=VAL1\nENV_VAR1=$ENV_VAR1_DEFAULT_VALUE\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1599720910636,
        "Solution_link_count":6.0,
        "Solution_readability":17.2,
        "Solution_reading_time":43.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":313.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3013536111,
        "Challenge_answer_count":3,
        "Challenge_body":"So the notebook is  still running, name of the notebook is  **Studio-Notebook:ml.t3.medium** ,please help me i am about to exceed the free tier limit . Please tell me how to fix this so that i wont be billed as i am not even using sagemaker rightnow.",
        "Challenge_closed_time":1679951160328,
        "Challenge_comment_count":1,
        "Challenge_created_time":1679950075455,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user accidentally deleted a Sagemaker notebook without stopping it, and it is still running. The notebook's name is Studio-Notebook:ml.t3.medium, and the user is concerned about exceeding the free tier limit and being billed for a service they are not currently using. They are seeking help to fix the issue and avoid being charged.",
        "Challenge_last_edit_time":1680297146784,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUoN4KlhnCRp-n47teJGVlWQ\/i-deleted-a-sagemaker-notebook-without-stopping-it",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.0,
        "Challenge_reading_time":3.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.3013536111,
        "Challenge_title":"I deleted a sagemaker notebook without stopping it",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":104.0,
        "Challenge_word_count":52,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"When you delete a notebook instance, SageMaker removes the ML compute instance, and deletes the ML storage volume as well as the network interface associated with the notebook instance.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1679951160328,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":2.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":386.3980555556,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n<img width=\"1430\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/5203025\/123860354-63399680-d958-11eb-9dc8-dc0a52d67cc2.png\">\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 109d9284-e234-5086-5da6-4155291361c8\r\n* Version Independent ID: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d\r\n* Content: [azureml.core.ScriptRunConfig class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.scriptrunconfig?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1626388206000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1624997173000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an exception while importing azureml.core after installing azure ml using a conda environment yml. The error message indicates a failure while loading azureml_run_type_providers due to a cryptography version issue. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1534",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":25.6,
        "Challenge_reading_time":13.49,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":386.3980555556,
        "Challenge_title":"Broken link in AML doc to azureml.core.runconfig.MpiConfiguration",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for submitting the issue. I am fixing this broken link now.  The links should be fixed on next SDK release on Aug 3rd",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":1.47,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":626.8732219444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Docker image in AWS ECR and I open my Sagemaker Notebook instance---&gt;go to terminal--&gt;docker run....\nThis is how I start my Docker container.<\/p>\n<p>Now, I want to automate this process(running my docker image on Sagemaker Notebook Instance) instead of typing the docker run commands.<\/p>\n<p>Can I create a cron job on Sagemaker? or Is there any other approach?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1626977403510,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624722520977,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to automate the process of running a Docker image on their Sagemaker Notebook Instance instead of manually typing the Docker run command. They are seeking advice on whether they can create a cron job on Sagemaker or if there is another approach to automate the process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68143997",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":626.3562591667,
        "Challenge_title":"Automate Docker Run command on Sagemaker's Notebook Instance",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":393.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1497621837832,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":230.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>For this you can create an inline Bash shell in your SageMaker notebook as follows. This will take your Docker container, create the image, ECR repo if it does not exist and push the image.<\/p>\n<pre><code>%%sh\n\n# Name of algo -&gt; ECR\nalgorithm_name=your-algo-name\n\ncd container #your directory with dockerfile and other sm components\n\nchmod +x randomForest-Petrol\/train #train file for container\nchmod +x randomForest-Petrol\/serve #serve file for container\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\n# Region, defaults to us-west-2\nregion=$(aws configure get region)\nregion=${region:-us-west-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\n# If the repository doesn't exist in ECR, create it.\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\naws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>I am contributing this on behalf of my employer, AWS. My contribution is licensed under the MIT license. See here for a more detailed explanation\n<a href=\"https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/\" rel=\"nofollow noreferrer\">https:\/\/aws-preview.aka.amazon.com\/tools\/stackoverflow-samples-license\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1626979264576,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":21.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":192.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":7804.7933702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried connecting through Sagemaker notebook to RDS. However, to connect to RDS, my public IP needs to be allowed for security reasons. I can see when I run this command: &quot;curl ifconfig.me&quot; on Sagemaker Notebook instance that public IP keeps changing from time to time.<\/p>\n<p>What is the correct way to connect to RDS with notebook on sagemaker? Do I need to crawl the RDS with AWS Glue and then use Athena on crawled tables and then take the query results from S3 with Sagemaker notebook?<\/p>",
        "Challenge_closed_time":1627578308223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599481052090,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting Sagemaker Jupyter Notebook to RDS due to the changing public IP address. The user is seeking advice on the correct way to connect to RDS with the notebook on Sagemaker. The user is also considering using AWS Glue to crawl RDS and then using Athena on crawled tables to take query results from S3 with Sagemaker notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63777462",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7804.7933702778,
        "Challenge_title":"Sagemaker Jupyter Notebook Cannot Connect to RDS",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":865.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509559597047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.<\/p>\n<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":7.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.8832972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to create a tabular dataset in a notebook with R kernel. The following code works with python kernel but how to do the same thing with R kernel ? Can anyone please help me ? Any help would be appreciated.     <\/p>\n<pre><code>from azureml.core import Workspace, Dataset  \n from azureml.core.dataset import Dataset  \n      \n subscription_id = 'abc'  \n resource_group = 'abcd'  \n workspace_name = 'xyz'  \n      \n workspace = Workspace(subscription_id, resource_group, workspace_name)  \n      \n dataset = Dataset.get_by_name(workspace, name='test')  \n      \n      \n # create tabular dataset from all parquet files in the directory  \n tabular_dataset_3 = Dataset.Tabular.from_parquet_files(path=(datastore,'\/UI\/09-17-2022_125003_UTC\/userdata1.parquet'))  \n<\/code><\/pre>",
        "Challenge_closed_time":1663571695407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663428115537,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help to create a tabular dataset in a notebook with R kernel. They have provided a code that works with python kernel but are unsure how to do the same thing with R kernel.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1012184\/how-to-create-tabular-dataset-in-notebook-with-r-k",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":39.8832972222,
        "Challenge_title":"How to create tabular dataset in notebook with R kernel",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=0274aa41-1ea9-4fdc-8434-c9c13c43307c\">@Ankit19 Gupta  <\/a> The Azure Machine Learning SDK for R was deprecated at the end of 2021 to make way for an improved R training and deployment experience using Azure Machine Learning CLI 2.0    <br \/>\nPlease refer the azureml SDK <a href=\"https:\/\/github.com\/Azure\/azureml-sdk-for-r\">repo<\/a> for more details which was deprecated at the end of last year. You can use CLI to register the dataset using specification file.    <\/p>\n<pre><code>az ml dataset register [--file]  \n                       [--output-metadata-file]  \n                       [--path]  \n                       [--resource-group]  \n                       [--show-template]  \n                       [--skip-validation]  \n                       [--subscription-id]  \n                       [--workspace-name]  \n<\/code><\/pre>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":13.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":106.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.935,
        "Challenge_answer_count":0,
        "Challenge_body":"`TypeError: object of type 'NoneType' has no len()` happens when suggested [VSCode configuration for kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. The error is due to commandline arguments being `None` when running pipeline directly through `run.py`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate_kedro_command\r\n    if len(from_inputs) > 0:\r\nTypeError: object of type 'NoneType' has no len()\r\n```",
        "Challenge_closed_time":1601893558000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1601890192000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the plugin they are using is only compatible with kedro-mlflow version less than 0.8.0, and the `context` package has been moved or refactored, causing an exception to be thrown.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/78",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":22.2,
        "Challenge_reading_time":48.48,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":0.935,
        "Challenge_title":"TypeError in _generate_kedro_command when debugging run in VSCode",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":212,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I see its fixed now so I'm closing this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":0.54,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1362580980910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Akron, OH, USA",
        "Answerer_reputation_count":4013.0,
        "Answerer_view_count":72.0,
        "Challenge_adjusted_solved_time":61.5306563889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Challenge_closed_time":1621009841940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620788331577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to mount an S3 container inside a docker image to be used as a ClearML agent. They have tried three solutions, including using prefabbed configuration in ClearML, mounting using s3fuse, and passing arguments to docker using docker_args and docker_bash_setup_script arguments to Task.create(). However, they have been unable to get any of these solutions to work.",
        "Challenge_last_edit_time":1621022250560,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67496760",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":21.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":61.5306563889,
        "Challenge_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":770.0,
        "Challenge_word_count":202,
        "Platform":"Stack Overflow",
        "Poster_created_time":1362580980910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Akron, OH, USA",
        "Poster_reputation_count":4013.0,
        "Poster_view_count":72.0,
        "Solution_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1621012052112,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":29.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":277.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.6353427778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,    <br \/>\ni deployed a real-time inference pipeline using ML Designer. Training and deploying works fine. But when I'm consuming\/testing my API it doesn't work. Postman gives me Errorcode 500 and &quot;Internal Server Error. Run: Server internal error is from Module Extract N-Gram Features from Text&quot;.    <\/p>\n<p>This is my training pipeline:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42733-image.png?platform=QnA\" alt=\"42733-image.png\" \/>    <\/p>\n<p>I read this: <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/algorithm-module-reference\/extract-n-gram-features-from-text.md#score-or-publish-a-model-that-uses-n-grams\">https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/algorithm-module-reference\/extract-n-gram-features-from-text.md#score-or-publish-a-model-that-uses-n-grams<\/a>    <\/p>\n<p>But I don't know how to achieve this.    <\/p>\n<p>Thanks in advance.    <\/p>",
        "Challenge_closed_time":1606367173387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606307286153,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue while trying to consume\/test their API for a real-time inference pipeline deployed using ML Designer. Postman gives an error code 500 and \"Internal Server Error\" related to the module \"Extract N-Gram Features from Text\". The user is seeking guidance on how to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/175242\/how-to-deploy-ml-designer-pipeline-as-real-time-in",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":18.5,
        "Challenge_reading_time":14.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":16.6353427778,
        "Challenge_title":"How to deploy ML Designer pipeline as real-time inference pipeline using N-Gram",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Once you create a real-time inference pipeline, please make the further modifications below:    <\/p>\n<ol>\n<li> Find the output <strong>Result_vocabulary<\/strong> dataset from <strong>Extract N-Gram Features from Text<\/strong> module.    <br \/>\n <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42884-findmoduleoutputdataset.png?platform=QnA\" alt=\"42884-findmoduleoutputdataset.png\" \/>    <\/li>\n<li> Register the dataset as with a name    <br \/>\n <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42905-registerdataset.png?platform=QnA\" alt=\"42905-registerdataset.png\" \/>    <\/li>\n<li> Update real-time inference pipeline like below:    <br \/>\n <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42865-inferencepipeline.png?platform=QnA\" alt=\"42865-inferencepipeline.png\" \/>    <\/li>\n<\/ol>\n<p>We will improve the documentation accordingly. Thanks for reporting the issue!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":19.7,
        "Solution_reading_time":12.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.7819525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a compute cluster in Azure ML and  I am able to see it in designer but not in the notebook. Can someone let me know the reason for this?<\/p>",
        "Challenge_closed_time":1608293911592,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608276696563,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user created a compute cluster in Azure ML, which is visible in the designer but not in the notebook. They are seeking assistance in understanding why this is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/203139\/compute-clusters-in-azure-ml-notebook",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":4.7819525,
        "Challenge_title":"Compute Clusters in Azure ML Notebook",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":36,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c0af539e-5b19-433b-85a1-2ca81772cd40\">@Srinivasan G  <\/a> This is an expected behavior while using notebooks on Azure ML portal. Compute clusters are used to train models and run experiments using the designer or pipelines. These cannot be used with notebooks.     <br \/>\nNotebooks are integrated to run on an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#notebookvm\">compute instance<\/a> which was previously termed as notebook VM. You can start\/stop the compute instance while using your notebook from Azure ML portal.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":7.71,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":70.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1361339272692,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"NYC",
        "Answerer_reputation_count":6281.0,
        "Answerer_view_count":958.0,
        "Challenge_adjusted_solved_time":3630.1468475,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Challenge_closed_time":1604879360048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591810831397,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between ScriptProcessor and Processor in AWS Sagemaker SDK. Both APIs can achieve the same goals, but ScriptProcessor supports docker command parameter while Processor supports docker entrypoint parameter. The user is asking if there are any other differences between the two.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":21.3,
        "Challenge_reading_time":11.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3630.1468475,
        "Challenge_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":400.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370231111260,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":5295.0,
        "Poster_view_count":455.0,
        "Solution_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.5,
        "Solution_reading_time":4.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":13264.2386313889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?<\/p>",
        "Challenge_closed_time":1523213115336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522449441927,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user wants to schedule a notebook on SageMaker to run every night and is seeking advice on the best way to do so, including the possibility of running a bash script and scheduling a Cron job from SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49582307",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":3.8,
        "Challenge_reading_time":2.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":14.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":212.1315025,
        "Challenge_title":"How to schedule tasks on SageMaker",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":17339.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420001102892,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":173.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=\"https:\/\/aws.amazon.com\/tools\/\" rel=\"noreferrer\">SDK<\/a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html<\/a> )<\/p>\n\n<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:<\/p>\n\n<ul>\n<li><strong>Stopping and Starting Notebook Instances<\/strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"noreferrer\">stop-notebook-instance<\/a> API at the end of the working day (8PM, for example), and the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StartNotebookInstance.html\" rel=\"noreferrer\">start-notebook-instance<\/a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).<\/li>\n<li><strong>Refreshing an ML Model<\/strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"noreferrer\">create-training-job<\/a> API from a scheduled Lambda function (or even from a <a href=\"https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/events\/WhatIsCloudWatchEvents.html\" rel=\"noreferrer\">CloudWatch Event<\/a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateModel.html\" rel=\"noreferrer\">create a refreshed model<\/a> that you can now deploy into an <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"noreferrer\">A\/B testing environment<\/a> .<\/li>\n<\/ul>\n\n<p>----- UPDATE (thanks to @snat2100 comment) -----<\/p>\n\n<ul>\n<li><strong>Creating and Deleting Real-time Endpoints<\/strong> - If your realtime endpoints are not needed 24\/7 (for example, serving internal company users working during workdays and hours), you can also <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpoint.html\" rel=\"noreferrer\">create the endpoints<\/a> in the morning and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DeleteEndpoint.html\" rel=\"noreferrer\">delete them<\/a> at night. <\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1570200701000,
        "Solution_link_count":11.0,
        "Solution_readability":15.5,
        "Solution_reading_time":39.08,
        "Solution_score_count":19.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":307.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1312912826288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":5843.0,
        "Answerer_view_count":153.0,
        "Challenge_adjusted_solved_time":0.9586563889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have deployed a build of mlflow to a pod in my kubernetes cluster. I'm able to port forward to the mlflow ui, and now I'm attempting to test it. To do this, I am running the following test on a jupyter notebook that is running on another pod in the same cluster.<\/p>\n<pre><code>import mlflow\n\nprint(&quot;Setting Tracking Server&quot;)\ntracking_uri = &quot;http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000&quot;\n\nmlflow.set_tracking_uri(tracking_uri)\n\nprint(&quot;Logging Artifact&quot;)\nmlflow.log_artifact('\/home\/test\/mlflow-example-artifact.png')\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p>When I run this though, I get<\/p>\n<pre><code>ConnectionError: HTTPConnectionPool(host='mlflow-tracking-server.default.svc.cluster.local', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get? (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>\n<p>The way I have deployed the mlflow pod is shown below in the yaml and docker:<\/p>\n<p>Yaml:<\/p>\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: &lt;ECR_IMAGE&gt;\n        ports:\n        - containerPort: 5000\n        env:\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;S3_BUCKET&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\n  annotations:\n    service.beta.kubernetes.io\/aws-load-balancer-type: nlb\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  selector:\n    app: mlflow-tracking-server\n  ports:\n    - name: http\n      port: 5000\n      targetPort: http\n<\/code><\/pre>\n<p>While the dockerfile calls a script that executes the mlflow server command: <code>mlflow server --default-artifact-root ${AWS_MLFLOW_BUCKET} --host 0.0.0.0 --port 5000<\/code>, I cannot connect to the service I have created using that mlflow pod.<\/p>\n<p>I have tried using the tracking uri <code>http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000<\/code>, I've tried using the service EXTERNAL-IP:5000, but everything I tried cannot connect and log using the service. Is there anything that I have missed in deploying my mlflow server pod to my kubernetes cluster?<\/p>",
        "Challenge_closed_time":1587498737656,
        "Challenge_comment_count":6,
        "Challenge_created_time":1587495286493,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has deployed an MLflow build to a pod in their Kubernetes cluster and is attempting to test it by running a Jupyter notebook on another pod in the same cluster. However, they are encountering a connection error when trying to set the tracking server using the mlflow library. The user has provided the YAML and Docker files used to deploy the MLflow pod and has tried various tracking URIs but cannot connect to the service. The user is seeking assistance in identifying any missed steps in deploying the MLflow server pod to their Kubernetes cluster.",
        "Challenge_last_edit_time":1599477403816,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61351024",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":14.0,
        "Challenge_reading_time":34.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.9586563889,
        "Challenge_title":"Kubernetes MLflow Service Pod Connection",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":276,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417012835812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":945.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>Your <strong>mlflow-tracking-server<\/strong> service should have <em>ClusterIP<\/em> type, not <em>LoadBalancer<\/em>. <\/p>\n\n<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer<\/em> Service type.<\/p>\n\n<blockquote>\n  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that\u2019s outside of your cluster.\n  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.<\/p>\n  \n  <p>Type values and their behaviors are:<\/p>\n  \n  <ul>\n  <li><p><strong>ClusterIP<\/strong>: Exposes the Service on a cluster-internal IP. Choosing this\n  value makes the Service only reachable from within the cluster. This\n  is the default ServiceType. <\/p><\/li>\n  <li><p><strong>NodePort<\/strong>: Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll > be able to contact the NodePort Service, from outside the cluster, by\n  requesting :. <\/p><\/li>\n  <li><strong>LoadBalancer<\/strong>: Exposes the Service\n  externally using a cloud provider\u2019s load balancer. NodePort and\n  ClusterIP Services, to which the external load balancer routes, are\n  automatically created. <\/li>\n  <li><strong>ExternalName<\/strong>: Maps the Service to the contents\n  of the externalName field (e.g. foo.bar.example.com), by returning a\n  CNAME record with its value. No proxying of any kind is set up.<\/li>\n  <\/ul>\n  \n  <p><a href=\"https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#publishing-services-service-types\" rel=\"nofollow noreferrer\">kubernetes.io<\/a><\/p>\n<\/blockquote>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1587499353943,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":21.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":206.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":142.3802777778,
        "Challenge_answer_count":0,
        "Challenge_body":"This [notebook](https:\/\/github.com\/Microsoft\/Recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to Azure ML SDK preview private index. \r\n\r\n    # Required packages for AzureML execution, history, and data preparation.\r\n    - --extra-index-url https:\/\/azuremlsdktestpypi.azureedge.net\/sdk-release\/Preview\/E7501C02541B433786111FE8E140CAA1\r\n\r\nGiven that Azure ML SDK is now available though regular PyPi as a GA product, and preview versions are unsupported, the extra-index-url should be removed.\r\n",
        "Challenge_closed_time":1548948415000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1548435846000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to open the R package locfit in Azure Machine Learning. They have followed the steps of downloading the package, creating a zip file, and uploading it to AML as a dataset. However, when executing the code, an error message is returned stating that there is no package called 'locfit_package'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/451",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":18.3,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":142.3802777778,
        "Challenge_title":"Remove azureml sdk preview private PyPi index from operationalize notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"hey @rastala thanks for the pointer, we are working on updating that notebook to a newer version of databricks and spark. @jreynolds01 is looking at this based on this issue https:\/\/github.com\/Microsoft\/Recommenders\/issues\/427 yes, this should be fixed with my PR. fixed with #438 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.2,
        "Solution_reading_time":3.51,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":15023.6469805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can someone explain or help me install <a href=\"https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/tree\/master\/python\" rel=\"nofollow noreferrer\">vowpalwabbit<\/a> (I'm interested in the python bindings) on an Amazon linux machine, either EC2 or SageMaker?\nfor some reason it is very hard and I can't find anything about it online...<\/p>\n\n<p>a <code>pip install vowpalwabbit<\/code> returns a <\/p>\n\n<pre><code>Using cached https:\/\/files.pythonhosted.org\/packages\/d1\/5a\/9fcd64fd52ad22e2d1821b2ef871e8783c324b37e2103e7ddefa776c2ed7\/vowpalwabbit-8.8.0.tar.gz\nBuilding wheels for collected packages: vowpalwabbit\n  Building wheel for vowpalwabbit (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0]= '\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-x0j85ac_ --python-tag cp36\n       cwd: \/tmp\/pip-install-tvp1174t\/vowpalwabbit\/\n<\/code><\/pre>\n\n<p>lower in the error I can also see a:<\/p>\n\n<pre><code>CMake Error at \/usr\/lib64\/python3.6\/dist-packages\/cmake\/data\/share\/cmake-3.13\/Modules\/FindBoost.cmake:2100 (message):\n    Unable to find the requested Boost libraries.\n\n    Boost version: 1.53.0\n\n    Boost include path: \/usr\/include\n\n    Could not find the following Boost libraries:\n\n            boost_python3\n\n    Some (but not all) of the required Boost libraries were found.  You may\n    need to install these additional Boost libraries.  Alternatively, set\n    BOOST_LIBRARYDIR to the directory containing Boost libraries or BOOST_ROOT\n    to the location of Boost.\n<\/code><\/pre>",
        "Challenge_closed_time":1634653018680,
        "Challenge_comment_count":1,
        "Challenge_created_time":1580567889550,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking help to install Vowpal Wabbit on an Amazon Linux machine, either EC2 or SageMaker, specifically the python bindings. The user has tried using \"pip install vowpalwabbit\" but encountered an error related to CMake and Boost libraries. The error message suggests that some of the required Boost libraries were not found and may need to be installed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60017893",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":15.4,
        "Challenge_reading_time":25.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15023.6469805556,
        "Challenge_title":"How to install Vowpal Wabbit on Amazon EC2 or SageMaker? (amazon linux)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442180190107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3203.0,
        "Poster_view_count":400.0,
        "Solution_body":"<p>Tested again 1.5 years later, and a <code>pip install vowpalwabbit<\/code> works fine on notebook instance. In training job, adding vowpalwabbit in a <code>requirements.txt<\/code> send to an AWS-managed Scikit learn container (<code>141502667606.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3<\/code>) also installs successfully. Both tested with vowpalwabbit-8.11.0<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":71.3516666667,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nI try to do multi-label classification with \"doc_classification_multilabel.py\". It worked at first. However when it came to `\"Train epoch 1\/1:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 17251\/26668 [10:19:41<4:04:28,  1.56s\/it]\"`, it stopped and report:\r\n\r\n```\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 672, in urlopen\r\n    chunked=chunked,\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 421, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 416, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 1331, in getresponse\r\n    response.begin()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 297, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 266, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n......\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\r\n```\r\n\r\n  I have checked that the Internet connection was ok. So I was confused why this error occured ?\r\n  \r\n\r\n**Error message**\r\nError that was thrown (if available)\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here, like type of downstream task, part of  etc.. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior\r\n\r\n**System:**\r\n - OS: \r\n - GPU\/CPU:\r\n - FARM version:\r\n",
        "Challenge_closed_time":1580393757000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1580136891000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a \"ModuleNotFoundError\" issue while running tests due to the absence of the \"mlflow\" module, which is required by the \"rikai.spark.sql.codegen.mlflow_logger\" module.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/deepset-ai\/FARM\/issues\/217",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.8,
        "Challenge_repo_contributor_count":36.0,
        "Challenge_repo_fork_count":231.0,
        "Challenge_repo_issue_count":844.0,
        "Challenge_repo_star_count":1598.0,
        "Challenge_repo_watch_count":56.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":71.3516666667,
        "Challenge_title":"MLFlowLogger: \"Connection aborted.\" - RemoteDisconnected Error",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":177,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @JiangYanting, \r\n\r\nAre you using our public mlflow server for logging (i.e. `ml_logger = MLFlowLogger(tracking_uri=\"https:\/\/public-mlflow.deepset.ai\/\")\r\n` in doc_classification_multilabel.py)? \r\n\r\nI would assume that your connection to that server was not available when the model tried to log the train_loss at step 17251. \r\n\r\nI see two solutions:\r\n- short term: you can log locally by setting `ml_logger = MLFlowLogger(tracking_uri=\"\")`\r\n- mid term: implementing a fix in FARM, so that we raise only a warning, if the logging doesn't succeed, but let the training continue. Let me know if you are interested in adding a PR for this. Otherwise, we can take care. It would be basically a try \/ catch block here: https:\/\/github.com\/deepset-ai\/FARM\/blob\/master\/farm\/utils.py#L126 @tholor By setting `ml_logger = MLFlowLogger(tracking_uri=\"\")` , it works. Thank you very much ! ^_^",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.4,
        "Solution_reading_time":10.9,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":117.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1314313109232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Technion, Israel",
        "Answerer_reputation_count":18777.0,
        "Answerer_view_count":2000.0,
        "Challenge_adjusted_solved_time":93.5721875,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>upon running <code>pip install trains<\/code> in my virtual env<\/p>\n<p>I am getting<\/p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying\n         cwd: \/tmp\/pip-install-owzh8lnl\/retrying\/\n    Complete output (10 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib\n    copying retrying.py -&gt; build\/lib\n    running install_lib\n    copying build\/lib\/retrying.py -&gt; \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\n    byte-compiling \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/retrying.py to retrying.cpython-38.pyc\n    error: [Errno 13] Permission denied: '\/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/__pycache__\/retrying.cpython-38.pyc.139678407381360'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying Check the logs for full command output.\n<\/code><\/pre>\n<p>I know that <a href=\"https:\/\/stackoverflow.com\/questions\/15028648\/is-it-acceptable-and-safe-to-run-pip-install-under-sudo\">I am not supposed to run under sudo when using a venv<\/a>, so I don't really understand the problem<\/p>\n<p>running for example <code>pip install pandas<\/code> does work.<\/p>\n<p>Python 3.8<\/p>\n<p>How to install trains?<\/p>\n<hr \/>\n<p>EDIT:<\/p>\n<p>running <code>pip install trains --user<\/code> or <code>pip install --user trains<\/code> gives<\/p>\n<pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n<\/code><\/pre>",
        "Challenge_closed_time":1602767930968,
        "Challenge_comment_count":3,
        "Challenge_created_time":1602431071093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install the package 'trains' using pip in their virtual environment. The error message suggests a permission issue while trying to install the package. The user is aware of not using sudo while using a virtual environment. The user has also tried installing another package 'pandas' which was successful. The user is seeking help to install the 'trains' package.",
        "Challenge_last_edit_time":1609353956110,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64305945",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":19.6,
        "Challenge_reading_time":39.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":93.5721875,
        "Challenge_title":"pip install trains fails",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1031.0,
        "Challenge_word_count":187,
        "Platform":"Stack Overflow",
        "Poster_created_time":1314313109232,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Technion, Israel",
        "Poster_reputation_count":18777.0,
        "Poster_view_count":2000.0,
        "Solution_body":"<p>The problem was a permissions problem for the venv.\nAnother problem was trains required some packages that were not yet available with wheels on Python3.8, so I had to downgrade Python to 3.7<\/p>\n<p>That venv was created using Pycharm, and for some reason it was created with low permissions.<\/p>\n<p>There was probably a way to elevate its permissions, but instead I just deleted it and created another one using command line by<\/p>\n<pre><code>python -m virtualenv --python=\/usr\/bin\/python3.7 venv\n<\/code><\/pre>\n<p>And now <code>pip install trains<\/code> worked.<\/p>\n<p>Very annoying.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1604308183096,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":7.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":86.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.0516666667,
        "Challenge_answer_count":1,
        "Challenge_body":"How does one choose or tune the hardware backend of a Sagemaker Studio Notebook?",
        "Challenge_closed_time":1576708404000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576675818000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to select or adjust the hardware configuration of a SageMaker Studio Notebook.",
        "Challenge_last_edit_time":1667955285675,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUp5wKTB0URcCPyBgUcAWMww\/how-to-tune-sagemaker-studio-notebooks-hardware-config",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":1.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":9.0516666667,
        "Challenge_title":"How to tune SageMaker Studio Notebooks hardware config?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":21,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"At the top right of a notebook (near the kernel ) there will be a resource configurations button, you'll be able to choose the instance you want to run the notebook on.\n\nA nice feature of that is that all the instances shares the same EFS mount (SageMaker studio uses EFS for notebook storage) if you save a dataframe to the local disk (EFS) you can change instance type during your work and continue from the place you've been in (Move from a GPU instance to a CPU instance for cost effectiveness \/ back to GPU for performance)",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925589624,
        "Solution_link_count":0.0,
        "Solution_readability":18.5,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":96.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":92.0835313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a pip install -r requirements.txt error but I get this error:<\/p>\n<pre><code>Processing \/tmp\/build\/80754af9\/arviz_1614019183254\/work\nERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '\/tmp\/build\/80754af9\/arviz_1614019183254\/work'\n<\/code><\/pre>\n<p>The requirements are generated from an Azure ML environment<\/p>\n<pre><code>absl-py==0.15.0\nadal==1.2.7\nadlfs==2022.7.0\naiohttp==3.8.1\naiohttp-cors==0.7.0\naiosignal==1.2.0\nalembic==1.8.1\nansiwrap==0.8.4\nantlr4-python3-runtime==4.9.3\nanyio==3.6.1\napplicationinsights==0.11.10\narch==4.14\nargcomplete==2.0.0\nargon2-cffi==21.3.0\nargon2-cffi-bindings==21.2.0\narviz @ file:\/\/\/tmp\/build\/80754af9\/arviz_1614019183254\/work\nastroid==2.11.7\nasttokens==2.0.5\nastunparse==1.6.3\nasync-timeout==4.0.2\nattrs==21.4.0\nauto-tqdm==1.0.2\nautokeras==1.0.16\nautopep8==1.6.0\nazure-appconfiguration==1.1.1\nazure-batch==12.0.0\nazure-cli==2.38.0\nazure-cli-core==2.38.0\nazure-cli-telemetry==1.0.6\nazure-common==1.1.28\nazure-core==1.22.1\nazure-cosmos==3.2.0\nazure-data-tables==12.4.0\nazure-datalake-store==0.0.52\nazure-graphrbac==0.61.1\nazure-identity==1.7.0\nazure-keyvault==1.1.0\nazure-keyvault-administration==4.0.0b3\nazure-keyvault-keys==4.5.1\nazure-loganalytics==0.1.1\nazure-mgmt-advisor==9.0.0\nazure-mgmt-apimanagement==3.0.0\nazure-mgmt-appconfiguration==2.1.0\nazure-mgmt-applicationinsights==1.0.0\nazure-mgmt-authorization==2.0.0\nazure-mgmt-batch==16.1.0\nazure-mgmt-batchai==7.0.0b1\nazure-mgmt-billing==6.0.0\nazure-mgmt-botservice==2.0.0b3\nazure-mgmt-cdn==12.0.0\nazure-mgmt-cognitiveservices==13.2.0\nazure-mgmt-compute==27.1.0\nazure-mgmt-consumption==2.0.0\nazure-mgmt-containerinstance==9.1.0\nazure-mgmt-containerregistry==10.0.0\nazure-mgmt-containerservice==19.1.0\nazure-mgmt-core==1.3.0\nazure-mgmt-cosmosdb==7.0.0b6\nazure-mgmt-databoxedge==1.0.0\nazure-mgmt-datalake-analytics==0.2.1\nazure-mgmt-datalake-nspkg==3.0.1\nazure-mgmt-datalake-store==0.5.0\nazure-mgmt-datamigration==10.0.0\nazure-mgmt-deploymentmanager==0.2.0\nazure-mgmt-devtestlabs==4.0.0\nazure-mgmt-dns==8.0.0\nazure-mgmt-eventgrid==9.0.0\nazure-mgmt-eventhub==10.1.0\nazure-mgmt-extendedlocation==1.0.0b2\nazure-mgmt-hdinsight==9.0.0\nazure-mgmt-imagebuilder==1.0.0\nazure-mgmt-iotcentral==10.0.0b1\nazure-mgmt-iothub==2.2.0\nazure-mgmt-iothubprovisioningservices==1.1.0\nazure-mgmt-keyvault==10.0.0\nazure-mgmt-kusto==0.3.0\nazure-mgmt-loganalytics==13.0.0b4\nazure-mgmt-managedservices==1.0.0\nazure-mgmt-managementgroups==1.0.0\nazure-mgmt-maps==2.0.0\nazure-mgmt-marketplaceordering==1.1.0\nazure-mgmt-media==9.0.0\nazure-mgmt-monitor==3.0.0\nazure-mgmt-msi==6.0.1\nazure-mgmt-netapp==8.0.0\nazure-mgmt-network==20.0.0\nazure-mgmt-nspkg==3.0.2\nazure-mgmt-policyinsights==1.1.0b2\nazure-mgmt-privatedns==1.0.0\nazure-mgmt-rdbms==10.0.0\nazure-mgmt-recoveryservices==2.0.0\nazure-mgmt-recoveryservicesbackup==5.0.0\nazure-mgmt-redhatopenshift==1.1.0\nazure-mgmt-redis==13.1.0\nazure-mgmt-relay==0.1.0\nazure-mgmt-reservations==2.0.0\nazure-mgmt-resource==21.1.0\nazure-mgmt-search==8.0.0\nazure-mgmt-security==2.0.0b1\nazure-mgmt-servicebus==7.1.0\nazure-mgmt-servicefabric==1.0.0\nazure-mgmt-servicefabricmanagedclusters==1.0.0\nazure-mgmt-servicelinker==1.0.0\nazure-mgmt-signalr==1.0.0b2\nazure-mgmt-sql==4.0.0b2\nazure-mgmt-sqlvirtualmachine==1.0.0b3\nazure-mgmt-storage==20.0.0\nazure-mgmt-synapse==2.1.0b2\nazure-mgmt-trafficmanager==1.0.0\nazure-mgmt-web==6.1.0\nazure-multiapi-storage==0.9.0\nazure-nspkg==3.0.2\nazure-storage-blob==12.9.0\nazure-storage-common==1.4.2\nazure-storage-queue==12.3.0\nazure-synapse-accesscontrol==0.5.0\nazure-synapse-artifacts==0.13.0\nazure-synapse-managedprivateendpoints==0.3.0\nazure-synapse-spark==0.2.0\nazureml-accel-models==1.43.0\nazureml-automl-core==1.43.0\nazureml-automl-dnn-nlp==1.43.0.post1\nazureml-automl-runtime==1.43.0\nazureml-cli-common==1.43.0\nazureml-contrib-automl-pipeline-steps==1.43.0\nazureml-contrib-dataset==1.43.0\nazureml-contrib-fairness==1.43.0\nazureml-contrib-notebook==1.43.0\nazureml-contrib-pipeline-steps==1.43.0\nazureml-contrib-reinforcementlearning==1.43.0\nazureml-contrib-server==1.43.0\nazureml-contrib-services==1.43.0\nazureml-core==1.43.0\nazureml-datadrift==1.43.0\nazureml-dataprep==4.0.4\nazureml-dataprep-native==38.0.0\nazureml-dataprep-rslex==2.6.3\nazureml-dataset-runtime==1.43.0.post2\nazureml-defaults==1.43.0\nazureml-explain-model==1.43.0\nazureml-inference-server-http==0.4.13\nazureml-interpret==1.43.0\nazureml-mlflow==1.43.0.post1\nazureml-opendatasets==1.43.0\nazureml-pipeline==1.43.0\nazureml-pipeline-core==1.43.0\nazureml-pipeline-steps==1.43.0\nazureml-responsibleai==1.43.0\nazureml-samples @ file:\/\/\/mnt\/jupyter-azsamples\nazureml-sdk==1.43.0\nazureml-telemetry==1.43.0\nazureml-tensorboard==1.43.0\nazureml-train==1.43.0\nazureml-train-automl==1.43.0\nazureml-train-automl-client==1.43.0\nazureml-train-automl-runtime==1.43.0\nazureml-train-core==1.43.0\nazureml-train-restclients-hyperdrive==1.43.0\nazureml-training-tabular==1.43.0\nazureml-widgets==1.43.0\nBabel==2.10.3\nbackcall==0.2.0\nbackports.functools-lru-cache @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/backports.functools_lru_cache_1618230623929\/work\nbackports.tempfile==1.0\nbackports.weakref==1.0.post1\nbackports.zoneinfo==0.2.1\nbcrypt==3.2.2\nbeautifulsoup4==4.11.1\nbleach==5.0.1\nblessed==1.19.1\nblis==0.4.1\nbokeh==2.4.3\nBoruta==0.3\nboto==2.49.0\nboto3==1.20.19\nbotocore==1.23.19\nBottleneck==1.3.5\ncachetools==5.2.0\ncatalogue==1.0.0\ncertifi @ file:\/\/\/opt\/conda\/conda-bld\/certifi_1655968806487\/work\/certifi\ncffi @ file:\/\/\/opt\/conda\/conda-bld\/cffi_1642701102775\/work\ncftime @ file:\/\/\/tmp\/build\/80754af9\/cftime_1638357901230\/work\nchardet==3.0.4\ncharset-normalizer==2.0.12\nclick==7.1.2\ncloudpickle @ file:\/\/\/Users\/ktietz\/demo\/mc3\/conda-bld\/cloudpickle_1629142150447\/work\ncolorama==0.4.5\ncolorful==0.5.4\ncolorlover==0.3.0\nconfigparser==3.7.4\ncontextlib2==21.6.0\nconvertdate @ file:\/\/\/tmp\/build\/80754af9\/convertdate_1634070773133\/work\ncoremltools @ git+https:\/\/github.com\/apple\/coremltools@13c064ed99ab1da7abea0196e4ddf663ede48aad\ncramjam==2.5.0\ncryptography==37.0.3\ncufflinks==0.17.3\ncycler @ file:\/\/\/tmp\/build\/80754af9\/cycler_1637851556182\/work\ncymem==2.0.6\nCython==0.29.17\ndask==2.30.0\ndask-sql==2022.6.0\ndatabricks-cli==0.17.0\ndataclasses==0.6\ndatasets==1.8.0\ndebugpy==1.6.0\ndecorator==5.1.1\ndefusedxml==0.7.1\nDeprecated==1.2.13\ndice-ml==0.8\ndill==0.3.5.1\ndistlib==0.3.5\ndistributed==2.30.1\ndistro==1.7.0\ndm-tree==0.1.7\ndocker==5.0.3\ndotnetcore2==3.1.23\ndowhy==0.7.1\neconml==0.12.0\nen-core-web-sm @ https:\/\/aka.ms\/automl-resources\/packages\/en_core_web_sm-2.1.0.tar.gz\nencrypted-inference==0.9\nentrypoints==0.4\nenvironments-utils==1.0.4\nephem @ file:\/\/\/tmp\/build\/80754af9\/ephem_1638942191467\/work\nerroranalysis==0.3.2\nexecuting==0.8.3\nfabric==2.7.1\nfairlearn==0.7.0\nfastai==1.0.61\nfastapi==0.79.0\nfastjsonschema==2.15.3\nfastparquet==0.8.1\nfastprogress==1.0.3\nfbprophet @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/fbprophet_1599365532360\/work\nfilelock==3.7.1\nfire==0.4.0\nflake8==4.0.1\nFlask==1.0.3\nFlask-Cors==3.0.10\nflatbuffers==2.0\nfonttools==4.25.0\nfrozenlist==1.3.0\nfsspec==2022.5.0\nfuncy==1.17\nfusepy==3.0.1\nfuture==0.18.2\ngast==0.3.3\ngensim==3.8.3\ngevent==1.3.6\ngitdb==4.0.9\nGitPython==3.1.27\ngoogle-api-core==2.8.2\ngoogle-auth==2.8.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.56.3\ngpustat==1.0.0rc1\ngreenlet==1.1.2\ngrpcio==1.47.0\ngunicorn==20.1.0\ngym==0.21.0\nh11==0.13.0\nh5py==3.7.0\nHeapDict==1.0.1\nhijri-converter @ file:\/\/\/tmp\/build\/80754af9\/hijri-converter_1634064010501\/work\nholidays==0.10.3\nhorovod==0.19.1\nhtmlmin==0.1.12\nhuggingface-hub==0.0.19\nhumanfriendly==10.0\nhumanize==4.2.3\nidna==3.3\nImageHash==4.2.1\nimageio==2.19.5\nimbalanced-learn==0.7.0\nimportlib-metadata==4.11.4\nimportlib-resources==5.8.0\ninference-schema==1.3.0\ninterpret-community==0.26.0\ninterpret-core==0.2.7\ninvoke==1.7.1\nipykernel==6.8.0\nipython==8.4.0\nipython-genutils==0.2.0\nipywidgets==7.7.1\nisodate==0.6.1\nisort==5.10.1\nitsdangerous==1.1.0\njavaproperties==0.5.2\njedi==0.18.0\njeepney==0.8.0\nJinja2==2.11.2\njmespath==0.10.0\njoblib==0.14.1\nJPype1==1.4.0\njson-logging-py==0.2\njson5==0.9.8\njsondiff==2.0.0\njsonpickle==2.2.0\njsonschema==4.6.0\njupyter==1.0.0\njupyter-client==6.1.12\njupyter-console==6.4.4\njupyter-core==4.10.0\njupyter-resource-usage==0.6.1\njupyter-server==1.18.1\njupyter-server-mathjax==0.2.6\njupyter-server-proxy==3.2.1\njupyterlab==3.2.4\njupyterlab-nvdashboard==0.7.0\njupyterlab-pygments==0.2.2\njupyterlab-server==2.15.0\njupyterlab-system-monitor==0.8.0\njupyterlab-topbar==0.6.1\njupyterlab-widgets==1.1.1\njupytext==1.14.0\nKeras==2.3.1\nKeras-Applications==1.0.8\nkeras-nightly==2.5.0.dev2021032900\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.3\nkeras2onnx==1.6.0\nkiwisolver==1.4.3\nkmodes==0.12.1\nknack==0.9.0\nkorean-lunar-calendar @ file:\/\/\/tmp\/build\/80754af9\/korean_lunar_calendar_1634063020401\/work\nkt-legacy==1.0.4\nlazy-object-proxy==1.7.1\nliac-arff==2.5.0\nlightgbm==3.2.1\nllvmlite==0.36.0\nlocket==1.0.0\nLunarCalendar @ file:\/\/\/tmp\/build\/80754af9\/lunarcalendar_1646383991234\/work\nlz4==4.0.1\nMako==1.2.1\nMarkdown==3.4.1\nmarkdown-it-py==2.1.0\nMarkupSafe==2.0.1\nmatplotlib==3.2.1\nmatplotlib-inline==0.1.3\nmccabe==0.6.1\nmdit-py-plugins==0.3.0\nmdurl==0.1.1\nmissingno==0.5.1\nmistune==0.8.4\nml-wrappers==0.2.0\nmlflow==1.27.0\nmlflow-skinny==1.26.1\nmlxtend==0.20.0\nmpmath==1.2.1\nmsal==1.18.0\nmsal-extensions==0.3.1\nmsgpack==1.0.4\nmsrest==0.6.21\nmsrestazure==0.6.4\nmultidict==6.0.2\nmultimethod==1.8\nmultiprocess==0.70.13\nmunkres==1.1.4\nmurmurhash==1.0.7\nnbclassic==0.4.3\nnbclient==0.6.6\nnbconvert==6.5.0\nnbdime==3.1.1\nnbformat==5.2.0\nndg-httpsclient==0.5.1\nnest-asyncio==1.5.5\nnetCDF4==1.5.7\nnetworkx==2.5\nnimbusml==1.8.0\nnltk==3.7\nnotebook==6.4.12\nnotebook-shim==0.1.0\nnumba==0.53.1\nnumexpr==2.8.3\nnumpy==1.19.0\nnvidia-ml-py==11.495.46\nnvidia-ml-py3==7.352.0\noauthlib==3.2.0\nolefile @ file:\/\/\/Users\/ktietz\/demo\/mc3\/conda-bld\/olefile_1629805411829\/work\nonnx==1.7.0\nonnxconverter-common==1.6.0\nonnxmltools==1.4.1\nonnxruntime==1.8.1\nopencensus==0.9.0\nopencensus-context==0.1.2\nopencensus-ext-azure==1.1.4\nopencv-python-headless==4.6.0.66\nopt-einsum==3.3.0\npackaging @ file:\/\/\/tmp\/build\/80754af9\/packaging_1637314298585\/work\npandas==1.1.5\npandas-ml==0.6.1\npandas-profiling==3.2.0\npandocfilters==1.5.0\npapermill==1.2.1\nparamiko==2.11.0\nparso==0.8.3\npartd==1.2.0\npathlib2==2.3.7.post1\npathspec==0.9.0\npatsy==0.5.2\npexpect==4.8.0\nphik==0.12.2\npickleshare==0.7.5\nPillow==6.2.1\npkginfo==1.8.3\nplac==1.1.3\nplatformdirs==2.5.2\nplotly==5.9.0\npluggy==1.0.0\npmdarima==1.7.1\nportalocker==2.4.0\npreshed==3.0.6\nprometheus-client==0.14.1\nprometheus-flask-exporter==0.20.2\nprompt-toolkit==3.0.28\nproperty-cached==1.6.4\nprotobuf==3.20.1\npsutil==5.9.1\npsycopg2 @ file:\/\/\/tmp\/build\/80754af9\/psycopg2_1612298147424\/work\nptyprocess==0.7.0\npure-eval==0.2.2\npy-spy==0.3.12\npy4j==0.10.9.5\npyarrow==3.0.0\npyasn1==0.4.8\npyasn1-modules==0.2.8\npycaret==2.3.10\npycocotools==2.0.2\npycodestyle==2.6.0\npycparser @ file:\/\/\/tmp\/build\/80754af9\/pycparser_1636541352034\/work\npydantic==1.9.1\npydocstyle==6.1.1\npydot==1.4.2\npyflakes==2.2.0\nPyGithub==1.55\nPygments==2.12.0\nPyJWT==2.4.0\npyLDAvis==3.3.1\npylint==2.14.5\nPyMeeus @ file:\/\/\/tmp\/build\/80754af9\/pymeeus_1634069098549\/work\nPyNaCl==1.5.0\npynndescent==0.5.7\npynvml==11.4.1\npyod==1.0.3\npyodbc @ file:\/\/\/tmp\/build\/80754af9\/pyodbc_1647408110185\/work\npyOpenSSL==22.0.0\npyparsing==3.0.9\npyreadline3==3.4.1\npyrsistent==0.18.1\nPySocks==1.7.1\npyspark==3.3.0\npystan @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/pystan_1598392747715\/work\npython-dateutil @ file:\/\/\/tmp\/build\/80754af9\/python-dateutil_1626374649649\/work\npython-jsonrpc-server==0.4.0\npython-language-server==0.35.0\npython-snappy==0.6.1\npytoolconfig==1.2.1\npytorch-transformers==1.0.0\npytz==2019.3\npytz-deprecation-shim==0.1.0.post0\nPyWavelets==1.3.0\nPyYAML==6.0\npyzmq==23.2.0\nqtconsole==5.3.1\nQtPy==2.1.0\nQuantLib==1.27\nquerystring-parser==1.2.4\nrai-core-flask==0.3.0\nraiutils==0.1.0\nraiwidgets==0.19.0\nray==1.13.0\nregex==2022.6.2\nrequests==2.28.0\nrequests-oauthlib==1.3.1\nresponsibleai==0.19.0\nrope==1.2.0\nrsa==4.8\ns3transfer==0.5.2\nsacremoses==0.0.53\nscikit-image==0.19.3\nscikit-learn==0.22.1\nscikit-plot==0.3.7\nscipy==1.5.3\nscp==0.13.6\nscrapbook==0.5.0\nseaborn==0.11.2\nSecretStorage==3.3.2\nsemver==2.13.0\nSend2Trash==1.8.0\nsentencepiece==0.1.96\nseqeval==1.2.2\nsetuptools-git==1.2\nshap==0.39.0\nsimpervisor==0.4\nsix==1.16.0\nskl2onnx==1.4.9\nsklearn-pandas==1.7.0\nslicer==0.0.7\nsmart-open==1.9.0\nsmmap==5.0.0\nsniffio==1.2.0\nsnowballstemmer==2.2.0\nsortedcontainers==2.4.0\nsoupsieve==2.3.2.post1\nspacy==2.2.4\nsparse==0.13.0\nSQLAlchemy==1.4.39\nsqlparse==0.4.2\nsrsly==1.0.5\nsshtunnel==0.1.5\nstack-data==0.3.0\nstarlette==0.19.1\nstatsmodels==0.11.0\nsympy==1.10.1\ntabulate==0.8.10\ntangled-up-in-unicode==0.2.0\ntblib==1.7.0\ntenacity==8.0.1\ntensorboard==2.2.2\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorboardX==2.5.1\ntensorflow==2.2.0\ntensorflow-estimator==2.2.0\ntensorflow-gpu==2.2.0\ntermcolor==1.1.0\nterminado==0.15.0\ntestpath==0.6.0\ntextblob==0.17.1\ntextwrap3==0.9.2\nthinc==7.4.0\nthreadpoolctl @ file:\/\/\/Users\/ktietz\/demo\/mc3\/conda-bld\/threadpoolctl_1629802263681\/work\ntifffile==2022.5.4\ntinycss2==1.1.1\ntokenizers==0.10.3\ntoml==0.10.2\ntomli==2.0.1\ntomlkit==0.11.1\ntoolz==0.11.2\ntorch==1.10.2\ntorch-tb-profiler==0.4.0\ntorchvision==0.9.1\ntornado==6.1\ntqdm @ file:\/\/\/opt\/conda\/conda-bld\/tqdm_1650891076910\/work\ntraitlets==5.3.0\ntransformers==4.5.1\ntyping-extensions==4.2.0\ntzdata==2022.1\ntzlocal==4.2\nujson==5.4.0\numap-learn==0.5.3\nurllib3==1.26.9\nuuid==1.30\nuvicorn==0.18.2\nvirtualenv==20.15.1\nvisions==0.7.4\nwaitress==2.1.1\nwasabi==0.9.1\nwcwidth @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/wcwidth_1600965781394\/work\nwebencodings==0.5.1\nwebsocket-client==1.3.3\nwebsockets==10.3\nWerkzeug==1.0.1\nwidgetsnbextension==3.6.1\nwordcloud==1.8.2.2\nwrapt==1.12.1\nxarray @ file:\/\/\/opt\/conda\/conda-bld\/xarray_1639166117697\/work\nxgboost==1.3.3\nxmltodict==0.13.0\nxxhash==3.0.0\nyapf==0.32.0\nyarl==1.7.2\nyellowbrick==1.4\nzict==2.2.0\nzipp==3.8.0\nzope.event==4.5.0\nzope.interface==5.4.0\n<\/code><\/pre>",
        "Challenge_closed_time":1660622121512,
        "Challenge_comment_count":7,
        "Challenge_created_time":1660310731870,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user encountered an oserror while attempting to install packages from a requirements.txt file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73334915",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":29.5,
        "Challenge_reading_time":196.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":86.4971227778,
        "Challenge_title":"How to install requirements.txt pip error ->Could not install packages due to an OSError: [Errno 2] No such file or directory",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":403.0,
        "Challenge_word_count":671,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<ul>\n<li>The installation process which was taken up is not having the existing location path of requirements.txt. As it is showing the location of<\/li>\n<\/ul>\n<p>\u201c<strong>\/tmp\/build\/80754af9\/arviz_1614019183254\/work<\/strong>\u201d.<\/p>\n<ul>\n<li><p>Mention the path of the requirements.txt file\u2019s path before the file name while trying to run it.<\/p>\n<p><strong><code>pip install -r \/path\/to\/requirements.txt<\/code><\/strong><\/p>\n<\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1660642232583,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":5.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.724685,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I need some help trying to understand why I can't see any GIT options (left panel and top selection drop down menu) in my Azure machine learning JupyterLab.<\/p>\n<p>I did the following steps:<\/p>\n<pre><code> jupyter labextension install @jupyterlab\/git\n pip install --upgrade jupyterlab-git\n jupyter serverextension enable --py jupyterlab_git\n jupyter lab build\n<\/code><\/pre>\n<p>I've restarted my jupyterLab a couple of times, if I check the command:<\/p>\n<pre><code> jupyter labextension list\n<\/code><\/pre>\n<p>I get that @jupyterlab\/git v0,20,0 is enabled and ok.  <br \/>\nWhat am I doing wrong?<\/p>\n<p>Thank you in advance,  <br \/>\nCarla<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/12015-issue1.png?platform=QnA\" alt=\"12015-issue1.png\" \/><\/p>",
        "Challenge_closed_time":1594762360103,
        "Challenge_comment_count":5,
        "Challenge_created_time":1594716551237,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning and jupyterlab git extension as they are unable to see any GIT options in their JupyterLab despite following the necessary steps to install the extension. The user has restarted JupyterLab multiple times and confirmed that the extension is enabled, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/46614\/azure-machine-learning-and-jupyterlab-git-extensio",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":10.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":12.724685,
        "Challenge_title":"Azure Machine Learning and jupyterlab git extension not working",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a>@ramr-msft<\/a> ,<\/p>\n<p>I did the steps mention in the link you gave me (<a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab-git\">https:\/\/github.com\/jupyterlab\/jupyterlab-git<\/a>) but still I can't open the Git extension from the Git tab on the left panel because it still doesn't exists.<\/p>\n<p>You mentioned we can still manage git repositories using the command line. Do you have any useful documentation on this approach?<\/p>\n<p>Once again, thank you in advance.  <br \/>\nCarla<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":6.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":375.8113888889,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThank you for creating such a helpful tool!\r\nThe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the MLFlow experiment artifacts dir. I think the issue is with inconsistent naming for the saved png for certain plot types.\r\nThank you for your help!\r\n<!--\r\n-->\r\n\r\n**To Reproduce**\r\n<!--\r\nAdd a Minimal, Complete, and Verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nIf the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=False, \r\n      silent=True,\r\n      verbose=False,\r\n      # for MLFlow logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = True, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**Expected behavior**\r\n<!--\r\n-->\r\nI expect ALL of the plot types to be logged under the MLFlow artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nHowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\r\nI think the issue is with inconsistent naming of the file. Here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:INFO:Saving 'calibration.png'\r\n2021-10-11 19:03:20,064:INFO:Visual Rendered Successfully\r\n2021-10-11 19:03:20,213:INFO:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:WARNING:[Errno 2] No such file or directory: 'Calibration Curve.png'\r\n```\r\nSo you can see that it is looking for 'Calibration Curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**Versions**\r\nPython 3.8.11\r\n\r\n<!--\r\nPlease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\nPycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1635405096000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1634052175000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a bug while trying to convert an experiment using the aim convert mlflow command with the experiment ID. The process failed with an error message. However, using the experiment name instead of the ID worked successfully. The expected behavior was to convert the experiment by ID. The user's environment included Aim Version 3.6, Python 3.8.1, pip3, and Ubuntu 20.04.3 LTS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1674",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":27.37,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":375.8113888889,
        "Challenge_title":"[BUG] some types plot types are not getting saved to the MLFlow experiment artifacts dir",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":268,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@ejohnson-amerilife Thank you so much for bringing this up. Would you like to submit a PR for this? ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.2,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.0739286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Challenge_closed_time":1661603882123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661517215980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"BAD_REQUEST\" error while trying to fetch runs from search_runs in mlflow while training a machine learning model in an Azure environment. The error message indicates an issue with the input string. The mlflow version being used is 1.28.0 and the user has successfully created and run jobs in Azure studio.",
        "Challenge_last_edit_time":1661625379892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24.0739286111,
        "Challenge_title":"Getting Bad request while searching run in mlflow",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582101477803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":171.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":17.0,
        "Solution_reading_time":16.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1431537216680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2243.0,
        "Answerer_view_count":231.0,
        "Challenge_adjusted_solved_time":5.7313725,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to interact with Azure Batch with python API, in the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azure.batch import BatchServiceClient\nbatch = BatchServiceClient('&lt;mycredential&gt;','https:\/\/&lt;mybatchaccount&gt;.&lt;region&gt;.batch.azure.com')\nnext(batch.job.list())\n<\/code><\/pre>\n<p>This is run in a ML Studio notebook.<\/p>\n<p>However the following error appears: <code>AttributeError: 'str' object has no attribute 'signed_session'<\/code>.<br \/>\nI am taking the url and credentials from my batch console UI:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lc9n4m.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lc9n4m.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As a credential I tried both Primary and Secondary access keys amd &quot;URL&quot; as batch url.<br \/>\nAm I doing anything wrong?<br \/>\nThanks<\/p>",
        "Challenge_closed_time":1622734661128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622712274873,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an 'AttributeError' while trying to interact with Azure Batch using python API in an ML Studio notebook. The error message states that the 'str' object has no attribute 'signed_session'. The user is using the URL and credentials from their batch console UI, but the error persists even after trying both Primary and Secondary access keys and \"URL\" as batch URL.",
        "Challenge_last_edit_time":1622714028187,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67818831",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":12.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.2184041667,
        "Challenge_title":"Azure Batch API rising 'AttributeError' in ML notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436006890427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Paesi Bassi",
        "Poster_reputation_count":959.0,
        "Poster_view_count":204.0,
        "Solution_body":"<p><code>&lt;mycredential&gt;<\/code> should not be your bare auth key string. You need to create a shared auth key object.<\/p>\n<pre><code>credentials = batchauth.SharedKeyCredentials(BATCH_ACCOUNT_NAME, BATCH_ACCOUNT_KEY)\nbatch_client = batch.BatchServiceClient(credentials, base_url=BATCH_ACCOUNT_URL)\n<\/code><\/pre>\n<p>Please see the <a href=\"https:\/\/docs.microsoft.com\/azure\/batch\/tutorial-parallel-python\" rel=\"nofollow noreferrer\">Azure Batch Python tutorial<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.4,
        "Solution_reading_time":6.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1430.2352777778,
        "Challenge_answer_count":0,
        "Challenge_body":"In a fresh conda environment, I get several warnings that halt the script execution:\r\n```\r\n...\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), Requirement.parse('docker<5.0.0'), {'azureml-core'}).\r\n...\r\n```\r\n\r\nMy environment is specified by:\r\n```yaml\r\nname: xxx\r\nchannels:\r\n  - anaconda\r\n  - pytorch-lts\r\ndependencies:\r\n  - python=3.6\r\n  - pandas=1.1.3\r\n  - numpy=1.19.2\r\n  - scikit-learn=0.23.2\r\n  - matplotlib\r\n  - mkl=2020.2\r\n  - pytorch=1.8.1\r\n  - cpuonly=1.0\r\n  - pip\r\n  - pip:\r\n      - azureml-sdk==1.31.0\r\n      - azureml-defaults==1.31.0\r\n      - azure-storage-blob==12.8.1\r\n      - mlflow==1.18.0\r\n      - azureml-mlflow==1.31.0\r\n      - pytorch-lightning==1.3.8\r\n      - onnxruntime==1.8.0\r\n      - docker<5.0.0 # this is the fix needed\r\n```\r\nThe fix is to specify `docker<5.0.0`. Perhaps, there are some wrong deps checks somewhere.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Challenge_closed_time":1630367426000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1625218579000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered a broken link in the AML doc to `azureml.core.runconfig.MpiConfiguration`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1537",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":21.68,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1430.2352777778,
        "Challenge_title":"Bug: Failure while loading azureml_run_type_providers",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":129,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for the report! azureml-sdk==1.13.0 does specify docker<5.0.0, while mlflow==1.18.0 requires 5.0.0. \r\n\r\nI'm going to close this issue as there is no action for azureml-sdk.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.22,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":168.8386111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nUsers get the error \"null is not an object\" when pop-ups are enabled in SWB (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620))\r\nThis error is illegible to the user and causes confusion. Can we make the error message more clear such as:\r\n\"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Diable pop-ups \r\n2. Connect to a workspace\r\n\r\n**Expected behavior**\r\nIf the workspace is unable to open, a more legible error message should be shown, such as \"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v4.3.1 and v5.0.0]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671209314000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1670601495000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the idle Sagemaker Notebook instances are not stopping automatically after the specified time. The autostop.py script used by the `on-start` lifecycle rule of the instance CFN template is not working due to missing packages. The expected behavior is that the idle instances should stop automatically after the specified time. The user is using Release Version v5.0.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1081",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.6,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":168.8386111111,
        "Challenge_title":"[Bug] More descriptive error message for \"null is not an object\" while trying to connect to Sagemaker notebook. ",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @simranmakwana, thank you for creating this issue. There are currently no plans to enrich the error messages in the UI; the recommendation is for you to customize the error messages within your installation of SWB as you see fit. Please reply back if there are any concerns with this approach. Thank you! ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":3.75,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":1229.6615519445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Azure ML. I am having some doubts .Could anyone please clarify my doubts listed below.<\/p>\n\n<ol>\n<li>What is the difference between Azure ML service Azure ML experimentation service.<\/li>\n<li>What is the difference between Azure ML workbench and Azure ML Studio.<\/li>\n<li>I want to use azure ML Experimentation service for building few models and creating web API's. Is it possible to do the same with ML studio. <\/li>\n<li>And also ML Experimentation service requires me to have a docker for windows installed for creating web services.\nCan i create web services without using docker?<\/li>\n<\/ol>",
        "Challenge_closed_time":1525629753687,
        "Challenge_comment_count":1,
        "Challenge_created_time":1521202972100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to Azure ML and has some doubts regarding the differences between Azure ML service and Azure ML experimentation service, as well as between Azure ML workbench and Azure ML Studio. They also want to know if it is possible to use Azure ML Studio for building models and creating web APIs, and if it is necessary to have Docker for Windows installed for creating web services using ML Experimentation service.",
        "Challenge_last_edit_time":1554674738927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49320679",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1229.6615519445,
        "Challenge_title":"Difference between Azure ML and Azure ML experimentation",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":763.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1422010576070,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom House, London, UK",
        "Poster_reputation_count":388.0,
        "Poster_view_count":124.0,
        "Solution_body":"<ol>\n<li><p>The AML Experimentation is one of our many new ML offerings, including data preparation, experimentation, model management, and operationalization. Workbench is a PREVIEW product that provides a GUI for some of these services. But it is just a installer\/wrapper for the CLI that is needed to run. The services are Spark and Python based. Other Python frameworks will work, and you can get a little hacky to call Java\/Scala from Python. Not really sure what you mean by an \"Azure ML Service\", perhaps you are referring to the operationalization service I mentioned above. This will quickly let you create new Python based APIs using Docker containers, and will connect with the model management account to keep track of the linage between your models and your services. All services here are still in preview and may breaking change before GA release. <\/p><\/li>\n<li><p>Azure ML Studio is an older product that is perhaps simpler for some(myself an engineer not a data scientist). It offers a drag and drop experience, but is limited in it's data size to about 10G. This product is GA. <\/p><\/li>\n<li><p>It is, but you need smaller data sizes, and the job flow is not spark based. I use this to do rapid PoC's. Also you will less control over the scalability of your scoring (batch or real time), because it is PaaS, compared to the newer service which is more IaaS. I would recommend looking at the new service instead of studio for most use cases. <\/p><\/li>\n<li><p>The web services are completely based on Docker. Needing docker for experimentation is more about running things locally, which I myself rarely do. But, for the real time service, everything you package is placed into a docker container so it can be deployed to an ACS cluster. <\/p><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":21.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":299.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1473257955532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":145.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":26.3318861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have read similar questions regarding azure app service, but I still can't find an answer. I was trying to deploy a model in azure kubernetes service, but I came across an error when importing cv2 (which is essential to me).<\/p>\n<p>Opencv-python is included in my environment .yaml file:<\/p>\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  # You must list azureml-defaults as a pip dependency\n  - azureml-defaults&gt;=1.0.45\n  - Cython\n  - matplotlib&gt;=3.2.2\n  - numpy&gt;=1.18.5\n  - opencv-python&gt;=4.1.2\n  - pillow\n  - PyYAML&gt;=5.3\n  - scipy&gt;=1.4.1\n  - torch&gt;=1.6.0\n  - torchvision&gt;=0.7.0\n  - tqdm&gt;=4.41.0\nchannels:\n- conda-forge\n<\/code><\/pre>\n<p>I am deploying as follows:<\/p>\n<pre><code>aks_service = Model.deploy(ws,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_config=gpu_aks_config,\n                       deployment_target=aks_target,\n                       name=aks_service_name)\n<\/code><\/pre>\n<p>And I get this error:<\/p>\n<pre><code>    Traceback (most recent call last):\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n    self.load_wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n    return self.load_wsgiapp()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n    __import__(module)\n  File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n    from app import main\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 23, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 8, in &lt;module&gt;\n    import cv2\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/cv2\/__init__.py&quot;, line 5, in &lt;module&gt;\n    from .cv2 import *\nImportError: libGL.so.1: cannot open shared object file: No such file or directory\nWorker exiting (pid: 41)\n<\/code><\/pre>",
        "Challenge_closed_time":1603897093470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603802298680,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when trying to import cv2 while deploying a model in Azure Kubernetes Service. The error message indicates that the libGL.so.1 file cannot be found, which is preventing the import of cv2. The user has already included opencv-python in their environment .yaml file and is deploying the model using Model.deploy() function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64554615",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":41.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":26.3318861111,
        "Challenge_title":"Import cv2 error when deploying in Azure Kubernetes Service - python",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":484.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473257955532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>This might go wrong at some point, but my workaround was installing opencv-python-headless instead of opencv.<\/p>\n<p>In the environment .yaml file, just replace:<\/p>\n<pre><code>- opencv-python&gt;=4.1.2\n<\/code><\/pre>\n<p>with:<\/p>\n<pre><code>- opencv-python-headless\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":3.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1540309037063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":404.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":72.6951308334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Want to have a copy of my Google Colab python notebook in my AWS Sagemaker Jupyter notebook(newbie to AWS Sagemaker)<\/p>\n\n<p>I tried selecting all cells in my Colab notebook and pasting in my sagemaker Jupyter notebook using copy paste icons and via cmd+C and cmd+V<\/p>\n\n<p>Cannot copy paste all selected cells at once between Colab and Sagemaker Jupyter notebooks<\/p>",
        "Challenge_closed_time":1561274854928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561274387650,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to copy or upload a Google Colab notebook onto an AWS Sagemaker instance but is unable to do so. They have tried selecting all cells and using copy-paste icons and keyboard shortcuts but it did not work.",
        "Challenge_last_edit_time":1561274899232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56721821",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.2,
        "Challenge_reading_time":5.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":0.1297994444,
        "Challenge_title":"Can I copy or upload a Google Colab notebook onto a AWS Sagemaker instance?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1621.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540309037063,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":404.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>While doing the drudgery work of copy pasting each cell between the notebooks(my bad), I realized that we could just <strong>download the notebook as .ipynb file on Colab<\/strong> and <strong>upload on the Sagemaker notebook instance using the <code>Upload button<\/code><\/strong>.<a href=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/z5wHt.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1561536601703,
        "Solution_link_count":2.0,
        "Solution_readability":11.4,
        "Solution_reading_time":5.86,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":41.2755555556,
        "Challenge_answer_count":0,
        "Challenge_body":"I was getting an error when azuremllogonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. So, I installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below.\r\n\r\ngrep executes but now the error says \"Dataset with name 'mnist_opendataset' is not found\".\r\n\r\nAny help troubleshooting this error will be appreciated, I am trying to demo this to a customer. next week.\r\n\r\n**TEXT of the OUTPUT when error is encountered:**\r\n\r\n\r\nInstalling amlarc-compute K8s extension was successful.\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nLibrary configuration succeeded\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\n\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nfound compute target: ARC-ml\r\n\"\r\n Training model:\r\n                               \r\n            .....                                             .....\r\n         .........                                           .........\r\n        .........                 (((((((((##                 .........\r\n       .....                      (((((((####                      .....\r\n      ......                      #((########                      ......\r\n     ....... .............        ###########        ............. .......\r\n     ......................       ###########       ......................\r\n    .................*.....       ###########       ....,*.................\r\n    .........*******......       (((((((((((         ......*******.........\r\n         ............          (((((((((((     (.         ............\r\n                            .(((((((((((     (((((\/\r\n                          ((((((((((((     #(((((((##\r\n                        \/\/\/\/(((((((*     ##############\r\n                      \/\/\/\/\/\/(((((.         ,#############.\r\n                   ,**\/\/\/\/\/\/\/((               #############\/\r\n                    *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%##########\r\n                    \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######(\r\n                     \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%####\r\n                     .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#.\r\n\r\n\"\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nWARNING: Command group 'ml job' is experimental and under development. Reference and support levels: https:\/\/aka.ms\/CLI_refstatus\r\nUploading src:   0%|                                                                                                                                | 0.00\/3.08k [00:00<?, ?B\/s]\r\n\r\n**ERROR: Code: UserError**\r\n**Message: Dataset with name 'mnist_opendataset' is not found.**\r\n**You cannot call a method on a null-valued expression.**\r\n**At C:\\Temp\\AzureMLLogonScript.ps1:267 char:4**\r\n**+    $RunId = ($Job | grep '\\\"name\\\":').Split('\\\"')[3]**\r\n**+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**\r\n    **+ CategoryInfo          : InvalidOperation: (:) [], RuntimeException**\r\n    **+ FullyQualifiedErrorId : InvokeMethodOnNull**\r\n\r\n**RunId:**\r\n**Training model, hold tight...**\r\n**ERROR: argument --name\/-n: expected one argument**_****\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n",
        "Challenge_closed_time":1631711922000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1631563330000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered broken links in the introduction section of the 11_exploring_hyperparameters_on_azureml notebook related to object detection. The links to two notebooks, 02_mask_rcnn.ipynb and 03_training_accuracy_vs_speed.ipynb, are not working. The user is working from the master branch of the repo and expects the notebooks to be present or the links to be removed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/758",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":10.4,
        "Challenge_reading_time":56.26,
        "Challenge_repo_contributor_count":62.0,
        "Challenge_repo_fork_count":369.0,
        "Challenge_repo_issue_count":1562.0,
        "Challenge_repo_star_count":527.0,
        "Challenge_repo_watch_count":26.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":41.2755555556,
        "Challenge_title":"error when installing AZURE ML training model piece",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":477,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @arturoqu77 - thanks for reaching out. We tried to repro this issue but couldn't.\r\n\r\nThis [line of code](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/AzureMLLogonScript.ps1#L266) leverages grep to parse the file name. `grep` should have been installed as part of the [bootstrap](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73). If  `grep` wasn't installed, this implies something must have interrupted the install before it got there.\r\n\r\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working. \r\n\r\nAre you seeing Postman installed - this happens [after `grep`](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73)? If not, this is probably what happened.\r\n\r\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above. Hello,\n\nThank you for your reply. I may have logged on before the bootstrap completed, I re-started the deployment to a new RG and seems to be working now.\n\nThanks for the help.\n\nRegards\n\n***@***.***\nArturo Quiroga\nSr. Cloud Solutions Architect (CSA)\nAzure Applications & Infrastructure\n***@***.******@***.***>\n[MSFT_logo_Gray DE sized SIG1.png]\n\n\nFrom: Raki ***@***.***>\nDate: Tuesday, September 14, 2021 at 6:22 PM\nTo: microsoft\/azure_arc ***@***.***>\nCc: Arturo Quiroga ***@***.***>, Mention ***@***.***>\nSubject: Re: [microsoft\/azure_arc] error when installing AZURE ML training model piece (#758)\n\nHi @arturoqu77<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Farturoqu77&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666347896%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=r1kAuKxYlYhONjoSTk83SERggUvNcbP1Hr4vmNh29io%3D&reserved=0> - thanks for reaching out. We tried to repro this issue but couldn't.\n\nThis line of code<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FAzureMLLogonScript.ps1%23L266&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=oAjL%2BfBBF4QXfnwN9gcM9UqEB4OA0ZZrzMuKilatz5A%3D&reserved=0> leverages grep to parse the file name. grep should have been installed as part of the bootstrap<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=V8dzJxj3W5a6IL8T%2BvB0mijBm5Ng4G46bb%2Fcdo2uvz4%3D&reserved=0>. If grep wasn't installed, this implies something must have interrupted the install before it got there.\n\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working.\n\nAre you seeing Postman installed - this happens after grep<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=XvpeFo2T7Kjr4qrIZYKO7eM0khlOddES9O3DGaw1yZ4%3D&reserved=0>? If not, this is probably what happened.\n\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fissues%2F758%23issuecomment-919554382&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ZBGNkrDGFcqvrdWHXy5iEGluQiq2Ph%2BZnfosqC3qTTU%3D&reserved=0>, or unsubscribe<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAHV4QUFA72NR7CEJ3UPS5NLUB7DLDANCNFSM5D6SSBHA&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ctEevpiqzC%2FQnTc6ho2hfr2PVGA%2FqwGJzj1pPUCEylY%3D&reserved=0>.\nTriage notifications on the go with GitHub Mobile for iOS<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=cMCZqYPB6q8c9n%2BgPTk9f3MCQr%2BlV4GsOW9iPFSZtgE%3D&reserved=0> or Android<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666387876%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=6B5T09q%2Bx2Q2rWftui6b32lD1VLrCRMPiLSrTUS7xnI%3D&reserved=0>.\n Great!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_readability":21.6,
        "Solution_reading_time":96.07,
        "Solution_score_count":null,
        "Solution_sentence_count":39.0,
        "Solution_word_count":416.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1290013527427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin",
        "Answerer_reputation_count":15929.0,
        "Answerer_view_count":1202.0,
        "Challenge_adjusted_solved_time":1080.3870858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Challenge_closed_time":1655459795692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651570402183,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a segfault error while trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. The error message indicates that there is no version information available for libncursesw.so.6, which is required by htop. The user has successfully used htop in other images before, but is facing issues with this particular image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72097417",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1080.3870858333,
        "Challenge_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":147.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Poster_created_time":1551431163127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paris, France",
        "Poster_reputation_count":494.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.0,
        "Solution_reading_time":2.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.8041516667,
        "Challenge_answer_count":1,
        "Challenge_body":"When I was building model for analyzing in SageMaker Canvas, it just run for 1h 2m and then I got this notification:\n\nModel building failed: Failed to run Neo compilation or generate explainability report. client_request_id is f76d5bf7-6780-4257-9631-500101632b1e\n\nWhere should I contact the admin for this issue? Thank you so much!",
        "Challenge_closed_time":1644863324414,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644763229468,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while building a model in SageMaker Canvas, which failed after running for 1 hour and 2 minutes. The error message indicated a failure to run Neo compilation or generate an explainability report, and the user is seeking guidance on where to report the issue to the admin.",
        "Challenge_last_edit_time":1668456865503,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUpsGPPvV7SbuofE1fnwC5AA\/where-should-i-report-to-when-encounter-a-trouble-at-sagemaker-canvas",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":5.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":27.8041516667,
        "Challenge_title":"Where should I report to when encounter a trouble at SageMaker Canvas?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":61,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Ailee, you can submit a ticket here and engineering will get back to you: https:\/\/t.corp.amazon.com\/create\/templates\/20b56bae-3fca-4281-9b94-69b6e50128cd. Thanks!",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1644863324414,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":2.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":12.2421333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie in this, and I am facing some problems with the Azure ML workspace. I ran a python code from the terminal, and then I opened another terminal to check the process. I got the following message in the terminal that checked the process:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<p>I appreciate any tips.<\/p>",
        "Challenge_closed_time":1651825622423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651781550743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Azure ML workspace while running a python code from the terminal. They opened another terminal to check the process and received a message stating \"Reconnecting terminal.\" The user is unsure of the meaning of the message and is concerned about losing processing time. They are seeking tips to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72133111",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":8.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.2421333334,
        "Challenge_title":"Azure ML: What means reconnecting terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575137776887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":21.0,
        "Solution_body":"<blockquote>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<\/blockquote>\n<ul>\n<li><code>Reconnecting terminal<\/code> message can appear for multiple reasons like intermittent connectivity issues, unused active terminal sessions, processing of different size\/format of data.<\/li>\n<li>Make sure you close any unused terminal sessions to preserve your compute instance's resources. Idle terminals may impact the performance of compute instances.<\/li>\n<\/ul>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#manage-terminal-sessions\" rel=\"nofollow noreferrer\">Access a compute instance terminal in your workspace<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\" rel=\"nofollow noreferrer\">Optimize data processing with Azure Machine Learning<\/a> and <a href=\"https:\/\/www.youtube.com\/watch?v=kiScfw9i4FM\" rel=\"nofollow noreferrer\">Azure ML: Speed up processing time<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":14.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":114.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.7170177778,
        "Challenge_answer_count":1,
        "Challenge_body":"I have been using Sagemaker Studio Notebook and suddenly it started hanging.\nWhen this happens, the notebook freezes completely. Than I have to wait some seconds (the delay duration is not constant and is common to reach about 30 seconds) and then it just freezes again, making its usage impossible.\nI was using a temporary account provided by Udacity and after trying different approaches to find and solve the problem, I switched to a personal account but the problem persists.\nApproaches I have tried so far:\n- Shutdow and start kernel\n- Restart kernel\n- Restart kernel and clear outputs\n- Log out and Login (from Sagemaker)\n- Log out and Login (from AWS)\n- Change region\n- Trying a different browser (I tried Chrome and Firefox)\n- Trying using other account (personal)\n\nI also checked CloudWatch logs but didn't find anything that seemed unusual.",
        "Challenge_closed_time":1657857148680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657750167416,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues with Sagemaker Studio Notebook as it keeps hanging and freezing, making it impossible to use. The user has tried various approaches to solve the problem, including restarting the kernel, changing the region, and using a different browser and account, but the issue persists. The user also checked CloudWatch logs but found nothing unusual.",
        "Challenge_last_edit_time":1667926330662,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbUkR0L2-Q1CcAHtTbLYJmg\/sagemaker-notebook-keeps-hanging-freezing",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":10.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":29.7170177778,
        "Challenge_title":"Sagemaker Notebook keeps hanging\/freezing",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":140,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The most likely cause of this from my experience is a **(very) large number of active git changes**.\n\nGiven your \"current\" working folder (the one you're navigated to in the folder sidebar menu), the jupyterlab-git integration regularly checks if you're inside a git repository and polls for changes in that repository if so.\n\nWhen this list is very large, I've sometimes seen it cause significant slowdowns in the overall UI because of the way the underlying (open-source) extension works. This has been discussed before for example [in this GitHub issue](https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/667) - which is now marked closed but I've still seen it happening.\n\nFor example, maybe you (like me \ud83d\ude05) forgot to [gitignore](https:\/\/git-scm.com\/docs\/gitignore) a data folder or node_modules and generated thousands of untracked files there: You might see a significant slowdown whenever you're navigated to a folder within the scope of that git repo.\n\nSuggested solution would be:\n\n- Use the folder sidebar to navigate anywhere other than the affected git repository (e.g. to your root folder?), and you should see the slowdown resolve pretty much immediately if this is the underlying cause\n- Now the tricky task of finding and clearing up the problemmatic folder(s) without navigating to them in the folder GUI:\n    - You could use a System Terminal, `cd` to the affected folder and run `git status` to see where the many changes are hiding, if you're not sure already\n    - Add a `.gitignore` file (or modify your existing one) to make git ignore those changes. Because it starts with a dot, `.gitignore` is hidden by default in the JupyterLab file browser anyway. I usually use a system terminal to e.g. `cp myrepo\/.gitignore gitignore.txt` to create a visible copy (somewhere other than the repository folder which you're trying to avoid navigating to!) and then `mv gitignore.txt myrepo\/.gitignore` to overwrite with my edited version\n\nAlternatively (if e.g. it's a folder full of new files that you no longer care about like `node_modules`) you could just slog through the slowness to delete the problemmatic folder in the UI - but of course the problem would return if you re-created them later without `.gitignore`.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1657857148683,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":27.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":346.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1420765480790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":340.0,
        "Answerer_view_count":61.0,
        "Challenge_adjusted_solved_time":2142.9330063889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Challenge_closed_time":1549414531500,
        "Challenge_comment_count":3,
        "Challenge_created_time":1541699972677,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to execute python from a conda environment using dvc run. Despite having a conda environment configured with python 3.6 and dvc installed, dvc is calling the python version of the main installation of conda and not finding the installed libraries.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2142.9330063889,
        "Challenge_title":"How to execute python from conda environment by dvc run",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":351.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420765480790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":340.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":0.69,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1276712500692,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tandil, Buenos Aires Province, Argentina",
        "Answerer_reputation_count":7226.0,
        "Answerer_view_count":637.0,
        "Challenge_adjusted_solved_time":7952.5209027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train my model using Bring your own container technique in sagemaker. My model training runs correctly without any issues locally. But my docker image takes env-file as an input that could change at different runs. But in sagemaker when passing the ECR image, I don't know how to pass this env-file. So instead, inside the <code>train<\/code> script, which is called by the sagemaker, I added <code>export KEY=value<\/code> statements to create my variables. Even that did not expose my variables. Another way I tried it was by executing <code>RUN source file.env<\/code> while building my image. Even this approach did not work out as I got an error <code>\/bin\/sh: 1: source: not found<\/code>.<\/p>\n<p>I could try <code>ENV<\/code> while building my image and that would probably work but this approach won't be flexible as my variables could change at different runs. Is there any way to pass docker run arguments from a sagemaker estimator or notebook? I checked out the documentation but I couldn't find anything.<\/p>",
        "Challenge_closed_time":1662661497360,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634032422110,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in passing arguments to their own docker container in Sagemaker while using the Bring Your Own Container technique. The docker image takes an env-file as input that could change at different runs, but the user is unsure how to pass this env-file while passing the ECR image. The user tried adding export KEY=value statements to create variables, but it did not expose the variables. The user also tried executing RUN source file.env while building the image, but it resulted in an error. The user is looking for a way to pass docker run arguments from a Sagemaker estimator or notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69538469",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7952.5209027778,
        "Challenge_title":"Is there a way to pass arguments to our own docker container in sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":292.0,
        "Challenge_word_count":180,
        "Platform":"Stack Overflow",
        "Poster_created_time":1633433905427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train<\/code> method states that:<\/p>\n<pre><code>environment (dict[str, str]) : Environment variables to be set for\n            use during training job (default: ``None``): \n<\/code><\/pre>\n<p>For reference, the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/5bc3ccf\/src\/sagemaker\/session.py#L569\" rel=\"nofollow noreferrer\">SDK source<\/a>.<\/p>\n<p>Because the SDK is a wrapper on top of <a href=\"https:\/\/pypi.org\/project\/boto3\/\" rel=\"nofollow noreferrer\">Boto3<\/a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=\"https:\/\/aws.amazon.com\/developer\/tools\/#SDKs\" rel=\"nofollow noreferrer\">Amazon Services SDK<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.2,
        "Solution_reading_time":11.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":92.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1450057717008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":21.4211194444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using AzureMLBatchExecution activity in Azure Data Factory, is it secure to pass the DB query as a global parameter to the AzureML web service? <\/p>",
        "Challenge_closed_time":1476431199480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476354083450,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning the security of passing a database query as a global parameter to the AzureML web service when using the AzureMLBatchExecution activity in Azure Data Factory.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40018320",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":21.4211194444,
        "Challenge_title":"Is it secure to pass the DB query to AzureML as a global parameter?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452608563363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>When you talk about \"secure\", are you worried about secure transmission between AML and ADF, or secure storage of your DB query information? For the former, all communication between these two services will be done with HTTPS. For the latter, our production storage has its strict access control. Besides, we only log the count of the global parameters and never the values. I believe it's secure to pass your DB query as a global parameter to the AzureML web service.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":5.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1375058329287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":171.1678108333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Summary<\/strong>: I am trying to define a <code>dvc<\/code> step using <code>dvc-run<\/code> where the command depends on some environment variables (for instance <code>$HOME<\/code>). The problem is that when I'm defining the step on machine A, then the variable is expanded when stored in the <code>.dvc<\/code> file. In this case, it won't be possible to reproduce the step on machine B. Did I hit a limitation of <code>dvc<\/code>? If that's not the case, what's the right approach?<\/p>\n\n<p><strong>More details<\/strong>: I faced the issue when trying to define a step for which the command is a <code>docker run<\/code>. Say that:<\/p>\n\n<ul>\n<li>on machine A <code>myrepo<\/code> is located at <code>\/Users\/user\/myrepo<\/code> and <\/li>\n<li>on machine B it is to be found at <code>\/home\/ubuntu\/myrepo<\/code>. <\/li>\n<\/ul>\n\n<p>Furthermore, assume I have a script <code>myrepo\/script.R<\/code> which processes a data file to be found at <code>myrepo\/data\/mydata.txt<\/code>. Lastly, assume that my step's command is something like: <\/p>\n\n<pre><code>docker run -v $HOME\/myrepo\/:\/prj\/ my_docker_image \/prj\/script.R \/prj\/data\/mydata.txt\n<\/code><\/pre>\n\n<p>If I'm running <code>dvc run -f step.dvc -d ... -d ... [cmd]<\/code> where <code>cmd<\/code> is the <code>docker<\/code> execution above, then in <code>step.dvc<\/code> the environment variable <code>$HOME<\/code> will be expanded. In this case, the step will be broken on machine B.<\/p>",
        "Challenge_closed_time":1563988872483,
        "Challenge_comment_count":6,
        "Challenge_created_time":1563703426437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while defining a dvc step using dvc-run where the command depends on environment variables. When defining the step on one machine, the variable is expanded and stored in the .dvc file, making it impossible to reproduce the step on another machine. The issue arises when the command is a docker run and the environment variable $HOME is expanded in the .dvc file, causing the step to break on another machine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57132106",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":8.6,
        "Challenge_reading_time":18.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":79.2905683334,
        "Challenge_title":"Expanding environment variables in the command part of a dvc run",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300789717227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11410.0,
        "Poster_view_count":1782.0,
        "Solution_body":"<p>From <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Use single quotes ' instead of \" to wrap the command if there are environment variables in it, that you want to be evaluated dynamically. E.g. dvc run -d script.sh '.\/myscript.sh $MYENVVAR'<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1564319630556,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":4.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1554060427012,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2433.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":9.9895388889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to build a new conda environment in our Sagemaker ec2 environment in a terminal session.  Packages in the original copy of the environment were corrupted, and the environment became unusable. The issue couldn't be fixed by removing packages and re-installing or using <code>conda update<\/code>.<\/p>\n<p>I nuked the environment with <code>conda env remove -n python3-cn<\/code> and then attempted to recreate the environment with:<\/p>\n<pre><code>conda env create -p \/home\/ec2-user\/SageMaker\/anaconda3\/envs\/python3-cn --file=${HOME}\/SageMaker\/efs\/.sagemaker\/python3-cn_environment.yml --force\n<\/code><\/pre>\n<p>This environment has been created a number of times in several ec2 instances for individual Sagemaker users.<\/p>\n<p>Conda logs the following:<\/p>\n<pre><code>Collecting package metadata (repodata.json): done\nSolving environment: done\n\nDownloading and Extracting Packages\npytest-arraydiff-0.2 | 14 KB     | ##################################################################################################### | 100% \npartd-0.3.8          | 32 KB     | ##################################################################################################### | 100% \n\n... several progress bar lines later...\n\npsycopg2-2.7.5       | 507 KB    | ##################################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nERROR conda.core.link:_execute(700): An error occurred while installing package 'defaults::mkl-2018.0.3-1'.\nRolling back transaction: done\n\n[Errno 28] No space left on device\n()\n<\/code><\/pre>\n<p>The <code>No space left on device<\/code> error is consistent. I've tried<\/p>\n<ul>\n<li><code>conda clean --all<\/code>, removing the environment, re-building the environment<\/li>\n<li>removing the caches, removing the environment, re-building the environment<\/li>\n<li>removing the environment, shutting down and restarting JuypiterLab (our Sagemaker is configured to create <code>python3-cn<\/code> if the environment doesn't exist when JupyterLab starts)<\/li>\n<\/ul>\n<p>In the first two, I get <code>Errno 28<\/code>.<\/p>\n<p>In the last one, the instance is not created, <code>conda env list<\/code> does not show the <code>python3-cn<\/code>, but I see there is a <code>python3-cn<\/code> directory in the <code>anaconda\/envs\/<\/code> directory. If I do <code>conda activate python3-cn<\/code>, I see the prompt change, but the environment is unusuable. If I try <code>conda update --all<\/code>, I get a notification that one of the package files has been corrupted.<\/p>\n<p>Not really sure what to do here. I'm looking for space hogs, but not really finding anything significant.<\/p>",
        "Challenge_closed_time":1594867069027,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594831106687,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to build a new conda environment in their Sagemaker ec2 environment, but is encountering a consistent \"No space left on device\" error. They have tried various solutions such as removing caches, removing the environment, and restarting JupyterLab, but the error persists. The user is unsure of what to do next and is currently looking for space hogs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62919671",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":35.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":9.9895388889,
        "Challenge_title":"conda env build fails with \"[Errno 28] No space left on device\"",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":4104.0,
        "Challenge_word_count":307,
        "Platform":"Stack Overflow",
        "Poster_created_time":1311046709507,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":56.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Try increasing the ebs volume amount of your notebook ... this blog explains it well: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-notebook-volume-size-up-to-16-tb-with-amazon-sagemaker\/<\/a><\/p>\n<p>Also, best practice is to use lifecycle configuration scripts to build\/add new dependencies ... official docs: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html<\/a><\/p>\n<p>This github page has some great template examples ... for example setting up specific configs like conda, etc: <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":31.7,
        "Solution_reading_time":14.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1641102333407,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":67.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":1.1414036111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I tried importing cv2 from opencv but i got error saying<\/p>\n<blockquote>\n<p>import error: libgthread-2.0.so.0: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n<p>Since Sagemaker Studio Labs doesn't support installation of Ubuntu packages i couldn't use <code>apt-get<\/code> or <code>yum<\/code> to install libglib2.0-0.<\/p>",
        "Challenge_closed_time":1641103613576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641099504523,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to import cv2 in SageMaker Studio Lab due to the missing shared object file libgthread-2.0.so.0. The user was unable to install the required package libglib2.0-0 as Sagemaker Studio Labs does not support the installation of Ubuntu packages.",
        "Challenge_last_edit_time":1641107240987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70553701",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.1414036111,
        "Challenge_title":"Couldn't import cv2 in SageMaker Studio Lab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":534.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589906719620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":63.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>With this line, you can install the glib dependency for Amazon Sagemaker Studio Lab. Just run it on your notebook cell.<\/p>\n<pre><code>! conda install glib=2.51.0 -y\n<\/code><\/pre>\n<p>You also can create another virtual environment for your session that contains glib:<\/p>\n<pre><code>! conda create -n glib-test -c defaults -c conda-forge python=3 glib=2.51.0` -y\n<\/code><\/pre>\n<p>After that maybe you need albumentations to import cv2:<\/p>\n<pre><code>! pip install albumentations\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1655773889523,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":390.0,
        "Answerer_view_count":240.0,
        "Challenge_adjusted_solved_time":6.1325663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Vertex AI notebook that contains a lot of python and jupyter notebook as well as pickled data files in it.  I need to move these files to another notebook.  There isn't a lot of documentation on google's help center.<\/p>\n<p>Has someone had to do this yet?  I'm new to GCP.<\/p>",
        "Challenge_closed_time":1658194851972,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658172774733,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in moving python and jupyter notebook files, along with pickled data files, from one GCP notebook instance to another. They are struggling to find documentation on Google's help center and are seeking advice from someone who has experience with this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73027674",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.2,
        "Challenge_reading_time":4.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.1325663889,
        "Challenge_title":"How does one move python and other types of files from one GCP notebook instance to another?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":114.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455270976143,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Grand Rapids, MI, USA",
        "Poster_reputation_count":1269.0,
        "Poster_view_count":261.0,
        "Solution_body":"<p>Can you try these steps in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/migrate\" rel=\"nofollow noreferrer\">article<\/a>. It says you can copy your files to a <a href=\"https:\/\/cloud.google.com\/storage\/\" rel=\"nofollow noreferrer\">Google Cloud Storage Bucket<\/a> then move it to a new notebook by using gsutil tool.<\/p>\n<p>In your notebook's terminal run this code to copy an object to your Google Cloud storage bucket:<\/p>\n<pre><code>gsutil cp -R \/home\/jupyter\/* gs:\/\/BUCKET_NAMEPATH\n<\/code><\/pre>\n<p>Then open a new terminal to the target notebook and run this command to copy the directory to the notebook:<\/p>\n<pre><code>gsutil cp gs:\/\/BUCKET_NAMEPATH* \/home\/jupyter\/\n<\/code><\/pre>\n<p>Just change the <code>BUCKET_NAMEPATH<\/code> to the name of your cloud storage bucket.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.1,
        "Solution_reading_time":10.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":98.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1357263005087,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Granada Hills, Los Angeles, CA, United States",
        "Answerer_reputation_count":6262.0,
        "Answerer_view_count":428.0,
        "Challenge_adjusted_solved_time":5.9137519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm on a Jupyter notebook using Python3 and trying to plot a tree with code like this:<\/p>\n\n<pre><code>import xgboost as xgb\nfrom xgboost import plot_tree\n\nplot_tree(model, num_trees=4)\n<\/code><\/pre>\n\n<p>On the last line I get:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/xgboost\/plotting.py in to_graphviz(booster, fmap, num_trees, rankdir, yes_color, no_color, **kwargs)\n196         from graphviz import Digraph\n197     except ImportError:\n--&gt; 198         raise ImportError('You must install graphviz to plot tree')\n199 \n200     if not isinstance(booster, (Booster, XGBModel)):\n\nImportError: You must install graphviz to plot tree\n<\/code><\/pre>\n\n<p>How do I install graphviz so I can see the plot_tree?<\/p>",
        "Challenge_closed_time":1552366926347,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552345636840,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to plot a tree using xgboost in Python3 on a Jupyter notebook on AWS Sagemaker. However, they are encountering an error message that says they need to install graphviz to plot the tree. The user is seeking guidance on how to install graphviz to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55112494",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":9.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.9137519445,
        "Challenge_title":"Install graphiz on AWS Sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":572.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1357263005087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Granada Hills, Los Angeles, CA, United States",
        "Poster_reputation_count":6262.0,
        "Poster_view_count":428.0,
        "Solution_body":"<p>I was finally able to learn that Conda has a package which can install it for you. I was able to get it installed by running the command:<\/p>\n\n<pre><code>!conda install python-graphviz --yes\n<\/code><\/pre>\n\n<p>Note the <code>--yes<\/code> is only needed if the installation needs to verify adding\/changing other packages since the Jupyter notebook is not interactive once it is running.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":4.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1384530039387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Ljubljana, Slovenia",
        "Answerer_reputation_count":2470.0,
        "Answerer_view_count":285.0,
        "Challenge_adjusted_solved_time":2971.3031783333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an interesting use case and a problem.<\/p>\n<p>We are leveraging <strong>Sagemaker Notebooks<\/strong> as a development environment for our data science teams. These notebooks are essentially EC2 instances with a (relatively) nice IDE (not as good as Cloud9, though).<\/p>\n<p>In addition, we are running some docker containers on these instances. However, we are forced to use <code>--network=host<\/code> mode, otherwise, the role assigned to the Notebook Instance is not assumed inside the docker container.<\/p>\n<p>On the host (here <code>1234567890<\/code> is our account number, and <code>DataScientist<\/code> is the role attached to the Sagemaker Notebook instance):<\/p>\n<pre><code>$ aws sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Running the same command inside a Docker container with <code>--network=host<\/code> produces the same result:<\/p>\n<pre><code>$ docker run --network host amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>However, it doesn't work with Docker <code>--network=bridge<\/code>:<\/p>\n<pre><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAIMGPPFPT5T6N7BYX6:i-0b2a9080d5ed1cb98&quot;,\n    &quot;Account&quot;: &quot;366152344081&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::366152344081:assumed-role\/BaseNotebookInstanceEc2InstanceRole\/i-0b2a9080d5ed1cb98&quot;\n}\n<\/code><\/pre>\n<p>As you can see, it's a completely different role being assumed. Notice the account number 366152344081 and the role ARN - it's sth internal to AWS.<\/p>\n<p>We would like to keep the default networking option for Docker (bridge) and at the same time be able to assume the correct role (the one attached to SageMaker Notebook instance e.g. <code>DataScientist<\/code> in our case) attached to the host system (Sagemaker Notebook). Are there any hacks (e.g. iptable rules, etc.) to achieve that?<\/p>",
        "Challenge_closed_time":1638951877412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628255185970,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a problem where the role assigned to the Sagemaker Notebook instance is not assumed inside a Docker container when using the default networking option for Docker (bridge). The user is forced to use the <code>--network=host<\/code> mode to assume the correct role. The user is looking for a solution to assume the correct role while keeping the default networking option for Docker.",
        "Challenge_last_edit_time":1639139946700,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68682085",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":30.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2971.3031783333,
        "Challenge_title":"Assume Sagemaker Notebook instance role from Docker container with default network mode",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":195.0,
        "Challenge_word_count":244,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384530039387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ljubljana, Slovenia",
        "Poster_reputation_count":2470.0,
        "Poster_view_count":285.0,
        "Solution_body":"<p>If we look at the networks that were created on a clean Sagemaker Notebook instance, we can notice a user-defined bridge network named <code>sagemaker-local<\/code>:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nf1d5a59a8c9e        bridge              bridge              local\n6142e6764495        host                host                local\n194adfb00f0a        none                null                local\n99de6c086aa8        sagemaker-local     bridge              local\n<\/code><\/pre>\n<p>If we then attach to this custom bridge, we will be able to assume the correct role (the one attached to the Sagemaker Notebook instance itself):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run --network sagemaker-local amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<hr \/>\n<p><strong>UPDATE<\/strong><\/p>\n<p>As of this writing (10 Dec 2021) you don't need to attach to <code>sagemaker-local<\/code> bridge network anymore, the default <code>bridge<\/code> will work as well (note <code>--network bridge<\/code> is implicit in this call):<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>$ docker run amazon\/aws-cli sts get-caller-identity\n{\n    &quot;UserId&quot;: &quot;AROAU2P5VGYMMVxxxxxJ:SageMaker&quot;,\n    &quot;Account&quot;: &quot;1234567890&quot;,\n    &quot;Arn&quot;: &quot;arn:aws:sts::1234567890:assumed-role\/DataScientist\/SageMaker&quot;\n}\n<\/code><\/pre>\n<p>Make sure you restart your SageMaker Notebook instance.<\/p>\n<p>Also, <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/setup.sh\" rel=\"nofollow noreferrer\">here<\/a> I found some manual patching (iptables etc.), but with the update it's already patched.<\/p>\n<p>Thanks to AWS who fixed this :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1639131288447,
        "Solution_link_count":1.0,
        "Solution_readability":21.9,
        "Solution_reading_time":24.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":178.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.6111111111,
        "Challenge_answer_count":0,
        "Challenge_body":"its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in sagemaker\r\nhow long is this going to even take man",
        "Challenge_closed_time":1667627477000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1667438077000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to install libraries in SageMaker Studio Lab that require root privileges, but they are not able to access root user. They have tried running `whoami` and `sudo` commands, but the latter is not found. They have also tried to install `sudo` by following a link, but they are prompted for a password which they do not have. The user is seeking help to gain root access or install libraries that require root access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/155",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":52.6111111111,
        "Challenge_title":"we are experiencing elevated fault rate in start runtime API. The SageMaker Studio Lab team is working to restore the service.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":51,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"up Ah finally btw, I can run the CPU but not GPU today\r\n @saleemmalik10835 Sorry for the long inconvenience of Studio Lab. As you know, the service was back and we confirmed that we can say it to you. I will close this issue because the mentioned problem is solved.\r\n\r\nBut as @aozorahime said, the GPU instance is sometime unavailable because of another instance allocation issue. Of course, we deal with this problem now. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":5.05,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":122.8697222222,
        "Challenge_answer_count":0,
        "Challenge_body":"On chart release v0.13.2 the default value for projectOperator.mlflow.image.tag is set to latest when it should be set to v0.13.2.\r\n\r\nCheck values.yml:\r\n\r\n```yaml\r\nprojectOperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullPolicy: IfNotPresent\r\n  mlflow:\r\n    image:\r\n      repository: konstellation\/mlflow\r\n      tag: latest\r\n      pullPolicy: IfNotPresent\r\n    volume:\r\n      storageClassName: standard\r\n      size: 1Gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullPolicy: IfNotPresent\r\n```",
        "Challenge_closed_time":1635871931000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635429600000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue where all MLflow experiments are visible to any user. To address this, they suggest creating a separate instance of MLflow for each project. They propose several steps to implement this solution, including creating a project operator, updating the KDL APP API, and adding the operator to the KDL server helm chart. They also plan to publish the project-operator in Docker Hub using GitHub workflows.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/623",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":15.5,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":122.8697222222,
        "Challenge_title":"Project operator mlflow image tag is set to \"latest\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":25.7031369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I attempt to Upgrade <strong>Pandas<\/strong> above version <strong>1.1.5<\/strong> on my <strong>AWS Sagemaker<\/strong> provided <strong>JupyterLab<\/strong> notebook I receive the error <strong>No Matching Distribution Found<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\n!{sys.executable} -m pip install --pre --upgrade pandas==1.3.5\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>ERROR: Could not find a version that satisfies the requirement pandas==1.3.5 (from versions: 0.1, 0.2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.25.2, 0.25.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5)\nERROR: No matching distribution found for pandas==1.3.5\n<\/code><\/pre>\n<h2>Background<\/h2>\n<p>I created a Notebook instance from the AWS Console via <strong>AWS Sagemaker -&gt; Notebook instances -&gt; Create Notebook instance<\/strong>.<\/p>\n<p>I then selected the Kernel <strong>conda_Python3<\/strong>.<\/p>\n<p>I use <strong>sys.executable<\/strong> to show the Kernel's Python, Pip and Pandas version.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -version\nPython 3.6.13\n\n!{sys.executable} -m pip show pip\nName: pip\nVersion: 21.3.1\nSummary: The PyPA recommended tool for installing Python packages.\nHome-page: https:\/\/pip.pypa.io\/\nAuthor: The pip developers\nAuthor-email: distutils-sig@python.org\nLicense: MIT\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: \nRequired-by: \n\n!{sys.executable} -m pip show pandas\nName: pandas\nVersion: 1.1.5\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: https:\/\/pandas.pydata.org\nAuthor: \nAuthor-email: \nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: numpy, python-dateutil, pytz\nRequired-by: autovizwidget, awswrangler, hdijupyterutils, odo, sagemaker, seaborn, shap, smclarify, sparkmagic, statsmodels\n<\/code><\/pre>\n<p>I cannot upgrade <strong>Pandas<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -m pip install --pre --upgrade pandas\nRequirement already satisfied: pandas in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (1.1.5)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy&gt;=1.15.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (1.18.5)\nRequirement already satisfied: six&gt;=1.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0)\n<\/code><\/pre>",
        "Challenge_closed_time":1654275686200,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654183154907,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to upgrade Pandas above version 1.1.5 on their AWS Sagemaker provided JupyterLab notebook. The error message \"No Matching Distribution Found\" is displayed. The user has created a Notebook instance from the AWS Console via AWS Sagemaker and selected the Kernel conda_Python3. The user is unable to upgrade Pandas and has provided the Python, Pip, and Pandas version details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72478572",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":43.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":25.7031369444,
        "Challenge_title":"Amazon Sagemaker JupiterLab Notebook - No matching distribution found for Pandas",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":315,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504724308867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington D.C., DC, United States",
        "Poster_reputation_count":97.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>see this - <a href=\"https:\/\/stackoverflow.com\/questions\/68750375\/no-matching-distribution-found-for-pandas-1-3-1\">No matching distribution found for pandas==1.3.1<\/a><\/p>\n<p>The latest version to support python 3.6 is 1.1.5.<\/p>\n<p>You can create a new conda environment with python version &gt;= 3.7 in your existing notebook, or move to notebooks with Amazon Linux 2 (see <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebook-instance-now-supports-amazon-linux-2\/\" rel=\"nofollow noreferrer\">blog post<\/a>). In the AL2 notebooks, <code>conda_python3<\/code> kernels come with Python 3.8.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":8.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1415906440767,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kyiv",
        "Answerer_reputation_count":12948.0,
        "Answerer_view_count":363.0,
        "Challenge_adjusted_solved_time":720.5025569444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use the following script to automate upgrading of my libraries.<\/p>\n\n<p><strong>My script (Start Notebook):<\/strong><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\n\necho 'Before:'\necho $PATH\n\nexport PATH=\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin:\/home\/ec2-user\/anaconda3\/bin\/:\/usr\/libexec\/gcc\/x86_64-amazon-linux\/4.8.5:\/usr\/local\/cuda\/bin:\/usr\/local\/bin:\/opt\/aws\/bin:\/usr\/local\/mpi\/bin:\/usr\/local\/bin:\/bin:\/usr\/bin:\/usr\/local\/sbin:\/usr\/sbin:\/sbin:\/opt\/aws\/bin:$PATH\n\necho 'After:'\necho $PATH\n\necho `pwd`\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate tensorflow_p36\n\npip install pandas --upgrade\n\npip install tensorflow-gpu --upgrade\n<\/code><\/pre>\n\n<p><strong>Error:<\/strong><\/p>\n\n<p>I get the following error, how can I point to the correct location(\/home\/ec2-user) of keras instead of <strong><em>\/root<\/em><\/strong><\/p>\n\n<pre><code>cp: cannot stat \u2018\/root\/.keras\/keras_tensorflow.json\u2019: No such file or directory \n<\/code><\/pre>\n\n<p><strong>Full Logs:<\/strong><\/p>\n\n<pre><code>Before:\n\/sbin:\/bin:\/usr\/sbin:\/usr\/bin\nAfter:\n\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin:\/home\/ec2-user\/anaconda3\/bin\/:\/usr\/libexec\/gcc\/x86_64-amazon-linux\/4.8.5:\/usr\/local\/cuda\/bin:\/usr\/local\/bin:\/opt\/aws\/bin:\/usr\/local\/mpi\/bin:\/usr\/local\/bin:\/bin:\/usr\/bin:\/usr\/local\/sbin:\/usr\/sbin:\/sbin:\/opt\/aws\/bin:\/sbin:\/bin:\/usr\/sbin:\/usr\/bin\n\/home\/ec2-user\ncp: cannot stat \u2018\/root\/.keras\/keras_tensorflow.json\u2019: No such file or directory\n<\/code><\/pre>\n\n<p><strong>Without lifecycle configuration:<\/strong><\/p>\n\n<p>All the commands in the above script works.<\/p>\n\n<p>Actual keras.json file is exising, under \/home\/ec2-user when I remove the lifecycle configuration with the following value.<\/p>\n\n<pre><code>sh-4.2$ cat .keras\/keras.json\n{\n    \"backend\": \"tensorflow\"\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1542128119243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1542016525347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while upgrading ML libraries using AWS Sagemaker Notebook's Lifecycle configurations. The user's script is failing with an error message \"cp: cannot stat \u2018\/root\/.keras\/keras_tensorflow.json\u2019: No such file or directory\". The user is looking for a solution to point to the correct location of keras instead of \/root. The script works fine without lifecycle configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53259647",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":22.1,
        "Challenge_reading_time":24.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":30.9983044445,
        "Challenge_title":"Issues in upgrading ML libraries using AWS Sagemaker Notebook's Lifecycle configurations",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":856.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1270801176840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Finland",
        "Poster_reputation_count":2406.0,
        "Poster_view_count":117.0,
        "Solution_body":"<p>Looks like <code>pip install<\/code> in your case executed \"outside\" of virtualenv  <\/p>\n\n<p>try to change from:  <\/p>\n\n<p><code>source \/home\/ec2-user\/anaconda3\/bin\/activate tensorflow_p36<\/code>  <\/p>\n\n<p>to: <\/p>\n\n<p><code>source \/home\/ec2-user\/anaconda3\/bin\/activate tensorflow_p36 &amp;&amp; pip install pandas tensorflow-gpu --upgrade<\/code>  <\/p>\n\n<p>and delete redundant lines<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1544610334552,
        "Solution_link_count":0.0,
        "Solution_readability":23.6,
        "Solution_reading_time":5.05,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":37.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.6852822222,
        "Challenge_answer_count":2,
        "Challenge_body":"I am trying to deploy the SageMaker Inference Endpoint by extending the Pre-built image. However, it failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\"\n\nMy Dockerfile\n\n```\nARG REGION=us-west-2\n\n# SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n\nRUN apt-get update\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, use the \/code subdirectory to store your user code.\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\n# Defines inference.py as script entrypoint \nENV SAGEMAKER_PROGRAM inference.py\n```\n\nCloudWatch Log From \/aws\/sagemaker\/Endpoints\/mytestEndpoint\n```\n2022-09-30T04:47:09.178-07:00\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module>\n    subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nTraceback (most recent call last): File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module> subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with Popen(*popenargs, **kwargs) as p: File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename)\n\n2022-09-30T04:47:13.409-07:00\nFileNotFoundError: [Errno 2] No such file or directory: 'serve'\n```",
        "Challenge_closed_time":1664616480772,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664542013756,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while deploying the SageMaker Inference Endpoint by extending the pre-built image. The deployment failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\". The CloudWatch log shows a traceback indicating that the error occurred in the docker-entrypoint.py file.",
        "Challenge_last_edit_time":1668616022040,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUR-uTDaDsQBGjMoAUcsi2sQ\/aws-sagemaker-extending-pre-built-container-deploy-endpoint-failed-no-such-file-or-directory-serve",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.8,
        "Challenge_reading_time":31.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":20.6852822222,
        "Challenge_title":"AWS SageMaker - Extending Pre-built Container, Deploy Endpoint Failed. No such file or directory: 'serve'\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":275.0,
        "Challenge_word_count":222,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Should use the Sagemaker image \n```\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-sagemaker\n```\ninstead of ec2\n\n```\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n```",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1664616495636,
        "Solution_link_count":0.0,
        "Solution_readability":35.9,
        "Solution_reading_time":3.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":195.7456172222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie to aws sagemaker.\nI am trying to setup a model in aws sagemaker using keras with GPU support.\nThe docker base image used to infer the model is given below<\/p>\n\n<pre><code>FROM tensorflow\/tensorflow:1.10.0-gpu-py3\n\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx curl\n...\n<\/code><\/pre>\n\n<p>This is the keras code I'm using to check if a GPU is identified by keras in flask.<\/p>\n\n<pre><code>import keras\n@app.route('\/ping', methods=['GET'])\ndef ping():\n\n    keras.backend.tensorflow_backend._get_available_gpus()\n\n    return flask.Response(response='\\n', status=200,mimetype='application\/json')\n<\/code><\/pre>\n\n<p>When I spin up a notebook instance in a sagemaker using the GPU the keras code shows available GPUs.\nSo, in order to access GPU in the inference phase(model) do I need to install any additional libraries in the docker file apart from the tensorflow GPU base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1545210167392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544505483170,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is a newbie to AWS Sagemaker and is trying to set up a model using Keras with GPU support. They are using a Docker base image with TensorFlow 1.10.0 GPU and are trying to check if a GPU is identified by Keras in Flask. The user is asking if they need to install any additional libraries in the Docker file to access the GPU in the inference phase.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53717800",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":12.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":195.7456172222,
        "Challenge_title":"Configuring GPU in aws sagemaker with keras and tensorflow as backend",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1958.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544503799112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":57.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>You shouldn't need to install anything else. Keras relies on TensorFlow for GPU detection and configuration.<\/p>\n\n<p>The only thing worth noting is how to use multiple GPUs during training. I'd recommend passing 'gpu_count' as an hyper parameter, and setting things up like so:<\/p>\n\n<pre><code>from keras.utils import multi_gpu_model\nmodel = Sequential()\nmodel.add(...)\n...\nif gpu_count &gt; 1:\n    model = multi_gpu_model(model, gpus=gpu_count)\nmodel.compile(...)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":6.08,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.6105555556,
        "Challenge_answer_count":0,
        "Challenge_body":"### Contact Details [Optional]\n\n_No response_\n\n### System Information\n\nZenml == 0.10.0\n\n### What happened?\n\nZenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### Reproduction steps\n\n1. Create a SageMaker pipeline.\r\n2. Create a s3 artifact store.\r\n3. Run the pipeline\r\n\n\n### Relevant log output\n\n```shell\nCreating run for pipeline: mnist_pipeline\r\nCache enabled for pipeline mnist_pipeline\r\nUsing stack sagemaker_stack to run pipeline mnist_pipeline...\r\nStep importer has started.\r\nUsing cached version of importer.\r\nStep importer has finished in 0.045s.\r\nStep trainer has started.\r\nINFO:botocore.credentials:Found credentials in shared credentials file: ~\/.aws\/credentials\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:752 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    749 \u2502   \u2502   \u2502   \u2502   \u2502   params[\"CreateBucketConfiguration\"] = {          \u2502\r\n\u2502    750 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \"LocationConstraint\": region_name            \u2502\r\n\u2502    751 \u2502   \u2502   \u2502   \u2502   \u2502   }                                                \u2502\r\n\u2502 >  752 \u2502   \u2502   \u2502   \u2502   await self._call_s3(\"create_bucket\", **params)       \u2502\r\n\u2502    753 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(\"\")                            \u2502\r\n\u2502    754 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(bucket)                        \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:302 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    299 \u2502   \u2502   \u2502   except Exception as e:                                   \u2502\r\n\u2502    300 \u2502   \u2502   \u2502   \u2502   err = e                                              \u2502\r\n\u2502    301 \u2502   \u2502   err = translate_boto_error(err)                              \u2502\r\n\u2502 >  302 \u2502   \u2502   raise err                                                    \u2502\r\n\u2502    303 \u2502                                                                    \u2502\r\n\u2502    304 \u2502   call_s3 = sync_wrapper(_call_s3)                                 \u2502\r\n\u2502    305                                                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:282 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    279 \u2502   \u2502   additional_kwargs = self._get_s3_method_kwargs(method, *akwa \u2502\r\n\u2502    280 \u2502   \u2502   for i in range(self.retries):                                \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   try:                                                     \u2502\r\n\u2502 >  282 \u2502   \u2502   \u2502   \u2502   out = await method(**additional_kwargs)              \u2502\r\n\u2502    283 \u2502   \u2502   \u2502   \u2502   return out                                           \u2502\r\n\u2502    284 \u2502   \u2502   \u2502   except S3_RETRYABLE_ERRORS as e:                         \u2502\r\n\u2502    285 \u2502   \u2502   \u2502   \u2502   logger.debug(\"Retryable error: %s\", e)               \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:198 in _make_api_call                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   195 \u2502   \u2502   \u2502   'has_streaming_input': operation_model.has_streaming_inpu \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   'auth_type': operation_model.auth_type,                   \u2502\r\n\u2502   197 \u2502   \u2502   }                                                             \u2502\r\n\u2502 > 198 \u2502   \u2502   request_dict = await self._convert_to_request_dict(           \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   api_params, operation_model, context=request_context)     \u2502\r\n\u2502   200 \u2502   \u2502   resolve_checksum_context(request_dict, operation_model, api_p \u2502\r\n\u2502   201                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:246 in _convert_to_request_dict                            \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   243 \u2502                                                                     \u2502\r\n\u2502   244 \u2502   async def _convert_to_request_dict(self, api_params, operation_mo \u2502\r\n\u2502   245 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      context=None):                 \u2502\r\n\u2502 > 246 \u2502   \u2502   api_params = await self._emit_api_params(                     \u2502\r\n\u2502   247 \u2502   \u2502   \u2502   api_params, operation_model, context)                     \u2502\r\n\u2502   248 \u2502   \u2502   request_dict = self._serializer.serialize_to_request(         \u2502\r\n\u2502   249 \u2502   \u2502   \u2502   api_params, operation_model)                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:275 in _emit_api_params                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502                                                                 \u2502\r\n\u2502   273 \u2502   \u2502   event_name = (                                                \u2502\r\n\u2502   274 \u2502   \u2502   \u2502   'before-parameter-build.{service_id}.{operation_name}')   \u2502\r\n\u2502 > 275 \u2502   \u2502   await self.meta.events.emit(                                  \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   event_name.format(                                        \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   service_id=service_id,                                \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   \u2502   operation_name=operation_name),                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\hooks.py:29 in _emit                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   26 \u2502   \u2502   \u2502   if asyncio.iscoroutinefunction(handler):                   \u2502\r\n\u2502   27 \u2502   \u2502   \u2502   \u2502   response = await handler(**kwargs)                     \u2502\r\n\u2502   28 \u2502   \u2502   \u2502   else:                                                      \u2502\r\n\u2502 > 29 \u2502   \u2502   \u2502   \u2502   response = handler(**kwargs)                           \u2502\r\n\u2502   30 \u2502   \u2502   \u2502                                                              \u2502\r\n\u2502   31 \u2502   \u2502   \u2502   responses.append((handler, response))                      \u2502\r\n\u2502   32 \u2502   \u2502   \u2502   if stop_on_response and response is not None:              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\botoc \u2502\r\n\u2502 ore\\handlers.py:243 in validate_bucket_name                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    240 \u2502   \u2502   \u2502   'Invalid bucket name \"%s\": Bucket name must match '      \u2502\r\n\u2502    241 \u2502   \u2502   \u2502   'the regex \"%s\" or be an ARN matching the regex \"%s\"' %  \u2502\r\n\u2502    242 \u2502   \u2502   \u2502   \u2502   bucket, VALID_BUCKET.pattern, VALID_S3_ARN.pattern)) \u2502\r\n\u2502 >  243 \u2502   \u2502   raise ParamValidationError(report=error_msg)                 \u2502\r\n\u2502    244                                                                      \u2502\r\n\u2502    245                                                                      \u2502\r\n\u2502    246 def sse_md5(params, **kwargs):                                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nParamValidationError: Parameter validation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\run-sagemaker.py:87 in       \u2502\r\n\u2502 <module>                                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   84 \u2502   \u2502   trainer=trainer(),                                             \u2502\r\n\u2502   85 \u2502   \u2502   evaluator=evaluator(),                                         \u2502\r\n\u2502   86 \u2502   )                                                                  \u2502\r\n\u2502 > 87 \u2502   pipeline.run()                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\pipelines\\base_pipeline.py:489 in run                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   486 \u2502   \u2502   self._reset_step_flags()                                      \u2502\r\n\u2502   487 \u2502   \u2502   self.validate_stack(stack)                                    \u2502\r\n\u2502   488 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 489 \u2502   \u2502   return stack.deploy_pipeline(                                 \u2502\r\n\u2502   490 \u2502   \u2502   \u2502   self, runtime_configuration=runtime_configuration         \u2502\r\n\u2502   491 \u2502   \u2502   )                                                             \u2502\r\n\u2502   492                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\stack\\stack.py:595 in deploy_pipeline                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   pipeline=pipeline, runtime_configuration=runtime_configur \u2502\r\n\u2502   593 \u2502   \u2502   )                                                             \u2502\r\n\u2502   594 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 595 \u2502   \u2502   return_value = self.orchestrator.run(                         \u2502\r\n\u2502   596 \u2502   \u2502   \u2502   pipeline, stack=self, runtime_configuration=runtime_confi \u2502\r\n\u2502   597 \u2502   \u2502   )                                                             \u2502\r\n\u2502   598                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:212 in run                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   pipeline=pipeline, pb2_pipeline=pb2_pipeline              \u2502\r\n\u2502   210 \u2502   \u2502   )                                                             \u2502\r\n\u2502   211 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 212 \u2502   \u2502   result = self.prepare_or_run_pipeline(                        \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   sorted_steps=sorted_steps,                                \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   pipeline=pipeline,                                        \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                                \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\local\\local_orchestrator.py:68 in prepare_or_run_pipeline    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   65 \u2502   \u2502                                                                  \u2502\r\n\u2502   66 \u2502   \u2502   # Run each step                                                \u2502\r\n\u2502   67 \u2502   \u2502   for step in sorted_steps:                                      \u2502\r\n\u2502 > 68 \u2502   \u2502   \u2502   self.run_step(                                             \u2502\r\n\u2502   69 \u2502   \u2502   \u2502   \u2502   step=step,                                             \u2502\r\n\u2502   70 \u2502   \u2502   \u2502   \u2502   run_name=runtime_configuration.run_name,               \u2502\r\n\u2502   71 \u2502   \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:316 in run_step                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   313 \u2502   \u2502   # This is where the step actually gets executed using the     \u2502\r\n\u2502   314 \u2502   \u2502   # component_launcher                                          \u2502\r\n\u2502   315 \u2502   \u2502   repo.active_stack.prepare_step_run()                          \u2502\r\n\u2502 > 316 \u2502   \u2502   execution_info = self._execute_step(component_launcher)       \u2502\r\n\u2502   317 \u2502   \u2502   repo.active_stack.cleanup_step_run()                          \u2502\r\n\u2502   318 \u2502   \u2502                                                                 \u2502\r\n\u2502   319 \u2502   \u2502   return execution_info                                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:340 in _execute_step                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   337 \u2502   \u2502   start_time = time.time()                                      \u2502\r\n\u2502   338 \u2502   \u2502   logger.info(f\"Step `{pipeline_step_name}` has started.\")      \u2502\r\n\u2502   339 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 > 340 \u2502   \u2502   \u2502   execution_info = tfx_launcher.launch()                    \u2502\r\n\u2502   341 \u2502   \u2502   \u2502   if execution_info and get_cache_status(execution_info):   \u2502\r\n\u2502   342 \u2502   \u2502   \u2502   \u2502   logger.info(f\"Using cached version of `{pipeline_step \u2502\r\n\u2502   343 \u2502   \u2502   except RuntimeError as e:                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:528 in launch                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   525 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self._pipeline_runtime_spe \u2502\r\n\u2502   526 \u2502                                                                     \u2502\r\n\u2502   527 \u2502   # Runs as a normal node.                                          \u2502\r\n\u2502 > 528 \u2502   execution_preparation_result = self._prepare_execution()          \u2502\r\n\u2502   529 \u2502   (execution_info, contexts,                                        \u2502\r\n\u2502   530 \u2502    is_execution_needed) = (execution_preparation_result.execution_i \u2502\r\n\u2502   531 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    execution_preparation_result.contexts,   \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:388 in _prepare_execution                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   385 \u2502   \u2502   \u2502     output_dict=output_artifacts,                           \u2502\r\n\u2502   386 \u2502   \u2502   \u2502     exec_properties=exec_properties,                        \u2502\r\n\u2502   387 \u2502   \u2502   \u2502     execution_output_uri=(                                  \u2502\r\n\u2502 > 388 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_executor_output_uri(execu \u2502\r\n\u2502   389 \u2502   \u2502   \u2502     stateful_working_dir=(                                  \u2502\r\n\u2502   390 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_stateful_working_director \u2502\r\n\u2502   391 \u2502   \u2502   \u2502     tmp_dir=self._output_resolver.make_tmp_dir(execution.id \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\outputs_utils.py:172 in get_executor_output_uri       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   169 \u2502   \"\"\"Generates executor output uri given execution_id.\"\"\"           \u2502\r\n\u2502   170 \u2502   execution_dir = os.path.join(self._node_dir, _SYSTEM, _EXECUTOR_E \u2502\r\n\u2502   171 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    str(execution_id))                   \u2502\r\n\u2502 > 172 \u2502   fileio.makedirs(execution_dir)                                    \u2502\r\n\u2502   173 \u2502   return os.path.join(execution_dir, _EXECUTOR_OUTPUT_FILE)         \u2502\r\n\u2502   174                                                                       \u2502\r\n\u2502   175   def get_driver_output_uri(self) -> str:                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\d \u2502\r\n\u2502 sl\\io\\fileio.py:80 in makedirs                                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    77                                                                       \u2502\r\n\u2502    78 def makedirs(path: PathType) -> None:                                 \u2502\r\n\u2502    79   \"\"\"Make a directory at the given path, recursively creating parents \u2502\r\n\u2502 >  80   _get_filesystem(path).makedirs(path)                                \u2502\r\n\u2502    81                                                                       \u2502\r\n\u2502    82                                                                       \u2502\r\n\u2502    83 def mkdir(path: PathType) -> None:                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\integrations\\s3\\artifact_stores\\s3_artifact_store.py:275 in makedirs       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502   Args:                                                         \u2502\r\n\u2502   273 \u2502   \u2502   \u2502   path: The path to create.                                 \u2502\r\n\u2502   274 \u2502   \u2502   \"\"\"                                                           \u2502\r\n\u2502 > 275 \u2502   \u2502   self.filesystem.makedirs(path=path, exist_ok=True)            \u2502\r\n\u2502   276 \u2502                                                                     \u2502\r\n\u2502   277 \u2502   def mkdir(self, path: PathType) -> None:                          \u2502\r\n\u2502   278 \u2502   \u2502   \"\"\"Create a directory at the given path.                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:85 in wrapper                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    82 \u2502   @functools.wraps(func)                                            \u2502\r\n\u2502    83 \u2502   def wrapper(*args, **kwargs):                                     \u2502\r\n\u2502    84 \u2502   \u2502   self = obj or args[0]                                         \u2502\r\n\u2502 >  85 \u2502   \u2502   return sync(self.loop, func, *args, **kwargs)                 \u2502\r\n\u2502    86 \u2502                                                                     \u2502\r\n\u2502    87 \u2502   return wrapper                                                    \u2502\r\n\u2502    88                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:65 in sync                                                        \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    62 \u2502   \u2502   # suppress asyncio.TimeoutError, raise FSTimeoutError         \u2502\r\n\u2502    63 \u2502   \u2502   raise FSTimeoutError from return_result                       \u2502\r\n\u2502    64 \u2502   elif isinstance(return_result, BaseException):                    \u2502\r\n\u2502 >  65 \u2502   \u2502   raise return_result                                           \u2502\r\n\u2502    66 \u2502   else:                                                             \u2502\r\n\u2502    67 \u2502   \u2502   return return_result                                          \u2502\r\n\u2502    68                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:25 in _runner                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    22 \u2502   if timeout is not None:                                           \u2502\r\n\u2502    23 \u2502   \u2502   coro = asyncio.wait_for(coro, timeout=timeout)                \u2502\r\n\u2502    24 \u2502   try:                                                              \u2502\r\n\u2502 >  25 \u2502   \u2502   result[0] = await coro                                        \u2502\r\n\u2502    26 \u2502   except Exception as ex:                                           \u2502\r\n\u2502    27 \u2502   \u2502   result[0] = ex                                                \u2502\r\n\u2502    28 \u2502   finally:                                                          \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:767 in _makedirs                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    764 \u2502                                                                    \u2502\r\n\u2502    765 \u2502   async def _makedirs(self, path, exist_ok=False):                 \u2502\r\n\u2502    766 \u2502   \u2502   try:                                                         \u2502\r\n\u2502 >  767 \u2502   \u2502   \u2502   await self._mkdir(path, create_parents=True)             \u2502\r\n\u2502    768 \u2502   \u2502   except FileExistsError:                                      \u2502\r\n\u2502    769 \u2502   \u2502   \u2502   if exist_ok:                                             \u2502\r\n\u2502    770 \u2502   \u2502   \u2502   \u2502   pass                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:758 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502    756 \u2502   \u2502   \u2502   \u2502   raise translate_boto_error(e)                        \u2502\r\n\u2502    757 \u2502   \u2502   \u2502   except ParamValidationError as e:                        \u2502\r\n\u2502 >  758 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"Bucket create failed %r: %s\" % (bu \u2502\r\n\u2502    759 \u2502   \u2502   else:                                                        \u2502\r\n\u2502    760 \u2502   \u2502   \u2502   # raises if bucket doesn't exist and doesn't get create  \u2502\r\n\u2502    761 \u2502   \u2502   \u2502   await self._ls(bucket)                                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nValueError: Bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': Parameter \r\nvalidation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1657782683000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657726485000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug in SWB 5.2.6 version of SageMaker Jupyter Notebook workspace where a study fails to mount. The error message indicates that the FUSE package failed to install during on-start. The user can resolve the issue by running \"sudo yum install fuse\" and then running \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json to mount the study.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":154.35,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":246.0,
        "Challenge_repo_issue_count":1160.0,
        "Challenge_repo_star_count":2570.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":15.6105555556,
        "Challenge_title":"[BUG]: SageMaker + S3 artifact store fails trying to create a new bucket",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":829,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @danguitavinas,\r\n\r\nI'm guessing from the stack trace that you're running on windows with the local orchestrator? If that's the case, my guess is that this issue should be fixed by #735.\r\n\r\nIf you're interested in trying this, you could install ZenML from that branch using the command `pip install git+https:\/\/github.com\/zenml-io\/zenml.git@bugfix\/windows-source-utils` @schustmi Thank you so much, that worked! Im closing the issue!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.41,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":26.9961138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When ticking any boxes inside Azure Machine Learning Studio - or in the YAML templates by specifying it's enabled by a true or false value- you can't specify which Application Insights instance to use - it will use the default one connected to that Azure Machine Learning Studio only.    <\/p>\n<p>For using something like Azure Machine Learning Studio as a solutions provider, and that might have multiple customers models within it, the ability to be able to specify a Applications Insights connection string to an instance OUTSIDE of the default in-built one would be a great addition to functionality.     <\/p>\n<p>With the current set up we are forced to have a different AML Studio \/ Storage Account \/ ACR \/ App Insights \/ Key vault for each customer to allow Applications Insights data collection to make any sense per customer.    <\/p>\n<p>Thanks    <\/p>",
        "Challenge_closed_time":1671640414100,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671543228090,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges while using Azure Machine Learning Studio as they are unable to specify which Application Insights instance to use. The default instance connected to Azure Machine Learning Studio is being used, which is not suitable for a scenario where multiple customer models are involved. The user suggests that the ability to specify an Application Insights connection string to an instance outside of the default one would be a great addition to functionality. Currently, the user is forced to have a different setup for each customer to allow Applications Insights data collection to make sense per customer.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1135890\/integration-with-application-insights-should-allow",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":11.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":26.9961138889,
        "Challenge_title":"Integration with Application Insights should allow you to specify a choice of which instance to use",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=0ec06fb6-513e-4f5c-9aff-281bc5e44e22\">@Neil McAlister  <\/a> Thanks for the feedback. I have forwarded to the product team to support near future to specify a Applications Insights connection string to an instance OUTSIDE of the default in-built.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.9641666667,
        "Challenge_answer_count":0,
        "Challenge_body":"![image](https:\/\/user-images.githubusercontent.com\/37505775\/120101493-4f481c80-c181-11eb-8a20-4a044c2bd51c.png)\r\n\r\n- lr\uc744 \uc81c\uc678\ud558\uace0 \ub098\uba38\uc9c0 value\uac00 \uc5c5\ub370\uc774\ud2b8\uac00 \ub418\uc9c0 \uc54a\ub294 \ubb38\uc81c \ubc1c\uc0dd\r\n- \uacc4\uc18d \uac12\uc774 \ucd94\uac00\ub418\ub294 \ub9ac\uc2a4\ud2b8\uc778 \uc904 \ubaa8\ub974\uace0 list[0]\uc73c\ub85c \uc778\ub371\uc2f1\ud574\uc11c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub77c\uace0 \uc0dd\uac01\ub428",
        "Challenge_closed_time":1622440667000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1622372396000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with importing wandb due to a module not being found on a server running centOS, specifically the 'six.moves.collections_abc' module. The issue cannot be reproduced on a local Intel i7 system.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pstage-ocr-team6\/ocr-teamcode\/issues\/5",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":3.11,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":43.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":0.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":18.9641666667,
        "Challenge_title":"[BUG] wandb value doesn't update",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":25,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"![image](https:\/\/user-images.githubusercontent.com\/26226101\/120102333-71439e00-c185-11eb-8a20-b113112b0b3f.png)\r\n\r\n\uc544\uc774\uac70 \uc65c \uadf8\ub7f0\uac8c \ud588\ub124\uc694.... \uc218\uc815\ud574 \uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.5,
        "Solution_reading_time":2.01,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":1.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2608333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Challenge_closed_time":1593108137000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593107198000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in changing instance types for SageMaker Studio as it seems to only support a limited number of types. They are seeking a way to change to other instance types, similar to how it can be done for SageMaker Notebook Instances.",
        "Challenge_last_edit_time":1668530553628,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sagemaker-studio",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":7.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.2608333333,
        "Challenge_title":"Notebook Instance Types for SageMaker Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":864.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The instance types you are seeing are Fast Launch Instances ( which are instance types designed to launch in under two minutes).\n\nIn order to see all the types of instances, click on the switch on top of the instance type list that says \"Fast Launch\", that should display the rest of available instances.\n\nHere is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html\n\nHope it helps!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925564584,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":5.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4417.9619444444,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Challenge_closed_time":1599677796000,
        "Challenge_comment_count":15,
        "Challenge_created_time":1583773133000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to install AWS stepfunctions using pip install in SageMaker Studio Notebook. The error message shows that the metadata generation failed due to an AttributeError, and the user received a CryptographyDeprecationWarning. The user expected to be able to install AWS stepfunctions, but the installation failed. The environment used was AWS Step Functions Data Science Python SDK version 2.3.0 and Python version 3.7.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Challenge_link_count":0,
        "Challenge_participation_count":15,
        "Challenge_readability":18.6,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4417.9619444444,
        "Challenge_title":"unable to kick off the sagemaker job",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":193,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@charlesa101  Thanks for trying out. I am assuming you have replaced input, output buckets and role Arn. \r\n\r\nWould you please run the following command provide the output ?\r\n\r\n```\r\nkubectl  get trainingjobs xgboost-mnist\r\nkubectl describe trainingjob xgboost-mnist\r\n``` @gautamkmr, here you go thank you! yeah i have my own bucket and sagemaker executor role\r\n\r\n```kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nxgboost-mnist                               2020-03-09T16:51:08Z ```\r\n\r\n```kubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400``` @charlesa101  Thanks for providing the output. It appears that operator is not running successfully on your k8s cluster.  you can verify that \r\n\r\n```\r\n kubectl get pods -A | grep -i sagemaker\r\n```\r\n\r\nYou can follow steps from [here](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#setup-and-operator-deployment) to install the operator, let us know if you face any issue. yeah that's what i noticed as well now\r\n\r\n```kubectl get pods -n sagemaker-k8s-operator-system\r\nNAME                                                         READY   STATUS    RESTARTS   AGE\r\nsagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8   0\/2     Pending   0          24h``` ```kubectl describe pod  -n sagemaker-k8s-operator-system                                             \r\nName:               sagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8\r\nNamespace:          sagemaker-k8s-operator-system\r\nPriority:           0\r\nPriorityClassName:  <none>\r\nNode:               <none>\r\nLabels:             control-plane=controller-manager\r\n                    pod-template-hash=5858fd7b8d\r\nAnnotations:        kubernetes.io\/psp: eks.privileged\r\nStatus:             Pending\r\nIP:                 \r\nControlled By:      ReplicaSet\/sagemaker-k8s-operator-controller-manager-5858fd7b8d\r\nContainers:\r\n  kube-rbac-proxy:\r\n    Image:      gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Port:       8443\/TCP\r\n    Host Port:  0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    Environment:\r\n      AWS_ROLE_ARN:                 arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\n  manager:\r\n    Image:      957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:  \r\n      AWS_ROLE_ARN:                    arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\nConditions:\r\n  Type           Status\r\n  PodScheduled   False \r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  sagemaker-k8s-operator-default-token-rwdkn:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  sagemaker-k8s-operator-default-token-rwdkn\r\n    Optional:    false\r\nQoS Class:       Burstable\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type     Reason            Age                   From               Message\r\n  ----     ------            ----                  ----               -------\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n my eks\/ecr is on us-east2, but it seems all the crd artifacts are coming from us-east1 could that be the issue?\r\n EKS can pull the image from other region too. I think in your case it seems that you don't have any worker node associated to cluster?  At least thats what below message says.\r\n```\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n```\r\n\r\nCan you run ?  \r\n```\r\nkubectl get node\r\n``` @charlesa101  did you get chance to review it again? ``` kubectl get nodes\r\nNAME                                           STATUS   ROLES    AGE     VERSION\r\nip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n yeah i did, recreated the cluster again but still the same issue\r\n @charlesa101   In previous describe output of `pod` it appears that cluster did not have any worker nodes available `(no nodes available to schedule pods)`.\r\n\r\nBut based on recent output it appears that you have three worker nodes available. \r\n\r\n> NAME                                           STATUS   ROLES    AGE     VERSION\r\n> ip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n\r\n\r\nCould you please describe each of these nodes and operator pod ?\r\n\r\n```\r\n# Describe nodes , assuming the names of nodes are same as you mentioned in previous comment.\r\nkubectl describe node ip-172-16-116-51.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-121-255.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-137-197.us-east-2.compute.internal \r\n```\r\n\r\n\r\n```\r\n#Get the operator pod name \r\nkubectl get pods -A | grep -i sagemaker\r\nkubectl describe pod <put the pod name here>  -n sagemaker-k8s-operator-system\r\n```\r\n\r\n\r\nIf operator has been deployed successfully and if trainingjob is still not yet running please attach the out put of describe trainingjob as well ? \r\n```\r\nkubectl describe trainingjob xgboost-mnist\r\n\r\n```\r\n\r\n i tried to look checked the operator pod, here is  the log @gautamkmr \r\n\r\n```\r\nkubectl logs -f sagemaker-k8s-operator-controller-manager-5858fd7b8d-2dk5c  -n sagemaker-k8s-operator-system manager\r\n2020-03-15T18:09:13.864Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    setup   starting manager\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"trainingjob\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"model\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"batchtransformjob\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hostingdeployment\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"endpointconfig\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hyperparametertuningjob\"}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"trainingjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"model\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Job status is empty, setting to intermediate status     {\"trainingjob\": \"default\/xgboost-mnist\", \"status\": \"SynchronizingK8sJobWithSageMaker\"}\r\n2020-03-15T19:09:19.963Z        INFO    controllers.TrainingJob Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"new-status\": {\"trainingJobStatus\":\"SynchronizingK8sJobWithSageMaker\",\"lastCheckTime\":\"2020-03-15T19:09:19Z\"}}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Adding generated name to spec   {\"trainingjob\": \"default\/xgboost-mnist\", \"new-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}\r\n2020-03-15T19:09:19.982Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:20.916Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:09:20.916Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\",\"lastCheckTime\":\"2020-03-15T19:09:20Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:09:20.924Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:42.150Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:11:42.150Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\",\"lastCheckTime\":\"2020-03-15T19:11:42Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:11:42.159Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n```\r\n @charlesa101  Thanks for sharing the log. You are on right track. I think the issue now is operator pod is unable to retrieve credentials from IAM service to talk to sagemaker. \r\n\r\n`\"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n`\r\n\r\nCould you please check your [trust.json](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#create-an-iam-role) basically **trust policy have three places to update cluster region and OIDC ID and one place to add your AWS account number.** Hi @charlesa101\r\n\r\nClosing this issue since there has been no activity in 90 days. Please re-open if you still need help\r\n\r\nThanks Hi, I'm having the exact same issue except that my pod is running fine. I setup my k8s cluster using terraform with 1 master node and 1 worker node. When I submit the trainingjob, there is no status or job name or anything else. I tried all the commands above and it looks like the scheduler was able to assign the pods to the worker node. Any help would be appreciated! Please see outputs for commands below:\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get pods -A                                                                                                                                                                                                                                                    \r\nNAMESPACE        NAME                                                         READY   STATUS    RESTARTS   AGE                                                                                                                                                                                                                \r\nkube-system      aws-node-67tgx                                               1\/1     Running   0          2d18h\r\nkube-system      aws-node-k2q7z                                               1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-cwfvj                                     1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-x5ld9                                     1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-54vm5                                             1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-r8j7j                                             1\/1     Running   0          2d18h\r\nkube-system      metrics-server-64cf6869bd-6nppx                              1\/1     Running   0          2d18h\r\nsagemaker-jobs   sagemaker-k8s-operator-controller-manager-855f498957-fhkvv   2\/2     Running   0          2d18h\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe pod sagemaker-k8s-operator-controller-manager-855f498957-fhkvv -n sagemaker-jobs\r\nName:         sagemaker-k8s-operator-controller-manager-855f498957-fhkvv\r\nNamespace:    sagemaker-jobs\r\nPriority:     0\r\nNode:         ip-10-0-1-245.us-west-2.compute.internal\/10.0.1.245\r\nStart Time:   Fri, 24 Jun 2022 22:26:03 +0000\r\nLabels:       control-plane=controller-manager\r\n              pod-template-hash=855f498957\r\nAnnotations:  kubernetes.io\/psp: eks.privileged\r\nStatus:       Running\r\nIP:           10.0.1.144\r\nIPs:\r\n  IP:           10.0.1.144\r\nControlled By:  ReplicaSet\/sagemaker-k8s-operator-controller-manager-855f498957\r\nContainers:\r\n  manager:\r\n    Container ID:  docker:\/\/d8fc52b3e20a050999d3f24ab914f1d865a84a168a8b038f3fa81ce59cccbced\r\n    Image:         957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Image ID:      docker-pullable:\/\/957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s@sha256:94ffbba68954249b1724fdb43f1e8ab13547114555b4a217849687d566191e23\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n      --namespace=sagemaker-jobs\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:09 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:\r\n      AWS_DEFAULT_REGION:              us-west-2\r\n      AWS_REGION:                      us-west-2\r\n      AWS_ROLE_ARN:                    arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nkube-rbac-proxy:\r\n    Container ID:  docker:\/\/4ecdaa395fdc70d5cead609465dbf21f6e11771a80ad5db0a6125053ab08b9d3\r\n    Image:         gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Image ID:      docker-pullable:\/\/gcr.io\/kubebuilder\/kube-rbac-proxy@sha256:297896d96b827bbcb1abd696da1b2d81cab88359ac34cce0e8281f266b4e08de\r\n    Port:          8443\/TCP\r\n    Host Port:     0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:11 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:\r\n      AWS_DEFAULT_REGION:           us-west-2\r\n      AWS_REGION:                   us-west-2\r\n      AWS_ROLE_ARN:                 arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             True\r\n  ContainersReady   True\r\n  PodScheduled      True\r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  kube-api-access-6j8rt:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io\/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io\/unreachable:NoExecute op=Exists for 300s\r\nEvents:                      <none>\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl logs sagemaker-k8s-operator-controller-manager-855f498957-fhkvv manager -n sagemaker-jobs\r\nI0624 22:26:11.339445       1 request.go:621] Throttling request took 1.046981399s, request: GET:https:\/\/172.20.0.1:443\/apis\/extensions\/v1beta1?timeout=32s\r\n2022-06-24T22:26:12.443Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2022-06-24T22:26:12.443Z        INFO    Starting manager in the namespace:      sagemaker-jobs\r\n2022-06-24T22:26:12.443Z        INFO    setup   starting manager\r\n2022-06-24T22:26:12.444Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.665Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\"}\r\n2022-06-24T22:26:12.746Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\"}\r\n2022-06-24T22:26:12.747Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nosic-test-run                               2022-06-24T22:38:13Z  \r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe trainingjob osic-test-run                                                                                                                                                                                                                             \r\nName:         osic-test-run                                                                                                                                                                                                                                                                                                   \r\nNamespace:    default                                                                                                                                                                                                                                                                                                         \r\nLabels:       <none>                                                                                                                                                                                                                                                                                                          \r\nAnnotations:  <none>                                                                                                                                                                                                                                                                                                          \r\nAPI Version:  sagemaker.aws.amazon.com\/v1                                                                                                                                                                                                                                                                                     \r\nKind:         TrainingJob                                                                                                                                                                                                                                                                                                     \r\nMetadata:                                                                                                                                                                                                                                                                                                                     \r\n  Creation Timestamp:  2022-06-24T22:38:13Z                                                                                                                                                                                                                                                                                   \r\n  Generation:          1                                                                                                                                                                                                                                                                                                      \r\n  Managed Fields:\r\n    API Version:  sagemaker.aws.amazon.com\/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:kubectl.kubernetes.io\/last-applied-configuration:\r\n      f:spec:\r\n        .:\r\n        f:algorithmSpecification:\r\n          .:\r\n          f:trainingImage:\r\n          f:trainingInputMode:\r\n        f:inputDataConfig:\r\n        f:outputDataConfig:\r\n          .:\r\n          f:s3OutputPath:\r\n        f:region:\r\n        f:resourceConfig:\r\n          .:\r\n          f:instanceCount:\r\n          f:instanceType:\r\n          f:volumeSizeInGB:\r\n        f:roleArn:\r\n        f:stoppingCondition:\r\n          .:\r\n          f:maxRuntimeInSeconds:\r\n        f:trainingJobName:\r\n    Manager:         kubectl-client-side-apply\r\n    Operation:       Update\r\n    Time:            2022-06-24T22:38:13Z\r\n  Resource Version:  3182\r\n  UID:               0a0880c0-baf9-4f1a-8aa3-37480520c3e2\r\nSpec:\r\n  Algorithm Specification:\r\nTraining Image:       438029713005.dkr.ecr.us-west-2.amazonaws.com\/model-training:latest\r\n    Training Input Mode:  File\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Data Source:\r\n      s3DataSource:\r\n        s3DataDistributionType:  FullyReplicated\r\n        s3DataType:              S3Prefix\r\n        s3Uri:                   s3:\/\/osic-full-including-override\r\n  Output Data Config:\r\n    s3OutputPath:  s3:\/\/osic-full-including-override\/experiments\r\n  Region:          us-west-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.p3.2xlarge\r\n    Volume Size In GB:  500\r\n  Role Arn:             arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  900\r\n  Training Job Name:         osic-test-run\r\nEvents:                      <none>\r\n```\r\n\r\nplease let me know if you need to see anything else!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":18.8,
        "Solution_reading_time":403.18,
        "Solution_score_count":null,
        "Solution_sentence_count":227.0,
        "Solution_word_count":2188.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":191.5183333333,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nIdle Sagemaker Notebook instances do not stop after specified time.\r\n\r\nSWB runs autostop.py script to automatically stop Sagemaker Notebook instance. The script is used by `on-start` lifecycle rule of the instance CFN template. According to LifecycleConfigOnStart logs, some packages are missing and autostop script doesn\u2019t work.\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Make sure AutoStopIdleTimeInMinutes parameter in workspace type config is set to a required time (30 minutes in our case)\r\n2. Create a new workspace with Sagemaker notebook instance\r\n3. Leave the instance idle for the time specified (AutoStopIdleTimeInMinutes )\r\n4. After the specified time see that the instance is not stopped\r\n\r\n**Expected behavior**\r\nIdle Sagemaker Notebook instance automatically stops after specified time.\r\n\r\n**Screenshots**\r\n<img width=\"1308\" alt=\"Screen Shot 2022-12-07 at 10 43 09 am\" src=\"https:\/\/user-images.githubusercontent.com\/47466926\/206049662-5ff12457-8bd4-42bd-b12f-ce68fdfacaf6.png\">\r\n\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed v5.0.0\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671059684000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1670370218000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the study folders are not mounted when launching a Sagemaker notebook with an associated study. Upon investigation, the user found that there is no credentials file in the `~\/.aws` folder, which is generated by the `mount_s3.sh` script. The expected behavior is for the study folders to be mounted using the assumed roles in the AWS credentials file. This issue may or may not be associated with another bug the user noted with mounting s3 studies folders.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":10.0,
        "Challenge_reading_time":15.67,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":191.5183333333,
        "Challenge_title":"[Bug] Sagemaker instance does not stop automatically",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you. We are aware of this issue and have a backlog item to resolve this! See https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065 for more information. Hi, please check the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5) for the fix to this issue.  Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! Hi Marianna,\nThank you. I am going to migrate to v5.2.5 tomorrow. If everything goes\nwell, I'll close the ticket soon.\n\n\nOn Tue, Dec 13, 2022 at 7:55 AM Marianna Ghirardelli <\n***@***.***> wrote:\n\n> Hi! I also want to note that you may need to stop and start any affected\n> instances after upgrade and deploying SWB v5.2.5.\n>\n> If this fixes your issue, please go ahead and close this issue. I am going\n> to mark as closing-soon-if-no-response so we will close in about 7 days if\n> we do not hear that this did not resolve the issue.\n>\n> Thank you for the report!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076#issuecomment-1347314897>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ALKETLSEQCFIMAF5HX4CAT3WM6GN3ANCNFSM6AAAAAASWEAJP4>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n Closing this issue since the fix was merged, please feel free to reopen the issue if it persists on your end.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":6.9,
        "Solution_reading_time":21.01,
        "Solution_score_count":null,
        "Solution_sentence_count":20.0,
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":35.0730002778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Challenge_closed_time":1579273412888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579147150087,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user encountered challenges while following the scikit_bring_your_own example for product recommendations, including errors when attempting to install the scikit-surprise package in the dockerfile.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":20.5,
        "Challenge_reading_time":212.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":223,
        "Challenge_solved_time":35.0730002778,
        "Challenge_title":"AWS Sagemaker scikit_bring_your_own example",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":341.0,
        "Challenge_word_count":1082,
        "Platform":"Stack Overflow",
        "Poster_created_time":1241005356852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Malaysia",
        "Poster_reputation_count":15794.0,
        "Poster_view_count":1032.0,
        "Solution_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":2.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":558.1344444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug Description\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nwhen I do the example:\r\nqrun qrun benchmarks\\GATs\\workflow_config_gats_Alpha158.yaml\r\n\r\nI got the error info:\r\n\r\n\r\n\r\n(py38) D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples>qrun benchmarks\\GATs\\workflow_config_gats_Alpha158_full02.yaml\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [config.py:413] - default_conf: client.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.workflow - [expm.py:31] - experiment manager uri is at file:D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples\\mlruns\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:\/Users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [expm.py:316] - <mlflow.tracking.client.MlflowClient object at 0x0000017B5D406F40>\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [exp.py:260] - Experiment 3 starts running ...\r\n[7724:MainThread](2022-10-14 07:53:34,124) INFO - qlib.workflow - [recorder.py:339] - Recorder 41d40d173e614811bad721127a3204b8 starts running under Experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,140) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,158) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,164) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff --cached`\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"Param value\", MAX_PARAM_VAL_LENGTH, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nThe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nIncorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    mlflow.log_param(\"depth\", 3)\r\n    mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will throw an MlflowException for overwriting a\r\nlogged parameter.\r\n\r\nCorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 3)\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:MainThread](2022-10-14 07:53:35,515) INFO - qlib.GATs - [pytorch_gats_ts.py:81] - GATs pytorch version...\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:100] - GATs parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : LSTM\r\nmodel_path : None\r\nvisible_GPU : 0\r\nuse_GPU : True\r\nseed : None\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:146] - model:\r\nGATModel(\r\n  (rnn): LSTM(158, 64, num_layers=2, batch_first=True, dropout=0.7)\r\n  (transformation): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc_out): Linear(in_features=64, out_features=1, bias=True)\r\n  (leaky_relu): LeakyReLU(negative_slope=0.01)\r\n  (softmax): Softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nThen the program re-run again.\r\nI am wondering how to fix it.\r\nThanks a lot.\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - Qlib version:\r\n - 0.8.6.99'\r\n - Python version:\r\n - 3.8.5\r\n - OS (`Windows`, `Linux`, `MacOS`):\r\n - windows 10\r\n - Commit number (optional, please provide it if you are using the dev version):\r\n\r\n## Additional Notes\r\n\r\n<!-- Add any other information about the problem here. -->\r\n",
        "Challenge_closed_time":1667718001000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1665708717000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a 500 internal server error when using the metadata docker image to interact with the Neptune database. When testing locally, the user found two problems: an error with the read_timeout argument and a MalformedQueryException error. The user suggests removing the read_timeout and write_timeout arguments and replacing Order.decr with Order.desc to fix the issues.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1317",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.5,
        "Challenge_reading_time":92.22,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":558.1344444444,
        "Challenge_title":"on qrun:\"mlflow.exceptions.MlflowException: Param value .... had length 780, which exceeded length limit of 500 \"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":583,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I had the same problem TT Same for all the example in `benchmarks\/LightGBM`. This is because mlflow limits the length of params since 1.28.0.\r\nWhile waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution. > This is because mlflow limits the length of params since 1.28.0. While waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution.\r\n\r\nThank you for help. Wish you have a good day.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":6.36,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":89.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":0.0008886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I initially had a notebook in one directory in AWS SageMaker JupyterLab, say <code>\/A<\/code>, but then moved it into <code>\/A\/B<\/code>. However, when I run <code>!pwd<\/code> in a jupyter notebook cell, I still get <code>\/A<\/code>. This happens even when I press 'restart kernel'. How does the notebook remember this, and is there a way to prevent or reset this?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1597057792807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597055996810,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has moved a Jupyter notebook from one directory to another in AWS SageMaker JupyterLab, but when running the command \"!pwd\" in a notebook cell, the previous directory is still displayed instead of the current one. The user is seeking a solution to prevent or reset this issue.",
        "Challenge_last_edit_time":1597057789608,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63338577",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4988880556,
        "Challenge_title":"Jupyter notebook seems to remember previous path (!pwd) after being moved to a different directory?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":118.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>I was actually using AWS SageMaker, and restarting the kernel from the toolbar was not enough. I needed to restart the kernel session, by pressing 'shut down' in the &quot;Running terminals and kernels&quot; section on the left navigation.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/934Fe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/934Fe.png\" alt=\"Shut down button seen on left navigation of JupterLab\" \/><\/a><\/p>\n<p>They are currently <a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab\/issues\/5989\" rel=\"nofollow noreferrer\">discussing<\/a> warning users about the need to restart the kernel when a notebook is moved.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.4,
        "Solution_reading_time":8.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":3103.0390416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Challenge_closed_time":1650761211590,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648821712310,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully created three different experiments using MLFlow and Jupyter Notebook on their local machine. However, when trying to run the MLFlow UI, they are unable to see any experiments. The user is seeking assistance in identifying the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71708147",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":15.0,
        "Challenge_reading_time":52.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":43,
        "Challenge_solved_time":538.7498,
        "Challenge_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":936.0,
        "Challenge_word_count":377,
        "Platform":"Stack Overflow",
        "Poster_created_time":1648820658172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659992652860,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.82,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":105.6016441667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Challenge_closed_time":1647022114532,
        "Challenge_comment_count":12,
        "Challenge_created_time":1646641948613,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run \"dvc pull\" on Google Colab. They have two repositories, A and B, where A is for their machine learning codes and B is for their dataset. They have successfully pushed their dataset to repository B with DVC and also managed to run \"dvc import\" on their local project of repository A. However, when they try to run \"dvc pull\" on Colab, they get an error stating that the repo failed to clone. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1652856778060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Challenge_link_count":7,
        "Challenge_participation_count":13,
        "Challenge_readability":13.5,
        "Challenge_reading_time":96.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":76,
        "Challenge_solved_time":105.6016441667,
        "Challenge_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":707.0,
        "Challenge_word_count":614,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525227015312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.5,
        "Solution_reading_time":10.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":99.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":123.0280555556,
        "Challenge_answer_count":0,
        "Challenge_body":"The team uses Azure ML CLI to deploy a container to AKS (az ml model deploy). Now and then (not always), they get an internal server error, see stack trace. They could not detect a clear pattern when this error occurs. Although it would be possible to create a retry loop in their Azure DevOps pipeline when this error occurs (as the error message also tells), this would not resolve the underlying issue.\r\n\r\n```\r\n2020-02-14T11:11:07.1739375Z ERROR: {'Azure-cli-ml Version': '1.0.85', 'Error': WebserviceException:\r\n\r\n2020-02-14T11:11:07.1739694Z \tMessage: Received bad response from Model Management Service:\r\n\r\n2020-02-14T11:11:07.1739785Z Response Code: 500\r\n\r\n2020-02-14T11:11:07.1740533Z Headers: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\r\n\r\n2020-02-14T11:11:07.1741400Z Content: b'{\"code\":\"InternalServerError\",\"statusCode\":500,\"message\":\"An internal server error occurred. Please try again. If the problem persists, contact support\"}'\r\n\r\n2020-02-14T11:11:07.1741516Z \tInnerException None\r\n\r\n2020-02-14T11:11:07.1741641Z \tErrorResponse \r\n\r\n2020-02-14T11:11:07.1741708Z {\r\n\r\n2020-02-14T11:11:07.1741813Z     \"error\": {\r\n\r\n2020-02-14T11:11:07.1742819Z         \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 500\\nHeaders: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"InternalServerError\\\",\\\"statusCode\\\":500,\\\"message\\\":\\\"An internal server error occurred. Please try again. If the problem persists, contact support\\\"}'\"\r\n\r\n2020-02-14T11:11:07.1743119Z     }\r\n\r\n2020-02-14T11:11:07.1743227Z }}\r\n```",
        "Challenge_closed_time":1583847372000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583404471000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an import error while running a sample notebook for image classification. The error occurs when trying to import 'Dataset' from 'azureml.core'. The user has provided a reference YAML file that includes the necessary dependencies, and is requesting assistance from Microsoft to investigate the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/841",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":123.0280555556,
        "Challenge_title":"Internal server error when deploying from Azure ML to AKS",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":201,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@robinvdheijden \r\n\r\nThanks for reaching out to us. This is forum for Machine Learning Notebook only. Please open a new forum thread in [MSDN forum](https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/home?forum=AzureMachineLearningService)as it could be better place to get help on your scenario. These forum community members could provide their expert guidance on your scenario based on their experience. Thanks.\r\n\r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":7.43,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1539831335196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation_count":137.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":870.7459847222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have this script where I want to get the callbacks to a separate CSV file in sagemaker custom script docker container. But when I try to run in local mode, it fails giving the following error. I have a hyper-parameter tuning job(HPO) to run and this keeps giving me errors. I need to get this local mode run correctly before doing the HPO. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/de522.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/de522.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the notebook I use the following code.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='lstm_model.py', \n                          role=role,\n                          code_location=custom_code_upload_location,\n                          output_path=model_artifact_location+'\/',\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1},\n                          base_job_name='hpo-lstm-local-test'\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n\n<p>In my <strong>lstm_model.py<\/strong> script the following code is used.<\/p>\n\n<pre><code>lgdir = os.path.join(model_dir, 'callbacks_log.csv')\ncsv_logger = CSVLogger(lgdir, append=True)\n\nregressor.fit(x_train, y_train, batch_size=batch_size,\n              validation_data=(x_val, y_val), \n              epochs=epochs,\n              verbose=2,\n              callbacks=[csv_logger]\n              )\n<\/code><\/pre>\n\n<p>I tried creating a file before hand like shown below using tensorflow backend. But it doesn't create a file. ( K : tensorflow Backend, tf: tensorflow )<\/p>\n\n<pre><code>filename = tf.Variable(lgdir , tf.string)\ncontent = tf.Variable(\"\", tf.string)\nsess = K.get_session()\ntf.io.write_file(filename, content)\n<\/code><\/pre>\n\n<p>I can't use any other packages like pandas to create the file as the TensorFlow docker container in SageMaker for custom scripts doesn't provide them. They give only a limited amount of packages. <\/p>\n\n<p>Is there a way I can write the csv file to the S3 bucket location, before the fit method try to write the callback. Or is that the solution to the problem? I am not sure. <\/p>\n\n<p>If you can even suggest other suggestions to get callbacks, I would even accept that answer. But it should be worth the effort. <\/p>\n\n<p>This docker image is really narrowing the scope. <\/p>",
        "Challenge_closed_time":1583860547412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580725861867,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to use Keras CSVLogger callbacks in Sagemaker script mode. The log file fails to write on S3, resulting in a \"No such file or directory\" error. The user has tried creating a file beforehand using TensorFlow backend, but it doesn't work. The user is looking for a solution to write the CSV file to the S3 bucket location before the fit method tries to write the callback. The user is open to other suggestions to get callbacks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60037376",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":31.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":870.7459847222,
        "Challenge_title":"Can't use Keras CSVLogger callbacks in Sagemaker script mode. It fails to write the log file on S3 ( error - No such file or directory )",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":412.0,
        "Challenge_word_count":292,
        "Platform":"Stack Overflow",
        "Poster_created_time":1517147266416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:<\/p>\n\n<pre><code># Downloads the TensorFlow library used to run the Python script\nFROM tensorflow\/tensorflow:2.0.0a0 # you would use the equivalent for your TF version\n\n# Contains the common functionality necessary to create a container compatible with Amazon SageMaker\nRUN pip install sagemaker-containers -q \n\n# Wandb allows us to customize and centralize logging while maintaining open-source agility\nRUN pip install wandb -q # here you would install pandas\n\n# Copies the training code inside the container to the design pattern created by the Tensorflow estimator\n# here you could copy over a callbacks csv\nCOPY mnist-2.py \/opt\/ml\/code\/mnist-2.py \nCOPY callbacks.py \/opt\/ml\/code\/callbacks.py \nCOPY wandb_setup.sh \/opt\/ml\/code\/wandb_setup.sh\n\n# Set the login script as the entry point\nENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py\n<\/code><\/pre>\n\n<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=\"https:\/\/www.wandb.com\/\" rel=\"nofollow noreferrer\">Weights and Biases<\/a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.6,
        "Solution_reading_time":19.1,
        "Solution_score_count":2.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":221.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.4035702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Working on deployment of 170 ml models using ML studio and azure Kubernetes service which is referred on the  below doc  link &quot;https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-deploy-azure-kubernetes-service.md&quot;.     <\/p>\n<p>We are training the model using python script with the custom environment and we are registering the ml model on the  Azure ML services. Once we register the mode we are deploying it on the AKS by using the container images.     <\/p>\n<p>While deploying the ML model we are able to deploy up 10 to 11 models per pods for each Node in AKS. When we try to deploy the model on the same node we are getting deployment timeout error and we are getting the below error message.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/129300-image-2021-09-04t13-25-12-512z.png?platform=QnA\" alt=\"129300-image-2021-09-04t13-25-12-512z.png\" \/>    <\/p>",
        "Challenge_closed_time":1630758725400,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630746472547,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to deploy 170 ml models using ML studio and Azure Kubernetes service. They are able to deploy up to 10-11 models per pod for each node in AKS, but when they try to deploy more models on the same node, they encounter a deployment timeout error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/540001\/how-many-models-can-be-deployed-in-single-node-in",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.4035702778,
        "Challenge_title":"how many models can be deployed in single node in azure kubernetes service?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=f023f08d-7d4a-4ac5-ba62-e9d37f7e7c70\">@suvedharan  <\/a>     <\/p>\n<p>The number of models to be deployed is limited to 1,000 models per deployment (per container).    <\/p>\n<p>Autoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">more details<\/a>    <\/p>\n<p>If the Answer is helpful, please click <code>Accept Answer<\/code> and <strong>up-vote<\/strong>, so that it can help others in the community looking for help on similar topics.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":9.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":86.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":674.2494444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I am running a lightly edited version of this pipeline example: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-azureml\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.`\r\n\r\nI am also getting this same warning in other pipelines I make and I cannot figure out what is causing it.\r\n\r\nHere is a slightly reduced MWE for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication\r\nfrom azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\r\nfrom azureml.core.conda_dependencies import CondaDependencies\r\nfrom azureml.core.compute import ComputeTarget, AmlCompute\r\nfrom azureml.core.compute_target import ComputeTargetException\r\nfrom azureml.data import OutputFileDatasetConfig\r\nfrom azureml.pipeline.steps import PythonScriptStep\r\nfrom azureml.pipeline.core import Pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\r\n                                    service_principal_id=os.environ['AML_PRINCIPAL_ID'],\r\n                                    service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = Workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = PythonScriptStep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=RunConfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom azureml.core import Run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = Pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = Experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=True)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nExpected a StepRun object but received <class 'azureml.core.run.Run'> instead.\r\nThis usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\r\nPlease check for package conflicts in your python environment\r\n```\r\n",
        "Challenge_closed_time":1626719342000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1624292044000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to deploy an ML model using the az ml model deploy command with additional files, but encounters an error stating that the Dockerfile instruction COPY is not allowed. The error message suggests that only certain instructions such as ARG, ENV, EXPOSE, LABEL, and RUN are allowed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1517",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":15.5,
        "Challenge_reading_time":36.56,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":674.2494444444,
        "Challenge_title":"AzureML Pipelines: Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":249,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@afogarty85 can you share the version of SDK you are using? ```\r\nimport azureml\r\nprint(azureml.core.__version__)\r\n1.31.0\r\n``` @afogarty85, I'm unable to reproduce the error you are seeing. Is the pipeline running despite the error\/warning? It is running\/working anyways and indeed -- on a different workspace, I too cannot reproduce it. I am not sure why it is a symptom of the one I am on. I am opening a bug for investigation and will update you when I have a response.  I am also running into this issue with code that was working previously. Had a weekly pipeline scheduled to run at the start of every Monday. It usually took around a couple of minutes  to finish but looking back at some logs it seems like after June 13  runs were taking 100+ hours and most timed out. I tried to manually run the pipeline and hit the exact same issue with Expecting StepRun object, not sure if there was some sort of update around the middle of June to the SDK?\r\n\r\n\r\n***EDIT Had to update the Azure ML SDK along with the azureml-automl-core, azureml-pipeline-core, and azureml-pipeline packages*** I'm sharing the investigation from engineering below. Since this is expected behavior, we will not be fixing it. Hope this helps. \r\n\r\nThis bug is activated if the user has a package version conflict in their local python environment, the PipelineRun.wait_for_completion() method may fail with an error 'Unexpected keyword argument timeout_seconds'. This is because the run rehydration fails and we receive a run object with the wrong type, which doesn't have this argument.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":18.91,
        "Solution_score_count":null,
        "Solution_sentence_count":17.0,
        "Solution_word_count":257.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6365.2066666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello, I can't open my project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? A screenshot is attached here to understand better. Thanks!\r\n<img width=\"1363\" alt=\"Screen Shot 2022-02-22 at 9 45 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Challenge_closed_time":1668499115000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1645584371000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug when attempting to clone a single notebook using the \"Open In in Sagemaker Studio Lab\" button. The error message \"Unable to copy notebook to project\" appears when the user selects \"Copy Notebook Only\" in the modal. Cloning the whole repository works without any issues. The expected behavior is for the notebook to open and appear as it would with cloning a directory. The user is using Windows 11 and Chrome version 97.0.4692.71.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6365.2066666667,
        "Challenge_title":"Can't open project on amazon sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Not sure if your issue has been resolved or not.\r\nA quick fix is to delete your account and recreate. You will by pass the approval process if you use the same email that has already been approved. @bsaha205 do you still have the problem to open the project? Please let us know about your situation. I'll close the issue. If you have the trouble. please try @MicheleMonclova solution.  Hi @icoxfog417, yes the issue is resolved. Thanks. I am glad to hear that. Please enjoy your ML journey!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":5.89,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6.3997222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\n    Warning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n    \u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\n    Making 'packages.html' ... done\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\n    yum install r-cran-rjava\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Challenge_closed_time":1527821535000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527798496000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with installing R libraries in SageMaker, specifically with the XML and rJava packages, which result in a non-zero exit status error. The user has tried a workaround that requires root access but cannot install due to lack of permission. The issue is not reproducible in a local RStudio environment, and the user is running the necessary system requirements in SageMaker. The user is seeking a solution to install these packages without root access.",
        "Challenge_last_edit_time":1668616977192,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sagemaker-that-receive-a-non-zero-exit-status",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.3997222222,
        "Challenge_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":744.0,
        "Challenge_word_count":189,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n`$ yum install r-cran-rjava`\n\nyou can try:\n\n`$ sudo yum install r-cran-rjava`\n\nwhich will allow you to impersonate the superuser (ie. root) for that command [1]\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results [2])\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n`$ sudo yum install -y R-java-devel.x86_64`\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n`$ sudo yum install -y libxml2-devel` [3]\n\nAfter which you can then open R (either as root user...)\n\n`$ sudo R`\n\nor personal\/local user\n\n`$ R`\n\nand execute the package installation:\n\n`> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")`\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n`$ sudo find \/ -iname libgomp.spec`\n\nwhich should be located at **\/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec** -- if so, you can manually create symlinks to fix this:\n\n```\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n```\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n`$ ls \/usr\/lib64\/libgomp*`\n\nOnce confirmed, you can run the install.package('rJava') command.\n\n[1]:https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/install-software.html\n[2]:https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/find-software.html\n[3]:https:\/\/stackoverflow.com\/questions\/29014062\/unable-to-install-xml-package-in-r-on-centos#29014363",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925576348,
        "Solution_link_count":4.0,
        "Solution_readability":14.1,
        "Solution_reading_time":23.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":223.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":0.1047952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was following a guide on mounting EFS in SageMaker studio, but when using the following as a notebook cell:<\/p>\n<pre><code>%%sh \n\nsudo mount -t nfs \\\n    -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\\n    172.31.5.227:\/ \\\n    ..\/efs\n\nsudo chmod go+rw ..\/efs\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>sh: 2: sudo: not found\nsh: 7: sudo: not found\n<\/code><\/pre>\n<p>Even in the terminal ('image terminal'), sudo is not found: <code># sudo \/bin\/sh: 1: sudo: not found<\/code><\/p>",
        "Challenge_closed_time":1596811524960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596811147697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to mount EFS in SageMaker studio. When using the given notebook cell, the user is getting an error message \"sudo: not found\" both in the notebook cell and the terminal.",
        "Challenge_last_edit_time":1596823164336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63304005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1047952778,
        "Challenge_title":"sudo: not found on AWS Sagemaker Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":980.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>I managed to get sudo working in the &quot;System Terminal&quot; instead. The image terminals don't seem to have access to sudo.<\/p>\n<p>Unrelated: But then when I tried to mount EFS onto the SageMaker studio app, it simply failed, saying mount target is not a directory. Looks like I'm not using Sagemaker Studio this year.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1596815281143,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.3284266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to run the notebook in studio, but can I export the studio as notebook and import it in another place?<\/p>",
        "Challenge_closed_time":1667268149163,
        "Challenge_comment_count":1,
        "Challenge_created_time":1667248966827,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if they can build their ML pipeline\/experiment in ML studio designer and export it as a Python and Jupyter notebook to be used in another place. They are able to run the notebook in studio but want to know if it can be exported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1069926\/can-i-build-my-ml-pipeline-experiment-in-ml-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":5.3284266667,
        "Challenge_title":"can I build my ML pipeline\/experiment in ML studio designer, and export it as a python and jupyter notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=a7efa3c2-0ff5-4cc8-8b14-63a031eb0713\">@usui gina  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A. Sorry, this is not support at this moment. But Azure Machine Learning Studio already has Notebook function just for your reference.     <\/p>\n<p>I will forward your feedback to product team and at the same time, I would highly recommend you provide your feedback in Azure Machine Learning portal to raise more visability - top right side as below screenshot    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/255817-image.png?platform=QnA\" alt=\"255817-image.png\" \/>    <\/p>\n<p>I hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":9.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1017638889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When users were creating a new compute in AML environment by default RStudio application was created.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238009-screenshot-2022-08-23-170502.png?platform=QnA\" alt=\"With RStudio\" \/>    <\/p>\n<p>However, from month of July, by default RStudio application is not getting created. Only JupyterLab, Jupyter, VS Code, Terminal, Notebook applications installed not a RStudio.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/237995-4.png?platform=QnA\" alt=\"without RStudio\" \/>    <\/p>\n<p>Is there any way to by default install RStudio application in azure ML compute instance?    <\/p>",
        "Challenge_closed_time":1662454114980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662442948630,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where RStudio application is not installed by default in Azure ML compute since July, and is seeking a solution to install it by default.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/995175\/rstudio-application-not-installed-in-azure-ml-comp",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":9.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.1017638889,
        "Challenge_title":"RStudio application not installed in Azure ML compute",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8876fdcf-bcee-403a-827a-7cff9a68f8ee\">@SHAIKH, Alif Abdul  <\/a> Yes, this is a recent change in the setup of compute instance that happens during the creation of compute instance. This change does not setup the rstudio community edition by default but you can set it up while creation by adding it as a custom application from advanced settings. This change is done as part of RStudio requirements to allow users to setup their own license key or use open studio version during creation. Please refer this documentation <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#add-custom-applications-such-as-rstudio-preview\">page<\/a> for details to setup licensed and open version of the studio.    <\/p>\n<p>If you add a license key then please use RStudio Workbench (bring your own license) option from the advanced options and enter your own license key.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238075-image.png?platform=QnA\" alt=\"238075-image.png\" \/>    <\/p>\n<p>If you need to add the open version you need to select custom application and enter the docker image and mount point details to setup the rstudio during creation.    <\/p>\n<p>Target\/Published port <code>8787<\/code>    <br \/>\nDocker image set to <code>ghcr.io\/azure\/rocker-rstudio-ml-verse:latest<\/code>    <br \/>\n<code>\/home\/azureuser\/cloudfiles<\/code> for Host path    <br \/>\n<code>\/home\/azureuser\/cloudfiles<\/code> for Container path    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238135-image.png?platform=QnA\" alt=\"238135-image.png\" \/>    <\/p>\n<p>Once the creation and setup is complete the options to use Rstudio for open and licensed version will be visible as links on the compute instances page.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238142-image.png?platform=QnA\" alt=\"238142-image.png\" \/>    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":14.3,
        "Solution_reading_time":30.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":240.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554060427012,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2433.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":68.9593552778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been using a jupyter notebook instance to spin up a training job (on separate instance) and deploy the endpoint (on another instance). I am using sagemaker tensorflow APIs for this as shown below:<\/p>\n<pre><code># create Tensorflow object and provide and entry point script\ntf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',\n                      train_instance_count=1, train_instance_type='ml.p2.xlarge',\n                      framework_version='1.12', py_version='py3')\n\n# train model on data on s3 and save model artifacts to s3\ntf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n\n# deploy model on another instance using checkpoints saved on S3\npredictor = estimator.deploy(initial_instance_count=1,\n                         instance_type='ml.c5.xlarge',\n                         endpoint_type='tensorflow-serving')\n<\/code><\/pre>\n<p>I have been doing all of these steps through a jupyter notebook instance. What AWS services I can use to get rid off the dependency of jupyter notebook instance and automate these tasks of training and deploying the model in serverless fashion?<\/p>",
        "Challenge_closed_time":1595624761856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595377179513,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is currently using a Jupyter notebook instance to train and deploy a model using Sagemaker TensorFlow APIs. They are looking for a way to automate these tasks without the dependency on a Jupyter notebook instance and deploy the model in a serverless fashion.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63024900",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":14.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":68.7728730556,
        "Challenge_title":"How to train and deploy model in script mode on Sagemaker without using jupyter notebook instance (serverless)?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":758.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378268842847,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, India",
        "Poster_reputation_count":4616.0,
        "Poster_view_count":592.0,
        "Solution_body":"<p>I recommend <code>AWS Step Functions<\/code>.  Been using it to schedule <code>SageMaker Batch Transform<\/code> and preprocessing jobs since it integrates with <code>CloudWatch<\/code> event rules.  It can also train models, perform hpo tuning, and integrates with <code>lambda<\/code>.  There is a SageMaker\/Step Functions SDK as well as you can use Step Functions directly by creating state machines. Some examples and documentation:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/<\/a><\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1595625433192,
        "Solution_link_count":4.0,
        "Solution_readability":21.9,
        "Solution_reading_time":12.5,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":1835.4266388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using SageMaker for distributed TensorFlow model training and serving.  I am trying to get the shape of the pre-processed datasets from the ScriptProcessor so I can provide it to the TensorFlow Environment.<\/p>\n<pre><code>script_processor = ScriptProcessor(command=['python3'],\n                image_uri=preprocess_img_uri,\n                role=role,\n                instance_count=1,\n                sagemaker_session=sm_session,\n                instance_type=preprocess_instance_type)\n\nscript_processor.run(code=preprocess_script_uri,\n                inputs=[ProcessingInput(\n                        source=source_dir + username + '\/' + dataset_name,\n                        destination='\/opt\/ml\/processing\/input')],\n                outputs=[\n                        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n                        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;)\n                ],\n\n                arguments = ['--filepath', dataset_name, '--labels', 'labels', '--test_size', '0.2', '--shuffle', 'False', '--lookback', '5',])\n\npreprocessing_job_description = script_processor.jobs[-1].describe()\n\noutput_config = preprocessing_job_description[&quot;ProcessingOutputConfig&quot;]\nfor output in output_config[&quot;Outputs&quot;]:\n    if output[&quot;OutputName&quot;] == &quot;train_data&quot;:\n        preprocessed_training_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]\n    if output[&quot;OutputName&quot;] == &quot;test_data&quot;:\n        preprocessed_test_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]\n<\/code><\/pre>\n<p>I would like to get the following data:<\/p>\n<pre><code>pre_processed_train_data_shape = script_processor.train_data_shape?\n<\/code><\/pre>\n<p>I am just not sure how to get the value out of the docker container.  I have reviewed the documentation here:<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>",
        "Challenge_closed_time":1647621141128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647581865580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using SageMaker for distributed TensorFlow model training and serving. They are trying to get the shape of the pre-processed datasets from the ScriptProcessor so they can provide it to the TensorFlow Environment, but they are not sure how to get the value out of the docker container. They have reviewed the documentation but are still unsure.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71522857",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":23.4,
        "Challenge_reading_time":25.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":10.9098744444,
        "Challenge_title":"Get Variable from SageMaker Script Processor",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":334.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1478923885800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>There are a few options:<\/p>\n<ol>\n<li><p>Write some data to a text file at <code>\/opt\/ml\/output\/message<\/code>, then call <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/describe-processing-job.html\" rel=\"nofollow noreferrer\">DescribeProcessingJob<\/a> (using Boto3 or the AWS CLI or API) and retrieve the <code>ExitMessage<\/code> value<\/p>\n<pre><code>aws sagemaker describe-processing-job \\\n  --processing-job-name foo \\\n  --output text \\\n  --query ExitMessage\n<\/code><\/pre>\n<\/li>\n<li><p>Add a new output to your processing job and send data there<\/p>\n<\/li>\n<li><p>If your <code>train_data<\/code> is in CSV, JSON, or Parquet then use an <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/s3api\/select-object-content.html\" rel=\"nofollow noreferrer\">S3 Select query<\/a> on <code>train_data<\/code> for it's # of rows\/columns<\/p>\n<pre><code>aws s3api select-object-content \\\n  --bucket foo \\\n  --key 'path\/to\/train_data.csv' \\\n  --expression &quot;SELECT count(*) FROM s3object&quot; \\\n  --expression-type 'SQL' \\\n  --input-serialization '{&quot;CSV&quot;: {}}' \\\n  --output-serialization '{&quot;CSV&quot;: {}}' \/dev\/stdout\n<\/code><\/pre>\n<\/li>\n<\/ol>\n<p>Set <code>expression<\/code> to <code>select * from s3object limit 1<\/code> to get the columns<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1654189401480,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":16.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":126.8551813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Notebook on Azure Machine Learning Studio and I am trying to install latest version of matplotlib, but it continues using version 3.2.1 I tried conda, pip, and also tried to uninstall and install again, nothing works.\nsample commands tried:<\/p>\n<pre><code>!pip install matplotlib\n!conda install matplotlib\n<\/code><\/pre>\n<p>Am I missing sth? thanks<\/p>",
        "Challenge_closed_time":1659334721720,
        "Challenge_comment_count":6,
        "Challenge_created_time":1658878043067,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to upgrade matplotlib beyond version 3.2.1 on Azure Machine Learning Studio despite trying various methods such as conda, pip, and uninstalling and reinstalling. They are seeking assistance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73130723",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":7.1,
        "Challenge_reading_time":5.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":126.8551813889,
        "Challenge_title":"matplotlib does not upgrade beyound 3.2.1",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525124885907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation_count":65.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>The version of <strong>Matplotlib<\/strong> will depend on the version of python. There is no support for the latest version of matplotlib library for <strong>python version 3.8<\/strong>. The latest version of matplotlib can be available only up to <strong>3.7 version<\/strong> of python. Even though we tried to upgrade it with <strong>pip, conda<\/strong> , by default <strong>version of python program<\/strong>, it will take the supporting version of matplotlib.<\/p>\n<p>For the current version of matplotlib in the current case of the situation, it is <strong>3.2<\/strong>. It is suggestable to check the version of python, whether it is 3.6 or not. <strong>Python version 3.6 supports 3.2 and 3.1 versions of matplotlib<\/strong>.<\/p>\n<p>If still the python version is showing the latest version of python, try to <strong>degrade<\/strong> the version and check.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.7,
        "Solution_reading_time":10.93,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":125.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1662621266503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":48.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":0.6069663889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been using SageMaker for a while and have performed several experiments already with distributed training. I am wondering if it is possible to test and run SageMaker distributed training in local mode (using SageMaker Notebook Instances)?<\/p>",
        "Challenge_closed_time":1662652096572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662649911493,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is familiar with SageMaker and has performed distributed training experiments. They are now seeking to know if it is possible to test and run SageMaker distributed training in local mode using SageMaker Notebook Instances.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73651368",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.6069663889,
        "Challenge_title":"SageMaker Distributed Training in Local Mode (inside Notebook Instances)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":19.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1662649653072,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>No, not possible yet. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">local mode<\/a> does not support the distributed training with <code>local_gpu<\/code>for Gzip compression, Pipe Mode, or manifest files for inputs<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.6,
        "Solution_reading_time":3.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.843735,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I am trying to create an Azure ML Environment using a Dockerfile but it contains the 'COPY' instruction.    <\/p>\n<p>From the documentation of Environment.from_dockerfile ( <a href=\"https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-\">https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-<\/a> ), I can not find a way to give it some files along with the Dockerfile itself.    <\/p>\n<p>So, how to pass context to enable using COPY in the Dockerfile ?    <\/p>\n<p>Thank you for your time !<\/p>",
        "Challenge_closed_time":1634774742763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634739305317,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges while creating an Azure ML Environment using a Dockerfile that contains the 'COPY' instruction. The user is unable to find a way to give files along with the Dockerfile to enable using COPY in the Dockerfile.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/597612\/(azure)(ml)(python-sdk)(environment)(docker)-docke",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.843735,
        "Challenge_title":"[Azure][ML][Python SDK][Environment][Docker] Docker copy missing context",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Docker context is not supported with AzureML Python SDK at the moment. Context support will added later this year<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1429811498812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":1910.0,
        "Answerer_view_count":316.0,
        "Challenge_adjusted_solved_time":17.1826736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a pre-trained ML model (saved as .h5 file) to Azure ML. I have created an AKS cluster and trying to deploy the model as shown below:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\n\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\n\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\nenv = Environment.get(workspace, name='AzureML-TensorFlow-1.13-GPU')\n\n# Installing packages present in my requirements file\nwith open('requirements.txt') as f:\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\ndependencies.append(&quot;azureml-defaults&gt;=1.0.45&quot;)\n\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=dependencies)\n\n# Including the source folder so that all helper scripts are included in my deployment\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# Deployment with suitable config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=4, memory_gb=32)\nmodel = Model(workspace, 'sketch-inference')\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<p>My main entry script requires some additional helper scripts, which I include by mentioning the source folder in my inference config.<\/p>\n<p>I was expecting that the helper scripts I add should be able to access the packages installed while setting up the environment during deployment, but I get ModuleNotFoundError.<\/p>\n<p>Here is the error output, along with the a couple of environment variables I printed while executing entry script:<\/p>\n<pre><code>    AZUREML_MODEL_DIR ----  azureml-models\/sketch-inference\/1\n    PYTHONPATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages:\/var\/azureml-server:\n    PATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/bin:\/opt\/miniconda\/bin:\/usr\/local\/nvidia\/bin:\/usr\/local\/cuda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/opt\/intel\/compilers_and_libraries\/linux\/mpi\/bin64\n    Exception in worker process\n    Traceback (most recent call last):\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n        worker.init_process()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n        self.load_wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n        self.wsgi = self.app.wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n        self.callable = self.load()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n        return self.load_wsgiapp()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n        return util.import_app(self.app_uri)\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n        __import__(module)\n    File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n        import create_app\n    File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n        from app import main\n    File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n        from aml_blueprint import AMLBlueprint\n    File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 25, in &lt;module&gt;\n        import main\n    File &quot;\/var\/azureml-app\/main.py&quot;, line 12, in &lt;module&gt;\n        driver_module_spec.loader.exec_module(driver_module)\n    File &quot;\/structure\/azureml-app\/ProcessImage\/app.py&quot;, line 16, in &lt;module&gt;\n        from ProcessImage.samples.coco.inference import run as infer\n    File &quot;\/var\/azureml-app\/ProcessImage\/samples\/coco\/inference.py&quot;, line 1, in &lt;module&gt;\n        import skimage.io\n    ModuleNotFoundError: No module named 'skimage'\n<\/code><\/pre>\n<p>The existing answers related to this aren't of much help. I believe there must be a simpler way to fix this, since AzureML specifically provides the feature to setup environment with pip\/conda packages installed either by supplying requirements.txt file or individually.<\/p>\n<p>What am I missing here? Kindly help.<\/p>",
        "Challenge_closed_time":1606249620652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606187763027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a pre-trained ML model to Azure ML using an AKS cluster. They have created an environment with required packages installed and included helper scripts in the deployment. However, the helper scripts are unable to access the installed packages, resulting in a ModuleNotFoundError. The user is seeking a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64979752",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":67.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":17.1826736111,
        "Challenge_title":"Unable to access python packages installed in Azure ML",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1381.0,
        "Challenge_word_count":396,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429811498812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA, USA",
        "Poster_reputation_count":1910.0,
        "Poster_view_count":316.0,
        "Solution_body":"<p>So, after some trial and error, creating a fresh environment and then adding the packages solved the problem for me. I am still not clear on why this didn't work when I tried to use <a href=\"http:\/\/from%20azureml.core%20import%20Workspace%20from%20azureml.core.model%20import%20Model%20from%20azureml.core.environment%20import%20Environment,%20DEFAULT_GPU_IMAGE%20from%20azureml.core.conda_dependencies%20import%20CondaDependencies%20from%20azureml.core.model%20import%20InferenceConfig%20from%20azureml.core.webservice%20import%20AksWebservice,%20LocalWebservice%20from%20azureml.core.compute%20import%20ComputeTarget%20%20%20#%201.%20Instantiate%20the%20workspace%20workspace%20=%20Workspace.from_config(path=%22config.json%22)%20%20#%202.%20Setup%20the%20environment%20env%20=%20Environment(%27sketchenv%27)%20with%20open(%27requirements.txt%27)%20as%20f:%20#%20Fetch%20all%20dependencies%20as%20a%20list%20%20%20%20%20dependencies%20=%20f.readlines()%20dependencies%20=%20%5Bx.strip()%20for%20x%20in%20dependencies%20if%20%27#%20%27%20not%20in%20x%5D%20env.docker.base_image%20=%20DEFAULT_GPU_IMAGE%20env.python.conda_dependencies%20=%20CondaDependencies.create(conda_packages=%5B%27numpy==1.17.4%27,%20%27Cython%27%5D,%20pip_packages=dependencies)%20%20#%203.%20Inference%20Config%20inference_config%20=%20InferenceConfig(entry_script=%27app.py%27,%20environment=env,%20source_directory=%27.\/ProcessImage%27)%20%20#%204.%20Compute%20target%20(using%20existing%20cluster%20from%20the%20workspacke)%20aks_target%20=%20ComputeTarget(workspace=workspace,%20name=%27sketch-ppt-vm%27)%20%20#%205.%20Deployment%20config%20deployment_config%20=%20AksWebservice.deploy_configuration(cpu_cores=6,%20memory_gb=100)%20%20#%206.%20Model%20deployment%20model%20=%20Model(workspace,%20%27sketch-inference%27)%20#%20Registered%20model%20(which%20contains%20model%20files\/folders)%20service%20=%20Model.deploy(workspace,%20%22process-sketch-dev%22,%20%5Bmodel%5D,%20inference_config,%20deployment_config,%20deployment_target=aks_target,%20overwrite=True)%20service.wait_for_deployment(show_output%20=%20True)%20print(service.state)\" rel=\"nofollow noreferrer\">Environment.from_pip_requirements()<\/a>. A detailed answer in this regard would be interesting to read.<\/p>\n<p>My primary task was inference - object detection given an image, and we have our own model developed by our team. There are two types of imports I wanted to have:<\/p>\n<p><strong>1. Standard python packages (installed through pip)<\/strong><br \/>\nThis was solved by creating conda dependencies and add it to env object (Step 2)<\/p>\n<p><strong>2. Methods\/vars from helper scripts<\/strong> (if you have pre\/post processing to be done during model inference):<br \/>\nThis was done by mentioning <code>source_directory<\/code> in InferenceConfig (step 3)<\/p>\n<p>Here is my updated script which combines Environment creation, Inference and Deployment configs and using existing compute in the workspace (created through portal).<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\n\n# 1. Instantiate the workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# 2. Setup the environment\nenv = Environment('sketchenv')\nwith open('requirements.txt') as f: # Fetch all dependencies as a list\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\nenv.docker.base_image = DEFAULT_GPU_IMAGE\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=['numpy==1.17.4', 'Cython'], pip_packages=dependencies)\n\n# 3. Inference Config\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\n# 4. Compute target (using existing cluster from the workspacke)\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# 5. Deployment config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=6, memory_gb=100)\n\n# 6. Model deployment\nmodel = Model(workspace, 'sketch-inference') # Registered model (which contains model files\/folders)\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<hr \/>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":25.5,
        "Solution_reading_time":63.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":271.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.8833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\u00a0\n\nI'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.\u00a0 I have enabled\u00a0aiplatform.googleapis.com.\n\nCommand:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifier\n\nUsing endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.\n\nPlease suggest what am I missing.",
        "Challenge_closed_time":1661575080000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661561100000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create a Vertex AI endpoint from a workbench notebook. The error message states that the project is not active, even though the user has enabled aiplatform.googleapis.com. The user is seeking suggestions on what they might be missing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project\/m-p\/460565#M543",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.8833333333,
        "Challenge_title":"Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":322.0,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":3.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":51.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1529408888483,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":79.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":2.6206047222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am getting the following error message when trying mlflow examples and running 'mlflow ui'.<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError: No module named 'fcntl' Running the mlflow server\n  failed. Please see the logs above for details<\/p>\n<\/blockquote>\n\n<p>Is anyone aware of a solution to this issue?<\/p>\n\n<p>I have tried the solutions suggested at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/1080\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/pull\/1080<\/a><\/p>\n\n<p>without success. Replacing the modified files in mlflow source code, it raises other issues for not finding what it is looking for with the following:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\thesis_mlflow\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\click\\core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\cli.py\", line 198, in ui\n    _run_server(backend_store_uri, default_artifact_root, \"127.0.0.1\", port, None, 1)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 90, in _run_server\n    exec_cmd(full_command, env=env_map, stream_output=True)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\site-packages\\mlflow\\utils\\process.py\", line 34, in exec_cmd\n    stdin=subprocess.PIPE, **kwargs)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 729, in __init__\n    restore_signals, start_new_session)\n  File \"c:\\programdata\\anaconda3\\envs\\thesis_mlflow\\lib\\subprocess.py\", line 1017, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n<\/code><\/pre>",
        "Challenge_closed_time":1562919127687,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562833873410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError: No module named 'fcntl'\" error when trying to run 'mlflow ui' on Windows. They have tried the solutions suggested on GitHub but have not been successful. When replacing the modified files in mlflow source code, it raises other issues for not finding what it is looking for.",
        "Challenge_last_edit_time":1562909693510,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56984854",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.6,
        "Challenge_reading_time":35.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":23.6817436111,
        "Challenge_title":"fcntl error with \u201cmlflow ui\u201d on windows - mlflow 1.0",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":725.0,
        "Challenge_word_count":213,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529408888483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":79.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Just solved the issue: for some reason, waitress was not installed in the running environment. After installing it, everything seems working fine with the solution #1080 linked above in the question.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1546431264350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Norway",
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":55.8339541667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are trying to develop an MLflow pipeline. We have our developing environment in a series of dockers (no local python environment &quot;whatsoever&quot;). This means that we have set up a docker container with MLflow and all requirements necessary to run pipelines. The issue we have is that when we write our MLflow project file we need to use &quot;docker_env&quot; to specify the environment. This figure illustrates what we want to achieve:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tLuw.jpg\" rel=\"nofollow noreferrer\">MLflow run dind<\/a><\/p>\n<p>MLflow inside the docker needs to access the docker daemon\/service so that it can either use the &quot;docker-image&quot; in the MLflow project file or pull it from docker hub. We are aware of the possibility of using &quot;conda_env&quot; in the MLflow project file but wish to avoid this.<\/p>\n<p>Our question is,<\/p>\n<p>Do we need to set some sort of &quot;docker in docker&quot; solution to achieve our goal?<\/p>\n<p>Is it possible to set up the docker container in which MLflow is running so that it can access the &quot;host machine&quot; docker daemon?<\/p>\n<p>I have been all over Google and MLflow's documentation but I can seem to find anything that can guide us. Thanks a lot in advance for any help or pointers!<\/p>",
        "Challenge_closed_time":1640385689187,
        "Challenge_comment_count":4,
        "Challenge_created_time":1640160788793,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to develop an MLflow pipeline within a docker container and needs to use \"docker_env\" to specify the environment in the MLflow project file. However, MLflow inside the docker needs to access the docker daemon\/service to use the \"docker-image\" in the project file or pull it from docker hub. The user is wondering if they need to set up a \"docker in docker\" solution or if it's possible to set up the docker container to access the host machine's docker daemon.",
        "Challenge_last_edit_time":1640184686952,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70445997",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":9.0,
        "Challenge_reading_time":17.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":62.4723316667,
        "Challenge_title":"MLflow run within a docker container - Running with \"docker_env\" in MLflow project file",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":779.0,
        "Challenge_word_count":211,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546431264350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I managed to create my pipeline using docker and docker_env in MLflow. It is not necessary to run d-in-d, the &quot;sibling approach&quot; is sufficient. This approach is described here:<\/p>\n<p><a href=\"https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/\" rel=\"nofollow noreferrer\">https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/<\/a><\/p>\n<p>and it is the preferred method to avoid d-in-d.<\/p>\n<p>One needs to be very careful when mounting volumes within the primary and secondary docker environments: all volume mounts happen in the host machine.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.87,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1572208191120,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":4337.3736888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm currently using Sagemaker notebook instance (not from Sagemaker Studio), and I want to run a notebook that is expected to take around 8 hours to finish. I want to leave it overnight, and see the output from each cell, the output is a combination of print statements and plots.<\/p>\n<p>Howevever, when I start running the notebook and make sure the initial cells run, I close the Jupyterlab tab in my browser, and some minutes after, I open it again to see how is it going, but the notebook is stopped.<\/p>\n<p>Is there any way where I can still use my notebook as it is, see the output from each cell (prints and plots) and do not have to keep the Jupyterlab tab open (turn my laptop off, etc)?<\/p>",
        "Challenge_closed_time":1662537228663,
        "Challenge_comment_count":3,
        "Challenge_created_time":1646922683383,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with running a Sagemaker notebook instance and being able to close the tab while still being able to see the output from each cell, which includes print statements and plots. The notebook stops running when the user closes the Jupyterlab tab in their browser, and they are looking for a solution to be able to leave the notebook running overnight without having to keep the tab open.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71425842",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":10.5,
        "Challenge_reading_time":9.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4337.3736888889,
        "Challenge_title":"Run Sagemaker notebook instance and be able to close tab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1154.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572208191120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":139.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>Answering my own question.<\/p>\n<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-run-notebook\" rel=\"nofollow noreferrer\">Sagemaker run notebook<\/a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.<\/p>\n<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.<\/p>\n<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.3,
        "Solution_reading_time":9.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":126.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.5077777778,
        "Challenge_answer_count":2,
        "Challenge_body":"Is there a solution for multi-user Notebook on Studio Notebook or Notebook Instances? Eg if we want several developers to interact on the same notebook at the same time",
        "Challenge_closed_time":1593009773000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592989945000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a solution to enable multiple developers to work on the same notebook simultaneously on SageMaker's Studio Notebook or Notebook Instances.",
        "Challenge_last_edit_time":1668624634692,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU4mxRvXy2QYmkYCvdqNVa2g\/is-there-a-solution-for-multi-user-notebook-on-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":2.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":5.5077777778,
        "Challenge_title":"Is there a solution for multi-user Notebook on SageMaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":881.0,
        "Challenge_word_count":37,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Notebook instances are not connected to the user. So if two users has the same access rights they will see and will be able to access the same instance (even in the same time). \n\nThe issue is - Jupyter Notebook is not ready for that, both users will have the same privileges, no tracking who did what, ... And working on the same notebook on the same time - basically they will overwrite each other saves.\n\nI had a need for similar thing (pair programming - data scientist and software engineer) - the only viable solution we were able to find was desktop sharing (like TeamViewer, ...)",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925546486,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":7.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":101.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1390832932187,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":249.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":19.5804397222,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Maybe I misunderstand the purpose of packaging but it doesn't seem to helpful in creating an artifact for production deployment because it only packages code. It leaves out the conf, data, and other directories that make the kedro project reproducible.<\/p>\n\n<p>I understand that I can use docker or airflow plugins for deployment but what about deploying to databricks. Do you have any advice here?<\/p>\n\n<p>I was thinking about making a wheel that could be installed on the cluster but I would need to package the conf first. Another option is to just sync a git workspace to the cluster and run kedro via a notebook.<\/p>\n\n<p>Any thoughts on a best practice?<\/p>",
        "Challenge_closed_time":1579581192127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579548059213,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in deploying a Kedro project to Databricks as packaging only packages code and leaves out important directories like conf and data. The user is seeking advice on the best practice for deploying Kedro to Databricks and is considering options like creating a wheel that includes conf or syncing a git workspace to the cluster and running Kedro via a notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59829640",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.7,
        "Challenge_reading_time":8.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":9.2035872222,
        "Challenge_title":"Kedro deployment to databricks",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1705.0,
        "Challenge_word_count":114,
        "Platform":"Stack Overflow",
        "Poster_created_time":1283979840287,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1152.0,
        "Poster_view_count":101.0,
        "Solution_body":"<p>If you are not using <code>docker<\/code> and just using kedro to deploy directly on a databricks cluster. This is how we have been deploying kedro to databricks.<\/p>\n\n<ol>\n<li><p>CI\/CD pipeline builds using <code>kedro package<\/code>. Creates a wheel file.<\/p><\/li>\n<li><p>Upload <code>dist<\/code> and <code>conf<\/code> to dbfs or AzureBlob file copy (if using Azure Databricks)<\/p><\/li>\n<\/ol>\n\n<p>This will upload everything to databricks on every <code>git push<\/code><\/p>\n\n<p>Then you can have a notebook with the following:<\/p>\n\n<ol>\n<li>You can have an init script in databricks something like:<\/li>\n<\/ol>\n\n<pre><code>from cargoai import run\nfrom cargoai.pipeline import create_pipeline\n\nbranch = dbutils.widgets.get(\"branch\")\n\nconf = run.get_config(\n    project_path=f\"\/dbfs\/project_name\/build\/cicd\/{branch}\"\n)\ncatalog = run.create_catalog(config=conf)\npipeline = create_pipeline()\n\n<\/code><\/pre>\n\n<p>Here <code>conf<\/code>, <code>catalog<\/code>, and <code>pipeline<\/code> will be available<\/p>\n\n<ol start=\"2\">\n<li><p>Call this init script when you want to run a branch or a <code>master<\/code> branch in production like: <br\/><code>%run \"\/Projects\/InitialSetup\/load_pipeline\" $branch=\"master\"<\/code><\/p><\/li>\n<li><p>For development and testing, you can run specific nodes<br\/><code>pipeline = pipeline.only_nodes_with_tags(*tags)<\/code><\/p><\/li>\n<li><p>Then run a full or a partial pipeline with just <code>SequentialRunner().run(pipeline, catalog)<\/code><\/p><\/li>\n<\/ol>\n\n<p>In production, this notebook can be scheduled by databricks. If you are on Azure Databricks, you can use <code>Azure Data Factory<\/code> to schedule and run this. <\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1579618548796,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":21.29,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":183.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1545360696800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Earth",
        "Answerer_reputation_count":1011.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":1.6492480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to set up a startup lifecycle configuration for a SageMaker sketchbook (which just ends up being a .sh file), and it seems like, regardless of what I do, my notebooks timeout on startup. I simplified everything as much as possible, to the point of commenting out all but <code>#!\/bin\/bash<\/code>, and I still get a timeout. Checking cloudwatch this shows up in the log:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnStart_2020-08-11-07-01jgfhhkwa: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>through testing, I also found that if I add a carriage return before <code>#!\/bin\/bash<\/code> I get this in the log:<\/p>\n<pre><code>\/tmp\/OnStart_2020-08-11-06-444y3fobzp: line 1: $'\\r': command not found\n<\/code><\/pre>\n<p>based on <a href=\"https:\/\/askubuntu.com\/questions\/966488\/how-do-i-fix-r-command-not-found-errors-running-bash-scripts-in-wsl\">this on the \\r error<\/a>, and <a href=\"https:\/\/stackoverflow.com\/questions\/14219092\/bash-script-and-bin-bashm-bad-interpreter-no-such-file-or-directory\">this on the ^M error<\/a>, this seems to be an incompatibility between windows and unix formatted text. However, I'm editing the lifecycle configuration through aws on my windows machine:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/A5oiU.png\" alt=\"screenshot of editing the lifecycle config\" \/><\/a><\/p>\n<p>is there some way that I can edit this field on my windows machine through AWS, but it be properly written in unix on the other end?<\/p>",
        "Challenge_closed_time":1597164941183,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597159003890,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in setting up a startup lifecycle configuration for a SageMaker sketchbook on Windows. The notebooks are timing out on startup, and the logs show errors related to incompatibility between Windows and Unix formatted text. The user is editing the lifecycle configuration through AWS on their Windows machine and is seeking a solution to properly write the configuration in Unix format.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63361229",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.6492480556,
        "Challenge_title":"How do you write lifecycle configurations for SageMaker on windows?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":608.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>This is, indeed, to do with special character representation in different os' based on <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/issues\/8\" rel=\"nofollow noreferrer\">this<\/a> you can use notepad++ to easily convert the dos representation a unix representation, then just &quot;paste as plain text&quot;, and it works fine<\/p>\n<ul>\n<li>copy to notepad++ view<\/li>\n<li>show symbol<\/li>\n<li>show all symbols<\/li>\n<li>replace &quot;\/r&quot; with nothing CRLF should become LF which is valid in unix<\/li>\n<li>copy and paste as plain text<\/li>\n<\/ul>\n<p>Doing this fixed the problem<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":20.5,
        "Solution_reading_time":8.29,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5700419445,
        "Challenge_answer_count":1,
        "Challenge_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Challenge_closed_time":1652688680111,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652686627960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring Sagemaker Built-in algorithms and is having difficulty finding research background and implementation details for specific algorithms. They are seeking help in finding this information.",
        "Challenge_last_edit_time":1668094438488,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":5.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.5700419445,
        "Challenge_title":"Sagemaker Built-in Algorithms",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\n - **BlazingText**: *[BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs](https:\/\/dl.acm.org\/doi\/10.1145\/3146347.3146354)*, Gupta et Khare\n - **[DeepAR](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar_how-it-works.html)** *[DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks](https:\/\/arxiv.org\/abs\/1704.04110)*, Salinas et al.\n - **[Factorization Machines](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines-howitworks.html)**\n - **[IP Insights](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ip-insights-howitworks.html)**\n - **[KMeans](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algo-kmeans-tech-notes.html)**\n - **[KNN](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/kNN_how-it-works.html)**\n - **[LDA](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lda-how-it-works.html)**\n - **[Linear Learner](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html)**\n - **[NTM](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html)**\n - **[Object2Vec](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object2vec-howitworks.html)**\n - **[Object Detection](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algo-object-detection-tech-notes.html)** (it's an SSD model)\n - **[PCA](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-pca-works.html)**\n - **[Random Cut Forest](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rcf_how-it-works.html)**: *[Robust Random Cut Forest Based Anomaly Detection On Streams](https:\/\/proceedings.mlr.press\/v48\/guha16.pdf)*, Guha et al\n - **[Semantic Segmentation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html)**\n - **[Seq2seq](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/seq-2-seq-howitworks.html)**\n - **[XGBoost](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-HowItWorks.html)**",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1652688680112,
        "Solution_link_count":18.0,
        "Solution_readability":37.8,
        "Solution_reading_time":28.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.7587716667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello! I'm newbie here, so sorry if i'm askinf some silly questions!  <\/p>\n<p>I'm trying to submit a job using Azure ML in Environment which is based on my docker container which is stored in Azure Container Registry.  So the problem is that my program doesnt start in the chosen docker, it starts somewhere else as I see it. The thing is that using code below it actually creates an Environment which builds provided docker image and then in this Environment (on Azure Portal) I can see docker logs and etc, and if I want to view this Environment in code editor I can also see correct docker: <\/p>\n<pre><code>{\n    &quot;assetId&quot;: null,\n    &quot;databricks&quot;: {\n        &quot;eggLibraries&quot;: [],\n        &quot;jarLibraries&quot;: [],\n        &quot;mavenLibraries&quot;: [],\n        &quot;pypiLibraries&quot;: [],\n        &quot;rcranLibraries&quot;: []\n    },\n    &quot;docker&quot;: {  #essentially docker which  runs everything locally\n        &quot;arguments&quot;: [],\n        &quot;baseDockerfile&quot;: null,\n        &quot;baseImage&quot;: &quot;somename&quot;,\n        &quot;baseImageRegistry&quot;: {\n            &quot;address&quot;: &quot;somevalue&quot;,\n            &quot;password&quot;: &quot;somevalue&quot;,\n            &quot;registryIdentity&quot;: null,\n            &quot;username&quot;: &quot;somevalue&quot;\n        },\n        &quot;buildContext&quot;: null,\n        &quot;enabled&quot;: true,\n        &quot;platform&quot;: {\n            &quot;architecture&quot;: &quot;amd64&quot;,\n            &quot;os&quot;: &quot;Linux&quot;\n        },\n        &quot;sharedVolumes&quot;: true,\n        &quot;shmSize&quot;: &quot;2g&quot;\n    },\n    &quot;environmentVariables&quot;: {\n        &quot;EXAMPLE_ENV_VAR&quot;: &quot;EXAMPLE_VALUE&quot;\n    },\n    &quot;inferencingStackVersion&quot;: null,\n    &quot;name&quot;: &quot;somename&quot;,\n    &quot;python&quot;: {\n        &quot;baseCondaEnvironment&quot;: null,\n        &quot;condaDependencies&quot;: {\n            &quot;channels&quot;: [\n                &quot;anaconda&quot;,\n                &quot;conda-forge&quot;\n            ],\n            &quot;dependencies&quot;: [\n                &quot;python=3.8.13&quot;,\n                {\n                    &quot;pip&quot;: [\n                        &quot;azureml-defaults&quot;\n                    ]\n                }\n            ],\n            &quot;name&quot;: &quot;project_environment&quot;\n        },\n        &quot;condaDependenciesFile&quot;: null,\n        &quot;interpreterPath&quot;: &quot;python&quot;,\n        &quot;userManagedDependencies&quot;: false\n    },\n    &quot;r&quot;: null,\n    &quot;spark&quot;: {\n        &quot;packages&quot;: [],\n        &quot;precachePackages&quot;: true,\n        &quot;repositories&quot;: []\n    },\n    &quot;version&quot;: null\n}\n\n<\/code><\/pre>\n<p>I use this container  locally to run the code, but for some reason when i'm trying to run the code in Azure ML it doesnt work for me. (it tell me that pandas is not installed, while it is installed in the provided docker) Below you can find code which I'm using. <\/p>\n<pre><code>docker_env = Environment(&quot;example&quot;)\n\ndocker_env.docker.enabled = True\ndocker_env.docker.base_image = docker_image_name\ndocker_env.docker.base_image_registry.address = docker_registry_address\ndocker_env.docker.base_image_registry.username = docker_registry_username\ndocker_env.docker.base_image_registry.password = docker_registry_password\ndocker_env.register(workspace=ws)\n\ncompute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6s_v3',)\n# create the cluster\ncompute_target = ComputeTarget.create(ws, cluster_name, compute_config)\ncompute_target.wait_for_completion(show_output=True)\n\n# Create an experiment\nexperiment_name = 'example'\nexperiment = Experiment(ws, name=experiment_name)\n\n# Set up for training script and parameters\nscript_folder = script_folder\n\n# Instantiate PyTorch estimator with upload of final model to a specified blob storage container (this can be anything)\nestimator = ScriptRunConfig(\n    source_directory=script_folder,\n    compute_target=compute_target,\n    command='.\/train_cloud.sh',\n    environment=docker_env,\n)\n\nrun = experiment.submit(estimator)\nprint(run.get_details())\n<\/code><\/pre>",
        "Challenge_closed_time":1684003046144,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683935514566,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to submit a job using Azure ML in an environment based on their Docker container stored in Azure Container Registry. However, the program does not start in the chosen Docker and starts somewhere else. The user has provided code that creates an environment that builds the provided Docker image, but when running the code in Azure ML, it does not work, and it tells the user that pandas is not installed, even though it is installed in the provided Docker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1283400\/submit-azureml-job-in-docker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":19.5,
        "Challenge_reading_time":49.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":18.7587716667,
        "Challenge_title":"Submit azureml job in docker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":313,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>It seems like your Azure ML job is not recognizing the environment you've defined and hence, not finding the required libraries like Pandas inside your Docker image.<\/p>\n<p>You've registered the environment with docker_env.register(workspace=ws). Ensure that this registration is successful and the environment is indeed available in your workspace.<\/p>\n<p>In the environment definition, userManagedDependencies is set to False. This means Azure ML is expecting to manage the Python dependencies for you. Since you are providing a Docker image with dependencies pre-installed, try setting userManagedDependencies to True:<\/p>\n<pre><code>docker_env.python.userManagedDependencies = True\n<\/code><\/pre>\n<p>You're also providing a conda dependencies file. This could be conflicting with the libraries in your Docker image. If your Docker image already includes all the necessary dependencies, try removing the python field from the environment definition or setting baseCondaEnvironment to None:<\/p>\n<pre><code>docker_env.python.baseCondaEnvironment = None\n\n<\/code><\/pre>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":13.74,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":133.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1579536819712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":18.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":4.6947408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I installed pyarrow using this command &quot;conda install pyarrow&quot;.\nI am running a sagemaker notebook and I am getting the error no module named pyarrow.\nI have python 3.8.3 installed on mac.<\/p>\n<p>I have numpy  1.18.5 , pandas 1.0.5 and pyarrow  0.15.1<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1604344726272,
        "Challenge_comment_count":1,
        "Challenge_created_time":1604343780117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user installed pyarrow using \"conda install pyarrow\" but is encountering an error of \"no module named pyarrow\" while running a sagemaker notebook on a Mac with Python 3.8.3 installed. The user has numpy 1.18.5, pandas 1.0.5, and pyarrow 0.15.1 installed.",
        "Challenge_last_edit_time":1604344440696,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64651724",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.1,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2628208333,
        "Challenge_title":"No Module named pyarrow",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":540.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468599558956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":107.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>I have not yet used AWS Sagemaker notebooks, but they may be similar to GCP 'AI Platform notebooks', which I have used quite extensively. Additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks (whether from command line or from GUI)?<\/p>\n<p>In GCP, I defaulted to using <code>pip install<\/code> for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments.<\/p>\n<p>Assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the <code>CONDA<\/code> tab and selecting which notebook uses which conda environment.\n-Spencer<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1604361341763,
        "Solution_link_count":0.0,
        "Solution_readability":16.1,
        "Solution_reading_time":11.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1389049461896,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":455.0,
        "Answerer_view_count":108.0,
        "Challenge_adjusted_solved_time":3574.5224019445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've so far seen people using tensorflow in Azure using in this <a href=\"http:\/\/www.mikelanzetta.com\/tensorflow-on-azure-using-docker.html\" rel=\"nofollow\">link<\/a>.\nAlso using the advantage of ubuntu in windows tensorflow can be run on\nwindows pc as well.Here is the <a href=\"http:\/\/www.hanselman.com\/blog\/PlayingWithTensorFlowOnWindows.aspx\" rel=\"nofollow\">link<\/a>.\nHowever during a conversation with Windows Azure engineer Hai Ning it came out\nthat \"Azure ML PaaS VMs use Windows OS; TensorFlow is not supported on Windows as of now.\"\nHence there is no direct way of running tensorflow in Azure ML.\nIs there any work around anyone figured out that allows running tensorflow in Azure ML.<\/p>",
        "Challenge_closed_time":1486144727963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1473271529687,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking a way to call Tensorflow in Azure ML, but has been informed by a Windows Azure engineer that it is not currently supported on Windows OS, which is used by Azure ML PaaS VMs. The user is asking if anyone has found a workaround to run Tensorflow in Azure ML.",
        "Challenge_last_edit_time":1473276447316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39376560",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3575.88841,
        "Challenge_title":"How to call Tensorflow in Azure ML",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1743.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1435075201580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":546.0,
        "Poster_view_count":59.0,
        "Solution_body":"<p>Quick update for you. As of TensorFlow r0.12 there is now a native TensorFlow package for Windows. I have it running successfully on my Windows 10 laptop. See this <a href=\"https:\/\/developers.googleblog.com\/2016\/11\/tensorflow-0-12-adds-support-for-windows.html\" rel=\"nofollow noreferrer\">blog post<\/a> from Google for more information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":4.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":39.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.6287383334,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello. I created a Notebook Job Definition, scheduled to run every hour. According to the status it is \"Active\". Whenever I manually trigger the job with the \"Run Job\" button, it creates a new Notebook Job and runs successfully. However, the notebook never runs on the schedule. It should execute every hour, but instead only executes when manually triggered.\n\nIs there anything I need to do for the notebook to obey the schedule? Thanks,",
        "Challenge_closed_time":1675405386807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675370723349,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a Notebook Job Definition scheduled to run every hour, but the notebook never runs on the schedule and only executes when manually triggered. The user is seeking advice on how to make the notebook obey the schedule.",
        "Challenge_last_edit_time":1675718147630,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUjxD8Cf8lTc2Tknbt4MAzFQ\/sagemaker-notebook-doesn-t-run-on-job-schedule",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.6287383334,
        "Challenge_title":"Sagemaker notebook doesn't run on Job schedule",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":90.0,
        "Challenge_word_count":80,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Are there any problems with IAM role settings, etc.?\n\nPlease check this document for reference only.\nhttps:\/\/aws.amazon.com\/jp\/blogs\/machine-learning\/operationalize-your-amazon-sagemaker-studio-notebooks-as-scheduled-notebook-jobs\/",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1675405386807,
        "Solution_link_count":1.0,
        "Solution_readability":21.9,
        "Solution_reading_time":3.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1452426675696,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":77.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":1.7254188889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am writing code in Jupyter Notebook in Azure ML Studio.<\/p>\n\n<p>At current moment every command causes kernel death. Even in new clear notebook I could not execute even <code>print 'hello'<\/code> - kernel died immediately.<\/p>\n\n<p>Also I could not use bash commands like <code>!ls<\/code> - It crashes kernel too.<\/p>\n\n<p>How could I restart my VM or restart session in Azure ML Studio with killing all running VM?<\/p>",
        "Challenge_closed_time":1458573398360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458551625417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues with kernel death while writing code in Jupyter Notebook in Azure ML Studio. They are unable to execute any command, including basic ones like print and bash commands like !ls. The user is seeking guidance on how to restart their VM or session in Azure ML Studio without killing all running VMs.",
        "Challenge_last_edit_time":1458567186852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36126897",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.9,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.0480397222,
        "Challenge_title":"How to restart VM in Azure ML Notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":488.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Poster_created_time":1452426675696,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":77.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I have found that if I go from notebook menu to File->Open, then I see all my notebooks, their statuses and I could shutdown them.<\/p>\n\n<p>Also I have found that some of my closed notebooks were still alive and have shut them down.<\/p>\n\n<p>After this my working notebook came back to life.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":3.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1621410539876,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":3579.0,
        "Answerer_view_count":1775.0,
        "Challenge_adjusted_solved_time":4.1121516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>got a folder called data-asset which contains a yaml file with the following<\/p>\n<pre><code>type: uri_folder\nname: &lt;name_of_data&gt;\ndescription: &lt;description goes here&gt;\npath: &lt;path&gt;\n<\/code><\/pre>\n<p>In a pipeline am referencing this using azure cli inline script using the following command az ml data create -f .yml but getting error<\/p>\n<p>full error-D:\\a\\1\\s\\ETL\\data-asset&gt;az ml data create -f data-asset.yml\nERROR: 'ml' is misspelled or not recognized by the system.<\/p>\n<p>Examples from AI knowledge base:\naz extension add --name anextension\nAdd extension by name<\/p>\n<p>trying to implement this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-data-assets?tabs=CLI<\/a><\/p>\n<p>how can a resolve this?<\/p>",
        "Challenge_closed_time":1659361184080,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659348144177,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Azure CLI not recognizing the command \"az ml data create -f <file-name>.yml\" when trying to reference a YAML file in a folder called \"data-asset\" in a pipeline. The user is receiving an error message stating that \"ml\" is misspelled or not recognized by the system. The user is seeking a resolution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73192053",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":12.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.6221952778,
        "Challenge_title":"azure cli not recognizing the following command az ml data create -f <file-name>.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606756004663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>One of the workaround you can follow to resolve the above issue;<\/p>\n<p>Based on this <a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/21390#issuecomment-1161782243\" rel=\"nofollow noreferrer\"><em><strong>GitHub issue<\/strong><\/em><\/a> as suggested by @<em>adba-msft<\/em> .<\/p>\n<blockquote>\n<p><strong>Please make sure that you have upgraded your azure cli to latest and<\/strong>\n<strong>Azure CLI ML extension v2 is being used.<\/strong><\/p>\n<\/blockquote>\n<p>To check and upgrade the cli we can use the below <code>cmdlts<\/code>:<\/p>\n<pre><code>az version\n\naz upgrade\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Uopde.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Uopde.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>For more information please refer this similar <a href=\"https:\/\/stackoverflow.com\/questions\/73110661\/create-is-misspelled-or-not-recognized-by-the-system-on-az-ml-dataset-create\"><em><strong>SO THREAD|'create' is misspelled or not recognized by the system on az ml dataset create<\/strong><\/em><\/a> .<\/p>\n<p>I did observe the same issue after trying the aforementioned suggestion by @<em>Dor Lugasi-Gal<\/em> it works for me with (in my case <code>az ml -h<\/code>) after installed the extension with  <code>az extension add -n ml -y<\/code> can able to get the result of <code>az ml -h<\/code> without any error.<\/p>\n<p><em><strong>SCREENSHOT FOR REFERENCE:-<\/strong><\/em><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/39LHa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/39LHa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659362947923,
        "Solution_link_count":6.0,
        "Solution_readability":12.5,
        "Solution_reading_time":21.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":161.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1614882423070,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":1.5591075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For some reason the jupyter notebooks on my VM are in the wrong environment (ie stuck in <code>(base)<\/code>). Furthermore, I can change the environment in the terminal but not in the notebook. Here is what happens when I attempt <code>!conda activate desired_env<\/code> in the notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n# conda environments:\n#\nbase                  *  \/anaconda\nazureml_py36             \/anaconda\/envs\/azureml_py36\nazureml_py38             \/anaconda\/envs\/azureml_py38\nazureml_py38_pytorch     \/anaconda\/envs\/azureml_py38_pytorch\nazureml_py38_tensorflow     \/anaconda\/envs\/azureml_py38_tensorflow\n<\/code><\/pre>\n<p>I tried the answers <a href=\"https:\/\/stackoverflow.com\/questions\/61915607\/commandnotfounderror-your-shell-has-not-been-properly-configured-to-use-conda\">here<\/a> (e.g., first running <code>!source \/anaconda\/etc\/profile.d\/conda.sh<\/code>).<\/p>\n<p>I also tried activating the environment using <code>source<\/code> rather than 'conda activate': <code>!source \/anaconda\/envs\/desired_env\/bin\/activate<\/code>. This runs but doesn't actually do anything when I see the current environment in <code>conda env list<\/code><\/p>\n<p>Edit: also adding that if I install a package in the <code>(base)<\/code> environment in the terminal, I still don't have access to it in jupyter notebook.<\/p>",
        "Challenge_closed_time":1636652640790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636646694700,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to change the virtual environment within Azure ML notebook and is stuck in the wrong environment. The user tried changing the environment in the terminal but encountered an error message. The user also tried activating the environment using 'source' but it did not work. Additionally, the user is unable to access packages installed in the (base) environment in the notebook.",
        "Challenge_last_edit_time":1636647028003,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69931411",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.6516916667,
        "Challenge_title":"Can't change virtual environment within Azure ML notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":672.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586979502910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":361.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>I'm the PM that released AzureML Notebooks, you can't activate a Conda env from a cell, you have to create a new kernel will the Conda Env. Here are the instructions: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels<\/a><\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":5.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":14.9379061111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a conda environment called <code>Foo<\/code>. After activating this environment I installed Kedro with <code>pip<\/code>, since <code>conda<\/code> was giving me a conflict. Even though I'm inside the <code>Foo<\/code> environment, when I run:<\/p>\n<pre><code>kedro jupyter lab\n<\/code><\/pre>\n<p>It picks up the modules from my <code>base<\/code> environment, not the <code>Foo<\/code> environment. Any idea, why this is happening, and how I can change what modules my notebook detect?<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>By mangling with my code I found out that on the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> it was calling the python from the base environment, not the <code>Foo<\/code> environment. I changed it manually, but is there a mode automatic way of setting the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> to use the current environment I'm on?<\/p>",
        "Challenge_closed_time":1652879216830,
        "Challenge_comment_count":1,
        "Challenge_created_time":1652795556263,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a conda environment called \"Foo\" and installed Kedro with pip while being inside the \"Foo\" environment. However, when running \"kedro jupyter lab\", the modules are being picked up from the \"base\" environment instead of \"Foo\". The user found out that the kernel.json file was calling the python from the base environment and manually changed it. The user is looking for an automatic way to set the kernel.json file to use the current environment.",
        "Challenge_last_edit_time":1652825440368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72275283",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":23.2390463889,
        "Challenge_title":"Kedro using wrong conda environment",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":115.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423164285360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p>The custom Kedro kernel spec is a feature that I recently added to Kedro. When you run <code>kedro jupyter lab\/notebook<\/code> it should automatically pick up on the conda environment without you needing to manually edit the kernel.json file. I tested this myself to check that it worked so I'm very interested in understanding what's going on here!<\/p>\n<p>The function <a href=\"https:\/\/github.com\/kedro-org\/kedro\/blob\/58c57c384f5257b998edebb99d94bff46574ae1e\/kedro\/framework\/cli\/jupyter.py#L99\" rel=\"nofollow noreferrer\"><code>_create_kernel<\/code><\/a> is what makes the the Kedro kernel spec. The docstring for that explains what's going on, but in short we delegate to <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L92\" rel=\"nofollow noreferrer\"><code>ipykernel.kernelspec.install<\/code><\/a>. This generates a kernelspec that points towards the Python path given by <code>sys.executable<\/code> (see <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L27\" rel=\"nofollow noreferrer\"><code>make_ipkernel_cmd<\/code><\/a>). In theory this should already point towards the correct Python path, which takes account of the conda environment.<\/p>\n<p>It's worth checking <code>which kedro<\/code> to see which conda environment that points to, and if we need to debug further then please do raise an issue on our <a href=\"https:\/\/github.com\/kedro-org\/kedro\" rel=\"nofollow noreferrer\">Github repo<\/a>. I'd definitely like to get to the bottom of this and understand where the problem is.<\/p>\n<p>P.S. you can also do a plain <code>jupyter lab\/notebook<\/code> to launch a kernel with the right conda environment and then run <code>%load_ext kedro.extras.extensions.ipython<\/code> in the first cell. This is basically equivalent to using the Kedro kernelspec, which loads the Kedro IPython extension automatically.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.8,
        "Solution_reading_time":25.37,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":217.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.4508038889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi everyone, I am using wandb with Huggingface in a AWS Sagemaker notebook and I am refering to the tutorial here: <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface\" class=\"inline-onebox\">Hugging Face Transformers | Weights &amp; Biases Documentation<\/a>.<\/p>\n<p>I tried to set the <code>WANDB_PROJECT<\/code> environment variable before setting up the <code>huggingface_estimator<\/code>, which will call <code>train.py<\/code>.<\/p>\n<p><code>train.py<\/code> is where I initialize the <code>Trainer<\/code>. The above tutorial mentions to make sure to set the project name before initializing the <code>Trainer<\/code>, and I think I am doing this correctly here.<\/p>\n<p>Here are some useful snippets of my code.<\/p>\n<pre><code class=\"lang-auto\">import wandb\nwandb.login()\n\nWANDB_PROJECT=my_project_name\n\n...\n\nhuggingface_estimator = HuggingFace(\n  image_uri=image_uri,\n  entry_point='train.py',\n  source_dir='.\/scripts',\n  instance_type='ml.g4dn.xlarge',\n  instance_count=1,\n  role=role,\n  py_version='py39',\n  hyperparameters=hyperparameters,\n)\n<\/code><\/pre>\n<p>train.py<\/p>\n<pre><code class=\"lang-auto\">    training_args = TrainingArguments(\n        output_dir=args.output_dir,\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        num_train_epochs=args.epochs,\n        learning_rate=args.learning_rate,\n        save_strategy=\"epoch\",\n        logging_strategy='epoch',\n        report_to=\"wandb\",\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        data_collator=collate_fn,\n        tokenizer=image_processor,\n    )\n\n    trainer.train()\n<\/code><\/pre>\n<p>I would greatly appreciate any guidance or advice on how to resolve this issue. Thank you very much in advance for your help!<\/p>",
        "Challenge_closed_time":1678799383235,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678765360341,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with setting the WANDB_PROJECT environment variable while using wandb with Huggingface in an AWS Sagemaker notebook. The user has followed the tutorial to set the project name before initializing the Trainer, but the project name is not being set properly. The user has provided code snippets for reference and is seeking guidance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/set-the-wandb-project-environment-variable-cannot-name-the-project-properly\/4055",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":15.9,
        "Challenge_reading_time":23.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":9.4508038889,
        "Challenge_title":"Set the WANDB_PROJECT environment variable cannot name the project properly",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":156,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/oschan77\">@oschan77<\/a> ,<\/p>\n<p>You can set the project name in your script like so:<\/p>\n<pre><code class=\"lang-auto\">import os\nos.environ[\"WANDB_PROJECT\"] = \"sentiment-analysis\"\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":3.03,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1395235213383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Norway",
        "Answerer_reputation_count":46807.0,
        "Answerer_view_count":3021.0,
        "Challenge_adjusted_solved_time":6.7343452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I recently (and sceptically) started messing around with <strong>Azure Machine Learning Studio<\/strong>. When I stumbled accross the menu option for a machine learning work-flow <strong>Open in a new Notebook<\/strong> (For Python 3, 2 or R) I thought it was too good to be true:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AYDbo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AYDbo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And it most likely is, since this option is seemingly only available for the first step of the process. The option still exists in the <strong>right-click menu<\/strong> elsewhere, but it's greyed out:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cy6MG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cy6MG.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does anyone know why it is like this? Do I have to activate something in the menus, or buy some sort of a premium license? Is the functionality only available for <em>some<\/em> of the machine learning algorithms? Or is it just not supposed to be an available option in the menus?<\/p>\n\n<p>By the way, if you click <kbd>Python 3<\/kbd> 3 in the first step, you get a corresponding Python 3 code snippet in a Jupyter Notebook where you immediately can start messing around with the dataset:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/gTHGF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gTHGF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I realize that making this functionality available for each step in each and every model that anyone chooses to design would be an extremely difficult and maybe even impossible thing to do. But again, why is the option still in the menu?<\/p>",
        "Challenge_closed_time":1529068177343,
        "Challenge_comment_count":1,
        "Challenge_created_time":1529043933700,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to open a new Python or R notebook in Azure Machine Learning Studio, as the option is greyed out in the right-click menu. The user is unsure if this is due to a premium license requirement or if the functionality is only available for certain machine learning algorithms. The option is available for the first step of the process, but not for subsequent steps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50870166",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":9.6,
        "Challenge_reading_time":22.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.7343452778,
        "Challenge_title":"Can not open new Python or R notebook in Azure Machine Learning Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1386.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395235213383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":46807.0,
        "Poster_view_count":3021.0,
        "Solution_body":"<p>Following the suggestion from @Jon in the comment section as well as a suggestion on <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/windowsdesktop\/en-US\/84a33ecc-1db2-4d11-83d2-3e96f0bcfaa7\/why-open-in-a-new-notebook-is-invalid?forum=MachineLearning\" rel=\"nofollow noreferrer\">microsoft.com<\/a>, I added a <strong>Convert to CSV Module<\/strong> at the end. After running the experiment, <strong>Open in a new Notebook<\/strong> is available when you right clik the <strong>Convert to CSV Module<\/strong>:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Bz7nJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Bz7nJ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What you get by clicking <kbd>Python 3<\/kbd> is this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ywbHz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ywbHz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The functionality is certainly not as magnificent as I was hoping, but it's still pretty cool. If anyone knows <em>anything<\/em> about other possibilites or plans for future development, please don't hesitate to contribute with an answer!<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.2,
        "Solution_reading_time":15.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":112.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.3555730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to create my own Kedro starter. I have tried to replicate the relevant portions of the pandas iris starter. I have a <code>cookiecutter.json<\/code> file with what I believe are appropriate mappings, and I have changed the repo and package directory names as well as any references to Kedro version such that they work with cookie cutter.<\/p>\n<p>I am able to generate a new project from my starter with <code>kedro new --starter=path\/to\/my\/starter<\/code>. <strong>However, the newly created project uses the default values for the project, package, and repo names, without prompting me for any input in the terminal<\/strong>.<\/p>\n<p>Have I misconfigured something? How can I create a starter that will prompt users to override the defaults when creating new projects?<\/p>\n<p>Here are the contents of <code>cookiecutter.json<\/code> in the top directory of my starter project:<\/p>\n<pre><code>{\n    &quot;project_name&quot;: &quot;default&quot;,\n    &quot;repo_name&quot;: &quot;{{ cookiecutter.project_name }}&quot;,\n    &quot;python_package&quot;: &quot;{{ cookiecutter.repo_name }}&quot;,\n    &quot;kedro_version&quot;: &quot;{{ cookiecutter.kedro_version }}&quot;\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1641488156140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641486876077,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create their own Kedro starter by replicating the relevant portions of the pandas iris starter. They have a cookiecutter.json file with appropriate mappings and have changed the repo and package directory names. However, when they generate a new project from their starter, it uses default values for project, package, and repo names without prompting for any input in the terminal. The user is seeking help to create a starter that will prompt users to override the defaults when creating new projects.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70610418",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":15.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.3555730556,
        "Challenge_title":"Why doesn't my Kedro starter prompt for input?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416091573812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Texas, USA",
        "Poster_reputation_count":197.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>I think you may be missing <code>prompts.yml<\/code>\n<a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml<\/a><\/p>\n<p>Full instructions can be found here:\n<a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":38.0,
        "Solution_reading_time":7.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":21.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1447151270223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":215.0267602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I created a docker image for training. In the dockerfile I have an entrypoint defined such that when <code>docker run<\/code> is executed, it will start running my python code.\nTo use this on aws sagemaker in my understanding I need to create a pytorch estimator in a jupyter notebook in sagemaker. I tried something like this:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nsagemaker_session = sagemaker.Session()\n\nrole = sagemaker.get_execution_role()\n\nestimator = PyTorch(entry_point='train.py',\n                    role=role,\n                    framework_version='1.3.1',\n                    image_name='xxx.ecr.eu-west-1.amazonaws.com\/xxx:latest',\n                    train_instance_count=1,\n                    train_instance_type='ml.p3.xlarge',\n                    hyperparameters={})\n\nestimator.fit({})\n\n<\/code><\/pre>\n\n<p>In the documentation I found that as image name I can specify the link the my docker image on aws ecr. When I try to execute this it keeps complaining<\/p>\n\n<pre><code>[Errno 2] No such file or directory: 'train.py'\n<\/code><\/pre>\n\n<p>It complains immidiatly, so surely I am doing something completely wrong. I would expect that first my docker image should run, and than it could find out that the entry point does not exist.<\/p>\n\n<p>But besides this, why do I need to specify an entry point, as in, should it not be clear that the entry to my training is simply <code>docker run<\/code>?<\/p>\n\n<p>For maybe better understanding. The entrypoint python file in my docker image looks like this:<\/p>\n\n<pre><code>if __name__=='__main__':\n    parser = argparse.ArgumentParser()\n\n    # Hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--batch_size', type=int, default=16)\n    parser.add_argument('--learning_rate', type=float, default=0.0001)\n\n    # Data and output directories\n    parser.add_argument('--output_data_dir', type=str, default=os.environ['OUTPUT_DATA_DIR'])\n    parser.add_argument('--train_data_path', type=str, default=os.environ['CHANNEL_TRAIN'])\n    parser.add_argument('--valid_data_path', type=str, default=os.environ['CHANNEL_VALID'])\n\n    # Start training\n    ...\n<\/code><\/pre>\n\n<p>Later I would like to specify the hyperparameters and data channels. But for now I simply do not understand what to put as entry point. In the documentation it says that the entrypoint is required and it should be a local\/global path to the entrypoint...<\/p>",
        "Challenge_closed_time":1579270087087,
        "Challenge_comment_count":3,
        "Challenge_created_time":1578494679740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created a custom docker image for training and defined an entrypoint in the Dockerfile. They are trying to use this image on AWS Sagemaker by creating a PyTorch estimator in a Jupyter notebook. However, when executing the code, they receive an error message stating that the entry point does not exist. The user is confused about why they need to specify an entry point and what to put as the entry point.",
        "Challenge_last_edit_time":1578495990750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59648275",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.7,
        "Challenge_reading_time":31.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":215.3909297222,
        "Challenge_title":"What to define as entrypoint when initializing a pytorch estimator with a custom docker image for training on AWS Sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":300,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447151270223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":141.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>If you really would like to use a complete separate by yourself build docker image, you should create an Amazon Sagemaker algorithm (which is one of the options in the Sagemaker menu). Here you have to specify a link to your docker image on amazon ECR as well as the input parameters and data channels etc. When choosing this options, you should <strong>not<\/strong> use the PyTorch estimater but the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/algorithm.html\" rel=\"nofollow noreferrer\">Algoritm estimater<\/a>. This way you indeed don't have to specify an entrypoint because it simple runs the docker when training and the default entrypoint can be defined in your docker file.<\/p>\n\n<p>The Pytorch estimator can be used when having you own model code, but you would like to run this code in an off-the-shelf Sagemaker PyTorch docker image. This is why you have to for example specify the PyTorch framework version. In this case the entrypoint file by default should be placed next to where your jupyter notebook is stored (just upload the file by clicking on the upload button). The PyTorch estimator inherits all options from the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"nofollow noreferrer\">framework estimator<\/a> where options can be found where to place the entrypoint and model, for example <em>source_dir<\/em>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.3,
        "Solution_reading_time":17.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":200.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6838888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nI am trying to launch an endpoint locally, to do couple inferences from my dev notebook (without having to wait for instanciation time of actual endpoint or batch training). I am running the following code:\n\n    # get trained model from s3\n    trained_S2S = SM.model.Model(\n        image=seq2seq,\n        model_data=('s3:\/\/XXXXXXXXXXXXX\/'\n            + 'output\/seq2seq-2018-07-30-16-55-12-521\/output\/model.tar.gz'),\n        role=role) \n    \n    S2S = trained_S2S.deploy(1, instance_type='local')    \n\nI get the following error (several hundreds of lines repeated):\n\n    WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa5c61eaac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': \/ping \n`RuntimeError: Giving up, endpoint: seq2seq-2018-08-01-14-18-06-555 didn't launch correctly`",
        "Challenge_closed_time":1533135863000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533133401000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"RuntimeError: Giving up, endpoint: didn't launch correctly\" error while trying to launch an endpoint locally to perform inferences from their dev notebook. The error message shows that the connection was broken and the endpoint failed to launch correctly.",
        "Challenge_last_edit_time":1668556657875,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU54PWM3V9QoybCiO5GvB03g\/sagemaker-local-deployment-runtimeerror-giving-up-endpoint-didn-t-launch-correctly",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":13.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.6838888889,
        "Challenge_title":"Sagemaker local deployment: \"RuntimeError: Giving up, endpoint: didn't launch correctly\"",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":431.0,
        "Challenge_word_count":102,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Currently, SageMaker local is supported only for SageMaker framework containers (MXNet, TensorFlow, PyTorch, Chainer and Spark) and not for the Builtin algorithms",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925544815,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":247.5842388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>I have a Kubernetes Service attached as an inference cluster to an azure machine learning workspace. I have deployed multiple models to that the AKS service, each with their own endpoints. I plan to configure this such that I just need to send the request to one main endpoint, which after applying some conditions, will redirect the request to one of the endpoints (e.g. redirect the request to the appropriate model). Are there any best practices to approach this problem?    <\/p>\n<p>There seems to be an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router\">Azure ML router using azureml-fe<\/a> that does something similar, but I cannot find any documentation about it.     <\/p>\n<p>Thanks,    <br \/>\nLawrence    <\/p>",
        "Challenge_closed_time":1601934562660,
        "Challenge_comment_count":3,
        "Challenge_created_time":1601043259400,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in routing requests within an inference cluster in Kubernetes Service attached to an Azure Machine Learning workspace. They have deployed multiple models with their own endpoints and want to redirect requests to the appropriate model through a main endpoint. The user is seeking best practices to approach this problem and has come across an Azure ML router using azureml-fe but cannot find any documentation about it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/107990\/best-practices-for-routing-requests-within-inferen",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":8.4,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":247.5842388889,
        "Challenge_title":"Best Practices for Routing Requests within Inference Clusters",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":118,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=888ae66b-34ae-4a26-b896-abb53c4a8a32\">@Lawrence Wong  <\/a> ,    <br \/>\nWe do have a solution for this in private preview (called Many Models solution accelerator).    <br \/>\nPlease send your email id to AzCommunity[at]microsoft[dot]com). Include title and link to this thread in the email (and reply here once you do for faster response) and we can take the conversation from there.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":0.4295236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently trying to open parquet files using Azure Jupyter Notebooks. I have tried both Python kernels (2 and 3).\nAfter the installation of <em>pyarrow<\/em> I can import the module only if the Python kernel is 2 (not working with Python 3)<\/p>\n\n<p>Here is what I've done so far (for clarity, I am not mentioning all my various attempts, such as using <em>conda<\/em> instead of <em>pip<\/em>, as it also failed):<\/p>\n\n<pre><code>!pip install --upgrade pip\n!pip install -I Cython==0.28.5\n!pip install pyarrow\n\nimport pandas  \nimport pyarrow\nimport pyarrow.parquet\n\n#so far, so good\n\nfilePath_parquet = \"foo.parquet\"\ntable_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n<\/code><\/pre>\n\n<p>This works well if I'm doing that off-line (using Spyder, Python v.3.7.0). But it fails using an Azure Notebook.<\/p>\n\n<pre><code> AttributeErrorTraceback (most recent call last)\n&lt;ipython-input-54-2739da3f2d20&gt; in &lt;module&gt;()\n      6 \n      7 #table_parquet_raw = pd.read_parquet(filePath_parquet, engine='pyarrow')\n----&gt; 8 table_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n\nAttributeError: 'module' object has no attribute 'read_parquet'\n<\/code><\/pre>\n\n<p>Any idea please?<\/p>\n\n<p>Thank you in advance !<\/p>\n\n<p>EDIT:<\/p>\n\n<p>Thank you very much for your reply Peter Pan !\nI have typed these  statements, here is what I got:<\/p>\n\n<p>1.<\/p>\n\n<pre><code>    print(pandas.__dict__)\n<\/code><\/pre>\n\n<p>=> read_parquet does not appear<\/p>\n\n<p>2.<\/p>\n\n<pre><code>    print(pandas.__file__)\n<\/code><\/pre>\n\n<p>=> I get:<\/p>\n\n<pre><code>    \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/pandas\/__init__.py\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li><p>import sys; print(sys.path) => I get:<\/p>\n\n<pre><code>['', '\/home\/nbuser\/anaconda3_23\/lib\/python34.zip',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/plat-linux',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/lib-dynload',\n'\/home\/nbuser\/.local\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/Sphinx-1.3.1-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/IPython\/extensions',\n'\/home\/nbuser\/.ipython']\n<\/code><\/pre><\/li>\n<\/ol>\n\n<p>Do you have any idea please ?<\/p>\n\n<p>EDIT 2:<\/p>\n\n<p>Dear @PeterPan, I have typed both <code>!conda update conda<\/code> and  <code>!conda update pandas<\/code> : when checking the Pandas version (<code>pandas.__version__<\/code>), it is still <code>0.19.2<\/code>.<\/p>\n\n<p>I have also tried with <code>!conda update pandas -y -f<\/code>, it returns:\n`Fetching package metadata ...........\nSolving package specifications: .<\/p>\n\n<p>Package plan for installation in environment \/home\/nbuser\/anaconda3_23:<\/p>\n\n<p>The following NEW packages will be INSTALLED:<\/p>\n\n<pre><code>pandas: 0.19.2-np111py34_1`\n<\/code><\/pre>\n\n<p>When typing:\n<code>!pip install --upgrade pandas<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Requirement already up-to-date: pandas in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\nRequirement already up-to-date: pytz&gt;=2011k in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: numpy&gt;=1.9.0 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: python-dateutil&gt;=2 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: six&gt;=1.5 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from python-dateutil&gt;=2-&gt;pandas)<\/code><\/p>\n\n<p>Finally, when typing:<\/p>\n\n<p><code>!pip install --upgrade pandas==0.24.0<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Collecting pandas==0.24.0\n  Could not find a version that satisfies the requirement pandas==0.24.0 (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0rc1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0rc2, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0rc1, 0.19.0, 0.19.1, 0.19.2, 0.20.0rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0rc1, 0.21.0, 0.21.1, 0.22.0)\nNo matching distribution found for pandas==0.24.0<\/code><\/p>\n\n<p>Therefore, my guess is that the problem comes from the way the packages are managed in Azure. Updating a package (here Pandas), should lead to an update to the latest version available, shouldn't it?<\/p>",
        "Challenge_closed_time":1545730275287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1545322944093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to read \".parquet\" files in Azure Jupyter Notebook using Python 2 and 3 kernels. After installing pyarrow, the module can only be imported with Python 2 kernel and not with Python 3. The user has tried various attempts such as using conda instead of pip, but it failed. The user has also tried updating pandas, but it did not work. The user suspects that the problem may be due to the way packages are managed in Azure.",
        "Challenge_last_edit_time":1547188516027,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53872444",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":60.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":113.1475538889,
        "Challenge_title":"Cannot read \".parquet\" files in Azure Jupyter Notebook (Python 2 and 3)",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1879.0,
        "Challenge_word_count":455,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545322329030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I tried to reproduce your issue on my Azure Jupyter Notebook, but failed. There was no any issue for me without doing your two steps <code>!pip install --upgrade pip<\/code> &amp; <code>!pip install -I Cython==0.28.5<\/code> which I think not matter.<\/p>\n\n<p>Please run some codes below to check your import package <code>pandas<\/code> whether be correct.<\/p>\n\n<ol>\n<li>Run <code>print(pandas.__dict__)<\/code> to check whether has the description of <code>read_parquet<\/code> function in the output.<\/li>\n<li>Run <code>print(pandas.__file__)<\/code> to check whether you imported a different <code>pandas<\/code> package.<\/li>\n<li>Run <code>import sys; print(sys.path)<\/code> to check the order of paths whether there is a same named file or directory under these paths.<\/li>\n<\/ol>\n\n<p>If there is a same file or directory named <code>pandas<\/code>, you just need to rename it and restart your <code>ipynb<\/code> to re-run. It's a common issue which you can refer to these SO threads <a href=\"https:\/\/stackoverflow.com\/questions\/35341363\/attributeerror-module-object-has-no-attribute-reader\">AttributeError: &#39;module&#39; object has no attribute &#39;reader&#39;<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/36250353\/importing-installed-package-from-script-raises-attributeerror-module-has-no-at\">Importing installed package from script raises &quot;AttributeError: module has no attribute&quot; or &quot;ImportError: cannot import name&quot;<\/a>.<\/p>\n\n<p>In Other cases, please update your post for more details to let me know.<\/p>\n\n<hr>\n\n<p>The latest <code>pandas<\/code> version should be <code>0.23.4<\/code>, not <code>0.24.0<\/code>.<\/p>\n\n<p>I tried to find out the earliest version of <code>pandas<\/code> which support the <code>read_parquet<\/code> feature via search the function name <code>read_parquet<\/code> in the documents of different version from <code>0.19.2<\/code> to <code>0.23.3<\/code>. Then, I found <code>pandas<\/code> supports <code>read_parquet<\/code> feature after the version <code>0.21.1<\/code>, as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The new features shown in the <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/whatsnew.html\" rel=\"nofollow noreferrer\"><code>What's New<\/code><\/a> of version <code>0.21.1<\/code>\n<a href=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>According to your <code>EDIT 2<\/code> description, it seems that you are using Python 3.4 in Azure Jupyter Notebook. Not all <code>pandas<\/code> versions support Python 3.4 version.<\/p>\n\n<p>The versions <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.21.1<\/code><\/a> &amp; <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.22\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.22.0<\/code><\/a> offically support Python 2.7,3.5, and 3.6, as below.\n<a href=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the <a href=\"https:\/\/pypi.org\/project\/pandas\/\" rel=\"nofollow noreferrer\">PyPI page for <code>pandas<\/code><\/a> also requires the Python version as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6613J.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6613J.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So you can try to install the <code>pandas<\/code> versions <code>0.21.1<\/code> &amp; <code>0.22.0<\/code> in the current notebook of Python 3.4. if failed, please create a new notebook in Python <code>2.7<\/code> or <code>&gt;=3.5<\/code> to install <code>pandas<\/code> version <code>&gt;= 0.21.1<\/code> to use the function <code>read_parquet<\/code>.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1547190062312,
        "Solution_link_count":14.0,
        "Solution_readability":10.9,
        "Solution_reading_time":52.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":51.0,
        "Solution_word_count":383.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":98.9333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"I was able to create a deep learning VM from the marketplace and when I open up the VM instance in the Console I see a metadata tag called `proxy-url` which has a format like `https:\/\/[alphanumeric string]-dot-us-central1.notebooks.googleusercontent.com\/lab`\n\nClicking on that link takes me to a JupyterLab UI that is running on my VM. Amazing! Unfortunately, when I try opening that link on an incognito window, I'm asked to sign in. If I sign in, I get a 403 forbidden.\n\nMy question now is, how can I make that link available to someone else?",
        "Challenge_closed_time":1643637480000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643281320000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a deep learning VM from the marketplace and found a metadata tag called `proxy-url` that takes them to a JupyterLab UI running on their VM. However, when they try to open the link on an incognito window, they are asked to sign in and get a 403 forbidden error. The user wants to know how to make the link available to someone else.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Make-deep-learning-VM-JupyterLab-publicly-available\/m-p\/386576#M191",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":98.9333333333,
        "Challenge_title":"Make deep learning VM JupyterLab publicly available?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi gopalv\n\nAs far as I understand, it sounds like your Jupyter Notebook isn't configured for remote access. since it doesn't work when trying to access it from the incognito window with a 403 error.\n\nYou can try looking here and here for details on how to set up a publicly accessible\/remote access notebook. There are additional troubleshooting steps in our documentation here as well.\n\nHope this helps!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":73.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1467237684900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":613.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":10.2121516667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to install spacy which is not available as part of the Sagemaker platform. How should can I pip install it?<\/p>",
        "Challenge_closed_time":1522941835116,
        "Challenge_comment_count":2,
        "Challenge_created_time":1522908559597,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to install the spacy module in Sagemaker, but it is not available in the platform. They are seeking guidance on how to pip install it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49665241",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.2,
        "Challenge_reading_time":2.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":9.2431997222,
        "Challenge_title":"How do I load python modules which are not available in Sagemaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2578.0,
        "Challenge_word_count":33,
        "Platform":"Stack Overflow",
        "Poster_created_time":1410972175307,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1124.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>When creating you model, you can specify the requirements.txt as an environment variable. <\/p>\n\n<p>For Eg. <\/p>\n\n<pre><code>env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/mybucket\/modelTarFile,\n                                  role = role,\n                                  entry_point = 'entry.py',\n                                  code_location = 's3:\/\/mybucket\/runtime-code\/',\n                                  source_dir = 'src',\n                                  env = env,\n                                  name = 'model_name',\n                                  sagemaker_session = sagemaker_session,\n                                 )\n<\/code><\/pre>\n\n<p>This would ensure that the requirements file is run after the docker container is created, before running any code on it. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1522945323343,
        "Solution_link_count":0.0,
        "Solution_readability":11.6,
        "Solution_reading_time":7.93,
        "Solution_score_count":10.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1476175026763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1241.0,
        "Answerer_view_count":68.0,
        "Challenge_adjusted_solved_time":0.9190636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to building a docker container with mlflow server inside, with poetry toml file for dependency.(the two toml are exactly the same, it was just a way to try  to figure out)<br>\ntree:<\/p>\n\n<p>\u251c\u2500\u2500 docker-entrypoint.sh <br>\n\u251c\u2500\u2500 Dockerfile<br>\n\u251c\u2500\u2500 files<br>\n\u2502   \u2514\u2500\u2500 pyproject.toml<br>\n\u251c\u2500\u2500 git.sh<br>\n\u251c\u2500\u2500 pyproject.toml<br>\n\u2514\u2500\u2500 README.md<br><\/p>\n\n<p>as you can see, my toml file is next to Dockerfile <code>COPY pyproject.toml .\/<\/code> don't work nevertheless<\/p>\n\n<p><strong>Dockerfile<\/strong><\/p>\n\n<pre><code>FROM python:3.6.10-alpine3.10 as base\nLABEL maintainer=\"\"\n\nENV PYTHONFAULTHANDLER 1 \nENV    PYTHONHASHSEED random \nENV    PYTHONUNBUFFERED 1\n\nENV MLFLOW_HOME .\/ \nENV SERVER_PORT 5000   \nENV    MLFLOW_VERSION 0.7.0 \nENV    SERVER_HOST 0.0.0.0  \nENV    FILE_STORE ${MLFLOW_HOME}\/fileStore  \nENV    ARTIFACT_STORE ${MLFLOW_HOME}\/artifactStore \nENV PIP_DEFAULT_TIMEOUT 100\nENV    PIP_DISABLE_PIP_VERSION_CHECK on\nENV    PIP_NO_CACHE_DIR  off \nENV    POETRY_VERSION  1.0.0 \n\nWORKDIR ${MLFLOW_HOME}\n\nFROM base as builder\n\nRUN apk update  \\\n    &amp;&amp; apk add --no-cache make gcc musl-dev python3-dev libffi-dev openssl-dev subversion\n#download project file from github  repo \nRUN    svn export https:\/\/github.com\/MChrys\/QuickSign\/trunk\/  \\\n    &amp;&amp; pip install poetry==${POETRY_VERSION} \\\n    &amp;&amp; mkdir -p ${FILE_STORE}  \\\n    &amp;&amp; mkdir -p ${ARTIFACT_STORE}\\\n    &amp;&amp; python -m venv \/venv\n\nCOPY  pyproject.toml .\/\nRUN poetry export -f requirements.txt | \/venv\/bin\/pip install -r  --allow-root-install \/dev\/stdin \n\nCOPY . .\nRUN poetry build &amp;&amp; \/venv\/bin\/pip install dist\/*.whl\n\nFROM base as final\n\nRUN apk add --no-cache libffi libpq\nCOPY --from=builder \/venv \/venv\nCOPY docker-entrypoint.sh .\/\n\nEXPOSE $SERVER_PORT\n\nVOLUME [\"${FILE_STORE}\", \"${ARTIFACT_STORE}\"]\n\nCMD [\".\/docker-entrypoint.sh\"]\n<\/code><\/pre>\n\n<p>the build command :<\/p>\n\n<pre><code>docker build - &lt; Dockerfile\n<\/code><\/pre>\n\n<p>I get this error  :<\/p>\n\n<pre><code>Step 21\/32 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder335195979\/pyproject.toml: no such file or   directory\n<\/code><\/pre>\n\n<p><strong>pyproject.toml<\/strong><\/p>\n\n<pre><code>requires = [\"poetry&gt;=1.0.0\", \"mlflow&gt;=0.7.0\", \"python&gt;=3.6\"]\nbuild-backend = \"poetry.masonry.api\"\n\n[tool.poetry]\nname = \"Sign\"\ndescription = \"\"\nversion = \"1.0.0\"\nreadme = \"README.md\"\nauthors = [\n  \"\"\n]\n\nlicense = \"MIT\"\n\n\n[tool.poetry.dependencies]\npython = \"3.6\"\nnumpy = \"1.14.3\"\nscipy = \"*\"\npandas = \"0.22.0\"\nscikit-learn = \"0.19.1\"\ncloudpickle = \"*\"\nmlflow =\"0.7.0\"\ntensorflow = \"^2.0.0\"\n\n\n[tool.poetry.dev-dependencies]\n\npylint = \"*\"\ndocker-compose = \"^1.25.0\"\ndocker-image-size-limit = \"^0.2.0\"\ntomlkit = \"^0.5.8\"\n\n<\/code><\/pre>\n\n<p><strong>docker-entrypoint.sh<\/strong><\/p>\n\n<pre><code>#!\/bin\/sh\n\nset -e\n\n. \/venv\/bin\/activate\n\nmlflow server \\\n    --file-store $FILE_STORE \\\n    --default-artifact-root $ARTIFACT_STORE \\\n    --host $SERVER_HOST \\\n    --port $SERVER_PORT\n<\/code><\/pre>\n\n<hr>\n\n<hr>\n\n<p>if i add <code>RUN pwd; ls<\/code> just befor the first <code>COPY<\/code> I obtain :<\/p>\n\n<pre><code>Step 20\/31 : RUN pwd; ls\n ---&gt; Running in e8ec36dd6ca8\n\/\nartifactStore\nbin\ndev\netc\nfileStore\nhome\nlib\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsrv\nsys\ntmp\ntrunk\nusr\nvar\nvenv\nRemoving intermediate container e8ec36dd6ca8\n ---&gt; d7bba641bd7c\nStep 21\/31 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder392824737\/pyproject.toml: no such file or directory\n\n<\/code><\/pre>",
        "Challenge_closed_time":1577175350592,
        "Challenge_comment_count":5,
        "Challenge_created_time":1577166859160,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to build a Docker container with an mlflow server inside, with poetry toml file for dependency. The user has placed the toml file next to the Dockerfile, but the \"COPY pyproject.toml .\/\" command does not work and blocks the docker build. The user has tried to add \"RUN pwd; ls\" before the first \"COPY\" command, but it still fails.",
        "Challenge_last_edit_time":1577172041963,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59464404",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":10.4,
        "Challenge_reading_time":44.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":2.3587311111,
        "Challenge_title":"COPY files - next to Dockerfile - don't work and block docker build",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":940.0,
        "Challenge_word_count":358,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459625723203,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Grenoble, France",
        "Poster_reputation_count":56.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>Try \n<code>docker build -t test .<\/code><\/p>\n\n<p>instead of\n<code>docker build - &lt; Dockerfile<\/code><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1327302732867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"USA",
        "Answerer_reputation_count":19711.0,
        "Answerer_view_count":1030.0,
        "Challenge_adjusted_solved_time":0.1674261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Challenge_closed_time":1597366468132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597366468133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering errors when attempting to train an XGBoost model using the AWS hosted container in a local Jupyter notebook. The errors include a KeyError related to S3DistributionType and a runtime error when attempting to run the docker container. The issue does not occur when using the Amazon cloud SageMaker environment. It is suspected that there is an issue with how the LocalSession() works with the hosted docker container.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.3,
        "Challenge_reading_time":56.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1174.0,
        "Challenge_word_count":293,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327302732867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"USA",
        "Poster_reputation_count":19711.0,
        "Poster_view_count":1030.0,
        "Solution_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1597367070867,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":32.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":235.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1352838205400,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":718.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":50.51511,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Docker image in Elastic Container Registry (ECR). It was created via a simple Dockerfile which I have control over.<\/p>\n<p>The image itself is fine, but I have a problem where the shared memory is insufficient when working inside a container in SageMaker Studio. Therefore I need to raise the shared memory of these containers.<\/p>\n<p>To raise the shared memory of a container, I believe the usual method is to pass the <code>--shm-size<\/code> argument to the <code>docker run<\/code> command when starting the container. However, I do not have control over this command, as SageMaker is doing that bit for me. The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">docs<\/a> say that SageMaker is running <code>docker run &lt;image&gt; train<\/code> when starting a container.<\/p>\n<p>Is it possible to work around this problem? Either via somehow providing additional arguments to the command, or specifying something when creating the Docker image (such as in the Dockerfile, deployment script to ECR).<\/p>",
        "Challenge_closed_time":1648732366736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648550512340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a Docker image in Elastic Container Registry (ECR) and needs to increase the shared memory of the containers in SageMaker Studio. However, they do not have control over the <code>docker run<\/code> command as it is done by SageMaker. The user is looking for a workaround to provide additional arguments to the command or specify something in the Docker image or deployment script to ECR.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71660619",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":14.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":50.51511,
        "Challenge_title":"How to alter shared memory for SageMaker Docker containers?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":456.0,
        "Challenge_word_count":162,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352838205400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":718.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>As pointed out by @rok (thank you!) it is not possible in this situation to pass arguments to <code>docker run<\/code>, although it would be if switching to ECS.<\/p>\n<p>It is however possible to pass the <code>--shm-size<\/code> argument to <code>docker build<\/code> when building the image to push to ECR. This seems to have fixed the problem, albeit it does require a new Docker image to be built and pushed whenever wanting to change this parameter.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":5.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1446841002932,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Trondheim, Norway",
        "Answerer_reputation_count":226.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":0.2383613889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an Azure ML Web service which outputs JSON response on request, and the structure of the sample request is as following:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"gender\",\n        \"age\",\n        \"income\"\n      ],\n      \"Values\": [\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ],\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>And the input parameters are supposedly like this:<\/p>\n\n<p>gender  String<br>\nage Numeric<br>\nincome  Numeric     <\/p>\n\n<p>My Post method looks like this:<\/p>\n\n<pre><code>    [HttpPost]\n        public ActionResult GetPredictionFromWebService()\n        {\n            var gender = Request.Form[\"gender\"];\n            var age = Request.Form[\"age\"];\n\n\n            if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n            {\n                var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = Int32.Parse(result[0, 2])\n                    };\n                }\n            }\n\n\n\n\n            return RedirectToAction(\"index\");\n        }\n<\/code><\/pre>\n\n<p>But for whatever reason; the Azure ML Webservice doesn\u2019t seem to respond anything to my request.\nDoes anyone know what the reason might be? I see no error or anything, just an empty response.<\/p>",
        "Challenge_closed_time":1456314151448,
        "Challenge_comment_count":0,
        "Challenge_created_time":1456313293347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an Azure ML Web service that outputs a JSON response on request. The user has created a post method to get predictions from the web service, but the web service is not responding to the request, and the user is not receiving any error messages.",
        "Challenge_last_edit_time":1456839908307,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35600907",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":16.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.2383613889,
        "Challenge_title":"Azure ML Web Service request not working in C#",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":480.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456309738852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":39.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>The answer to your problem is that the \u201cNumeric\u201d datatype which is written in the input parameters in Azure ML is in fact a float and not an integer for your income measure. So when trying to request a response from Azure ML, you are not providing it the \u201cadequate\u201d information needed in the right format for it to respond correctly, resulting in it not giving you any response.<\/p>\n\n<p>I believe your model would look something similar to this based on your input parameters:<\/p>\n\n<pre><code>public class Person\n    {\n        public string Gender { get; set; }\n        public int Age { get; set; }\n        public int Income { get; set; }\n\n\n        public override string ToString()\n        {\n            return Gender + \",\" + Age + \",\" + Income;\n        }\n    }\n<\/code><\/pre>\n\n<p>You would have to change your Income datatype into float like so:<\/p>\n\n<pre><code>public class Person\n{\n    public string Gender { get; set; }\n    public int Age { get; set; }\n    public float Income { get; set; }\n\n    public override string ToString()\n    {\n        return Gender + \",\" + Age + \",\" + Income;\n    }\n}\n<\/code><\/pre>\n\n<p>And then your post-method would look something like this:<\/p>\n\n<pre><code>    [HttpPost]\n    public ActionResult GetPredictionFromWebService()\n    {\n        var gender = Request.Form[\"gender\"];\n        var age = Request.Form[\"age\"];\n\n        if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n        {\n            var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n                };\n            }\n        }\n\n        ViewBag.myData = PersonResult.Income.ToString();\n        return View(\"Index\");\n    }\n<\/code><\/pre>\n\n<p>The key here is simply:<\/p>\n\n<pre><code>Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n<\/code><\/pre>\n\n<p>Rather than your legacy <\/p>\n\n<pre><code>Income = Int32.Parse(result[0, 2])\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":24.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":221.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1393509037328,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":768.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":765.2169341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/mlops\/mlops_first_steps\" rel=\"nofollow noreferrer\">ClearML<\/a>.<\/p>\n<p>The only line in my file is<\/p>\n<pre><code>from allegroai import Dataset, DatasetVersion\n<\/code><\/pre>\n<p>which yields<\/p>\n<pre><code>ModuleNotFoundError: No module named 'allegroai'\n<\/code><\/pre>\n<p>Looks like some pip package is missing, but I couldn't for the life of me find it in the docs.<\/p>\n<p>What should I pip install?<\/p>\n<p><strong>Not working:<\/strong><\/p>\n<ul>\n<li><code>pip install clearml-agent<\/code><\/li>\n<li><code>pip install clearml<\/code> and <code>clearml-init<\/code> as in <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/ds\/ds_first_steps\" rel=\"nofollow noreferrer\">here<\/a><\/li>\n<li><code>pip install allegroai<\/code><\/li>\n<\/ul>",
        "Challenge_closed_time":1657799581543,
        "Challenge_comment_count":2,
        "Challenge_created_time":1655044800580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleNotFoundError while trying to use ClearML and import the 'allegroai' module. They have tried installing various packages including clearml-agent, clearml, and allegroai, but none of them have resolved the issue. The user is seeking guidance on what package they should install to resolve the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72593187",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":11.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":765.2169341667,
        "Challenge_title":"ModuleNotFoundError: No module named 'allegroai'",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1314313109232,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Technion, Israel",
        "Poster_reputation_count":18777.0,
        "Poster_view_count":2000.0,
        "Solution_body":"<p>Allegroai package should be taken from ClearML PyPi server.\nThis is only for paying customers (I think), and the way to retrieve it is by:<\/p>\n<ol>\n<li>Going to ClearML website (login with username\/company).<\/li>\n<li>Press the ? on the top right of the screen (next to your user icon) and choose the first one\n<a href=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" alt=\"1\" \/><\/a><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":5.83,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":60.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1351534968768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":823.0,
        "Answerer_view_count":114.0,
        "Challenge_adjusted_solved_time":43.0164288889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know how to access packages in <code>R<\/code> scripts in <code>Azure machine<\/code> learning by either using the <code>Azure<\/code> supported ones or by zipping up the packages.<\/p>\n\n<p>My problem now is that <code>Azure<\/code> machine learning does not support the <code>h2o package<\/code> and when I tried using the zipped file - it gave an <code>error<\/code>. <\/p>\n\n<p>Has anyone figured out how to use <code>h2o<\/code> in <code>R<\/code> in <code>Azure machine<\/code> learning?<\/p>",
        "Challenge_closed_time":1487237805672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487082559403,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in running the h2o package in R script in Azure machine learning. Azure machine learning does not support the h2o package and using the zipped file resulted in an error. The user is seeking help to figure out how to use h2o in R in Azure machine learning.",
        "Challenge_last_edit_time":1487082946528,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42228715",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":43.1239636111,
        "Challenge_title":"Running h2o in R script in Azure machine learning",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351534968768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":823.0,
        "Poster_view_count":114.0,
        "Solution_body":"<p>So since there was no reply to my question, I made some research and came up with the following:<\/p>\n\n<p>H2O cannot be ran in a straightforward manner in Azure machine learning embedded R scripts. A workaround the problem is to consider using an Azure created environment - specially for H2O. The options available are:<\/p>\n\n<ol>\n<li>Spinning up an H2O Artificial Intelligence Virtual Machine solution<\/li>\n<li>Using an H2O application for HDInsight<\/li>\n<\/ol>\n\n<p>For more reading, you can go to: <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html\" rel=\"nofollow noreferrer\">http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":8.43,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1589205020747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":76.5197022222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to upgrade the default Ubuntu version that comes with the Compute Instance in Azure ML.<\/p>\n\n<p>Anyone has any guide on safely upgrading to the latest LTS?<\/p>",
        "Challenge_closed_time":1591531320408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591255849480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to upgrade the default Ubuntu version that comes with the Compute Instance in Azure ML to the latest LTS and is seeking guidance on how to do it safely.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62189103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":3.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":76.5197022222,
        "Challenge_title":"Azure ML Compute Instance: How can I safely upgrade the default Azure Ubuntu 16.04 LTS to the latest LTS?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449542867716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1302.0,
        "Poster_view_count":157.0,
        "Solution_body":"<p>Any specific reason you want to do this?<\/p>\n\n<p>Since there are some heavy dependencies (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#contents\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#contents<\/a>), my guess is you have to try it yourself.<\/p>\n\n<p>Create a new one and run:<\/p>\n\n<pre><code>$ sudo apt update \n$ sudo apt upgrade\n$ sudo apt dist-upgrade\n<\/code><\/pre>\n\n<p>Let us know what happened.<\/p>\n\n<p>BTW: Are Compute Instance also Docker images? If so, the upgrade might be working, if not, there might be many drivers that need to be upgraded too. The ones from the GPU would be the easiest...<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.21,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0780555556,
        "Challenge_answer_count":0,
        "Challenge_body":"This question was resolved and discussed on Polyaxon Slack. Posting for visibility if someone stumbles upon the same issue.\n\nGiven setup:\nDocker-registry provider: Amazon Elastic Container Registry (ECR)\nPolyaxon version: 1.7.3 CE\nDeployed with Kubernetes on AWS\nAnd credentials setup from Kaniko github: Pushing to Amazon ECR\nAnd Kaniko integration in polyaxon-config.yml:\nconnections:\n  - name: docker-registry\n    kind: registry\n    description: \"aws docker repository\"\n    schema:\n      url: https:\/\/ID.dkr.ecr.SOME-REGION.amazonaws.com\n    secret:\n      name: docker-conf\n      mountPath: \/kaniko\/.docker\nAnd polyaxonfile.yml from polyaxon example:\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: docker-registry\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"tensorflow\/tensorflow:2.0.1-py3\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\",\"polytune\"]'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\nThen warning was raised:\n\nand job would be stuck like this until manually stopped.\n\nTYPE     STATUS    REASON              MESSAGE                                                           LAST_UPDATE_TIME    LAST_TRANSITION_TIME\n-------  --------  ------------------  ----------------------------------------------------------------  ------------------  ----------------------\nwarning  True      ContainersNotReady  containers with unready status: [polyaxon-main polyaxon-sidecar]  a few seconds ago   a few seconds ago",
        "Challenge_closed_time":1619182673000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619182392000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Kaniko integration in Polyaxon, where the image push is stuck on ContainersNotReady containers with unready status. The setup includes Amazon Elastic Container Registry (ECR) as the Docker-registry provider, Polyaxon version 1.7.3 CE, and Kubernetes on AWS. The warning message indicates that the job is stuck and cannot proceed until manually stopped.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1296",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":12.5,
        "Challenge_reading_time":19.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.0780555556,
        "Challenge_title":"Image push with Kaniko stuck on ContainersNotReady containers with unready status: [polyaxon-main polyaxon-sidecar]",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":150,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Resolution:\n\nThe issue was with aws-secret type.\n\nAt first we had the type of secret that expires every 12 hours.\nChanging it to one that does not expire allowed successful connection and push of built image.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":2.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1623510794510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":23.3032630556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to launch some experiments in DVC. But when I set values of experiment parameters, DVC deletes file 'params.yaml', and experiment doesn't set in queue.<\/p>\n<p>Simplified code for example:\nPython file 'test.py':<\/p>\n<pre><code>import numpy as np\nimport json\nimport yaml\n\nparams = yaml.safe_load(open('params.yaml'))[&quot;test&quot;]\n\nprecision = np.random.random()\nrecall = params['value']\naccuracy = np.random.random()\n \n\nrows = {'precision': precision,\n        'recall': recall,\n        'accuracy': accuracy}\n\n\nwith open(params['metrics_path'], 'w') as outfile:\n    json.dump(rows, outfile)\n\nfpr = 10*np.random.random((1,10)).tolist()\ntpr = 10*np.random.random((1,10)).tolist()\n\nwith open('plot.json', 'w') as outfile2:\n    json.dump(\n      {\n        &quot;roc&quot;: [ {&quot;fpr&quot;: f, &quot;tpr&quot;: t} for f, t in zip(fpr, tpr) ]\n      }, \n      outfile2\n      )\n<\/code><\/pre>\n<p>params.yaml:<\/p>\n<pre><code>test:\n  metrics_path: &quot;scores.json&quot;\n  value: 1\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>stages:\n  test:\n    cmd: python test.py\n    deps:\n    - test.py\n    params:\n    - test.metrics_path\n    - test.value\n    metrics:\n    - scores.json:\n        cache: false\n    plots:\n    - plot.json:\n        cache: false\n        x: fpr\n        y: tpr\n<\/code><\/pre>\n<p>It is strange behavior. Is it possible to fix it?<\/p>",
        "Challenge_closed_time":1654244934587,
        "Challenge_comment_count":8,
        "Challenge_created_time":1654161042840,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while launching experiments in DVC. When they set values of experiment parameters, DVC deletes the file 'params.yaml', and the experiment doesn't set in the queue. The user has shared simplified code for an example, including Python file 'test.py', 'params.yaml', and 'dvc.yaml'. The user is seeking help to fix this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72473641",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":9.1,
        "Challenge_reading_time":16.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":23.3032630556,
        "Challenge_title":"How do launch experiments in DVC?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623510794510,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>I solved my problem. It is necessary, that all files (executable scripts, 'dvc.yaml', 'params.yaml') be tracked by git. In this case <code>dvc exp run<\/code> command works correctly.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":27.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":2.3951186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've got a basic ScriptStep in my AML Pipeline and it's just trying to read an attached dataset. When i execute this simple example, the pipeline fails with the following in the driver log:<\/p>\n\n<blockquote>\n  <p>ImportError: azureml-dataprep is not installed. Dataset cannot be used\n  without azureml-dataprep. Please make sure\n  azureml-dataprep[fuse,pandas] is installed by specifying it in the\n  conda dependencies. pandas is optional and should be only installed if\n  you intend to create a pandas DataFrame from the dataset.<\/p>\n<\/blockquote>\n\n<p>I then modified my step to include the conda package but then the driver fails with \"ResolvePackageNotFound: azureml-dataprep\". The entire log file can be accessed <a href=\"https:\/\/www.dropbox.com\/s\/372ht6jkvzu9loo\/conda.err.txt?dl=0\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<pre><code># create a new runconfig object\nrun_config = RunConfiguration()\nrun_config.environment.docker.enabled = True\nrun_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\nrun_config.environment.python.user_managed_dependencies = False\nrun_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['azureml-dataprep[pandas,fuse]'])\n\nsource_directory = '.\/read-step'\nprint('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\nstep2 = PythonScriptStep(name=\"read_step\",\n                         script_name=\"Read.py\", \n                         arguments=[\"--dataFilePath\", dataset.as_named_input('local_ds').as_mount() ],\n                         compute_target=aml_compute, \n                         source_directory=source_directory,\n                         runconfig=run_config,\n                         allow_reuse=False)\n<\/code><\/pre>\n\n<p>I'm out of ideas, would deeply appreciate any help here!<\/p>",
        "Challenge_closed_time":1587162956780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587154334353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with AzureML where a ScriptStep in their AML Pipeline is failing to read an attached dataset due to the missing installation of azureml-dataprep. The user tried modifying the step to include the conda package, but the driver fails with \"ResolvePackageNotFound: azureml-dataprep\".",
        "Challenge_last_edit_time":1591825691630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61279914",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":22.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":2.3951186111,
        "Challenge_title":"AzureML: ResolvePackageNotFound azureml-dataprep",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1244.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>The <code>azureml-sdk<\/code> isn't available on conda, you need to install it with <code>pip<\/code>.<\/p>\n\n<pre><code>myenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies().add_pip_package(\"azureml-dataprep[pandas,fuse]\")\nmyenv.python.conda_dependencies=conda_dep\nrun_config.environment = myenv\n<\/code><\/pre>\n\n<p>For more information, about this error, the logs tab has a log named <code>20_image_build_log.txt<\/code> which Docker build logs. It contains the error where <code>conda<\/code> failed to failed to find <code>azureml-dataprep<\/code><\/p>\n\n<p>EDIT:<\/p>\n\n<p>Soon, you won't have to specify this dependency anymore. the Azure Data4ML team says <code>azureml-dataprep[pandas,fuse]<\/code> is getting added as a dependency for <code>azureml-defaults<\/code> which is automatically installed on all images. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1587416360076,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":10.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":30.9986213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to use a compute instance as my develop machine.\nAre there any best practices on how to handle custom Anaconda enviroments on these machines?<\/p>\n\n<p>So far, I do it this way:<\/p>\n\n<pre><code>conda create --name testenv python=3\nconda activate testenv\nconda install ipykernel\nipython kernel install --user --name=testenv\nsudo systemctl restart jupyter.service\n<\/code><\/pre>\n\n<p>--> Reload the JupyterHub in your browser.<\/p>\n\n<p>Do you see any drawbacks by doing it this way? I know, some special package combinations in the standard env are lost, but I'd like to know what I've installed in my system.\nOf course, one could combine it with an <code>environment.yml<\/code>.<\/p>\n\n<p>What do you think?<\/p>",
        "Challenge_closed_time":1591290256907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591178661870,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking best practices for handling custom Anaconda environments on a compute instance used as a development machine. The user currently creates a new environment using conda, installs necessary packages, and restarts the Jupyter service. The user is concerned about losing special package combinations in the standard environment but wants to know what is installed in the system. The user is considering combining the process with an environment.yml file.",
        "Challenge_last_edit_time":1591865477772,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62170192",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":9.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":30.9986213889,
        "Challenge_title":"Compute Instance: Best practice for custom Anaconda env",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":116.0,
        "Challenge_word_count":114,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589205020747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":163.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Your workaround is the best option as of now. But I know that the Azure ML product group has been working on exactly this problem, but I can't make any promises as to timeline.<\/p>\n\n<p>I share your dream of an easily configurable data science cloud development environment that allows for Git repo cloning and environment creation w\/ a conda yml. We're so close especially given all the press &amp; announcements around Visual Studio Codespaces!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":5.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":74.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":503.9755555556,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\n$ dvc repro run_benchmarks\r\nERROR: 'dvc.lock' is git-ignored.\r\n```\r\n\r\n`.dvc.lock` in `.gitignore` causes Exceptions at running benchmark. Delete this line solves this problem. And because of #168 maybe we need some better ways to deal with `dvc.lock`.",
        "Challenge_closed_time":1621495987000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619681675000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered several issues while running experiments using `example-dvc-experiments`. These issues include dvc not being installed by `pip install -r requirements.txt`, an error while running `dvc pull`, all images being listed when running the `extract` stage using `dvc exp run`, and unclear instructions regarding the use of `dvc repro` and `dvc exp run`. The user suggests including `dvc` in `requirements.txt` and clarifying the instructions for using `dvc repro` and `dvc exp run`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/255",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.8,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":503.9755555556,
        "Challenge_title":"ERROR: 'dvc.lock' is git-ignored.",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1262052472408,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Melbourne VIC, Australia",
        "Answerer_reputation_count":13830.0,
        "Answerer_view_count":1372.0,
        "Challenge_adjusted_solved_time":31.96977,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I tried to start a R notebook in Sagemaker and I typed<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>install.packages(\"disk.frame\")\n<\/code><\/pre>\n\n<p>and it gave me the error<\/p>\n\n<pre><code>also installing the dependencies \u2018listenv\u2019, \u2018dplyr\u2019, \u2018rlang\u2019, \u2018furrr\u2019, \n\u2018future.apply\u2019, \u2018fs\u2019, \u2018pryr\u2019, \u2018fst\u2019, \u2018globals\u2019, \u2018future\u2019\n\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018rlang\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018fs\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018pryr\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018fst\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018dplyr\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018disk.frame\u2019 had non-zero exit status\u201d\nUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n<\/code><\/pre>\n\n<p>How do I install R packages on Sagemaker?<\/p>",
        "Challenge_closed_time":1566562240968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566445052783,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to install the R package \"disk.frame\" on a SageMaker Notebook instance but is encountering errors related to the installation of dependencies such as \"rlang\", \"fs\", \"pryr\", \"fst\", and \"dplyr\". The user is seeking guidance on how to install R packages on SageMaker.",
        "Challenge_last_edit_time":1566447149796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57601733",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.1,
        "Challenge_reading_time":15.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":32.5522736111,
        "Challenge_title":"How do I install R packages on the SageMaker Notebook instance?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2786.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1262052472408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Melbourne VIC, Australia",
        "Poster_reputation_count":13830.0,
        "Poster_view_count":1372.0,
        "Solution_body":"<p>I think you just need to specify a repo. For example, setting the RStudio CRAN repo, I can install perfectly fine.<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>install.packages(\"disk.frame\", repo=\"https:\/\/cran.rstudio.com\/\")\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":3.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1408356046196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bonn, Deutschland",
        "Answerer_reputation_count":594.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":0.2372688889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to test my service and to do so I deploy it locally and until now everything worked fine. However, for some unrelated reason I was forced to delete all my docker images and since then I'm unable to deploy the service locally. Upon deployment I receive the following error:<\/p>\n\n<blockquote>\n  <p>404 Client Error: Not Found for url:\n  http+docker:\/\/localnpipe\/v1.39\/images\/471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\/json<\/p>\n<\/blockquote>\n\n<p>And also:<\/p>\n\n<blockquote>\n  <p>ImageNotFound: 404 Client Error: Not Found (\"no such image: \n  471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814: No\n  such image:\n  sha256:471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\")<\/p>\n<\/blockquote>\n\n<p>What I did to deploy the model:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.webservice import LocalWebservice\nfrom azureml.core.model import InferenceConfig\n\nws = Workspace.from_config(\"config.json\")\n\ndeployment_config = LocalWebservice.deploy_configuration(port=8890)\n\ninference_config = InferenceConfig(runtime= \"python\", \n                               entry_script=\"score.py\",\n                               conda_file=\"env.yml\")\n\nmodel_box = Model(ws, \"box\")\nmodel_view = Model(ws, \"view_crop\")\nmodel_damage = Model(ws, \"damage_crop\")\n\nservice = Model.deploy(ws, \"test-service\", [model_box, model_view, model_damage], inference_config, deployment_config)\n\nservice.wait_for_deployment(True)\n<\/code><\/pre>\n\n<p>I understand why there is no image present, but I would expect that it is downloaded in that case.<\/p>\n\n<p>Is there a way to force the build process to re-download the docker base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1559043123528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559042269360,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to deploy their service locally after deleting all their docker images. They receive a \"no such image\" error and a 404 client error when attempting to deploy the model. The user is looking for a way to force the build process to re-download the docker base image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56341012",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":22.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":0.2372688889,
        "Challenge_title":"Docker image not found during local deployment (\"no such image\")",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":3685.0,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408356046196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bonn, Deutschland",
        "Poster_reputation_count":594.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>I just found the problem and corresponding solution:<\/p>\n\n<p>I deleted all images but there where still some containers based on deleted images present. Deleting the corresponding container had the desired effect that the docker image is reloaded from the server.<\/p>\n\n<p>You can delete all containers with <code>docker kill $(docker ps -q)<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.45,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":51.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.6107855556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have the following error while running <a href=\"http:\/\/wandb.me\/prompts-quickstart\" rel=\"noopener nofollow ugc\">W&amp;B_Prompts_Quickstart notebook<\/a> <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/8\/88da9eb34b5ef62baf4fcd367d69c08d5579c825.png\" data-download-href=\"\/uploads\/short-url\/jwFkh0P5adLx7fGnRM3YtxxLn4p.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/8\/88da9eb34b5ef62baf4fcd367d69c08d5579c825.png\" alt=\"image\" data-base62-sha1=\"jwFkh0P5adLx7fGnRM3YtxxLn4p\" width=\"690\" height=\"289\" data-dominant-color=\"F4F4F4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">774\u00d7325 8.32 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1684232017936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684222619108,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running the W&B_Prompts_Quickstart notebook, as shown in the attached image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/error-in-w-b-prompts-quickstart-notebook\/4411",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":28.9,
        "Challenge_reading_time":15.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.6107855556,
        "Challenge_title":"Error in W&B_Prompts_Quickstart notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tinsae\">@tinsae<\/a> thanks for reporting this issue. There\u2019s a breaking change with newer LangChain version, and the Growth team is working on a fix.<\/p>\n<p>You could run the Prompts Quickstart notebook for now by pinning a previous LangChain version which I just tested and seems to be working fine. Could you please change the installation section as follows:<\/p>\n<pre><code class=\"lang-auto\">!pip install \"wandb&gt;=0.15.2\" -qqq\n!pip install \"langchain==v0.0.158\" openai\n<\/code><\/pre>\n<p>Would this work for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":7.04,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5526797223,
        "Challenge_answer_count":1,
        "Challenge_body":"According to the doc ( https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/smd_model_parallel_general.html ), there are different parameters depending on the version of `smdistributed-modelparallel` module \/ package. However, I am unable to find a way to check the version (e.g. via sagemaker python SDK) or just from the training container documentation (e.g. https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md#huggingface-training-containers ).\n\nAny idea?\n\nThanks!",
        "Challenge_closed_time":1660807962126,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660805972479,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to check the version of the `smdistributed-modelparallel` module\/package, as different parameters are available depending on the version. However, they are unable to find a way to check the version through the sagemaker python SDK or the training container documentation.",
        "Challenge_last_edit_time":1668434749756,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUsfpWY8CuRsiyHg_x7qyJzw\/how-to-check-smdistributed-modelparallel-version",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":20.2,
        "Challenge_reading_time":7.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.5526797223,
        "Challenge_title":"How to check smdistributed-modelparallel version?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Have not yet found a programmatic way to check the version.\n\nHowever, for each DLC (Deep Learning Container) available at \nhttps:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md , we can look at the corresponding docker build files.\n\nE.g. for `PyTorch 1.10.2 with HuggingFace transformers` DLC, the corresponding dockerfile is here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/huggingface\/pytorch\/training\/docker\/1.10\/py3\/cu113\/Dockerfile.gpu\n\nAnd we can see that the version: `smdistributed_modelparallel-1.8.1-cp38-cp38-linux_x86_64.whl`.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1660807962126,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":3.4106091667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Created a new compute instance in Azure ML and trained a model with out any issue. I wanted to draw a pairplot using <code>seaborn<\/code> but I keep getting the error <code>&quot;ImportError: No module named seaborn&quot;<\/code><\/p>\n<p>I ran <code>!conda list<\/code> and I can see seaborn in the list<\/p>\n<pre><code># packages in environment at \/anaconda:\n#\n# Name                    Version                   Build  Channel\n_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0  \nalabaster                 0.7.12                   py37_0  \nanaconda                  2018.12                  py37_0  \nanaconda-client           1.7.2                    py37_0  \nanaconda-navigator        1.9.6                    py37_0  \nanaconda-project          0.8.2                    py37_0  \napplicationinsights       0.11.9                    &lt;pip&gt;\nasn1crypto                0.24.0                   py37_0  \nastroid                   2.1.0                    py37_0  \nastropy                   3.1              py37h7b6447c_0  \natomicwrites              1.2.1                    py37_0  \nattrs                     18.2.0           py37h28b3542_0  \nbabel                     2.6.0                    py37_0  \nbackcall                  0.1.0                    py37_0  \nbackports                 1.0                      py37_1  \nbackports.os              0.1.1                    py37_0  \nbackports.shutil_get_terminal_size 1.0.0                    py37_2  \nbeautifulsoup4            4.6.3                    py37_0  \nbitarray                  0.8.3            py37h14c3975_0  \nbkcharts                  0.2                      py37_0  \nblas                      1.0                         mkl  \nblaze                     0.11.3                   py37_0  \nbleach                    3.0.2                    py37_0  \nblosc                     1.14.4               hdbcaa40_0  \nbokeh                     1.0.2                    py37_0  \nboto                      2.49.0                   py37_0  \nbottleneck                1.2.1            py37h035aef0_1  \nbzip2                     1.0.6                h14c3975_5  \nca-certificates           2020.7.22                     0    anaconda\ncairo                     1.14.12              h8948797_3  \ncertifi                   2020.6.20                py37_0    anaconda\ncffi                      1.11.5           py37he75722e_1  \nchardet                   3.0.4                    py37_1  \nclick                     7.0                      py37_0  \ncloudpickle               0.6.1                    py37_0  \nclyent                    1.2.2                    py37_1  \ncolorama                  0.4.1                    py37_0  \nconda                     4.5.12                   py37_0  \nconda-build               3.17.6                   py37_0  \nconda-env                 2.6.0                         1  \nconda-verify              3.1.1                    py37_0  \ncontextlib2               0.5.5                    py37_0  \ncryptography              2.4.2            py37h1ba5d50_0  \ncurl                      7.63.0            hbc83047_1000  \ncycler                    0.10.0                   py37_0    anaconda\ncython                    0.29.2           py37he6710b0_0  \ncytoolz                   0.9.0.1          py37h14c3975_1  \ndask                      1.0.0                    py37_0  \ndask-core                 1.0.0                    py37_0  \ndatashape                 0.5.4                    py37_1  \ndbus                      1.13.12              h746ee38_0    anaconda\ndecorator                 4.3.0                    py37_0  \ndefusedxml                0.5.0                    py37_1  \ndistributed               1.25.1                   py37_0  \ndocutils                  0.14                     py37_0  \nentrypoints               0.2.3                    py37_2  \net_xmlfile                1.0.1                    py37_0  \nexpat                     2.2.9                he6710b0_2    anaconda\nfastcache                 1.0.2            py37h14c3975_2  \nfilelock                  3.0.10                   py37_0  \nflask                     1.0.2                    py37_1  \nflask-cors                3.0.7                    py37_0  \nfontconfig                2.13.0               h9420a91_0    anaconda\nfreetype                  2.10.2               h5ab3b9f_0    anaconda\nfribidi                   1.0.5                h7b6447c_0  \nfuture                    0.17.1                   py37_0  \nget_terminal_size         1.0.0                haa9412d_0  \ngevent                    1.3.7            py37h7b6447c_1  \nglib                      2.56.2               hd408876_0    anaconda\nglob2                     0.6                      py37_1  \ngmp                       6.1.2                h6c8ec71_1  \ngmpy2                     2.0.8            py37h10f8cd9_2  \ngraphite2                 1.3.12               h23475e2_2  \ngreenlet                  0.4.15           py37h7b6447c_0  \ngst-plugins-base          1.14.0               hbbd80ab_1    anaconda\ngstreamer                 1.14.0               hb453b48_1    anaconda\nh5py                      2.8.0            py37h989c5e5_3  \nharfbuzz                  1.8.8                hffaf4a1_0  \nhdf5                      1.10.2               hba1933b_1  \nheapdict                  1.0.0                    py37_2  \nhtml5lib                  1.0.1                    py37_0  \nicu                       58.2                 he6710b0_3    anaconda\nidna                      2.8                      py37_0  \nimageio                   2.4.1                    py37_0  \nimagesize                 1.1.0                    py37_0  \nimportlib_metadata        0.6                      py37_0  \nintel-openmp              2019.1                      144  \nipykernel                 5.1.0            py37h39e3cac_0  \nipython                   7.2.0            py37h39e3cac_0  \nipython_genutils          0.2.0                    py37_0  \nipywidgets                7.4.2                    py37_0  \nisort                     4.3.4                    py37_0  \nitsdangerous              1.1.0                    py37_0  \njbig                      2.1                  hdba287a_0  \njdcal                     1.4                      py37_0  \njedi                      0.13.2                   py37_0  \njeepney                   0.4                      py37_0  \njinja2                    2.10                     py37_0  \njpeg                      9b                   habf39ab_1    anaconda\njsonschema                2.6.0                    py37_0  \njupyter                   1.0.0                    py37_7  \njupyter_client            5.2.4                    py37_0  \njupyter_console           6.0.0                    py37_0  \njupyter_core              4.4.0                    py37_0  \njupyterlab                0.35.3                   py37_0  \njupyterlab_server         0.2.0                    py37_0  \nkeyring                   17.0.0                   py37_0  \nkiwisolver                1.2.0            py37hfd86e86_0    anaconda\nkrb5                      1.16.1               h173b8e3_7  \nlazy-object-proxy         1.3.1            py37h14c3975_2  \nlcms2                     2.11                 h396b838_0    anaconda\nld_impl_linux-64          2.33.1               h53a641e_7    anaconda\nlibarchive                3.3.3                h5d8350f_5  \nlibcurl                   7.63.0            h20c2e04_1000  \nlibedit                   3.1.20191231         h14c3975_1    anaconda\nlibffi                    3.3                  he6710b0_2    anaconda\nlibgcc-ng                 9.1.0                hdf63c60_0    anaconda\nlibgfortran-ng            7.3.0                hdf63c60_0  \nliblief                   0.9.0                h7725739_1  \nlibpng                    1.6.37               hbc83047_0    anaconda\nlibsodium                 1.0.16               h1bed415_0  \nlibssh2                   1.8.0                h1ba5d50_4  \nlibstdcxx-ng              8.2.0                hdf63c60_1  \nlibtiff                   4.1.0                h2733197_1    anaconda\nlibtool                   2.4.6                h7b6447c_5  \nlibuuid                   1.0.3                h1bed415_2    anaconda\nlibxcb                    1.14                 h7b6447c_0    anaconda\nlibxml2                   2.9.10               he19cac6_1    anaconda\nlibxslt                   1.1.32               h1312cb7_0  \nllvmlite                  0.26.0           py37hd408876_0  \nlocket                    0.2.0                    py37_1  \nlxml                      4.2.5            py37hefd8a0e_0  \nlz4-c                     1.9.2                he6710b0_1    anaconda\nlzo                       2.10                 h49e0be7_2  \nmarkupsafe                1.1.0            py37h7b6447c_0  \nmatplotlib                3.3.1                         0    anaconda\nmatplotlib-base           3.3.1            py37h817c723_0    anaconda\nmccabe                    0.6.1                    py37_1  \nmistune                   0.8.4            py37h7b6447c_0  \nmkl                       2019.1                      144  \nmkl-service               1.1.2            py37he904b0f_5  \nmkl_fft                   1.0.10           py37ha843d7b_0    anaconda\nmkl_random                1.0.2            py37hd81dba3_0    anaconda\nmore-itertools            4.3.0                    py37_0  \nmpc                       1.1.0                h10f8cd9_1  \nmpfr                      4.0.1                hdf1c602_3  \nmpmath                    1.1.0                    py37_0  \nmsgpack-python            0.5.6            py37h6bb024c_1  \nmultipledispatch          0.6.0                    py37_0  \nnavigator-updater         0.2.1                    py37_0  \nnbconvert                 5.4.0                    py37_1  \nnbformat                  4.4.0                    py37_0  \nncurses                   6.2                  he6710b0_1    anaconda\nnetworkx                  2.2                      py37_1  \nnltk                      3.4                      py37_1  \nnose                      1.3.7                    py37_2  \nnotebook                  5.7.4                    py37_0  \nnumba                     0.41.0           py37h962f231_0  \nnumexpr                   2.6.8            py37h9e4a6bb_0  \nnumpy                     1.16.2           py37h7e9f1db_0    anaconda\nnumpy-base                1.16.2           py37hde5b4d6_0    anaconda\nnumpydoc                  0.8.0                    py37_0  \nodo                       0.5.1                    py37_0  \nolefile                   0.46                       py_0    anaconda\nopenpyxl                  2.5.12                   py37_0  \nopenssl                   1.1.1g               h7b6447c_0    anaconda\npackaging                 18.0                     py37_0  \npandas                    1.1.1            py37he6710b0_0    anaconda\npandoc                    1.19.2.1             hea2e7c5_1  \npandocfilters             1.4.2                    py37_1  \npango                     1.42.4               h049681c_0  \nparso                     0.3.1                    py37_0  \npartd                     0.3.9                    py37_0  \npatchelf                  0.9                  he6710b0_3  \npath.py                   11.5.0                   py37_0  \npathlib2                  2.3.3                    py37_0  \npatsy                     0.5.1                    py37_0  \npcre                      8.44                 he6710b0_0    anaconda\npep8                      1.7.1                    py37_0  \npexpect                   4.6.0                    py37_0  \npickleshare               0.7.5                    py37_0  \npillow                    7.2.0            py37hb39fc2d_0    anaconda\npip                       20.2.2                   py37_0    anaconda\npixman                    0.34.0               hceecf20_3  \npkginfo                   1.4.2                    py37_1  \npluggy                    0.8.0                    py37_0  \nply                       3.11                     py37_0  \nprometheus_client         0.5.0                    py37_0  \nprompt_toolkit            2.0.7                    py37_0  \npsutil                    5.4.8            py37h7b6447c_0  \nptyprocess                0.6.0                    py37_0  \npy                        1.7.0                    py37_0  \npy-lief                   0.9.0            py37h7725739_1  \npycodestyle               2.4.0                    py37_0  \npycosat                   0.6.3            py37h14c3975_0  \npycparser                 2.19                     py37_0  \npycrypto                  2.6.1            py37h14c3975_9  \npycurl                    7.43.0.2         py37h1ba5d50_0  \npyflakes                  2.0.0                    py37_0  \npygments                  2.3.1                    py37_0  \npylint                    2.2.2                    py37_0  \npyodbc                    4.0.25           py37he6710b0_0  \npyopenssl                 18.0.0                   py37_0  \npyparsing                 2.4.7                      py_0    anaconda\npyqt                      5.9.2            py37h22d08a2_1    anaconda\npysocks                   1.6.8                    py37_0  \npytables                  3.4.4            py37ha205bf6_0  \npytest                    4.0.2                    py37_0  \npytest-arraydiff          0.3              py37h39e3cac_0  \npytest-astropy            0.5.0                    py37_0  \npytest-doctestplus        0.2.0                    py37_0  \npytest-openfiles          0.3.1                    py37_0  \npytest-remotedata         0.3.1                    py37_0  \npython                    3.7.9                h7579374_0    anaconda\npython-dateutil           2.8.1                      py_0    anaconda\npython-libarchive-c       2.8                      py37_6  \npytz                      2020.1                     py_0    anaconda\npywavelets                1.0.1            py37hdd07704_0  \npyyaml                    3.13             py37h14c3975_0  \npyzmq                     17.1.2           py37h14c3975_0  \nqt                        5.9.7                h5867ecd_1    anaconda\nqtawesome                 0.5.3                    py37_0  \nqtconsole                 4.4.3                    py37_0  \nqtpy                      1.5.2                    py37_0  \nreadline                  8.0                  h7b6447c_0    anaconda\nrequests                  2.21.0                   py37_0  \nrope                      0.11.0                   py37_0  \nruamel_yaml               0.15.46          py37h14c3975_0  \nscikit-image              0.14.1           py37he6710b0_0  \nscikit-learn              0.20.1           py37hd81dba3_0  \nscipy                     1.2.1            py37h7c811a0_0    anaconda\nseaborn                   0.10.1                     py_0    anaconda\nsecretstorage             3.1.0                    py37_0  \nsend2trash                1.5.0                    py37_0  \nsetuptools                49.6.0                   py37_0    anaconda\nsimplegeneric             0.8.1                    py37_2  \nsingledispatch            3.4.0.3                  py37_0  \nsip                       4.19.24          py37he6710b0_0    anaconda\nsix                       1.15.0                     py_0    anaconda\nsnappy                    1.1.7                hbae5bb6_3  \nsnowballstemmer           1.2.1                    py37_0  \nsortedcollections         1.0.1                    py37_0  \nsortedcontainers          2.1.0                    py37_0  \nsphinx                    1.8.2                    py37_0  \nsphinxcontrib             1.0                      py37_1  \nsphinxcontrib-websupport  1.1.0                    py37_1  \nspyder                    3.3.2                    py37_0  \nspyder-kernels            0.3.0                    py37_0  \nsqlalchemy                1.2.15           py37h7b6447c_0  \nsqlite                    3.33.0               h62c20be_0    anaconda\nstatsmodels               0.9.0            py37h035aef0_0  \nsympy                     1.3                      py37_0  \ntblib                     1.3.2                    py37_0  \nterminado                 0.8.1                    py37_1  \ntestpath                  0.4.2                    py37_0  \ntk                        8.6.10               hbc83047_0    anaconda\ntoolz                     0.9.0                    py37_0  \ntornado                   6.0.4            py37h7b6447c_1    anaconda\ntqdm                      4.28.1           py37h28b3542_0  \ntraitlets                 4.3.2                    py37_0  \nunicodecsv                0.14.1                   py37_0  \nunixodbc                  2.3.7                h14c3975_0  \nurllib3                   1.24.1                   py37_0  \nwcwidth                   0.1.7                    py37_0  \nwebencodings              0.5.1                    py37_1  \nwerkzeug                  0.14.1                   py37_0  \nwheel                     0.35.1                     py_0    anaconda\nwidgetsnbextension        3.4.2                    py37_0  \nwrapt                     1.10.11          py37h14c3975_2  \nwurlitzer                 1.0.2                    py37_0  \nxlrd                      1.2.0                    py37_0  \nxlsxwriter                1.1.2                    py37_0  \nxlwt                      1.3.0                    py37_0  \nxz                        5.2.5                h7b6447c_0    anaconda\nyaml                      0.1.7                had09818_2  \nzeromq                    4.2.5                hf484d3e_1  \nzict                      0.1.3                    py37_0  \nzlib                      1.2.11               h7b6447c_3    anaconda\nzstd                      1.4.4                h0b5b093_3    anaconda\n<\/code><\/pre>",
        "Challenge_closed_time":1599452239416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599439961223,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"the user encountered an importerror when attempting to draw a pairplot using seaborn in a new compute instance.",
        "Challenge_last_edit_time":1603385843288,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63770171",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.4,
        "Challenge_reading_time":102.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":282,
        "Challenge_solved_time":3.4106091667,
        "Challenge_title":"\"ImportError: No module named seaborn\" in Azure ML",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1167.0,
        "Challenge_word_count":958,
        "Platform":"Stack Overflow",
        "Poster_created_time":1245726715288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cumming, GA",
        "Poster_reputation_count":77230.0,
        "Poster_view_count":6359.0,
        "Solution_body":"<p>I just did the following and wasn't able to reproduce your error:<\/p>\n<ol>\n<li>make a new compute instance<\/li>\n<li>open it up using JupyterLab<\/li>\n<li>open a new terminal<\/li>\n<li><code>conda activate azureml_py36<\/code><\/li>\n<li><code>conda install seaborn -y<\/code><\/li>\n<li>open a new notebook and run <code>import seaborn as sns<\/code><\/li>\n<\/ol>\n<h3>Spitballing<\/h3>\n<ol>\n<li>Are you using the kernel, <code>Python 3.6 - AzureML<\/code> (i.e. the <code>azureml_py36<\/code> conda env)?<\/li>\n<li>Have you tried restarting the kernel and\/or creating a new compute instance?<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kPbLH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kPbLH.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":9.92,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.83,
        "Challenge_answer_count":0,
        "Challenge_body":"logs: \r\n\r\n```\r\nRun papermill notebooks\/sklearn\/train-diabetes-mlproject.ipynb out.ipynb -k python\r\nInput Notebook:  notebooks\/sklearn\/train-diabetes-mlproject.ipynb\r\nOutput Notebook: out.ipynb\r\n\r\nExecuting:   0%|          | 0\/7 [00:00<?, ?cell\/s]Executing notebook with kernel: python\r\n\r\nExecuting:  14%|\u2588\u258d        | 1\/7 [00:01<00:07,  1.33s\/cell]\r\nExecuting:  29%|\u2588\u2588\u258a       | 2\/7 [00:02<00:07,  1.43s\/cell]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4\/7 [00:05<00:03,  1.32s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:07<00:01,  1.34s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:08<00:01,  1.40s\/cell]\r\nTraceback (most recent call last):\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/bin\/papermill\", line 8, in <module>\r\n    sys.exit(papermill())\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/cli.py\", line 240, in papermill\r\n    execute_notebook(\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 110, in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 222, in raise_for_execution_errors\r\n    raise error\r\npapermill.exceptions.PapermillExecutionError: \r\n---------------------------------------------------------------------------\r\nException encountered at \"In [5]\":\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-5-ef514d3992f5> in <module>\r\n----> 1 run = mlflow.projects.run(\r\n      2     uri=str(project_uri),\r\n      3     parameters=***\"alpha\": 0.3***,\r\n      4     backend=\"azureml\",\r\n      5     backend_config=backend_config,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in run(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\r\n    271     )\r\n    272 \r\n--> 273     submitted_run_obj = _run(\r\n    274         uri=uri,\r\n    275         experiment_id=experiment_id,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in _run(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, use_conda, storage_dir, synchronous)\r\n     98         backend = loader.load_backend(backend_name)\r\n     99         if backend:\r\n--> 100             submitted_run = backend.run(\r\n    101                 uri,\r\n    102                 entry_point,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/mlflow\/_internal\/projects.py in run(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id)\r\n    240         if compute and compute != _LOCAL and compute != _LOCAL.upper():\r\n    241             remote_environment = _load_remote_environment(mlproject)\r\n--> 242             remote_environment.register(workspace=workspace)\r\n    243             cpu_cluster = _load_compute_target(workspace, backend_config)\r\n    244             src.run_config.target = cpu_cluster.name\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/environment.py in register(self, workspace)\r\n    803         environment_client = EnvironmentClient(workspace.service_context)\r\n    804         environment_dict = Environment._serialize_to_dict(self)\r\n--> 805         response = environment_client._register_environment_definition(environment_dict)\r\n    806         env = Environment._deserialize_and_add_to_object(response)\r\n    807 \r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/_restclient\/environment_client.py in _register_environment_definition(self, environment_dict)\r\n     75             message = \"Error registering the environment definition. Code: ***\\n: ***\".format(response.status_code,\r\n     76                                                                                             response.text)\r\n---> 77             raise Exception(message)\r\n     78 \r\n     79     def _get_image_details(self, name, version=None):\r\n\r\nException: Error registering the environment definition. Code: 409\r\n: ***\r\n  \"error\": ***\r\n    \"code\": \"TransientError\",\r\n    \"severity\": null,\r\n    \"message\": \"Etag conflict on 0e149764-3720-4610-b0f3-3e3f974544ac\/8f54aa7d6c05b2722ba149d8ea3185c263ecf5310eb2d7271569d1918c736972 with etag .\",\r\n    \"messageFormat\": null,\r\n    \"messageParameters\": null,\r\n    \"referenceCode\": null,\r\n    \"detailsUri\": null,\r\n    \"target\": null,\r\n    \"details\": [],\r\n    \"innerError\": null,\r\n    \"debugInfo\": null\r\n  ***,\r\n  \"correlation\": ***\r\n    \"operation\": \"db22e6e6bfa07f499f1749f708b798c9\",\r\n    \"request\": \"f470e9430c5ed842\"\r\n  ***,\r\n  \"environment\": \"eastus\",\r\n  \"location\": \"eastus\",\r\n  \"time\": \"2020-10-01T20:17:52.8383774+00:00\",\r\n  \"componentName\": \"environment-management\"\r\n***\r\n\r\nError: Process completed with exit code 1.\r\n```",
        "Challenge_closed_time":1601658581000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601583593000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an MlflowException while running a double ensemble on Google Colab. They were able to solve some issues by cloning the repo and reinstalling numpy, but they are unsure how to solve this particular issue. The error message indicates an invalid value for metric 'IC' and a value error related to duplicate bin edges. The user followed instructions to download cn data and provided a yaml file with configuration details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1170",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":20.3,
        "Challenge_reading_time":69.38,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":20.83,
        "Challenge_title":"mlflow.projects.run failing consistently with etag error ",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":338,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1306431192840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Rafael, CA, USA",
        "Answerer_reputation_count":5369.0,
        "Answerer_view_count":637.0,
        "Challenge_adjusted_solved_time":27.9719833334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Thanks for getting back to me.<\/p>\n\n<p>Basically I subscribed to a Cluster API service (cortana analytics). This is the sample application as per Microsoft Machine Learning site<\/p>\n\n<p><a href=\"http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx\" rel=\"nofollow\">http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx<\/a><\/p>\n\n<p>As you could see there are 2 arguments to be passed on<\/p>\n\n<p>Input<\/p>\n\n<p>K<\/p>\n\n<p>Where input could be 10;5;2,18;1;6,7;5;5,22;3;4,12;2;1,10;3;4 (each row is separated by semi colon)<\/p>\n\n<p>And K is cluster number: 5 (for example)<\/p>\n\n<p>So to consume this API I use PowerBI Edit Query, <\/p>\n\n<p>So go to Get Data > More > Azure > Microsoft Data MarketPlace, I can see the list of APIs I subscribed to, one of them is the one I referred to in the link above.<\/p>\n\n<p>So I load that as Function lets called it \"Score\"<\/p>\n\n<p>Then I got energy table which I loaded in from a csv file, I want to cluster energy consumption into 5 clusters.<\/p>\n\n<p>So my data layout is<\/p>\n\n<p>Year   Energy<\/p>\n\n<p>2001   6.28213<\/p>\n\n<p>2002  14.12845<\/p>\n\n<p>2003   5.55851<\/p>\n\n<p>and so on, lets say I got 100 rows of the data.<\/p>\n\n<p>So I tried to pass \"6.28213;14.12845;5.55851\", \"5\" to Score function but I dont know how to <\/p>\n\n<ol>\n<li><p>Convert my table into records<\/p><\/li>\n<li><p>pass 2 argument records and constant value 5 as K.<\/p><\/li>\n<\/ol>\n\n<p>Hope this makes sense.<\/p>\n\n<p>Please help! :)<\/p>\n\n<p>Thank you in advance.<\/p>",
        "Challenge_closed_time":1476896300823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476795601683,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble consuming a Cluster API service from Cortana Analytics using PowerBI Edit Query. They have loaded the API as a function and have a table of energy consumption data that they want to cluster into 5 clusters. However, they are unsure how to convert their table into records and pass two argument records and a constant value of 5 as K to the Score function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40108999",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":19.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":27.9719833334,
        "Challenge_title":"Consume Microsoft Cluster API using PowerBI",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":137.0,
        "Challenge_word_count":215,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427899203630,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":403.0,
        "Poster_view_count":200.0,
        "Solution_body":"<p>To convert a column of numbers into a semicolon delimited text, do this to your table:<\/p>\n\n<ol>\n<li>Convert your Energy column is type text.<\/li>\n<li>Add <code>[Energy]<\/code> after the name of your table, which gives you a list of the numbers.<\/li>\n<li>Use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/mt253358.aspx\" rel=\"nofollow\"><code>Text.Combine<\/code><\/a> to turn the list into a text value seperated by <code>;<\/code><\/li>\n<\/ol>\n\n<p>Here's a mashup that does that:<\/p>\n\n<pre><code>let\n    Source = Table.FromRows(Json.Document(Binary.Decompress(Binary.FromText(\"NcjBCQAgDAPAXfKWYqKR7iLdfw1F8J63N9Q70bBCKQ5Ue6VbnEHl9L9xz2GniaoD\", BinaryEncoding.Base64), Compression.Deflate)), let _t = ((type text) meta [Serialized.Text = true]) in type table [Year = _t, Energy = _t]),\n    #\"Changed Type\" = Table.TransformColumnTypes(Source,{{\"Year\", Int64.Type}, {\"Energy\", type text}}),\n    Custom1 = #\"Changed Type\"[Energy],\n    Custom2 = Text.Combine(Custom1, \";\")\nin\n    Custom2\n<\/code><\/pre>\n\n<hr>\n\n<p>Once you have a function, you'll invoke it like <code>YourFunction(Custum2, 5)<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":14.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":108.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":54.3333333333,
        "Challenge_answer_count":4,
        "Challenge_body":"Hello,\n\nI am running into an issue with timeout value for environment specific webhooks in Dialogflow CX.\n\nWe have extended the timeout value to 15 seconds for the webhook and we have multiple environment specific webhook URLs set as well.\u00a0 You can see precise setup as part of this issue posted by another user facing the exact same issue 3 months back :\u00a0https:\/\/issuetracker.google.com\/issues\/261683010\n\nIt seems like the webhook timeout value is not being considered for environment specific URLs.\u00a0 I see it does work for the agent level URL which is not what I would like to use as we have for obvious reasons different URL for non-prod and production apps.\n\nIs there any workaround or possible solution to this problem?\n\nCheers, jags",
        "Challenge_closed_time":1678347120000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678151520000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the timeout value for environment-specific webhooks in Dialogflow CX. Despite extending the timeout value to 15 seconds, the webhook timeout value is not being considered for environment-specific URLs. The user is looking for a possible solution or workaround to this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-environment-specific-webhook-timeout-is-not\/m-p\/529553#M1383",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":9.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":54.3333333333,
        "Challenge_title":"Dialogflow CX environment specific webhook timeout is not considered",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":130,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Found a suitable workaround:\n\nUpdate the default webhook URL to production with timeout 15 seconds.\u00a0 Remove the environment specific webhook URL.\nRelease a version of the flow and use it with the production environment.\nUpdate the default webhook URL back to test endpoint with timeout 15 seconds.\nRepeat the steps before releasing the new version for production usage.\n\nWhen you release the version of a flow, Dialogflow CX would create the snapshot of webhook configuration as well along with flow.\u00a0 And when you update the default webhook URL again, it would only update for unpublished\/draft version and it does not affect the webhook URL in the released version of the flow.\u00a0 It's verified and working approach!\n\nCheers, jags\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":9.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1559607319220,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":510.8353358333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create Sagemaker notebook instance using cloudformation template. Just wanted to see if there is any way i can associate codecommit repo to that note book instance.<\/p>\n\n<p>I know simple way to create repo and associate it using sagemaker via GUI easily.. However, is there any way we can associate via template. <\/p>\n\n<p>I found similar info on <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html<\/a> But it doesn't show with cloudformation<\/p>",
        "Challenge_closed_time":1559608178472,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557513388300,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a Sagemaker notebook instance using a cloudformation template and wants to know if there is a way to associate a CodeCommit repo to the instance through the template. The user has found information on how to do this through the Sagemaker GUI but has not found any information on how to do it through the cloudformation template.",
        "Challenge_last_edit_time":1557769171263,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56083148",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":581.8861588889,
        "Challenge_title":"Associate CodeCommit repo While creating Sagemaker notebook Instance",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1194.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_created_time":1553287213987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>SageMaker now supports associating code repository including CodeCommit and any other Git repository with Notebook Instances via CloudFormation. <\/p>\n\n<p>Here's the link for more information: <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-coderepository.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-coderepository.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":30.7,
        "Solution_reading_time":6.18,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":320.9530555556,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi, I am trying to install some libraries in Studio Lab which requires root privileges. \r\n\r\nBelow I have run `whoami` to check if I am root user. (I am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nBelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\nI followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\nOn running `su -` , It asks for the password, but we don't have any password for Studio Lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\nCan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nPlease let me know if my query is not clear. ",
        "Challenge_closed_time":1655933766000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1654778335000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to open a database file and is encountering an unexpected error while saving a file. They have deleted some unwanted notebooks from the studio lab's files and are now unable to install libraries with pip, create new files, or start the kernel.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/118",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.06,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":320.9530555556,
        "Challenge_title":"How to get root access in SageMaker Studio Lab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":122,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you for trying Studio Lab. Now Studio Lab does not allow `sudo` (similar issue: https:\/\/github.com\/aws\/studio-lab-examples\/issues\/40#issuecomment-1005305538). We can use `pip` and `conda` instead. Please refer the following issue.\r\n\r\nWhat software do you try to install? Some libraries will be available in `conda-forge` . Here is the sample of search.\r\n\r\nhttps:\/\/anaconda.org\/search?q=gym Thanks for the reply, I will try to explain the issue.\r\n I am trying to run bipedal robot from Open-ai gym. \r\n```\r\n!pip install gym\r\n!apt-get update\r\n!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> \/dev\/null\r\n!apt-get install xvfb\r\n!pip install pyvirtualdisplay \r\n!pip -q install pyglet\r\n!pip -q install pyopengl\r\n!apt-get install swig\r\n!pip install box2d box2d-kengz\r\n!pip install pybullet\r\n```\r\nThese are the libraries which I need to install for the code to work. \r\nIt works fine on google-colab: (screenshot below)\r\n![colab_gym_ss](https:\/\/user-images.githubusercontent.com\/91401599\/173034039-1973ee00-6c0b-4f49-adad-9bb324c59b8c.png)\r\n\r\nBut it throws error when I run it on SageMaker Studio Lab: (screenshot below)\r\n![sagemaker1](https:\/\/user-images.githubusercontent.com\/91401599\/173034679-f49340dc-de46-42bb-be1b-7c7e3616dac4.png)\r\n\r\nError Log (I have made gym_install.sh file which installs everything described above, I am running it below.): \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ sh gym_install.sh\r\nRequirement already satisfied: gym in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (0.24.1)\r\nRequirement already satisfied: gym-notices>=0.0.4 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (0.0.7)\r\nRequirement already satisfied: numpy>=1.18.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (1.22.4)\r\nRequirement already satisfied: cloudpickle>=1.2.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (2.1.0)\r\nReading package lists... Done\r\nE: List directory \/var\/lib\/apt\/lists\/partial is missing. - Acquire (13: Permission denied)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nRequirement already satisfied: pyvirtualdisplay in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.0)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nCollecting box2d\r\n  Using cached Box2D-2.3.2.tar.gz (427 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting box2d-kengz\r\n  Using cached Box2D-kengz-2.3.3.tar.gz (425 kB)\r\n  Preparing metadata (setup.py) ... done\r\nBuilding wheels for collected packages: box2d, box2d-kengz\r\n  Building wheel for box2d (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d\r\n  Running setup.py clean for box2d\r\n  Building wheel for box2d-kengz (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d-kengz\r\n  Running setup.py clean for box2d-kengz\r\nFailed to build box2d box2d-kengz\r\nInstalling collected packages: box2d-kengz, box2d\r\n  Running setup.py install for box2d-kengz ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Running setup.py install for box2d-kengz did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [18 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running install\r\n      \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n        warnings.warn(\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: legacy-install-failure\r\n\r\n\u00d7 Encountered error while trying to install package.\r\n\u2570\u2500> box2d-kengz\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for output from the failure.\r\nRequirement already satisfied: pybullet in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.2.5)\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\n### To be specific I am getting error in this line:\r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ apt-get install xvfb\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\n```\r\nSo as mentioned by you I tried to find xvfb in conda-forge, but couldn't find it. \r\nThere are some wrapper xvfb on conda-forge but installing them didn't help with the error. \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ python check.py\r\nTraceback (most recent call last):\r\n  File \"\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/GM_\/ARS_src\/check.py\", line 9, in <module>\r\n    display = Display(visible=0, size=(1024, 768))\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/display.py\", line 54, in __init__\r\n    self._obj = cls(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/xvfb.py\", line 44, in __init__\r\n    AbstractDisplay.__init__(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/abstractdisplay.py\", line 85, in __init__\r\n    helptext = get_helptext(program)\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/util.py\", line 13, in get_helptext\r\n    p = subprocess.Popen(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 966, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 1842, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\nTo reproduce above error run below code (check.py) :->\r\n```\r\nimport os\r\nimport numpy as np\r\nimport gym\r\nfrom gym import wrappers\r\nimport pyvirtualdisplay\r\nfrom pyvirtualdisplay import Display\r\n\r\nif __name__ == \"__main__\":\r\n    display = Display(visible=0, size=(1024, 768))\r\n```\r\n Thank you for sharing the error message. Studio Lab does not allow `apt install` so that the `gym_install.sh` does not work straightly. We have to prepare the environment by `conda` and `pip`.\r\n\r\nAt first we can install `swig` from `conda`. We can not install `xvfb` from `conda`, is this necessary to run the code?  Basically I have to capture the video output using `Display` of `pyvirtualdisplay` library. Everything else works. \r\nAs you can see `display = Display(visible=0, size=(1024, 768))` this line throws error  `FileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'` , so I am trying to install `Xvfb`.\r\n\r\nThanks, @ar8372 Sorry for the late reply. I raised the #124 to work OpenAI Gym in Studio Lab. Please comment to #124 if you have additional information. We need your insight to solve the issue. If you do not mind, please close this issue to suppress the duplication of issues.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.2,
        "Solution_reading_time":134.31,
        "Solution_score_count":null,
        "Solution_sentence_count":129.0,
        "Solution_word_count":1049.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1489644560420,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Planet Earth",
        "Answerer_reputation_count":791.0,
        "Answerer_view_count":253.0,
        "Challenge_adjusted_solved_time":3.8482858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to install python package cassandra driver in Azure Machine Learning studio. I am following this answer from <a href=\"https:\/\/stackoverflow.com\/questions\/44371692\/install-python-packages-in-azure-ml\">here<\/a>. Unfortunately i don't see any wheel file for cassandra-driver <a href=\"https:\/\/pypi.python.org\/pypi\/cassandra-driver\/\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/cassandra-driver\/<\/a> so i downloaded the .tar file and converted to zip.<\/p>\n<p>I included this .zip file as dataset and connected to python script<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" alt=\"jpg1\" \/><\/a><\/p>\n<p>But when i run it, it says No module named cassandra\n<a href=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" alt=\"jpg2\" \/><\/a><\/p>\n<p>Does this work only with wheel file? Any solution is much appreciated.<\/p>\n<p>I am using Python Version :  Anoconda 4.0\/Python 3.5<\/p>",
        "Challenge_closed_time":1519124227916,
        "Challenge_comment_count":2,
        "Challenge_created_time":1519110374087,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an ImportError while trying to install the cassandra driver python package in Azure Machine Learning Studio. They downloaded the .tar file and converted it to a zip file, included it as a dataset, and connected it to a python script. However, when they run it, it says \"No module named cassandra\". The user is using Python Version: Anoconda 4.0\/Python 3.5 and is seeking a solution to the issue.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48879595",
        "Challenge_link_count":7,
        "Challenge_participation_count":3,
        "Challenge_readability":10.9,
        "Challenge_reading_time":14.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":3.8482858333,
        "Challenge_title":"ImportError: No module named cassandra in Azure Machine Learning Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":500.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489644560420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Planet Earth",
        "Poster_reputation_count":791.0,
        "Poster_view_count":253.0,
        "Solution_body":"<p>I got it working. Changed the folder inside .zip file to <code>\"cassandra\"<\/code> (just like cassandra package). <\/p>\n\n<p>And in the Python script, i added <\/p>\n\n<pre><code>from cassandra import *\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":29.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":15.5657958333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have done quite a few google searches but have not found a clear answer to the following use case. Basically, I would rather use cloud 9 (most of the time) as my IDE rather than Jupyter. What I am confused\/not sure about is, how I could executed long running jobs like (Bayesian) hyper parameter optimisation from there. Can I use Sagemaker capabilities? Should I use docker and deploy to ECR (looking for the cheapest-ish option)? Any pointers w.r.t. to this particular issue would be very much appreciated. Thanks.<\/p>",
        "Challenge_closed_time":1641735920212,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641644416603,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for guidance on how to execute long running jobs like hyper parameter optimization using Cloud 9 as their IDE instead of Jupyter. They are unsure if they can use Sagemaker capabilities or if they should use docker and deploy to ECR. They are seeking advice on the most cost-effective option.",
        "Challenge_last_edit_time":1641679883347,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70632239",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":25.4176691667,
        "Challenge_title":"cloud 9 and sagemaker - hyper parameter optimisation",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>You could use whatever IDE you choose (including your laptop).<br \/>\nSaegMaker tuning job (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex.html\" rel=\"nofollow noreferrer\">example<\/a>) is <strong>asynchronous<\/strong>, so you can safely close your IDE after launching it. You can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-monitor.html\" rel=\"nofollow noreferrer\">monitor the job the AWS web console,<\/a> or with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeHyperParameterTuningJob.html\" rel=\"nofollow noreferrer\">DescribeHyperParameterTuningJob API call<\/a>.<\/p>\n<p>You can launch TensorFlow, PyTorch, XGBoost, Scikit-learn, and other <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/frameworks.html\" rel=\"nofollow noreferrer\">popular ML frameworks<\/a>, using one of the built-in framework containers, avoiding the extra work of bringing your own container.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":20.2,
        "Solution_reading_time":13.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.9523508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am learning about Kubernetes online endpoints and my objective is to monitor my resources which are deployed using Kubernetes endpoints. Is there any provision to get out-of-the-box monitoring to Kubernetes online endpoints to check the performance. I am new to this domain. Any help is appreciated.<\/p>",
        "Challenge_closed_time":1652337837120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652334408657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to monitor resources deployed using Kubernetes online endpoints and is looking for out-of-the-box monitoring options. They are new to this domain and are seeking assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72210628",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.9523508333,
        "Challenge_title":"How to manage out of the box monitoring using Kubernetes online endpoints",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652333709927,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>The general monitoring is available and supportive in AKS, but the out of the box implementation was not supportive unfortunately. Check the below documentation and screenshot to refer supportive formats of monitoring in AKS<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0uxW5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0uxW5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.8,
        "Solution_reading_time":8.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1659144422092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":166.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":1.1739147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>l'am running a ipynb file on sagemaker, however the error of occurs.\nl have used 'pip install tqdm' in terminals to install the tqdm so l've no idea what's happening. Is it running in a different environment?\nThanks for any answer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bO6zS.png\" rel=\"nofollow noreferrer\">error report from my ipynb file <\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5tP1J.png\" rel=\"nofollow noreferrer\">what l've done in terminal<\/a><\/p>",
        "Challenge_closed_time":1659155782776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659151516027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to import a certain module while running an ipynb file on Sagemaker. They have tried installing the module using 'pip install' in the terminal but are still encountering an error. They are unsure if the file is running in a different environment.",
        "Challenge_last_edit_time":1659151556683,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73172644",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":6.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.1852080556,
        "Challenge_title":"Running ipynb file, but can't import a certain module",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1658300099523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>There is a possibility you may be executing &quot;pip&quot; in a different environment.<\/p>\n<p>Try executing &quot;!pip install tqdm&quot; or &quot;!pip3 install tqdm&quot; as a code cell in the Sagemaker document itself.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":2.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1587281590603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":473.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":84.1130386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<h2>The problem<\/h2>\n<p>I have a jupyter notebook with an <code>ipydatagrid<\/code> widget that displays a dataframe. This notebook works correctly when run locally, but not when run in AWS SageMaker Studio. When run in SageMaker Studio, instead of showing the widget it simply shows the text <code>Loading widget...<\/code><\/p>\n<p>How does one use a <code>ipydatagrid<\/code> widget in the SageMaker Studio environment?<\/p>\n<h2>Details<\/h2>\n<p>Python version:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ python --version\nPython 3.7.10\n<\/code><\/pre>\n<p>Run at start:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip install -r requirements.txt\n$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n$ jupyter nbextension install --py --symlink --sys-prefix ipydatagrid\n$ jupyter nbextension enable --py --sys-prefix ipydatagrid\n<\/code><\/pre>\n<p>File <code>requirements.txt<\/code>:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>ipydatagrid==1.1.11\npandas==1.0.1\n<\/code><\/pre>\n<p>Notebook contents:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># %%\nimport pandas as pd\nfrom ipydatagrid import DataGrid\nfrom IPython.display import display\nimport ipywidgets\n\n# %%\ndata = [\n    (&quot;potato&quot;, 1.2, True),\n    (&quot;sweet potato&quot;, 0.8, False),\n    (&quot;french fries&quot;, 4.5, True),\n    (&quot;waffle fries&quot;, 4.9, True)\n]\ndf = pd.DataFrame(\n    data,\n    columns=[&quot;food&quot;, &quot;stars&quot;, &quot;is_available&quot;]\n)\n\n# %%\ngrid = DataGrid(df)\n\n# %%\ndisplay(grid)\n\n# %%\n# SANITY CHECK:\nbutton = ipywidgets.Button(\n    description=&quot;Button&quot;,\n    disabled=False\n)\ndef on_click(b):\n    print(&quot;CLICK&quot;)\nbutton.on_click(on_click)\ndisplay(button)\n<\/code><\/pre>\n<h3>Error messages<\/h3>\n<p>If I use the Google Chrome developer tools, I can see more logs in the browser that give some error messages, most of which are repeated:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>manager.js:305 Uncaught (in promise) Error: Module ipydatagrid, semver range ^1.1.11 is not registered as a widget module\n    at C.loadClass (manager.js:305:19)\n    at C.&lt;anonymous&gt; (manager-base.js:263:46)\n    at l (manager-base.js:44:23)\n    at Object.next (manager-base.js:25:53)\n    at manager-base.js:19:71\n    at new Promise (&lt;anonymous&gt;)\n    at Rtm6.k (manager-base.js:15:12)\n    at C.e._make_model (manager-base.js:257:16)\n    at C.&lt;anonymous&gt; (manager-base.js:246:45)\n    at l (manager-base.js:44:23)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>utils.js:119 Error: Could not create a model.\n    at n (utils.js:119:27)\n    at async C._handleCommOpen (manager.js:61:51)\n    at async v._handleCommOpen (default.js:994:100)\n    at async v._handleMessage (default.js:1100:43)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>manager-base.js:273 Could not instantiate widget\n<\/code><\/pre>\n<p>However, there is no overt error message that's immediately obvious to the user, including in the log where <code>print<\/code> statements send their output.<\/p>",
        "Challenge_closed_time":1648549758096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648246951157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where the ipydatagrid widget is not displaying in AWS SageMaker Studio, despite working correctly when run locally. The user has provided details of the Python version, the requirements, and the notebook contents. The user has also shared error messages that appear in the browser console, but there is no overt error message visible to the user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71623732",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":39.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":84.1130386111,
        "Challenge_title":"ipydatagrid widget does not display in SageMaker Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1560606498248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Provo, UT, USA",
        "Poster_reputation_count":528.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab<\/em>), and per the <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid#Installation\" rel=\"nofollow noreferrer\">ipydatagrid installation instructions<\/a>, current\/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.<\/p>\n<p>I had a quick look at the past releases of <code>ipydatagrid<\/code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/0.2.16\" rel=\"nofollow noreferrer\">v0.2.16<\/a> and <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/1.0.1\" rel=\"nofollow noreferrer\">v1.0.1<\/a> (which are adjacent on GitHub).<\/p>\n<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package<\/code> and also note that versions &lt;1.0 don't seem to be present <a href=\"https:\/\/libraries.io\/pypi\/ipydatagrid\/versions\" rel=\"nofollow noreferrer\">on PyPI<\/a>.<\/p>\n<p>So unfortunately I think (unless\/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.3,
        "Solution_reading_time":18.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1526889513900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":24.1450022222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to write a small program using the AzureML Python SDK (v1.0.85) to register an Environment in AMLS and use that definition to construct a local Conda environment when experiments are being run (for a pre-trained model). The code works fine for simple scenarios where all dependencies are loaded from Conda\/ public PyPI, but when I introduce a private dependency (e.g. a utils library) I am getting a InternalServerError with the message \"Error getting recipe specifications\".<\/p>\n\n<p>The code I am using to register the environment is (after having authenticated to Azure and connected to our workspace):<\/p>\n\n<pre><code>environment_name = config['environment']['name']\npy_version = \"3.7\"\nconda_packages = [\"pip\"]\npip_packages = [\"azureml-defaults\"]\nprivate_packages = [\".\/env-wheels\/utils-0.0.3-py3-none-any.whl\"]\n\nprint(f\"Creating environment with name {environment_name}\")\nenvironment = Environment(name=environment_name)\nconda_deps = CondaDependencies()\n\nprint(f\"Adding Python version: {py_version}\")\nconda_deps.set_python_version(py_version)\n\nfor conda_pkg in conda_packages:\n    print(f\"Adding Conda denpendency: {conda_pkg}\")\n    conda_deps.add_conda_package(conda_pkg)\n\nfor pip_pkg in pip_packages:\n    print(f\"Adding Pip dependency: {pip_pkg}\")\n    conda_deps.add_pip_package(pip_pkg)\n\nfor private_pkg in private_packages:\n    print(f\"Uploading private wheel from {private_pkg}\")\n    private_pkg_url = Environment.add_private_pip_wheel(workspace=ws, file_path=Path(private_pkg).absolute(), exist_ok=True)\n    print(f\"Adding private Pip dependency: {private_pkg_url}\")\n    conda_deps.add_pip_package(private_pkg_url)\n\nenvironment.python.conda_dependencies = conda_deps\nenvironment.register(workspace=ws)\n<\/code><\/pre>\n\n<p>And the code I am using to create the local Conda environment is:<\/p>\n\n<pre><code>amls_environment = Environment.get(ws, name=environment_name, version=environment_version)\n\nprint(f\"Building environment...\")\namls_environment.build_local(workspace=ws)\n<\/code><\/pre>\n\n<p>The exact error message being returned when <code>build_local(...)<\/code> is called is:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 814, in build_local\n    raise error\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 807, in build_local\n    recipe = environment_client._get_recipe_for_build(name=self.name, version=self.version, **payload)\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\_restclient\\environment_client.py\", line 171, in _get_recipe_for_build\n    raise Exception(message)\nException: Error getting recipe specifications. Code: 500\n: {\n  \"error\": {\n    \"code\": \"ServiceError\",\n    \"message\": \"InternalServerError\",\n    \"detailsUri\": null,\n    \"target\": null,\n    \"details\": [],\n    \"innerError\": null,\n    \"debugInfo\": null\n  },\n  \"correlation\": {\n    \"operation\": \"15043e1469e85a4c96a3c18c45a2af67\",\n    \"request\": \"19231be75a2b8192\"\n  },\n  \"environment\": \"westeurope\",\n  \"location\": \"westeurope\",\n  \"time\": \"2020-02-28T09:38:47.8900715+00:00\"\n}\n\nProcess finished with exit code 1\n<\/code><\/pre>\n\n<p>Has anyone seen this error before or able to provide some guidance around what the issue may be?<\/p>",
        "Challenge_closed_time":1583243758048,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583156836040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while building a local AMLS environment with a private wheel. The user is trying to register an environment in AMLS and use that definition to construct a local Conda environment when experiments are being run. The code works fine for simple scenarios where all dependencies are loaded from Conda\/public PyPI, but when the user introduces a private dependency, they are getting an InternalServerError with the message \"Error getting recipe specifications\". The user is seeking guidance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60490195",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":42.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":24.1450022222,
        "Challenge_title":"Unable to build local AMLS environment with private wheel",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":239.0,
        "Challenge_word_count":291,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526889513900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The issue was with out firewall blocking the required requests between AMLS and the storage container (I presume to get the environment definitions\/ private wheels).<\/p>\n\n<p>We resolved this by updating the firewall with appropriate ALLOW rules for the AMLS service to contact and read from the attached storage container.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":4.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4348.4297222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Challenge_closed_time":1586730089000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1571075742000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while using PyTorch MNIST Notebook with SageMaker Studio. They had to add two commands to the notebook, but even after that, SageMaker local mode still showed some errors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":16.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":91.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4348.4297222222,
        "Challenge_title":"Can not compile SageMaker examples",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":19,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is reported by user and the problem is kubeflow pipeline has some breaking changes on parameters but we always install latest KFP pipeline which is not compatible. \r\n\r\nShort term. use lower kfp version\r\n```\r\n!pip install https:\/\/storage.googleapis.com\/ml-pipeline\/release\/0.1.29\/kfp.tar.gz --upgrade\r\n```\r\n\r\nLong term, update examples and make sure it leverages latest features of KFP.  Will check on the [SageMaker example](https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/01438d181f502504056eac89bfc0eb091733e9a8\/notebooks\/05_Kubeflow_Pipeline\/05_04_Pipeline_SageMaker.ipynb) and file a PR to make it leverage the latest features of KFP. And the master example of [SageMaker Kubeflow Pipeline](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/samples\/contrib\/aws-samples\/mnist-kmeans-sagemaker), will try to use [master yaml file](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/aws\/sagemaker). After so, will try to use latest version 2.05 of kfp to make it compatible. Potential SageMaker example issues with users: [1st](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1401) and [2nd](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1642). But the issue description is not that informative. Will talk with users if necessary. Let's not put time on this one. I will ask SM team to fix Op issue and we can concentrate on others. Since the updated SageMaker example has been merged, let's close this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":10.4,
        "Solution_reading_time":18.52,
        "Solution_score_count":null,
        "Solution_sentence_count":17.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":1.4920061111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'd like to control kedro parameters via command line.<\/p>\n<p>According to <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/03_configuration.html#specifying-parameters-at-runtime\" rel=\"nofollow noreferrer\">docs<\/a>, kedro can specify runtime parameters as follows:<\/p>\n<pre><code>kedro run --params key:value\n&gt; {'key': 'value'}\n<\/code><\/pre>\n<p>It works. In the same way, I try to specify <strong>list<\/strong> parameters like this:<\/p>\n<pre><code>kedro run --params keys:['value1']\n&gt; {'keys': '[value1]'}\n<\/code><\/pre>\n<p>It doesn't work because kedro interplets <strong>not list but str<\/strong>. Probably, <a href=\"https:\/\/stackoverflow.com\/a\/61455703\/9489217\">this answer<\/a> could be related.<\/p>\n<p>Hope to mention a couple things to make kedro evaluates list parameters like yaml.<\/p>",
        "Challenge_closed_time":1592700529052,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592695157830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to pass list parameters from the command line in Kedro, but it is not working as Kedro interprets it as a string. The user is looking for a solution to make Kedro evaluate list parameters like YAML.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62492785",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":11.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.4920061111,
        "Challenge_title":"Kedro: How to pass \"list\" parameters from command line?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1413.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521002414380,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Japan",
        "Poster_reputation_count":13.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>By default the <code>kedro<\/code> command line won't typecast parameters beyond simpler numeric types. More complicated parameters should be handled via the <code>parameters.yml<\/code> file.<\/p>\n<p>That said, if you <em>really<\/em> want to do this, you can modify your <code>kedro_cli.py<\/code> to support this. Specifically, you'd want to modify the <code>_split_params<\/code> callback function in the file. The easiest thing here would likely be to change the line that reads<\/p>\n<pre><code>result[key] = _try_convert_to_numeric(value)\n<\/code><\/pre>\n<p>which handles parsing simple numeric types to<\/p>\n<pre><code>result[key] = json.loads(value)\n<\/code><\/pre>\n<p>to make it parse a wider range of types. That is, parse the CLI parameter you pass in as <code>json<\/code> (so you'll also need to be mindful of quotes and making sure that you pass in valid <code>json<\/code> syntax.<\/p>\n<p>If that doesn't work, you can try adding your own syntax and parsing it in that function. However, my recommendation is to avoid depending on fragile string parameter evaluation from CLI and use the <code>parameters.yml<\/code> instead.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":14.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":153.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1366551199192,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1789.0,
        "Answerer_view_count":102.0,
        "Challenge_adjusted_solved_time":1.5206341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am interested if one can import sagemaker packages on your own local Python environment or whether they are restricted to AWS Sagemaker?<\/p>\n<pre><code>from sagemaker_automl import AutoMLInteractiveRunner, AutoMLLocalCandidate\n<\/code><\/pre>\n<p>For instance can I somehow download the sagemaker_automl?<\/p>\n<p>I know the there are no sagemaker packages available in the conda repository. Perhaps there is some other way to get them.<\/p>",
        "Challenge_closed_time":1606601111440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606595637157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if it is possible to import sagemaker packages on their local Python environment or if they are limited to AWS Sagemaker. They are specifically interested in downloading the sagemaker_automl package and are unsure if it is available outside of AWS Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65054187",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":6.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.5206341667,
        "Challenge_title":"Can you use sagemaker Python libraries on my localhost?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":162.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310821356880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":14913.0,
        "Poster_view_count":1093.0,
        "Solution_body":"<p>The Sagemaker Python SDK is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">open source and on GitHub<\/a>, as well as published on <a href=\"https:\/\/pypi.org\/project\/sagemaker\/\" rel=\"nofollow noreferrer\">Pypi<\/a>.<\/p>\n<p>You can install it by running:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install sagemaker\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.0,
        "Solution_reading_time":4.99,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":34.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1445975382243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":6831.0,
        "Answerer_view_count":653.0,
        "Challenge_adjusted_solved_time":3.9043655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>SageMaker seems to give examples of using two different serving stacks for serving custom docker images:<\/p>\n\n<ol>\n<li>NGINX + Gunicorn + Flask<\/li>\n<li>NGINX + TensorFlow Serving<\/li>\n<\/ol>\n\n<p>Could someone explain to me at a very high level (I have very little knowledge of network engineering) what responsibilities these different components have? And since the second stack has only two components instead of one, can I rightly assume that TensorFlow Serving does the job (whatever that may be) of both Gunicorn and Flask? <\/p>\n\n<p>Lastly, I've read that it's possible to use Flask and TensorFlow serving at the same time. Would this then be NGINX -> Gunicorn -> Flask -> TensorFlow Serving? And what are there advantages of this?<\/p>",
        "Challenge_closed_time":1547801639500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547670814743,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is seeking an explanation of the two different serving stacks for serving custom docker images in SageMaker, namely NGINX + Gunicorn + Flask and NGINX + TensorFlow Serving. They are also curious about the responsibilities of these different components and whether TensorFlow Serving does the job of both Gunicorn and Flask. Additionally, the user is interested in knowing the advantages of using Flask and TensorFlow serving at the same time and whether it would be NGINX -> Gunicorn -> Flask -> TensorFlow Serving.",
        "Challenge_last_edit_time":1547802223467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54224934",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":9.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":36.3402102778,
        "Challenge_title":"SageMaker TensorFlow serving stack comparisons",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":741.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. <\/p>\n\n<p>I'll go over the different components from bottom to top:<\/p>\n\n<p><strong>TensorFlow Serving<\/strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and\/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.<\/p>\n\n<p><strong>Flask<\/strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. <\/p>\n\n<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1<\/strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2<\/strong>)(<strong>3<\/strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4<\/strong>). You could visualize this as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/67EXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/67EXW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Gunicorn<\/strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.<\/p>\n\n<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. <\/p>\n\n<p><strong>Nginx<\/strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. <\/p>\n\n<p><a href=\"https:\/\/serverfault.com\/questions\/331256\/why-do-i-need-nginx-and-something-like-gunicorn\">This answer<\/a> might be helpful as well. <\/p>\n\n<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1547816279183,
        "Solution_link_count":3.0,
        "Solution_readability":10.2,
        "Solution_reading_time":36.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":444.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":167.4691666667,
        "Challenge_answer_count":0,
        "Challenge_body":"![10a2a57d-e765-4359-915e-a60163bd6ec8](https:\/\/user-images.githubusercontent.com\/58698728\/205865653-bf35fb85-19cb-4e95-958e-619d13015db0.jpg)\r\n",
        "Challenge_closed_time":1670919923000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670317034000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to call \"dvc-cc run\" as the dvc servername and url are not found. This is due to the dvc\/config file being created with whitespaces, which dvc-cc cannot read. The user has provided additional context including the versions of dvc, faice, and dvc-cc being used.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/csia-pme\/csia-pme\/issues\/39",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":24.5,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":4.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":167.4691666667,
        "Challenge_title":"Resolve DVC Bad request with Minio",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":6,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":1833.7168480556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I trained a model with the built-in RESnet18 docker image, and now I want to deploy the model to an endpoint and classify ~ 1 million images. I have all my training, validation, and test images stored on S3 in RecordIO format (converted with <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/basic\/data.html?highlight=im2rec\" rel=\"nofollow noreferrer\">im2rec.py<\/a>). According to the <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>The Amazon SageMaker Image Classification algorithm supports both RecordIO (application\/x-recordio) and image (application\/x-image) content types for training. The algorithm supports only\u00a0application\/x-image\u00a0for inference.<\/p>\n<\/blockquote>\n\n<p>So I cannot perform inference on my training data in RecordIO format. To overcome this I copied all the raw .jpg images (~ 2GB) onto my Sagemaker Jupyter Notebook instance and performed inference one at a time in the following way:<\/p>\n\n<pre><code>img_list = os.listdir('temp_data') # list of all ~1,000,000 images\n\nfor im in img_list:\n    with open('temp_data\/'+im, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n                                       ContentType='application\/x-image', \n                                       Body=payload)\n\n    etc...\n<\/code><\/pre>\n\n<p>Needless to say, transferring all the data onto my Notebook instance took a long time and I would prefer not having to do that before running inference. Why does the SageMaker Image Classification not support RecordIO for inference? And more importantly, what is the best way to run inference on many images without having to move them from S3?<\/p>",
        "Challenge_closed_time":1531369355823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524767975170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model using the RESnet18 docker image in Sagemaker and wants to classify around 1 million images stored in S3 in RecordIO format. However, the Sagemaker Image Classification algorithm only supports application\/x-image for inference, not RecordIO. Therefore, the user had to copy all the raw .jpg images onto their Sagemaker Jupyter Notebook instance and perform inference one at a time, which took a long time. The user is seeking a better way to run inference on many images without having to move them from S3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50049928",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":22.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":1833.7168480556,
        "Challenge_title":"Sagemaker image classification: Best way to perform inference on many images in S3?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2600.0,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Poster_created_time":1474520506390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":832.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>The RecordIO format is designed to pack a large number of images into a single file, so I don't think it would work well for predicting single images.<\/p>\n\n<p>When it comes to prediction, you definitely don't have to copy images to a notebook instance or to S3. You just have to load them from anywhere and inline them in your prediction requests.<\/p>\n\n<p><strong>If you want HTTP-based prediction, here are your options:<\/strong><\/p>\n\n<p>1) Use the SageMaker SDK Predictor.predict() API on any machine (as long as it has proper AWS credentials) <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<p>2) Use the AWS Python SDK (aka boto3) API invoke_endpoint() on any machine (as long as it has proper AWS credentials)<\/p>\n\n<p>You can even build a simple service to perform pre-processing or post-processing with Lambda. Here's an example: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>\n\n<p><strong>If you want batch prediction:<\/strong>\n the simplest way is to retrieve the trained model from SageMaker, write a few lines of ad-hoc MXNet code to load it and run all your predictions. Here's an example: <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.1,
        "Solution_reading_time":20.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":177.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1407449881432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":228.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":732.1807,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Challenge_closed_time":1539818808056,
        "Challenge_comment_count":3,
        "Challenge_created_time":1538879533133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with AWS NoCredentials in training while attempting to run example code for Amazon Sagemaker on a local GPU. The script fails on m.fit in Docker container with the error \"Unable to locate credentials\". The user suspects that the issue has to do with passing the AWS credentials to the Docker container and is unsure if the AWS credentials should be passed in as environmental variables. The user has checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though they have an ~\/.aws\/config and ~\/.aws\/credentials file. The user has updated their sagemaker install and checked their .aws\/config and .aws\/credentials files, but the issue persists. The",
        "Challenge_last_edit_time":1539893383687,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Challenge_link_count":3,
        "Challenge_participation_count":7,
        "Challenge_readability":16.6,
        "Challenge_reading_time":96.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":63,
        "Challenge_solved_time":260.9097008333,
        "Challenge_title":"AWS NoCredentials in training",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1374.0,
        "Challenge_word_count":621,
        "Platform":"Stack Overflow",
        "Poster_created_time":1380496984176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gloucester, VA, USA",
        "Poster_reputation_count":544.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1542529234207,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":12.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.9835497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>According to the adoption plan, we need to rebuild everything, there is no quick way to push <a href=\"https:\/\/github.com\/Azure\/Azure-Machine-Learning-Adoption-Framework\">https:\/\/github.com\/Azure\/Azure-Machine-Learning-Adoption-Framework<\/a>    <\/p>\n<p>Am I correct?     <\/p>\n<p>It\u2019s not user friendly if I am not wrong.<\/p>",
        "Challenge_closed_time":1656630131776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656615790997,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering challenges with the Azure Machine Learning Adoption Framework, as they need to rebuild everything according to the adoption plan and there is no quick way to push changes. The user also finds the framework not user-friendly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/909961\/azure-machine-learning-adoption-framework",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.9835497222,
        "Challenge_title":"Azure-Machine-Learning-Adoption-Framework",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ce84de18-8973-44f3-8101-f191f9216b1f\">@Alexandre  <\/a>    <\/p>\n<p>Thanks for reaching out to us, I have answered this question as well in your other post. I think you are talking about move from Studio classc to Designer, please refer to below document:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview<\/a>    <\/p>\n<p>Basically yes for your other thread, you need to rebuild the whole pipeline since we can not copy - paste your orignal structure to Designer.    <\/p>\n<p>I am sorry for the inconveniences since the new studio has a disfferent structure to make this migration not that easy. Please let me know if you have any question during this process, we will provide help.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.3,
        "Solution_reading_time":12.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":126.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565022238448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":370.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":3.4553277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to be able to debug a running <code>entry_script.py<\/code> script in VSCode. This code runs in the container created through <code>az ml deploy<\/code> with its own docker run command. This is a local deployment so I'm using a deployment config that looks like this:<\/p>\n\n<pre><code>{\n    \"computeType\": \"LOCAL\",\n    \"port\": 32267\n}\n<\/code><\/pre>\n\n<p>I was thinking about using <code>ptvsd<\/code> to set up a VSCode server but I need to also expose\/map the 5678 port in addition to that 32267 port for the endpoint itself. So it's not clear to me how to map an additional exposed port (typically using the <code>-p<\/code> or <code>-P<\/code> flags in the <code>docker run<\/code> command). <\/p>\n\n<p>Sure, I can <code>EXPOSE<\/code> it in the <code>extra_dockerfile_steps<\/code> configuration but that won't actually map it to a host port that I can connect to\/attach to in VSCode.<\/p>\n\n<p>I tried to determine the run command and maybe modify it but I couldn't find out what that run command is. If I knew how to run the image that's created through AzureML local deployment then I could modify these flags. <\/p>\n\n<p>Ultimately it felt too hacky - if there was a more supported way through <code>az ml deploy<\/code> or through the deployment configuration that would be preferred.<\/p>\n\n<p>This is the code I'm using at the start of the entry_script to enable attachment via <code>ptvsd<\/code>: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># 5678 is the default attach port in the VS Code debug configurations\nprint(\"Waiting for debugger attach\")\nptvsd.enable_attach(address=('localhost', 5678), redirect_output=True)\n<\/code><\/pre>",
        "Challenge_closed_time":1566419274223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566406835043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to debug a running script in VSCode that runs in a container created through \"az ml deploy\" with its own docker run command. The user wants to expose\/map an additional port (5678) in addition to the endpoint port (32267) for the VSCode server using \"ptvsd\", but is unsure how to map an additional exposed port using the \"-p\" or \"-P\" flags in the \"docker run\" command. The user tried to modify the run command but couldn't find it and is looking for a more supported way through \"az ml deploy\" or the deployment configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57596224",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":3.4553277778,
        "Challenge_title":"How to expose port from locally-deployed AzureML container?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":206.0,
        "Challenge_word_count":250,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254279877887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA",
        "Poster_reputation_count":153.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>Unfortunately az ml deploy local doesn't support binding any ports other then the port hosting the scoring server. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":1.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1623222646127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":1.2744991667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have been struggling to run this code snippet in my Jupyter notebook in Sagemaker studio when I can run it locally. I previously imported Sagemaker. I can import Sagemaker smoothly and I pip installed it previously.<\/p>\n<pre><code>from sagemaker.workflow.lambda_step import LambdaStep\n<\/code><\/pre>\n<p>Here's the error thrown when I attempted to run it:<\/p>\n<pre><code>ModuleNotFoundError: No module named 'sagemaker.workflow.lambda_step'\n<\/code><\/pre>\n<p>I also tried different ways of importing such as 'from sagemaker.workflow import lambda_step' or 'sagemaker.workflow.lambda_step.LambdaStep' straight away when calling the function. Those ways also did not help me.<\/p>\n<p>I'm also not sure why I can't import LambdaStep when I can run the code snippet below smoothly in Sagemaker studio:<\/p>\n<pre><code>from sagemaker.workflow.pipeline import Pipeline\n<\/code><\/pre>\n<p>I am not sure how to solve this problem and would greatly appreciate if someone can help me out!<\/p>",
        "Challenge_closed_time":1628756732887,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628752144690,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where they are unable to import LambdaStep in their Jupyter notebook in Sagemaker studio, despite being able to do so when running the notebook locally. They have tried different ways of importing but have not been successful. They are unsure of the reason for this issue and are seeking help to resolve it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68753045",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":13.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.2744991667,
        "Challenge_title":"Why is it that I can import LambdaStep when I run my notebook locally but not when I run it in Sagemaker studio?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623222646127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Strangely, my issue is solved after I installed sagemaker again but by using this code snippet<\/p>\n<pre><code>!pip install -U sagemaker\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":1.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    }
]