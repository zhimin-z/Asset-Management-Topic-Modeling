[
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to GCP's Vertex AI and suspect I am running into an error from my lack of experience, but Googling the answer has brought me no fruitful information.<\/p>\n<p>I created a Jupyter Notebook in AI Platform but wanted to schedule it to run at a set period of time. So I was hoping to use Vertex AI's Execute function. At first when I tried accessing Vertex I was unable to do so because the API had not been enabled in GCP. My IT team then enabled the Vertex AI API and I can now utilize Vertex. Here is a picture showing it is enabled. <a href=\"https:\/\/i.stack.imgur.com\/pUSRO.png\" rel=\"nofollow noreferrer\">Enabled API Picture<\/a><\/p>\n<p>I uploaded my notebook to a JupyterLab instance in Vertex, and when I click on the Execute button, I get an <a href=\"https:\/\/i.stack.imgur.com\/jnUDv.png\" rel=\"nofollow noreferrer\">error message<\/a> saying I need to &quot;Enable necessary APIs&quot;, specifically for Vertex AI API. I'm not sure why this is considering it's already been enabled. I try to click Enable, but it just spins and spins, and then I can only get out of it by closing or reloading the tab.<\/p>\n<p>One other thing I want to call out in case it's a settings issue is that currently my <a href=\"https:\/\/i.stack.imgur.com\/6UxKb.png\" rel=\"nofollow noreferrer\">Managed Notebooks tab says &quot;PREVIEW&quot;<\/a> in the Workbench. I started thinking maybe this was an indicator that there was a separate feature that needed to be enabled to use Managed Notebooks (which is where I can access the Execute button from). When I click on the User-Managed Notebooks and open JupyterLab from there, I don't have the Execute button.<\/p>\n<p>The GCP account I'm using does have billing enabled.<\/p>\n<p>Can anyone point me in the right direction to getting the Execute button to work?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":8,
        "Challenge_created_time":1648067877907,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1648155867247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71593747",
        "Challenge_link_count":3,
        "Challenge_participation_count":9,
        "Challenge_readability":9.0,
        "Challenge_reading_time":22.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"GCP Vertex AI \"Enable necessary APIs\" when already enabled",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":518.0,
        "Challenge_word_count":299,
        "Platform":"Stack Overflow",
        "Poster_created_time":1646671161350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Based on @JamesS comments, the issue was solved by adding necessary permissions on his individual account since it is the account configured on OP's <code>Managed Notebook Instance<\/code> in which has an access mode of <code>Single user only<\/code>.<\/p>\n<p>Based on my testing when I tried to replicate the scenario, &quot;Enable necessary APIs&quot; message box will continue to show when the user has no <em>&quot;Vertex AI User&quot;<\/em> role assigned to it. And in conclusion of my testing, below are the minimum <strong>roles<\/strong> required when trying to create a <em>Scheduled run<\/em> on a <code>Managed Notebook Instance<\/code>.<\/p>\n<ul>\n<li><strong>Notebook Admin<\/strong> - For access of the notebook instance and open it through Jupyter. User will be able to run written codes in the Notebook as well.<\/li>\n<li><strong>Vertex AI User<\/strong> - So that the user can <strong>create schedule run<\/strong> on the notebook instance since the creation of the scheduled run is under the Vertex AI API itself.<\/li>\n<li><strong>Storage Admin<\/strong> - Creation of scheduled run will require a Google Cloud Storage bucket location where the job will be saved<\/li>\n<\/ul>\n<p>Posting the answer as <em>community wiki<\/em> for the benefit of the community that might encounter this use case in the future.<\/p>\n<p>Feel free to edit this answer for additional information.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.0,
        "Solution_reading_time":17.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8,
        "Solution_word_count":200,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"The team uses Azure ML CLI to deploy a container to AKS (az ml model deploy). Now and then (not always), they get an internal server error, see stack trace. They could not detect a clear pattern when this error occurs. Although it would be possible to create a retry loop in their Azure DevOps pipeline when this error occurs (as the error message also tells), this would not resolve the underlying issue.\r\n\r\n```\r\n2020-02-14T11:11:07.1739375Z ERROR: {'Azure-cli-ml Version': '1.0.85', 'Error': WebserviceException:\r\n\r\n2020-02-14T11:11:07.1739694Z \tMessage: Received bad response from Model Management Service:\r\n\r\n2020-02-14T11:11:07.1739785Z Response Code: 500\r\n\r\n2020-02-14T11:11:07.1740533Z Headers: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\r\n\r\n2020-02-14T11:11:07.1741400Z Content: b'{\"code\":\"InternalServerError\",\"statusCode\":500,\"message\":\"An internal server error occurred. Please try again. If the problem persists, contact support\"}'\r\n\r\n2020-02-14T11:11:07.1741516Z \tInnerException None\r\n\r\n2020-02-14T11:11:07.1741641Z \tErrorResponse \r\n\r\n2020-02-14T11:11:07.1741708Z {\r\n\r\n2020-02-14T11:11:07.1741813Z     \"error\": {\r\n\r\n2020-02-14T11:11:07.1742819Z         \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 500\\nHeaders: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"InternalServerError\\\",\\\"statusCode\\\":500,\\\"message\\\":\\\"An internal server error occurred. Please try again. If the problem persists, contact support\\\"}'\"\r\n\r\n2020-02-14T11:11:07.1743119Z     }\r\n\r\n2020-02-14T11:11:07.1743227Z }}\r\n```",
        "Challenge_closed_time":1583847,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583404471000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/841",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Internal server error when deploying from Azure ML to AKS",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":201,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@robinvdheijden \r\n\r\nThanks for reaching out to us. This is forum for Machine Learning Notebook only. Please open a new forum thread in [MSDN forum](https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/home?forum=AzureMachineLearningService)as it could be better place to get help on your scenario. These forum community members could provide their expert guidance on your scenario based on their experience. Thanks.\r\n\r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":8.9,
        "Solution_reading_time":7.43,
        "Solution_score_count":null,
        "Solution_sentence_count":7,
        "Solution_word_count":80,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I tried to import causalImpact library from github using \"devtools\" in AzureML studio for one of my projects.\ncode used was:<\/p>\n\n<pre><code>library(devtools)\ndevtools::install_github(\"google\/CausalImpact\")\n<\/code><\/pre>\n\n<p>Unfortunately, Azure doesn't support this.So tried importing it following the procedure in this <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow\">blog<\/a>.It is giving multiple errors on the name of dependent packages of casualImpact(i.e. BOOM, BH etc.). Can anyone help me out in importing this package on Azure?<\/p>\n\n<p>This is the R-script I used following the link given above:<\/p>\n\n<pre><code>library(assertthat)\nlibrary(dplyr)\nlibrary(hflights)\nlibrary(Lahman)\nlibrary(magrittr)\nlibrary(LGPL)\ninstall.packages(\"src\/BH_1.55.0-3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BH \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\nlibrary(BH)\ninstall.packages(\"src\/Boom_0.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"Boom \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/BoomSpikeSlab.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BoomSpikeSlab\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/bsts_0.5.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"bsts\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\nlibrary(zoo)\nlibrary(xts)\ninstall.packages(\"src\/CausalImpact.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"CausalImpact\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459257130213,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1459402444996,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36285329",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":23.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"How to import a third party library \"causalImpact\" using R script in AzureML studio?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":326.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423214734843,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore",
        "Poster_reputation_count":55.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>You will have to upload all dependent packages of casualImpact as a zip file - see sample <a href=\"http:\/\/gallery.azureml.net\/Details\/7507f907deb845d9b9b193b455a8615d\" rel=\"nofollow\">here<\/a> which shows uploading two packages required for xgboost<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.9,
        "Solution_reading_time":3.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":27,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI have a problem with plx CLI, a param of type str with value False that gets converted automatically, but I want to have a string parameter:\n\n- {name: add_noise,       type: str,   isOptional: true, value: \"False\"}\n\nPolyaxon compiles the YAML to\n\ninputs:\n  - ...\n  - name: add_noise\n    type: str\n    value: false  <-----\n    isOptional: true\n\nrun:\n  ...\n  container:\n    args:\n      - ...\n      - '--add_noise=false' <----",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649330562000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1477",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use boolean params as string values without converting to a boolean values",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To force the string value, you should use double quotation to avoid Yaml\/Json interpreting the value as bool \"...\":\n\n- {name: add_noise, type: str, isOptional: true, value: '\"False\"'}",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.2,
        "Solution_reading_time":2.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":26,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1432655047272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":463.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have to do large scale feature engineering on some data. My current approach is to spin up an instance using <code>SKLearnProcessor<\/code> and then scale the job by choosing a larger instance size or increasing the number of instances. I require using some packages that are not installed on Sagemaker instances by default and so I want to install the packages using .whl files.<\/p>\n<p>Another hurdle is that the Sagemaker role does not have internet access.<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nsess = sagemaker.Session()\nsess.default_bucket()        \n\nregion = boto3.session.Session().region_name\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session = sess,\n                                     instance_type=&quot;ml.t3.medium&quot;,\n                                     instance_count=1)\n\nsklearn_processor.run(code='script.py')\n<\/code><\/pre>\n<p><strong>Attempted resolutions:<\/strong><\/p>\n<ol>\n<li>Upload the packages to a CodeCommit repository and clone the repo into the SKLearnProcessor instances. Failed with error <code>fatal: could not read Username for 'https:\/\/git-codecommit.eu-west-1.amazonaws.com': No such device or address<\/code>. I tried cloning the repo into a sagemaker notebook instance and it works, so its not a problem with my script.<\/li>\n<li>Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off <a href=\"https:\/\/medium.com\/@shadidc\/installing-custom-python-package-to-sagemaker-notebook-b7b897f4f655\" rel=\"nofollow noreferrer\">this post<\/a>. But the packages never get copied, and an error message is not thrown.<\/li>\n<li>Also looked into using the package <code>s3fs<\/code> but it didn't seem suitable to copy the wheel files.<\/li>\n<\/ol>\n<p><strong>Alternatives<\/strong><\/p>\n<p>My client is hesitant to spin up containers from custom docker images. Any alternatives?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601912020397,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64211755",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":25.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"How to upload packages to an instance in a Processing step in Sagemaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1071.0,
        "Challenge_word_count":242,
        "Platform":"Stack Overflow",
        "Poster_created_time":1535553190827,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Johannesburg",
        "Poster_reputation_count":438.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p><code>2. Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off this post. But the packages never get copied, and an error message is not thrown.<\/code><\/p>\n<p>This approach seems sound.<\/p>\n<p>You may be better off overriding the <code>command<\/code> field on the <code>SKLearnProcessor<\/code> to <code>\/bin\/bash<\/code>, run a bash script like <code>install_and_run_my_python_code.sh<\/code> that installs the wheel containing your python dependencies, then runs your main python entry point script.<\/p>\n<p>Additionally, instead of using AWS S3 calls to download your code in a script, you could use a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ProcessingInput\" rel=\"nofollow noreferrer\">ProcessingInput<\/a> to download your code rather than doing this with AWS CLI calls in a bash script, which is what the <code>SKLearnProcessor<\/code> does to download your entry point <code>script.py<\/code> code across all the instances.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.2,
        "Solution_reading_time":13.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9,
        "Solution_word_count":132,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515259002820,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":153.0,
        "Answerer_view_count":74.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am learning AWS SageMaker which is supposed to be a serverless compute environment for Machine Learning. In this type of serverless compute environment, who is supposed to ensure the software package consistency and update the versions?<\/p>\n\n<p>For example, I ran the demo program that came with SageMaker, deepar_synthetic. In this second cell, it executes the following: !conda install -y s3fs<\/p>\n\n<p>However, I got the following warning message:<\/p>\n\n<p>Solving environment: done\n==> WARNING: A newer version of conda exists. &lt;==\n  current version: 4.4.10\n  latest version: 4.5.4\nPlease update conda by running\n    $ conda update -n base conda<\/p>\n\n<p>Since it is serverless compute, am I still supposed to update the software packages myself?<\/p>\n\n<p>Another example is as follows. I wrote a few simple lines to find out the package versions in Jupyter notebook:<\/p>\n\n<p>import platform<\/p>\n\n<p>import tensorflow as tf<\/p>\n\n<p>print(platform.python_version())<\/p>\n\n<p>print (tf.<strong>version<\/strong>)<\/p>\n\n<p>However, I got the following warning messages:<\/p>\n\n<p>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/importlib\/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\nreturn f(*args, **kwds)\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/h5py\/<strong>init<\/strong>.py:36: FutureWarning: Conversion of the second argument of issubdtype from <code>float<\/code> to <code>np.floating<\/code> is deprecated. In future, it will be treated as <code>np.float64 == np.dtype(float).type<\/code>.\nfrom ._conv import register_converters as _register_converters<\/p>\n\n<p>The prints still worked and I got the results shown beolow:<\/p>\n\n<p>3.6.4\n1.4.0<\/p>\n\n<p>I am wondering what I have to do to get the package consistent so that I don't get the warning messages. Thanks.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526869228233,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1531211855607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50441181",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":25.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"How to ensure software package version consistency in AWS SageMaker serverless compute?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":926.0,
        "Challenge_word_count":239,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461112434223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Jose, CA, United States",
        "Poster_reputation_count":1075.0,
        "Poster_view_count":181.0,
        "Solution_body":"<p>Today, SageMaker Notebook Instances are managed EC2 instances but users still have full control over the the Notebook Instance as root. You have full capabilities to install missing libraries through the Jupyter terminal. <\/p>\n\n<p>To access a terminal, open your Notebook Instance to the home page and click the drop-down on the top right: \u201cNew\u201d -> \u201cTerminal\u201d. \nNote: By default, conda installs to the root environment. <\/p>\n\n<p>The following are instructions you can follow <a href=\"https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html\" rel=\"nofollow noreferrer\">https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html<\/a> on how to install libraries in the particular conda environment. <\/p>\n\n<p>In general you will need following commands, <\/p>\n\n<pre><code>conda env list \n<\/code><\/pre>\n\n<p>which list all of your conda environments <\/p>\n\n<pre><code>source activate &lt;conda environment name&gt; \n<\/code><\/pre>\n\n<p>e.g. source activate python3 <\/p>\n\n<pre><code>conda list | grep &lt;package&gt; \n<\/code><\/pre>\n\n<p>e.g. conda list | grep numpy \nlist what are the current package versions <\/p>\n\n<pre><code>pip install numpy \n<\/code><\/pre>\n\n<p>Or <\/p>\n\n<pre><code>conda install numpy \n<\/code><\/pre>\n\n<p>Note: Periodically the SageMaker team releases new versions of libraries onto the Notebook Instances. To get the new libraries, you can stop and start your Notebook Instance. <\/p>\n\n<p>If you have recommendations on libraries you would like to see by default, you can create a forum post under <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285<\/a> . Alternatively, you can bootstrap your Notebook Instances with Lifecycle Configurations to install custom libraries. More details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateNotebookInstanceLifecycleConfig.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6,
        "Solution_readability":13.4,
        "Solution_reading_time":26.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16,
        "Solution_word_count":220,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>The following documentation specifies this option...    <\/p>\n<blockquote>\n<p>To reference an existing environment, use the azureml:&lt;environment-name&gt;:&lt;environment-version&gt; syntax.    <\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/learn.microsoft.com\/en-gb\/azure\/machine-learning\/reference-yaml-deployment-managed-online#:%7E:text=To%20reference%20an%20existing%20environment%2C%20use%20the%20azureml%3A%3Cenvironment%2Dname%3E%3A%3Cenvironment%2Dversion%3E%20syntax\">https:\/\/learn.microsoft.com\/en-gb\/azure\/machine-learning\/reference-yaml-deployment-managed-online#:~:text=To%20reference%20an%20existing%20environment%2C%20use%20the%20azureml%3A%3Cenvironment%2Dname%3E%3A%3Cenvironment%2Dversion%3E%20syntax<\/a>.    <\/p>\n<p>Can anyone help me work out what I need to put into the YAML deployment config file please - tried multiple things and nothing works. I tried to highlight this over at the documentation GitHub but they told me to come here - not a documentation issue apparently! There are no example around.    <\/p>\n<p>This is my deployment YAML file - I want to reference the Azure Machine Learning existing environment which is called <strong>AzureML-sklearn-1.0-ubuntu20.04-py38-cpu<\/strong> version <strong>32<\/strong>    <\/p>\n<pre><code>$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json  \nmodel: azureml:my-model:1  \ncode_configuration:  \n  code: models\/my-model\/  \n  scoring_script: score.py  \nenvironment:   \n ???????  \n  conda_file: models\/my-model\/conda.yml  \ninstance_type: Standard_DS2_v2  \ninstance_count: 2  \napp_insights_enabled: true  \n<\/code><\/pre>\n<p>Thank you in advance    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671542161167,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1135929\/cli-v2-managed-online-endpoint-deployment-yaml-fil",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.8,
        "Challenge_reading_time":23.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"CLI v2 managed online endpoint deployment YAML file referencing an existing inbuilt environment - how?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":134,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=0ec06fb6-513e-4f5c-9aff-281bc5e44e22\">@Neil McAlister  <\/a> Thanks for the question. Here is the sample to create a Deployment YAML Definition.    <\/p>\n<pre><code>$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json \u200b  \n  \nname: blue \u200b  \n  \nendpoint_name: my-endpoint \u200b  \n  \nmodel: \u200b  \n  \n    path: ..\/..\/model-1\/model\/ \u200b  \n  \ncode_configuration: \u200b  \n  \n    code: ..\/..\/model-1\/onlinescoring\/ \u200b  \n  \n    scoring_script: score.py \u200b  \n  \nenvironment: \u200b  \n  \n    conda_file: ..\/..\/model-1\/environment\/conda.yml \u200b  \n  \n    image: mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:20210727.v1 \u200b  \n  \ninstance_type: Standard_DS2_v2 \u200b  \n  \ninstance_count: 1  \n<\/code><\/pre>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":18.1,
        "Solution_reading_time":8.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9,
        "Solution_word_count":43,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1538275960603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I'm following the guidelines (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a>) to use a custom docker file on Azure. My script to create the environment looks like this:<\/p>\n<pre><code>from azureml.core.environment import Environment\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n<\/code><\/pre>\n<p>Upon execution, this is totally ignored and libgl1 is not installed. Any ideas why?<\/p>\n<p>EDIT: Here's the rest of my code:<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n    script_params = script_params,\n    use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n    pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    )\n\nrun = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1594687513407,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1594731646430,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62886435",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":16.1,
        "Challenge_reading_time":19.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Using a custom docker with Azure ML",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1819.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366237908703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>This should work :<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.environment import Environment\nfrom azureml.train.estimator import Estimator\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core import Experiment\n\nws = Workspace (...)\nexp = Experiment(ws, 'test-so-exp')\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n\n## You need to instead put your packages in the Environment definition instead... \n## see below for some changes too\n\nmyenv.python.conda_dependencies = CondaDependencies.create(pip_packages = ['scipy==1.1.0', 'torch==1.5.1'])\n<\/code><\/pre>\n<p>Finally you can build your estimator a bit differently :<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n#     script_params = script_params,\n#     use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n#     pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    environment_definition=myenv\n    )\n<\/code><\/pre>\n<p>And submit it :<\/p>\n<pre><code>run = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>Let us know if that works.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":14.0,
        "Solution_reading_time":18.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":19,
        "Solution_word_count":121,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":46.5198344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to train <strong>Azure Machine Learning model<\/strong> on azure using <strong>Azure Machine Learning Service.<\/strong> But I want to use the <strong>custom Docker image<\/strong> for deploying the model on azure. I am not able to understand how to deploy Machine Learning models using Custom Docker Image.<\/p>\n\n<p>Please share me if there is any tutorial or blog about the deploy ml models using a custom image.<\/p>\n\n<p>Please check the below Docker file commands:-<\/p>\n\n<pre><code># Set locale\nRUN apt-get update\nRUN apt-get install locales\nRUN locale-gen en_US.UTF-8\nRUN update-locale LANG=en_US.UTF-8\n\n# Install MS SQL v13 driver for PyOdbc\nRUN apt-get install -y curl\nRUN apt-get install apt-transport-https\nRUN curl https:\/\/packages.microsoft.com\/keys\/microsoft.asc | apt-key add - \nRUN curl https:\/\/packages.microsoft.com\/config\/ubuntu\/16.04\/prod.list &gt; \/etc\/apt\/sources.list.d\/mssql-release.list\nRUN exit\nRUN apt-get update\n\nRUN ACCEPT_EULA=Y apt-get install -y msodbcsql\nRUN apt-get install -y unixodbc-dev\n<\/code><\/pre>\n\n<p>I want to use the <strong>Azure Container Registry<\/strong> for push the docker image and use the <strong>Custom Docker Image.<\/strong> Please let me know if there is any way.<\/p>\n\n<p><strong>Is there any way to Deploy Azure ML Models using Custom docker images?<\/strong><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569236101160,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1569244969488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58060865",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploy Azure Machine Learning models using Custom Docker Image on Azure Container Regisrty",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":864.0,
        "Challenge_word_count":184,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554466050936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":219.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>You can do following:<\/p>\n\n<ol>\n<li>Create an [Environment][1] with the coordinates of your custom Docker image specified in Docker section.<\/li>\n<li>Create [InferenceConfig][2] with that Environment as argument, and use it when deploying the model.<\/li>\n<\/ol>\n\n<p>For example, assuming you have a model already and eliding other arguments:<\/p>\n\n<pre><code>from azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment(name=\"myenv\")\nenv.docker.base_image = \"mybaseimage\"\nenv.docker.base_image_registry.address = \"ip-address\"\nenv.docker.base_image_registry.username = \"my-username\"\nenv.docker.base_image_registry.password = \"my-password\"\n\nic = InferenceConfig(\u2026,environment = env)\nmodel.deploy(\u2026,inference_config = ic)\n<\/code><\/pre>\n\n<pre><code>\n  [1]: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py\n  [2]: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1569412440892,
        "Solution_link_count":2,
        "Solution_readability":20.2,
        "Solution_reading_time":14.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11,
        "Solution_word_count":77,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":0.4295236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently trying to open parquet files using Azure Jupyter Notebooks. I have tried both Python kernels (2 and 3).\nAfter the installation of <em>pyarrow<\/em> I can import the module only if the Python kernel is 2 (not working with Python 3)<\/p>\n\n<p>Here is what I've done so far (for clarity, I am not mentioning all my various attempts, such as using <em>conda<\/em> instead of <em>pip<\/em>, as it also failed):<\/p>\n\n<pre><code>!pip install --upgrade pip\n!pip install -I Cython==0.28.5\n!pip install pyarrow\n\nimport pandas  \nimport pyarrow\nimport pyarrow.parquet\n\n#so far, so good\n\nfilePath_parquet = \"foo.parquet\"\ntable_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n<\/code><\/pre>\n\n<p>This works well if I'm doing that off-line (using Spyder, Python v.3.7.0). But it fails using an Azure Notebook.<\/p>\n\n<pre><code> AttributeErrorTraceback (most recent call last)\n&lt;ipython-input-54-2739da3f2d20&gt; in &lt;module&gt;()\n      6 \n      7 #table_parquet_raw = pd.read_parquet(filePath_parquet, engine='pyarrow')\n----&gt; 8 table_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n\nAttributeError: 'module' object has no attribute 'read_parquet'\n<\/code><\/pre>\n\n<p>Any idea please?<\/p>\n\n<p>Thank you in advance !<\/p>\n\n<p>EDIT:<\/p>\n\n<p>Thank you very much for your reply Peter Pan !\nI have typed these  statements, here is what I got:<\/p>\n\n<p>1.<\/p>\n\n<pre><code>    print(pandas.__dict__)\n<\/code><\/pre>\n\n<p>=> read_parquet does not appear<\/p>\n\n<p>2.<\/p>\n\n<pre><code>    print(pandas.__file__)\n<\/code><\/pre>\n\n<p>=> I get:<\/p>\n\n<pre><code>    \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/pandas\/__init__.py\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li><p>import sys; print(sys.path) => I get:<\/p>\n\n<pre><code>['', '\/home\/nbuser\/anaconda3_23\/lib\/python34.zip',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/plat-linux',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/lib-dynload',\n'\/home\/nbuser\/.local\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/Sphinx-1.3.1-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/IPython\/extensions',\n'\/home\/nbuser\/.ipython']\n<\/code><\/pre><\/li>\n<\/ol>\n\n<p>Do you have any idea please ?<\/p>\n\n<p>EDIT 2:<\/p>\n\n<p>Dear @PeterPan, I have typed both <code>!conda update conda<\/code> and  <code>!conda update pandas<\/code> : when checking the Pandas version (<code>pandas.__version__<\/code>), it is still <code>0.19.2<\/code>.<\/p>\n\n<p>I have also tried with <code>!conda update pandas -y -f<\/code>, it returns:\n`Fetching package metadata ...........\nSolving package specifications: .<\/p>\n\n<p>Package plan for installation in environment \/home\/nbuser\/anaconda3_23:<\/p>\n\n<p>The following NEW packages will be INSTALLED:<\/p>\n\n<pre><code>pandas: 0.19.2-np111py34_1`\n<\/code><\/pre>\n\n<p>When typing:\n<code>!pip install --upgrade pandas<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Requirement already up-to-date: pandas in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\nRequirement already up-to-date: pytz&gt;=2011k in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: numpy&gt;=1.9.0 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: python-dateutil&gt;=2 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: six&gt;=1.5 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from python-dateutil&gt;=2-&gt;pandas)<\/code><\/p>\n\n<p>Finally, when typing:<\/p>\n\n<p><code>!pip install --upgrade pandas==0.24.0<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Collecting pandas==0.24.0\n  Could not find a version that satisfies the requirement pandas==0.24.0 (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0rc1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0rc2, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0rc1, 0.19.0, 0.19.1, 0.19.2, 0.20.0rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0rc1, 0.21.0, 0.21.1, 0.22.0)\nNo matching distribution found for pandas==0.24.0<\/code><\/p>\n\n<p>Therefore, my guess is that the problem comes from the way the packages are managed in Azure. Updating a package (here Pandas), should lead to an update to the latest version available, shouldn't it?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1545322944093,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1547188516027,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53872444",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":60.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":null,
        "Challenge_title":"Cannot read \".parquet\" files in Azure Jupyter Notebook (Python 2 and 3)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1879.0,
        "Challenge_word_count":455,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545322329030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I tried to reproduce your issue on my Azure Jupyter Notebook, but failed. There was no any issue for me without doing your two steps <code>!pip install --upgrade pip<\/code> &amp; <code>!pip install -I Cython==0.28.5<\/code> which I think not matter.<\/p>\n\n<p>Please run some codes below to check your import package <code>pandas<\/code> whether be correct.<\/p>\n\n<ol>\n<li>Run <code>print(pandas.__dict__)<\/code> to check whether has the description of <code>read_parquet<\/code> function in the output.<\/li>\n<li>Run <code>print(pandas.__file__)<\/code> to check whether you imported a different <code>pandas<\/code> package.<\/li>\n<li>Run <code>import sys; print(sys.path)<\/code> to check the order of paths whether there is a same named file or directory under these paths.<\/li>\n<\/ol>\n\n<p>If there is a same file or directory named <code>pandas<\/code>, you just need to rename it and restart your <code>ipynb<\/code> to re-run. It's a common issue which you can refer to these SO threads <a href=\"https:\/\/stackoverflow.com\/questions\/35341363\/attributeerror-module-object-has-no-attribute-reader\">AttributeError: &#39;module&#39; object has no attribute &#39;reader&#39;<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/36250353\/importing-installed-package-from-script-raises-attributeerror-module-has-no-at\">Importing installed package from script raises &quot;AttributeError: module has no attribute&quot; or &quot;ImportError: cannot import name&quot;<\/a>.<\/p>\n\n<p>In Other cases, please update your post for more details to let me know.<\/p>\n\n<hr>\n\n<p>The latest <code>pandas<\/code> version should be <code>0.23.4<\/code>, not <code>0.24.0<\/code>.<\/p>\n\n<p>I tried to find out the earliest version of <code>pandas<\/code> which support the <code>read_parquet<\/code> feature via search the function name <code>read_parquet<\/code> in the documents of different version from <code>0.19.2<\/code> to <code>0.23.3<\/code>. Then, I found <code>pandas<\/code> supports <code>read_parquet<\/code> feature after the version <code>0.21.1<\/code>, as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The new features shown in the <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/whatsnew.html\" rel=\"nofollow noreferrer\"><code>What's New<\/code><\/a> of version <code>0.21.1<\/code>\n<a href=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>According to your <code>EDIT 2<\/code> description, it seems that you are using Python 3.4 in Azure Jupyter Notebook. Not all <code>pandas<\/code> versions support Python 3.4 version.<\/p>\n\n<p>The versions <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.21.1<\/code><\/a> &amp; <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.22\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.22.0<\/code><\/a> offically support Python 2.7,3.5, and 3.6, as below.\n<a href=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the <a href=\"https:\/\/pypi.org\/project\/pandas\/\" rel=\"nofollow noreferrer\">PyPI page for <code>pandas<\/code><\/a> also requires the Python version as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6613J.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6613J.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So you can try to install the <code>pandas<\/code> versions <code>0.21.1<\/code> &amp; <code>0.22.0<\/code> in the current notebook of Python 3.4. if failed, please create a new notebook in Python <code>2.7<\/code> or <code>&gt;=3.5<\/code> to install <code>pandas<\/code> version <code>&gt;= 0.21.1<\/code> to use the function <code>read_parquet<\/code>.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1547190062312,
        "Solution_link_count":14,
        "Solution_readability":10.9,
        "Solution_reading_time":52.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":51,
        "Solution_word_count":383,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"I was able to create a deep learning VM from the marketplace and when I open up the VM instance in the Console I see a metadata tag called `proxy-url` which has a format like `https:\/\/[alphanumeric string]-dot-us-central1.notebooks.googleusercontent.com\/lab`\n\nClicking on that link takes me to a JupyterLab UI that is running on my VM. Amazing! Unfortunately, when I try opening that link on an incognito window, I'm asked to sign in. If I sign in, I get a 403 forbidden.\n\nMy question now is, how can I make that link available to someone else?",
        "Challenge_closed_time":1643637,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643281320000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Make-deep-learning-VM-JupyterLab-publicly-available\/m-p\/386576#M191",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Make deep learning VM JupyterLab publicly available?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi gopalv\n\nAs far as I understand, it sounds like your Jupyter Notebook isn't configured for remote access. since it doesn't work when trying to access it from the incognito window with a 403 error.\n\nYou can try looking here and here for details on how to set up a publicly accessible\/remote access notebook. There are additional troubleshooting steps in our documentation here as well.\n\nHope this helps!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":73,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1429811498812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":1910.0,
        "Answerer_view_count":316.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a pre-trained ML model (saved as .h5 file) to Azure ML. I have created an AKS cluster and trying to deploy the model as shown below:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\n\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\n\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\nenv = Environment.get(workspace, name='AzureML-TensorFlow-1.13-GPU')\n\n# Installing packages present in my requirements file\nwith open('requirements.txt') as f:\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\ndependencies.append(&quot;azureml-defaults&gt;=1.0.45&quot;)\n\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=dependencies)\n\n# Including the source folder so that all helper scripts are included in my deployment\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# Deployment with suitable config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=4, memory_gb=32)\nmodel = Model(workspace, 'sketch-inference')\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<p>My main entry script requires some additional helper scripts, which I include by mentioning the source folder in my inference config.<\/p>\n<p>I was expecting that the helper scripts I add should be able to access the packages installed while setting up the environment during deployment, but I get ModuleNotFoundError.<\/p>\n<p>Here is the error output, along with the a couple of environment variables I printed while executing entry script:<\/p>\n<pre><code>    AZUREML_MODEL_DIR ----  azureml-models\/sketch-inference\/1\n    PYTHONPATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages:\/var\/azureml-server:\n    PATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/bin:\/opt\/miniconda\/bin:\/usr\/local\/nvidia\/bin:\/usr\/local\/cuda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/opt\/intel\/compilers_and_libraries\/linux\/mpi\/bin64\n    Exception in worker process\n    Traceback (most recent call last):\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n        worker.init_process()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n        self.load_wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n        self.wsgi = self.app.wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n        self.callable = self.load()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n        return self.load_wsgiapp()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n        return util.import_app(self.app_uri)\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n        __import__(module)\n    File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n        import create_app\n    File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n        from app import main\n    File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n        from aml_blueprint import AMLBlueprint\n    File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 25, in &lt;module&gt;\n        import main\n    File &quot;\/var\/azureml-app\/main.py&quot;, line 12, in &lt;module&gt;\n        driver_module_spec.loader.exec_module(driver_module)\n    File &quot;\/structure\/azureml-app\/ProcessImage\/app.py&quot;, line 16, in &lt;module&gt;\n        from ProcessImage.samples.coco.inference import run as infer\n    File &quot;\/var\/azureml-app\/ProcessImage\/samples\/coco\/inference.py&quot;, line 1, in &lt;module&gt;\n        import skimage.io\n    ModuleNotFoundError: No module named 'skimage'\n<\/code><\/pre>\n<p>The existing answers related to this aren't of much help. I believe there must be a simpler way to fix this, since AzureML specifically provides the feature to setup environment with pip\/conda packages installed either by supplying requirements.txt file or individually.<\/p>\n<p>What am I missing here? Kindly help.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606187763027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64979752",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":67.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to access python packages installed in Azure ML",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1381.0,
        "Challenge_word_count":396,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429811498812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA, USA",
        "Poster_reputation_count":1910.0,
        "Poster_view_count":316.0,
        "Solution_body":"<p>So, after some trial and error, creating a fresh environment and then adding the packages solved the problem for me. I am still not clear on why this didn't work when I tried to use <a href=\"http:\/\/from%20azureml.core%20import%20Workspace%20from%20azureml.core.model%20import%20Model%20from%20azureml.core.environment%20import%20Environment,%20DEFAULT_GPU_IMAGE%20from%20azureml.core.conda_dependencies%20import%20CondaDependencies%20from%20azureml.core.model%20import%20InferenceConfig%20from%20azureml.core.webservice%20import%20AksWebservice,%20LocalWebservice%20from%20azureml.core.compute%20import%20ComputeTarget%20%20%20#%201.%20Instantiate%20the%20workspace%20workspace%20=%20Workspace.from_config(path=%22config.json%22)%20%20#%202.%20Setup%20the%20environment%20env%20=%20Environment(%27sketchenv%27)%20with%20open(%27requirements.txt%27)%20as%20f:%20#%20Fetch%20all%20dependencies%20as%20a%20list%20%20%20%20%20dependencies%20=%20f.readlines()%20dependencies%20=%20%5Bx.strip()%20for%20x%20in%20dependencies%20if%20%27#%20%27%20not%20in%20x%5D%20env.docker.base_image%20=%20DEFAULT_GPU_IMAGE%20env.python.conda_dependencies%20=%20CondaDependencies.create(conda_packages=%5B%27numpy==1.17.4%27,%20%27Cython%27%5D,%20pip_packages=dependencies)%20%20#%203.%20Inference%20Config%20inference_config%20=%20InferenceConfig(entry_script=%27app.py%27,%20environment=env,%20source_directory=%27.\/ProcessImage%27)%20%20#%204.%20Compute%20target%20(using%20existing%20cluster%20from%20the%20workspacke)%20aks_target%20=%20ComputeTarget(workspace=workspace,%20name=%27sketch-ppt-vm%27)%20%20#%205.%20Deployment%20config%20deployment_config%20=%20AksWebservice.deploy_configuration(cpu_cores=6,%20memory_gb=100)%20%20#%206.%20Model%20deployment%20model%20=%20Model(workspace,%20%27sketch-inference%27)%20#%20Registered%20model%20(which%20contains%20model%20files\/folders)%20service%20=%20Model.deploy(workspace,%20%22process-sketch-dev%22,%20%5Bmodel%5D,%20inference_config,%20deployment_config,%20deployment_target=aks_target,%20overwrite=True)%20service.wait_for_deployment(show_output%20=%20True)%20print(service.state)\" rel=\"nofollow noreferrer\">Environment.from_pip_requirements()<\/a>. A detailed answer in this regard would be interesting to read.<\/p>\n<p>My primary task was inference - object detection given an image, and we have our own model developed by our team. There are two types of imports I wanted to have:<\/p>\n<p><strong>1. Standard python packages (installed through pip)<\/strong><br \/>\nThis was solved by creating conda dependencies and add it to env object (Step 2)<\/p>\n<p><strong>2. Methods\/vars from helper scripts<\/strong> (if you have pre\/post processing to be done during model inference):<br \/>\nThis was done by mentioning <code>source_directory<\/code> in InferenceConfig (step 3)<\/p>\n<p>Here is my updated script which combines Environment creation, Inference and Deployment configs and using existing compute in the workspace (created through portal).<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\n\n# 1. Instantiate the workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# 2. Setup the environment\nenv = Environment('sketchenv')\nwith open('requirements.txt') as f: # Fetch all dependencies as a list\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\nenv.docker.base_image = DEFAULT_GPU_IMAGE\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=['numpy==1.17.4', 'Cython'], pip_packages=dependencies)\n\n# 3. Inference Config\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\n# 4. Compute target (using existing cluster from the workspacke)\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# 5. Deployment config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=6, memory_gb=100)\n\n# 6. Model deployment\nmodel = Model(workspace, 'sketch-inference') # Registered model (which contains model files\/folders)\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<hr \/>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":25.5,
        "Solution_reading_time":63.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":32,
        "Solution_word_count":271,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1502815666600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Memphis, TN, USA",
        "Answerer_reputation_count":5028.0,
        "Answerer_view_count":957.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.<\/p>\n\n<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\nmlflow from https:\/\/files.pythonhosted.org\/packages\/01\/ec\/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270\/mlflow-1.0.0-py3-none-any.whl#sha256=0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4:\n    Expected sha256 0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4\n         Got        eb34ea16ecfe02d474ce50fd1f88aba82d56dcce9e8fdd30193ab39edf32ac9e\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1560545573400,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1560799785056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56604989",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":10.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to install mlflow using pip install",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355343131932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5655.0,
        "Poster_view_count":629.0,
        "Solution_body":"<p>It is trying to check cache for packages. They were likely compiled in linux or some other OS and you are trying to install them in Windows.<\/p>\n\n<p>This should fix your issue:<\/p>\n\n<pre><code>pip install --no-cache-dir mlflow\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.7,
        "Solution_reading_time":3.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":37,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625466236780,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68251533",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":266.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408744280996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2758.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.9,
        "Solution_reading_time":6.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":60,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I launched ipython session and trying to load a dataset.<br \/>\nI am running<br \/>\ndf = catalog.load(&quot;test_dataset&quot;)<br \/>\nFacing the below error<br \/>\n<code>NameError: name 'catalog' is not defined<\/code><\/p>\n<p>I also tried %reload_kedro but got the below error<\/p>\n<p><code>UsageError: Line magic function `%reload_kedro` not found.<\/code><\/p>\n<p>Even not able to load context either.\nI am running the kedro environment from a Docker container.\nI am not sure where I am going wrong.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637670434953,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70080915",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"kedro context and catalog missing from ipython session",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":617.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>new in 0.17.5 there is a fallback option, please run the following commands in your Jupyter\/IPython session:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>%load_ext kedro.extras.extensions.ipython\n%reload_kedro &lt;path_to_project_root&gt;\n<\/code><\/pre>\n<p>This should help you get up and running.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.2,
        "Solution_reading_time":4.1,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3,
        "Solution_word_count":32,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Please see the screenshots below. Once it said terminated but without reason:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/140829-screenshot-2021-10-13-221133.png?platform=QnA\" alt=\"140829-screenshot-2021-10-13-221133.png\" \/>    <br \/>\nThe other time there was nothing just stopped:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/140846-screenshot-2021-10-13-215523.png?platform=QnA\" alt=\"140846-screenshot-2021-10-13-215523.png\" \/>    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634306015143,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/592153\/my-script-stops-running-without-any-message-explai",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.6,
        "Challenge_reading_time":7.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"my script stops running without any message explaining the reason",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>Hope you have solved this issue and we are sorry not seeing your response. Since this issue happened without any error details, support ticket would be the best way to debug that. Please let me know if you still need that. Thanks.  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.4,
        "Solution_reading_time":3.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":48,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1579536819712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":18.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":4.6947408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I installed pyarrow using this command &quot;conda install pyarrow&quot;.\nI am running a sagemaker notebook and I am getting the error no module named pyarrow.\nI have python 3.8.3 installed on mac.<\/p>\n<p>I have numpy  1.18.5 , pandas 1.0.5 and pyarrow  0.15.1<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1604343780117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1604344440696,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64651724",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.1,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"No Module named pyarrow",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":540.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468599558956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":107.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>I have not yet used AWS Sagemaker notebooks, but they may be similar to GCP 'AI Platform notebooks', which I have used quite extensively. Additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks (whether from command line or from GUI)?<\/p>\n<p>In GCP, I defaulted to using <code>pip install<\/code> for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments.<\/p>\n<p>Assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the <code>CONDA<\/code> tab and selecting which notebook uses which conda environment.\n-Spencer<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1604361341763,
        "Solution_link_count":0,
        "Solution_readability":16.1,
        "Solution_reading_time":11.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":131,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1428454496052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a docker container that is using Sagemaker via the java sdk. This container is deployed on a k8s cluster with several replicas. <\/p>\n\n<p>The container is doing simple requests to Sagemaker to list some models that we have trained and deployed. However we are now having issues with some java certificate. I am quite novice with k8s and certificates so I will appreciate if you could provide some help to fix the issue.<\/p>\n\n<p>Here are some traces from the log when it tries to list the endpoints:<\/p>\n\n<pre><code>org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:394)\n    at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:353)\n    at com.amazonaws.http.conn.ssl.SdkTLSSocketFactory.connectSocket(SdkTLSSocketFactory.java:132)\n    at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:141)\n    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n    at com.amazonaws.http.conn.$Proxy67.connect(Unknown Source)\n    at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n    at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1236)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056)\n    ... 70 common frames omitted\nCaused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:397)\n    at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:302)\n    at sun.security.validator.Validator.validate(Validator.java:262)\n    at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324)\n    at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229)\n    at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:124)\n    at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1621)\n    ... 97 common frames omitted\nCaused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)\n    at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)\n    at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280)\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:392)\n    ... 103 common frames omitted \n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1543802602820,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53586515",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":37.1,
        "Challenge_reading_time":51.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker certificate issue with Kubernetes",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":488.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>I think I have found the answer to my problem. I have set up another k8s cluster and deployed the container there as well. They are working fine and the certificate issues does not happen. When investigating more I have noticed that they were some issues with DNS resolution on the first k8s cluster. In fact the containers with certificate issues could not ping google.com for example.\nI fixed the DNS issue by not relying on core-dns and setting the DNS configuration in the deployment.yaml file. I am not sure to understand why exactly but this seems to have fixed the certificate issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.1,
        "Solution_reading_time":7.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8,
        "Solution_word_count":103,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello, I have an Databricks account on Azure, and the goal is to compare different image tagging services from GCP and other providers via corresponding API calls, with Python notebook. I have problems with GCP vision API calls, specifically with credentials: as far as I understand, the one necessary step is to set 'GOOGLE_APPLICATION_CREDENTIALS' environment variable in my databricks notebook with something like\u00a0\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='\/folder1\/credentials.json'\u00a0\n\nwhere '\/folder1\/credentials.json' is the place my notebook looks for json file with credentials (notebook is in the same folder,\u00a0\/folder1\/notebook_api_test).\n\nI am getting this path by looking into\u00a0Workspace->\u00a0Copy file path\u00a0in the Databricks web page.\n\nBut this approach doesn't work, when cell is executed, I am getting this error:\u00a0\n\nDefaultCredentialsError: File \/folder1\/credentials.json was not found.\u00a0\n\nWhat is the right way to deal with credentials to access google vision API from Databricks notebook?",
        "Challenge_closed_time":1682007,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681889880000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-is-the-best-way-to-use-credentials-for-API-calls-from\/m-p\/545303#M1700",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.9,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"What is the best way to use credentials for API calls from databricks notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":170.0,
        "Challenge_word_count":152,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Ok, here is a trick: in my case, the file with GCP credentials is stored in notebook workspace storage, which is not visible to os.environ() command. So solution is to read a content of this file, and save it to the cluster storage attached to the notebook, which is created with the cluster and is erased when cluster is gone (so we need to repeat this procedure every time the cluster is re-created). According to this doc, we can read the content of the credentials json file stored in notebook workspace with\n\n\u00a0 \u00a0 \u00a0 \u00a0 with open('\/Workspace\/folder1\/cred.json'): #note that I need a full path here, for some reason\n\u00a0 \u00a0 \u00a0 \u00a0 content = f.read()\n\nand then according to this doc, we need to save it on another place in a new file (with the same name in my case, cred.json), namely on cluster storage attached to the notebook (which is visible to os-related functions, like os.environ()), with\n\n\u00a0 \u00a0 \u00a0 \u00a0 fd = os.open(\"cred.json\", os.O_RDWR|os.O_CREAT)\n\u00a0 \u00a0 \u00a0 \u00a0 ret = os.write(fd,content.encode())\n\u00a0 \u00a0 \u00a0 \u00a0 #need to add .encode(), or will get TypeError: a bytes-like object is required, not 'str'\n\u00a0 \u00a0 \u00a0 \u00a0 os.close(fd)\n\nOnly after that we can continue with setting an environment variable, required for GCP authentication:\n\n\u00a0 \u00a0 \u00a0 \u00a0 os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='.\/cred.json'\n\nand then API calls should work fine, without DefaultCredentialsError.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.3,
        "Solution_reading_time":16.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13,
        "Solution_word_count":202,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\r\nThe name of the inputs are:\r\n1. `input_image` \r\n2. `input_image_meta` \r\n3. `input_anchors` \r\n\r\n\r\nand the name of outputs are:\r\n1.  `output_detections`\r\n2.  `output_mrcnn_class`\r\n3.  `output_mrcnn_bbox`\r\n4.  `output_mrcnn_mask`\r\n5.  `output_rois`\r\n\r\nI have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting `{'error': \"Missing 'inputs' or 'instances' key\"}` in return.\r\n \r\nI have made a model.tar.gz file which has the following structure:\r\n\r\n    mymodel\r\n        |__1\r\n            |__variables\r\n            |__saved_model.pb\r\n\r\n    code\r\n        |__inference.py\r\n        |__requirements.txt\r\n\r\nAs specified in the documentation, inference.py has input_handler and output handler functions. From the client-side, I pass the S3 link of the image which then transforms to the three inputs for the model. \r\n\r\nThe structure of input_handler is as follows:\r\n\r\n```\r\ndef input_handler(data, context):\r\n     input_data = json.loads(data.read().decode('utf-8'))\r\n\r\n    obj = bucket.Object(input_data['img_link'])\r\n    tmp = tempfile.NamedTemporaryFile()\r\n    \r\n    # download image from AWS S3\r\n    with open(tmp.name, 'wb') as f:\r\n        obj.download_fileobj(f)\r\n        image=mpimg.imread(tmp.name)\r\n    \r\n    # make preprocessing\r\n    image = Image.fromarray(image)\r\n     \r\n     ...... # some more transformations \r\n     return = {\"input_image\": Python list for image,\r\n                    \"input_image_meta: Python list for input image meta,\r\n                    \"input_anchors\": Python list for input anchors}\r\n\r\n```\r\nThe deifinition of output_handler is as follows:\r\n\r\n```\r\ndef output_handler(data, context):\r\n      output_string = data.content.decode('unicode-escape')\r\n      return output_string, context.accept_header\r\n```\r\n\r\nThe sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\r\nOn the client side, I call the predictor using follwoing code:\r\n\r\n```\r\nrequest = {}\r\nrequest[\"img_link\"] = \"image.jpg\"\r\nresult = predictor.predict(request)\r\n```\r\n\r\nBut when I print the result the following gets printed out, `{'error': \"Missing 'inputs' or 'instances' key\"}`\r\nAll the bucket connections for loading the image are in inference.py\r\n      ",
        "Challenge_closed_time":1571957,
        "Challenge_comment_count":0,
        "Challenge_created_time":1567172491000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/73",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":28.7,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":104.0,
        "Challenge_repo_issue_count":229.0,
        "Challenge_repo_star_count":160.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":283,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello @janismdhanbad,\r\n\r\nI believe your inference requests will have to follow the TensorFlow serving REST API specifications defined here: https:\/\/www.tensorflow.org\/tfx\/serving\/api_rest#request_format_2\r\n\r\n```\r\nThe request body for predict API must be JSON object formatted as follows:\r\n\r\n{\r\n  \/\/ (Optional) Serving signature to use.\r\n  \/\/ If unspecifed default serving signature is used.\r\n  \"signature_name\": <string>,\r\n\r\n  \/\/ Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\r\n  \/\/ A request can have either of them but NOT both.\r\n  \"instances\": <value>|<(nested)list>|<list-of-objects>\r\n  \"inputs\": <value>|<(nested)list>|<object>\r\n}\r\n``` closing due to inactivity. feel free to reopen if necessary.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.3,
        "Solution_reading_time":8.78,
        "Solution_score_count":null,
        "Solution_sentence_count":7,
        "Solution_word_count":80,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1264671735676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA",
        "Answerer_reputation_count":8619.0,
        "Answerer_view_count":1286.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to use a post-startup script to create a Vertex AI User Managed Notebook whose Jupyter Lab has a dedicated virtual environment and corresponding computing kernel when first launched. I have had success creating the instance and then, as a second manual step from within the Jupyter Lab &gt; Terminal, running a bash script like so:<\/p>\n<pre><code>#!\/bin\/bash\ncd \/home\/jupyter\nmkdir -p env\ncd env\npython3 -m venv envName --system-site-packages\nsource envName\/bin\/activate\nenvName\/bin\/python3 -m pip install --upgrade pip\npython -m ipykernel install --user --name=envName\npip3 install geemap --user \npip3 install earthengine-api --user \npip3 install ipyleaflet --user \npip3 install folium --user \npip3 install voila --user \npip3 install jupyterlab_widgets\ndeactivate\njupyter labextension install --no-build @jupyter-widgets\/jupyterlab-manager jupyter-leaflet\njupyter lab build --dev-build=False --minimize=False\njupyter labextension enable @jupyter-widgets\/jupyterlab-manager\n<\/code><\/pre>\n<p>However, I have not had luck using this code as a post-startup script (being supplied through the console creation tools, as opposed to command line, thus far). When I open Jupyter Lab and look at the relevant structures, I find that there is no environment or kernel. Could someone please provide a working example that accomplishes my aim, or otherwise describe the order of build steps that one would follow?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1662640804323,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73649262",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":17.8,
        "Challenge_reading_time":18.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Create custom kernel via post-startup script in Vertex AI User Managed notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":206,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459350905808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Post startup scripts run as root.\nWhen you run:<\/p>\n<pre><code>python -m ipykernel install --user --name=envName\n<\/code><\/pre>\n<p>Notebook is using current user which is <code>root<\/code> vs when you use Terminal, which is running as <code>jupyter<\/code> user.<\/p>\n<p>Option 1) Have 2 scripts:<\/p>\n<ul>\n<li>Script A. Contents specified in original post. Example: <code>gs:\/\/newsml-us-central1\/so73649262.sh<\/code><\/li>\n<li>Script B. Downloads script and execute it as <code>jupyter<\/code>. Example: <code>gs:\/\/newsml-us-central1\/so1.sh<\/code> and use it as post-startup script.<\/li>\n<\/ul>\n<pre><code>#!\/bin\/bash\n\nset -x\n\ngsutil cp gs:\/\/newsml-us-central1\/so73649262.sh \/home\/jupyter\nchown jupyter \/home\/jupyter\/so73649262.sh\nchmod a+x \/home\/jupyter\/so73649262.sh\nsu -c '\/home\/jupyter\/so73649262.sh' jupyter\n<\/code><\/pre>\n<p>Option 2) Create a file in bash using EOF. Write the contents into a single file and execute it as mentioned above.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.5,
        "Solution_reading_time":12.3,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14,
        "Solution_word_count":108,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1510046220943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":292.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>My search didn't yield anything useful so I was wondering if there is any easy way to copy notebooks from one instance to another instance on Sagemaker? Of course other than manually downloading the notebooks on one instance and uploading to the other one!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1537397018090,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1537397797832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52415136",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":9.9,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to copy notebooks between different Sagemaker instances?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":5303.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444524456120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation_count":973.0,
        "Poster_view_count":82.0,
        "Solution_body":"<p>The recommended way to do this (as of 12\/16\/2018) would be to use the newly- launched Git integration for SageMaker Notebook Instances.<\/p>\n\n<ol>\n<li>Create a Git repository for your notebooks<\/li>\n<li>Commit and push changes from Notebook Instance #1 to your Git repo<\/li>\n<li>Start Notebook Instance #2 using the same Git repo<\/li>\n<\/ol>\n\n<p>This way your notebooks are persisted in the Git repo rather than on the  instance, and the Git repo can be shared by multiple instances. <\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":19.5,
        "Solution_reading_time":11.4,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4,
        "Solution_word_count":82,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I recently started using the <code>wandb.Api()<\/code> in order not to manually download all the Charts in <code>.csv<\/code> format.<\/p>\n<p>The problem is that I cannot get consistent results, most of the times that I call the API  in a jupyter-notebook I get different results.<\/p>\n<p>I have made public one of my dashboards to tackle this issue. Here is a screenshot with a reproducible example:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" data-download-href=\"\/uploads\/short-url\/8x7Rm9lNkSyg4pi6edKG0wNOxgE.png?dl=1\" title=\"2022-05-16-165542_647x517_scrot\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" alt=\"2022-05-16-165542_647x517_scrot\" data-base62-sha1=\"8x7Rm9lNkSyg4pi6edKG0wNOxgE\" width=\"625\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">2022-05-16-165542_647x517_scrot<\/span><span class=\"informations\">647\u00d7517 50.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>In order to obtain the <code>csv_val_f1<\/code> variable one just needs to download the <code>Val F1<\/code> chart. Two things can be seen here:<\/p>\n<ol>\n<li>Multiple runs of the same code produce different results<\/li>\n<li>The maximum value obtained by the API differs from the maximum value obtained by manually downloading the <code>.csv<\/code> version of the Chart.<\/li>\n<\/ol>\n<p>Any ideas on what I\u2019m missing?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652713229122,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/run-history-returns-different-values-on-almost-each-call\/2431",
        "Challenge_link_count":3,
        "Challenge_participation_count":5,
        "Challenge_readability":14.1,
        "Challenge_reading_time":26.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Run.history() returns different values on almost each call",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":866.0,
        "Challenge_word_count":165,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jaeheelee\">@jaeheelee<\/a> and <a class=\"mention\" href=\"\/u\/carloshernandezp\">@carloshernandezp<\/a>,<br>\nI believe you are seeing this because we sample the data points when you call <code>run.history()<\/code>. You can use <code>run.scan_history()<\/code> if you would like to have the entire history returned. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#sampling\">Here<\/a> is some more information on this.<\/p>\n<p>Let me know if this solves the issue for you.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":9.5,
        "Solution_reading_time":7.07,
        "Solution_score_count":null,
        "Solution_sentence_count":6,
        "Solution_word_count":59,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\n$ dvc repro run_benchmarks\r\nERROR: 'dvc.lock' is git-ignored.\r\n```\r\n\r\n`.dvc.lock` in `.gitignore` causes Exceptions at running benchmark. Delete this line solves this problem. And because of #168 maybe we need some better ways to deal with `dvc.lock`.",
        "Challenge_closed_time":1621495,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619681675000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/255",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.8,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"ERROR: 'dvc.lock' is git-ignored.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1552934828727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":254.0,
        "Answerer_view_count":62.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1618407986697,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":30.4,
        "Challenge_reading_time":154.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":56,
        "Challenge_solved_time":null,
        "Challenge_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1218.0,
        "Challenge_word_count":510,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558009697176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cergy-Pontoise, Cergy, France",
        "Poster_reputation_count":53.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.3,
        "Solution_reading_time":1.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":11,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":8283.79234,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?<\/p>\n\n<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"noreferrer\">SageMaker containers<\/a>, so my understanding is that I need to build my own.<\/p>\n\n<p>What is the best Base image to use? Example Dockerfile?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1565186451297,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1565186790803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57396212",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":13,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker and TensorFlow 2.0",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":4136.0,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Poster_created_time":1446859510543,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":3259.0,
        "Poster_view_count":233.0,
        "Solution_body":"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.<\/strong><\/p>\n<ul>\n<li>SageMaker + TensorFlow docs: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html<\/a><\/li>\n<li>Supported Tensorflow versions (and Docker URIs): <a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images<\/a><\/li>\n<\/ul>\n<hr \/>\n<p><em>Original answer<\/em><\/p>\n<p>Here is an example Dockerfile that uses <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">the underlying SageMaker Containers library<\/a> (this is what is used in the official pre-built Docker images):<\/p>\n<pre><code>FROM tensorflow\/tensorflow:2.0.0b1\n\nRUN pip install sagemaker-containers\n\n# Copies the training code inside the container\nCOPY train.py \/opt\/ml\/code\/train.py\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>For more information on this approach, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html<\/a><\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1595008443227,
        "Solution_link_count":7,
        "Solution_readability":25.0,
        "Solution_reading_time":19.1,
        "Solution_score_count":10.0,
        "Solution_sentence_count":13,
        "Solution_word_count":94,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1494171603136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":73187.0,
        "Answerer_view_count":8473.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure ML Workspace which comes by default with some pre-installed packages.<\/p>\n<p>I tried to install<\/p>\n<pre><code>!pip install -U imbalanced-learn\n<\/code><\/pre>\n<p>But I got this error<\/p>\n<pre><code>Requirement already up-to-date: scikit-learn in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (0.24.2)\nRequirement already satisfied, skipping upgrade: scipy&gt;=0.19.1 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.18.5)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (2.1.0)\nCollecting imbalanced-learn\n  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: scipy&gt;=1.1.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (1.4.1)\nERROR: Could not find a version that satisfies the requirement scikit-learn&gt;=1.0.1 (from imbalanced-learn) (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\nERROR: No matching distribution found for scikit-learn&gt;=1.0.1 (from imbalanced-\n<\/code><\/pre>\n<p>learn)<\/p>\n<p>Not sure how to solve this, I have read in other posts to use conda, but that didnt work either.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1644933997077,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1644960360047,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71127858",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.6,
        "Challenge_reading_time":31.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"Cant install imbalanced-learn on an Azure ML Environment",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":219.0,
        "Challenge_word_count":225,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p><a href=\"https:\/\/pypi.org\/project\/scikit-learn\/1.0.1\/\" rel=\"nofollow noreferrer\"><code>scikit-learn<\/code> 1.0.1<\/a> and up require Python &gt;= 3.7; you use Python 3.6. You need to upgrade Python or downgrade <code>imbalanced-learn<\/code>. <a href=\"https:\/\/pypi.org\/project\/imbalanced-learn\/0.8.1\/\" rel=\"nofollow noreferrer\"><code>imbalanced-learn<\/code> 0.8.1<\/a> allows Python 3.6 so<\/p>\n<pre><code>!pip install -U &quot;imbalanced-learn &lt; 0.9&quot;\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.3,
        "Solution_reading_time":6.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8,
        "Solution_word_count":39,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1445975382243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":6831.0,
        "Answerer_view_count":653.0,
        "Challenge_adjusted_solved_time":3.9043655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>SageMaker seems to give examples of using two different serving stacks for serving custom docker images:<\/p>\n\n<ol>\n<li>NGINX + Gunicorn + Flask<\/li>\n<li>NGINX + TensorFlow Serving<\/li>\n<\/ol>\n\n<p>Could someone explain to me at a very high level (I have very little knowledge of network engineering) what responsibilities these different components have? And since the second stack has only two components instead of one, can I rightly assume that TensorFlow Serving does the job (whatever that may be) of both Gunicorn and Flask? <\/p>\n\n<p>Lastly, I've read that it's possible to use Flask and TensorFlow serving at the same time. Would this then be NGINX -> Gunicorn -> Flask -> TensorFlow Serving? And what are there advantages of this?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547670814743,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":1547802223467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54224934",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":9.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker TensorFlow serving stack comparisons",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":741.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. <\/p>\n\n<p>I'll go over the different components from bottom to top:<\/p>\n\n<p><strong>TensorFlow Serving<\/strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and\/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.<\/p>\n\n<p><strong>Flask<\/strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. <\/p>\n\n<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1<\/strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2<\/strong>)(<strong>3<\/strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4<\/strong>). You could visualize this as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/67EXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/67EXW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Gunicorn<\/strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.<\/p>\n\n<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. <\/p>\n\n<p><strong>Nginx<\/strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. <\/p>\n\n<p><a href=\"https:\/\/serverfault.com\/questions\/331256\/why-do-i-need-nginx-and-something-like-gunicorn\">This answer<\/a> might be helpful as well. <\/p>\n\n<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1547816279183,
        "Solution_link_count":3,
        "Solution_readability":10.2,
        "Solution_reading_time":36.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":25,
        "Solution_word_count":444,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an issue where vscode when connected to a VM on GCP cannot see packages installed in <code>\/opt\/conda\/lib\/python3.7\/site-packages.<\/code> I have created the VM using Vertex AI. When I open the jupyter notebook through the UI in a the browser I can see all the packages via <code>pip3 list<\/code>. But when I am connected to the VM through SSH in vscode I cannot see the packages installed such as nltk, spacy etc. and when I try to load it gives me <code>ModuleNotFoundError<\/code>. This error does not show up when I use the jupyter notebook from the Vertex AI UI. The site-packages folder is in my system path and the python that I am using is <code>\/opt\/conda\/bin\/python3<\/code>.<\/p>\n<p>Any help is appreciated. Please do let me know if my question is clear.<\/p>\n<p>EDIT: I figured out that my packages are running on a container in the VM. Is there a way for me to access those packages via jupyter notebook in vscode?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1638473538253,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1638550660460,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70205432",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.4,
        "Challenge_reading_time":11.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"VSCode cannot see packages on a GCP VM",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":230.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580840045043,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":85.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Posting the answer as community wiki. As confirmed by @Abhishek, he was able to make it work by installing a docker extension on the VM then attach VS code to the container.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.4,
        "Solution_reading_time":2.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":32,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nCloning a single notebook using the \"Open In in Sagemaker Studio Lab\" fails.  Cloning the whole repo works.  \r\n\r\nUsing sagemaker's sample, https:\/\/github.com\/aws\/studio-lab-examples\/tree\/main\/open-in-studio-lab, I get this error:\r\n```\r\nUnable to copy notebook to project.\r\nThe link to this notebook is broken or blocked. If this is a private GitHub notebook, sign in to GitHub before copying the notebook.aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb\r\n```\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create MD cell with `[![Open in SageMaker Studio Lab](https:\/\/studiolab.sagemaker.aws\/studiolab.svg)](https:\/\/studiolab.sagemaker.aws\/import\/github\/aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb)`  and run it\r\n2. Click on the button that appears once you run the cell.  Will open new tab in browser\r\n3. In the new pop up tab, click \"Copy to Project\".  Will open new tab in browser\r\n4. In the new pop up tab's modal, select \"Copy Notebook Only\"\r\n5. Error will now appear\r\n\r\n**Expected behavior**\r\nMy notebook will open and appear, just as it would with cloning a directory\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/46935140\/151415892-7d033f97-f98c-4ac8-9c50-99223253b1ee.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [Windows 11]\r\n - Browser [Chrome]\r\n - Version [97.0.4692.71]\r\n\r\n",
        "Challenge_closed_time":1643319,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643305784000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/54",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":19.98,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] \"Open In in Sagemaker Studio Lab\" button process fails when attempting to \"Copy Notebooks Only\"",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":177,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Interesting - this works fine on my end, but I'm using a Mac. I'll create a ticket for you. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.07,
        "Solution_score_count":null,
        "Solution_sentence_count":2,
        "Solution_word_count":18,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"mlflow raises error if length of key\/value exceeds 250. If the length of gbdt parameters or cat_columns is long, experiment_gbdt will raise an exception.\r\n\r\nPossible option:\r\n- catch and ignore all errors from mlflow\r\n- truncate logging parameters automatically ",
        "Challenge_closed_time":1580353,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580251523000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/nyanp\/nyaggle\/issues\/19",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":4.0,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":30.0,
        "Challenge_repo_issue_count":92.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"experiment_gbdt raise errors with long parameters and mlflow",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":44,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1405882600928,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":115.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an existing project, cloned with <code>git clone<\/code>.<\/p>\n\n<p>After I <code>pip install kedro<\/code> I can run <code>kedro info<\/code> fine but I dont seem to have access to the projects CLI for example if I try to run<code>kedro install<\/code> I get the following error:<\/p>\n\n<pre><code>Usage: kedro [OPTIONS] COMMAND [ARGS]...\nTry 'kedro -h' for help.\n\nError: No such command 'install'.\n<\/code><\/pre>\n\n<p>Any clues on what to do for existing projects are much appreciated.<\/p>\n\n<p>Not sure if this matters but I am working inside a conda environment which is inside a docker container.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1589564204173,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1589721006283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61825202",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Accessing Kedro CLI from an existing project",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1175.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1387377887347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Edinburgh, United Kingdom",
        "Poster_reputation_count":1240.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>Project CLIs are available if you run <code>kedro<\/code> at your Kedro project directory. <\/p>\n\n<ol>\n<li><p>Run <code>kedro new<\/code> to create a Kedro project<\/p><\/li>\n<li><p><code>cd &lt;your-kedro-project&gt;<\/code><\/p><\/li>\n<li><p><code>kedro<\/code> at the project directory<\/p><\/li>\n<\/ol>\n\n<p>And you should see the project level CLIs<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Also for your existing project, check if you have <code>kedro_cli.py<\/code> at your project directory.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":11.7,
        "Solution_reading_time":8.23,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5,
        "Solution_word_count":62,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Sorry if this is covered by docs  couldn\u2019t understand something about wandb jobs (wandb launch);<\/p>\n<p>Basically, does the Dockerfile itself must have an <code>Entrypoint<\/code> attr, which runs a program? Or is it enough to have a docker image which has WANDB_DOCKER, wandb_api_key env variable set, having a python directory codebase which can start wandb runs?? (can\u2019t create a job this way though\u2026), if possible without logging the code.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679664874173,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/confused-about-wandb-launch-usage-with-docker\/4119",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":6.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Confused about wandb launch usage with docker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":74,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Cleared:<br>\nYeah, the Dockerfile should be complete end to end such that just running the docker file should run the program, it must have <code>Entrypoint<\/code> attr in the Dockerfile, which actually can be changed from the overrides when we pass it to queue.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":17.7,
        "Solution_reading_time":3.33,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":43,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1548341556520,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mumbai, Maharashtra, India",
        "Answerer_reputation_count":2907.0,
        "Answerer_view_count":238.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm not able to upload statsmodels 0.9rc1 python package in Azure ML studio for Time series analysis.<\/p>\n\n<p>I have downloaded <a href=\"https:\/\/files.pythonhosted.org\/packages\/df\/6f\/df6cf5faecd8082ee23916ff45d396dfee5a1f17aa275da7bab4f5c8926a\/statsmodels-0.9.0rc1-cp36-cp36m-win_amd64.whl\" rel=\"nofollow noreferrer\">statsmodels 0.9rc1<\/a>, unzipped contents and added statsmodels folder and model.pkl file to zip folder.<\/p>\n\n<p>But, while uploading to Microsoft Azure ML studio it says <strong>failed to build schema and visualization<\/strong><\/p>\n\n<p>I'm using this external package in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts\" rel=\"nofollow noreferrer\">Execute Python script<\/a><\/p>\n\n<p>PS: I have succesfully uploaded packages like Adal, dateutils etc.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570616046857,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1570619708227,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58301879",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.5,
        "Challenge_reading_time":11.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to upload statsmodels 0.9rc1 python package in Azure ML studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":141.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1548341556520,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mumbai, Maharashtra, India",
        "Poster_reputation_count":2907.0,
        "Poster_view_count":238.0,
        "Solution_body":"<p>I have switched to Azure Jupyter Notebook where I installed package using pip<\/p>\n\n<pre><code>!pip install statsmodels==0.9.0rc1\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2,
        "Solution_word_count":17,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I work in SM Studio, and I do not understand why CPU and memory usage do not appear in the notebook toolbar. These metrics should be there, at least given this description:\n\nhttps:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/notebooks-menu.html\n\nWhen I open a notebook in SM Studio, I see the same toolbar but without CPU and memory usage listed. Moreover, I see 'cluster' before the kernel's name in my toolbar.\n\nHas anyone experienced sth similar? I assume an alternative for me would be to use CloudWatch.",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652797037698,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668563929559,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGQfGnTgqQcyNbWVb3U9V8Q\/cpu-memory-usage-missing-from-sm-studio-notebook-toolbar",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"CPU + memory usage missing from SM Studio notebook toolbar",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":614.0,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, you should be able to see your CPU and Memory on the bottom toolbar, looks like `Kernel: Idle | Instance MEM`. You can click on that text to show the kernel and instance usage metrics.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1652798353412,
        "Solution_link_count":0,
        "Solution_readability":6.6,
        "Solution_reading_time":2.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":35,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am quite new to docker. My problem in short is to create a docker file that contains python with sklearn and pandas which can be used on aws sagemaker.<\/p>\n<p>My current docker file looks like the following:<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>However when i try to create this image I get an error at line <code>pip3 install sagemaker-training<\/code>. The error is the following:<\/p>\n<pre><code>error: command 'gcc' failed: No such file or directory\n\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-o5rzjscd\/install-record.txt --single-version-externally-managed --compile --install-headers \/opt\/conda\/include\/python3.9\/sagemaker-training Check the logs for full command output.\n\nThe command '\/bin\/bash -o pipefail -c pip3 install sagemaker-training' returned a non-zero code: 1\n<\/code><\/pre>\n<p>If there is a more suitable base image can someone point that out to me? I am generally trying to follow this page <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit<\/a>.<\/p>\n<p>Note: I realise I can use some sagemaker pre-built containers without using my own docker file. However I am trying to do this for my own learning so I know what to do for projects that can't utilise them.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":5,
        "Challenge_created_time":1634912008467,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1634913542568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69678393",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":11.5,
        "Challenge_reading_time":29.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Creating docker file which installs python with sklearn and pandas that can be used on sagemaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":161.0,
        "Challenge_word_count":227,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737124656,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milton Keynes",
        "Poster_reputation_count":738.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>I adjusted your Dockerfile and it builds successfully for me.<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\nARG defaultuser=jovyan\nUSER root\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update &amp;&amp; \\\n    apt-get -y install gcc mono-mcs &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\nUSER $defaultuser\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>(I had to adjust for the fact that the default user from the base container isn't root, when installing GCC)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.2,
        "Solution_reading_time":6.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":65,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to create a tabular dataset in a notebook with R kernel. The following code works with python kernel but how to do the same thing with R kernel ? Can anyone please help me ? Any help would be appreciated.     <\/p>\n<pre><code>from azureml.core import Workspace, Dataset  \n from azureml.core.dataset import Dataset  \n      \n subscription_id = 'abc'  \n resource_group = 'abcd'  \n workspace_name = 'xyz'  \n      \n workspace = Workspace(subscription_id, resource_group, workspace_name)  \n      \n dataset = Dataset.get_by_name(workspace, name='test')  \n      \n      \n # create tabular dataset from all parquet files in the directory  \n tabular_dataset_3 = Dataset.Tabular.from_parquet_files(path=(datastore,'\/UI\/09-17-2022_125003_UTC\/userdata1.parquet'))  \n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663428115537,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1012184\/how-to-create-tabular-dataset-in-notebook-with-r-k",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to create tabular dataset in notebook with R kernel",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=0274aa41-1ea9-4fdc-8434-c9c13c43307c\">@Ankit19 Gupta  <\/a> The Azure Machine Learning SDK for R was deprecated at the end of 2021 to make way for an improved R training and deployment experience using Azure Machine Learning CLI 2.0    <br \/>\nPlease refer the azureml SDK <a href=\"https:\/\/github.com\/Azure\/azureml-sdk-for-r\">repo<\/a> for more details which was deprecated at the end of last year. You can use CLI to register the dataset using specification file.    <\/p>\n<pre><code>az ml dataset register [--file]  \n                       [--output-metadata-file]  \n                       [--path]  \n                       [--resource-group]  \n                       [--show-template]  \n                       [--skip-validation]  \n                       [--subscription-id]  \n                       [--workspace-name]  \n<\/code><\/pre>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":13.9,
        "Solution_reading_time":13.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":106,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1559888978768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":569.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created the following <code>environment.yml<\/code> file from my local Anaconda that contains an openjdk package.<\/p>\n<pre><code>name: venv\nchannels:\n  - defaults\ndependencies:\n  - openjdk=11.0.6\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6AlVr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6AlVr.png\" alt=\"Anaconda openjdk\" \/><\/a><\/p>\n<p>However, Azure Machine Learning couldn't install the openjdk package from the <code>environment.yml<\/code> file as module is not found.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gxuS6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gxuS6.png\" alt=\"ResolvePackageNotFound\" \/><\/a><\/p>\n<p>Backstory:<\/p>\n<p>I'm building a machine learning model using H2O.ai Python library. Unfortunately, H2O.ai is written in Java so it requires Java to run. I've installed openjdk to my local Anaconda venv for running H2O.ai locally - it runs perfectly. However, I couldn't deploy this model to Azure Machine Learning because it couldn't install openjdk from requirements.txt or environment.yml as module not found.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1619503641653,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1619802630132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67277764",
        "Challenge_link_count":4,
        "Challenge_participation_count":4,
        "Challenge_readability":10.6,
        "Challenge_reading_time":14.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"How to install OpenJDK library?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1389.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559888978768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":569.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>Solution:<\/p>\n<p>Install openjdk through conda but specify conda-forge as the channel to install the package from.<\/p>\n<pre><code>name: venv\nchannels:\n  - defaults\n  - conda-forge\ndependencies:\n  - conda-forge::openjdk=11.0.9.1\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AHCye.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AHCye.png\" alt=\"Conda Forge\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":14.0,
        "Solution_reading_time":5.21,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5,
        "Solution_word_count":32,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Similar question as <a href=\"https:\/\/stackoverflow.com\/questions\/43176442\/install-r-packages-in-azure-ml\">here<\/a> but now on Python packages. Currently, the CVXPY is missing in Azure ML. I am also trying to get other solvers such as GLPK, CLP and COINMP working in Azure ML.<\/p>\n<p><strong>How can I install Python packages in Azure ML?<\/strong><\/p>\n<hr \/>\n<p><em>Update about trying to install the Python packages not found in Azure ML.<\/em><\/p>\n<blockquote>\n<p>I did as instructed by Peter Pan but I think the 32bits CVXPY files are wrong for the Anaconda 4 and Python 3.5 in Azure ML, logs and errors are <a href=\"https:\/\/pastebin.com\/zN5QrPtL\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>[Information]         Running with Python 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)]\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9glSm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9glSm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/blockquote>\n<p><em>Update 2 with win_amd64 files (paste <a href=\"https:\/\/pastebin.com\/tisWuP5C\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code>[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         cvxopt-1.1.9-cp35-cp35m-win_amd64.whl          2017-06-07 01:03:34      1972074\n[Information]         __MACOSX\/                                      2017-06-07 01:26:28            0\n[Information]         __MACOSX\/._cvxopt-1.1.9-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:34          452\n[Information]         cvxpy-0.4.10-py3-none-any.whl                  2017-06-07 00:25:36       300880\n[Information]         __MACOSX\/._cvxpy-0.4.10-py3-none-any.whl       2017-06-07 00:25:36          444\n[Information]         ecos-2.0.4-cp35-cp35m-win_amd64.whl            2017-06-07 01:03:40        56522\n[Information]         __MACOSX\/._ecos-2.0.4-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:40          450\n[Information]         numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl   2017-06-07 01:25:02    127909457\n[Information]         __MACOSX\/._numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl 2017-06-07 01:25:02          459\n[Information]         scipy-0.19.0-cp35-cp35m-win_amd64.whl          2017-06-07 01:05:12     12178932\n[Information]         __MACOSX\/._scipy-0.19.0-cp35-cp35m-win_amd64.whl 2017-06-07 01:05:12          452\n[Information]         scs-1.2.6-cp35-cp35m-win_amd64.whl             2017-06-07 01:03:34        78653\n[Information]         __MACOSX\/._scs-1.2.6-cp35-cp35m-win_amd64.whl  2017-06-07 01:03:34          449\n[Information]         [ READING ] 0:00:00\n[Information]         Input pandas.DataFrame #1:\n[Information]         Empty DataFrame\n[Information]         Columns: [1]\n[Information]         Index: []\n[Information]         [ EXECUTING ] 0:00:00\n[Information]         [ WRITING ] 0:00:00\n<\/code><\/pre>\n<p>where <code>import cvxpy<\/code>, <code>import cvxpy-0.4.10-py3-none-any.whl<\/code> or <code>cvxpy-0.4.10-py3-none-any<\/code> do not work so<\/p>\n<p><strong>How can I use the following wheel files downloaded from <a href=\"http:\/\/www.lfd.uci.edu\/%7Egohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\">here<\/a> to use the external Python packages not found in Azure ML?<\/strong><\/p>\n<\/blockquote>\n<p><em>Update about permission problem about importing cvxpy (paste <a href=\"https:\/\/pastebin.com\/3kTKgLfc\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code> [Error]         ImportError: No module named 'canonInterface'\n<\/code><\/pre>\n<p>where the ZIP Bundle is organised a bit differently, the content of each wheel downloaded to a folder and the content having all zipped as a ZIP Bundle.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496674339213,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44371692",
        "Challenge_link_count":11,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":49.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":null,
        "Challenge_title":"Install Python Packages in Azure ML?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":12625.0,
        "Challenge_word_count":346,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts#limitations\" rel=\"nofollow noreferrer\"><code>Limitations<\/code><\/a> and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of <code>Execute Python Script<\/code> tutorial, the only way to add custom Python modules is via the zip file mechanism to package the modules and all dependencies.<\/p>\n\n<p>For example to install <code>CVXPY<\/code>, as below.<\/p>\n\n<ol>\n<li>Download the wheel file of <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\"><code>CVXPY<\/code><\/a> and its dependencies like <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxopt\" rel=\"nofollow noreferrer\"><code>CVXOPT<\/code><\/a>.<\/li>\n<li>Decompress these wheel files, and package these files in the path <code>cvxpy<\/code> and <code>cvxopt<\/code>, etc as a zipped file with your script.<\/li>\n<li>Upload the zip file as a dataset and use it as the script bundle.<\/li>\n<\/ol>\n\n<p>If you were using IPython, you also can try to install the Python Package via the code <code>!pip install cvxpy<\/code>.<\/p>\n\n<p>And there are some similar SO threads which may be helpful for you, as below.<\/p>\n\n<ol>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/44285641\/azure-ml-python-with-script-bundle-cannot-import-module\">Azure ML Python with Script Bundle cannot import module<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/8663046\/how-to-install-a-python-package-from-within-ipython\">How to install a Python package from within IPython?<\/a><\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>\n\n<hr>\n\n<p>Update:<\/p>\n\n<p>For IPython interface of Azure ML, you move to the <code>NOTEBOOKS<\/code> tab to create a notebook via <code>ADD TO PROJECT<\/code> button at the bottom of the page, as the figure below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Or you can directly login to the website <code>https:\/\/notebooks.azure.com<\/code> to use it.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1496760284030,
        "Solution_link_count":9,
        "Solution_readability":10.8,
        "Solution_reading_time":28.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":21,
        "Solution_word_count":215,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I cannot execute sagemaker notebook anymore.<br>\nThe following error occurs.<\/p>\n\n<pre><code>Failed to start kernel\nAn error occurred (ThrottlingException) when calling the CreateApp operation (reached max retries: 4): \nRate exceeded\n<\/code><\/pre>\n\n<p>I checked my app list and there are only two.\nOne app is trying to delete but never stops, this could be one of the problem.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/M0iqo.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590373205720,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61994821",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Cannot execute AWS Sagemaker Notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":965.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532422348876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Happened to me too. Contact support and ask them to delete the kernel behind the scenes.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.1,
        "Solution_reading_time":1.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":16,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy an R inference script to Azure ML Service Endpoint as an Azure Container Instance. I have made the following steps:<\/p>\n<ul>\n<li>   created a custom Docker image from scratch and pushed it to the Azure Container Registry (associated with AML Workspace)<\/li>\n<li>   registered a custom environment in AML Workspace, based on the image in ACR<\/li>\n<li>   deployed R entry script (just a simple hello world script with init() and run() functions defined)\n<ul>\n<li>   the inference configuration uses the custom AML environment<\/li>\n<li>   deployment is made with Azure ML R SDK<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>The container instance is created, but the endpoint startup runs into error. Here is the output from the container instance:<\/p>\n<pre><code>2020-10-16T12:56:21,639812796+00:00 - gunicorn\/run \n2020-10-16T12:56:21,639290594+00:00 - iot-server\/run \n2020-10-16T12:56:21,640405198+00:00 - rsyslog\/run \n2020-10-16T12:56:21,735291424+00:00 - nginx\/run \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2020-10-16T12:56:23,736657191+00:00 - iot-server\/finish 1 0\n2020-10-16T12:56:23,834747728+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 20.0.4\nListening at: http:\/\/127.0.0.1:31311 (11)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 38\n\/bin\/bash: \/root\/miniconda3\/lib\/libtinfo.so.6: no version information available (required by \/bin\/bash)\nSPARK_HOME not set. Skipping PySpark Initialization.\nException in worker process\nTraceback (most recent call last):\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 43, in &lt;module&gt;\n    from azureml.api.exceptions.ClientSideException import ClientSideException\nModuleNotFoundError: No module named 'azureml.api'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/util.py&quot;, line 383, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.8\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1014, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 671, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 783, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n    from app import main\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 45, in &lt;module&gt;\n    from azure.ml.api.exceptions.ClientSideException import ClientSideException\nModuleNotFoundError: No module named 'azure.ml'\nWorker exiting (pid: 38)\nShutting down: Master\nReason: Worker failed to boot.\n2020-10-16T12:56:39,434787859+00:00 - gunicorn\/finish 3 0\n2020-10-16T12:56:39,435715063+00:00 - Exit code 3 is not normal. Killing image.\n<\/code><\/pre>\n<p>How do I install the azureml.api dependency, which can not be found? It doesn't seem to be part of the Azure ML SDK. I have installed the following dependencies in my Dockerfile:<\/p>\n<pre><code>RUN apt-get -y install python3-flask python3-rpy2 python3-azure python3-applicationinsights\nRUN pip install azureml-core\n<\/code><\/pre>\n<p>I also have Miniconda installed. Pip refers to Miniconda's pip.<\/p>\n<p>Or, is this dependency available to install at all? Should I use some pre-defined AML environment as the base Docker image? (Note: I am currently using bare FROM: ubuntu). Suggestions how to find and use the base images are also welcome, since this is not documented very well.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602855221470,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/129038\/r-model-deployment-with-custom-docker-image-module",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":62.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":null,
        "Challenge_title":"R model deployment with custom Docker image: \"ModuleNotFoundError: No module named 'azureml.api'\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":500,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c02d5983-5261-45cb-9bb0-3e5f19b42ae9\">@Lauri Lehman  <\/a> Thanks for the question. Here is the <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fmedium.com%2Fmicrosoftazure%2Fhow-to-create-custom-docker-base-images-for-azure-machine-learning-environments-86aa4c7bc7b9&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7C49414af813754671ec7908d83863d1b5%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637321350687268940&amp;sdata=f5ebitkOUNxY8v7cOS2vFy12mBfuP%2BJVVzLbhAKVyXE%3D&amp;reserved=0\">document<\/a>  to Create Custom Docker Base Images for Azure Machine Learning Environments for R people.    <br \/>\nWe have used the AzureML <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.r_script_step.rscriptstep?view=azure-ml-py\">RScriptStep<\/a>  pipeline feature which allows you to point to CRAN or Github or custom URLS, but this requires authoring the pipeline in python or YAML.. In R You can also  these arguments in the Azuremlsdk R estimator function:  <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html\">https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html<\/a>    <br \/>\nAnother option that are not available through conda install as part of the R script with install.packages(\u201cpath\/*.tar.gz\u201d, repos=NULL))    <\/p>\n<p>One of the challenges is that the build at runtime can take a while to prepare the environment.  R likes to compile packages on Linux environments and a large package could have lots of dependencies which would take a while.  This is an R on Linux\/PaaS thing, rather than specific to AzureML    <\/p>\n<p>To make start up fast we created a custom docker image where you can tightly control the image ahead of runtime.  If you want to go in this direction you can find an example Dockerfile to get you started here..    <br \/>\n<a href=\"https:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile\">https:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile<\/a>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":15.4,
        "Solution_reading_time":27.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12,
        "Solution_word_count":191,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Is there a solution for multi-user Notebook on Studio Notebook or Notebook Instances? Eg if we want several developers to interact on the same notebook at the same time",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592989945000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668624634692,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU4mxRvXy2QYmkYCvdqNVa2g\/is-there-a-solution-for-multi-user-notebook-on-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":2.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a solution for multi-user Notebook on SageMaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":881.0,
        "Challenge_word_count":37,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Notebook instances are not connected to the user. So if two users has the same access rights they will see and will be able to access the same instance (even in the same time). \n\nThe issue is - Jupyter Notebook is not ready for that, both users will have the same privileges, no tracking who did what, ... And working on the same notebook on the same time - basically they will overwrite each other saves.\n\nI had a need for similar thing (pair programming - data scientist and software engineer) - the only viable solution we were able to find was desktop sharing (like TeamViewer, ...)",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925546486,
        "Solution_link_count":0,
        "Solution_readability":8.8,
        "Solution_reading_time":7.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":101,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1466260908296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the Azure Recommendation API sample there is a snippet like this:<\/p>\n\n<pre><code>     if (itemSets.RecommendedItemSetInfo != null)\n        {\n            ...\n        }\n        else\n        {\n            Console.WriteLine(\"No recommendations found.\");\n        }\n<\/code><\/pre>\n\n<p>So I assume that nullable recommended set means no recommendations. But what is the case with this set being not nullable but still empty ( as I am having it running the example)?<\/p>\n\n<p>I provided my own usages and catalog files. I have not too many entries there however for i2i recommendations I have results and for u2i there is an empty set.\nAllowColdItemPlacement doesn't change a think here.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1477648288960,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40302499",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Recommendation API: what is the difference between null results and empty results",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1354118434116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Wroc\u0142aw, Poland",
        "Poster_reputation_count":393.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>We did not mean to convey a difference in meaning between null recommendations and empty recommendations. I will check why we may be sending two different types of results. Either way, don't treat those two cases as different cases. <\/p>\n\n<p>If you are not getting results for user-to-item recommendations, most likely there was no data for that user when the build was created or the items that the user interacted with do not have enough co-occurrences with other items in the usage.<\/p>\n\n<p>What to do when you get empty recommendations is up to you, you may decide to not show any recommendations, or back-fill with popular items you may want to promote.<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Luis Cabrera\nProgram Manager - Recommendations API.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":119,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1370286859500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16981.0,
        "Answerer_view_count":1733.0,
        "Challenge_adjusted_solved_time":21792.3181611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need the sacred package for a new code base I downloaded. It requires sacred. \n<a href=\"https:\/\/pypi.python.org\/pypi\/sacred\" rel=\"noreferrer\">https:\/\/pypi.python.org\/pypi\/sacred<\/a><\/p>\n\n<p>conda install sacred fails with \nPackageNotFoundError: Package missing in current osx-64 channels: \n  - sacred<\/p>\n\n<p>The instruction on the package site only explains how to install with pip. What do you do in this case?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1501710168027,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45471477",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.2,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"python package can be installed by pip but not conda",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":7845.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1321905325720,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California, United States",
        "Poster_reputation_count":3278.0,
        "Poster_view_count":399.0,
        "Solution_body":"<p>That package is not available as a conda package at all. You can search for packages on anaconda.org: <a href=\"https:\/\/anaconda.org\/search?q=sacred\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=sacred<\/a> You can see the type of package in the 4th column. Other Python packages may be available as conda packages, for instance, NumPy: <a href=\"https:\/\/anaconda.org\/search?q=numpy\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=numpy<\/a><\/p>\n\n<p>As you can see, the conda package numpy is available from a number of different channels (the channel is the name before the slash). If you wanted to install a package from a different channel, you can add the option to the install\/create command with the <code>-c<\/code>\/<code>--channel<\/code> option, or you can add the channel to your configuration <code>conda config --add channels channel-name<\/code>.<\/p>\n\n<p>If no conda package exists for a Python package, you can either install via pip (if available) or <a href=\"https:\/\/docs.conda.io\/projects\/conda-build\/en\/latest\/user-guide\/tutorials\/building-conda-packages.html\" rel=\"nofollow noreferrer\">build your own conda package<\/a>. This isn't usually too difficult to do for pure Python packages, especially if one can use <code>skeleton<\/code> to build a recipe from a package on PyPI.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1580162513407,
        "Solution_link_count":5,
        "Solution_readability":11.0,
        "Solution_reading_time":16.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":12,
        "Solution_word_count":163,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":27.5167869444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am still new in AWS sagemaker. Working on a architecture where we would have an AWS sagemaker notebook. There would be multiple users, I want that students don`t see each other work. would I need to do that in terminal? or we can do that in notebook itself?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584978635243,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60816944",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":3.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker Notebook with multiple users",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1907.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1513883236660,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Noida, Uttar Pradesh, India",
        "Poster_reputation_count":465.0,
        "Poster_view_count":46.0,
        "Solution_body":"<p>The simplest way is to create a small notebook instance for each student. This way you can have the needed isolation and also the responsibility of each student for their notebook to stop them when they are not in use.<\/p>\n\n<p>The smallest instance type <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">costs<\/a> $0.0464 per hour. If you have it running 24\/7 it costs about $30 per month. But if the students are responsible and stop their instances when they are not using them, it can be about $1 for 20 hours of work.<\/p>\n\n<p>If you want to enable isolation to the notebooks, you can use the ability to presign the URL that is used to open the Jupyter interface. See here on the way to use the CLI to create the URL: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-presigned-notebook-instance-url.html<\/a>. It is also supported in other SDK.<\/p>\n\n<pre><code>create-presigned-notebook-instance-url\n--notebook-instance-name &lt;student-instance-name&gt;\n--session-expiration-duration-in-seconds 3600\n<\/code><\/pre>\n\n<p>You can integrate it into the internal portal that you have in your institute. <\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1585077695676,
        "Solution_link_count":3,
        "Solution_readability":10.8,
        "Solution_reading_time":16.57,
        "Solution_score_count":3.0,
        "Solution_sentence_count":12,
        "Solution_word_count":160,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an azureml studio with a notebook and suddenly since today, I cant run notebooks cells anymore.  It says kernel not connected.  <br \/>\nI cant either open the terminal it never loads.  <\/p>\n<p>I restarted the compute instance several time, but that didnt fix the problem  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1645626985700,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/747823\/kernel-not-connected",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.0,
        "Challenge_reading_time":3.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Kernel not connected",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=0b725e52-0000-0003-0000-000000000000\">@Luis Valencia  <\/a>    <\/p>\n<p>There was an issue causing the &quot;kernal not connected&quot; issue, but it has been fixed. Please let us know if you are still blocked by this issue. Thanks a lot!    <\/p>\n<p>Regards,    <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.1,
        "Solution_reading_time":3.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":40,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Azure ML, I'm trying to execute a Python module that needs to import the module pyxdameraulevenshtein (<a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>).<\/p>\n\n<p>I followed the usual way, which is to create a zip file and then import it; however for this specific module, it seems to never be able to find it. The error message is as usual:<\/p>\n\n<p><em>ImportError: No module named 'pyxdameraulevenshtein'<\/em><\/p>\n\n<p>Has anyone included this pyxdameraulevenshtein module in Azure ML with success ?<\/p>\n\n<p>(I took the package from <a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>.)<\/p>\n\n<p>Thanks for any help you can provide,<\/p>\n\n<p>PH<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496235911927,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":1496278915287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44285641",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":11.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Python with Script Bundle cannot import module",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2395.0,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1496233557192,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I viewed the <code>pyxdameraulevenshtein<\/code> module page, there are two packages you can download which include a wheel file for MacOS and a source code tar file. I don't think you can directly use the both on Azure ML, because the MacOS one is just a share library <code>.so<\/code> file for darwin which is not compatible with Azure ML, and the other you need to first compile it.<\/p>\n\n<p>So my suggestion is as below for using <code>pyxdameraulevenshtein<\/code>.<\/p>\n\n<ol>\n<li>First, compile the source code of <code>pyxdameraulevenshtein<\/code> to a DLL file on Windows, please refer to the document for Python <a href=\"https:\/\/docs.python.org\/2\/extending\/windows.html\" rel=\"nofollow noreferrer\">2<\/a>\/<a href=\"https:\/\/docs.python.org\/3\/extending\/windows.html\" rel=\"nofollow noreferrer\">3<\/a> or search for doing this.<\/li>\n<li>Write a Python script using the DLL you compiled to implement your needs, please refer to the SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/252417\/how-can-i-use-a-dll-file-from-python\">How can I use a DLL file from Python?<\/a> for how to use DLL from Python and refer to the Azure offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts\" rel=\"nofollow noreferrer\">tutorial<\/a> to write your Python script<\/li>\n<li>Package your Python script and DLL file as a zip file, then to upload the zip file to use it in <code>Execute Python script<\/code> model of Azure ML.<\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":9.5,
        "Solution_reading_time":19.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12,
        "Solution_word_count":192,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1524630627116,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":154.0,
        "Answerer_view_count":27.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to install a package <code>aclocal<\/code> on <code>Amazon SageMmaker<\/code> (which is a requirement for <code>teseract<\/code>) using the command <code>sudo yum install aclocal<\/code><\/p>\n<p>But it gives me the following error.<\/p>\n<p><code>No package aclocal available<\/code><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1662965527117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73685446",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"No package aclocal available",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":27.0,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524630627116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":154.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>To install <code>tesseract<\/code> in SageMaker you can simply follow the instructions here: <a href=\"https:\/\/tesseract-ocr.github.io\/tessdoc\/Compiling.html#linux\" rel=\"nofollow noreferrer\">https:\/\/tesseract-ocr.github.io\/tessdoc\/Compiling.html#linux<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":24.1,
        "Solution_reading_time":3.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":16,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>I am doing the Challenge. <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/intro-to-azure-machine-learning-service\/\">https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/intro-to-azure-machine-learning-service\/<\/a>    <\/p>\n<p>Please see what I have installed:    <\/p>\n<blockquote>\n<p>pip install azureml-sdk    <\/p>\n<\/blockquote>\n<p>I am getting the following messages at the end:    <\/p>\n<blockquote>\n<p>ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.    <\/p>\n<p>We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.    <\/p>\n<p>jupyterlab 2.2.9 requires jupyterlab-server&lt;2.0,&gt;=1.1.5, which is not installed.    <br \/>\nSuccessfully installed applicationinsights-0.11.9 azure-identity-1.4.1 azureml-automl-core-1.19.0 azureml-dataprep-2.6.3 azureml-dataprep-native-26.0.0 azureml-dataprep-rslex-1.4.0 azureml-dataset-runtime-1.19.0.post1 azureml-pipeline-1.19.0 azureml-pipeline-core-1.19.0 azureml-pipeline-steps-1.19.0 azureml-sdk-1.19.0 azureml-telemetry-1.19.0 azureml-train-1.19.0 azureml-train-automl-client-1.19.0 azureml-train-core-1.19.0 azureml-train-restclients-hyperdrive-1.19.0 distro-1.5.0 dotnetcore2-2.1.20 fusepy-3.0.1 msal-1.8.0 msal-extensions-0.2.2 numpy-1.19.3 portalocker-1.7.1 pyarrow-1.0.1 pywin32-227    <\/p>\n<\/blockquote>\n<p>Now I am trying to start up and type the following in .py file in Visual Studio Code    <\/p>\n<blockquote>\n<p>from azureml.core import Workspace    <\/p>\n<\/blockquote>\n<p>This is the error message I am getting:    <\/p>\n<blockquote>\n<p> File &quot;c:\/Users\/User\/OneDrive\/Desktop\/New folder\/Build AI Solution\/automl_python.py&quot;, line 1, in &lt;module&gt;    <br \/>\n    from azureml.core import Workspace    <br \/>\nModuleNotFoundError: No module named 'azureml'    <\/p>\n<\/blockquote>\n<p>Please could you help me?    <\/p>\n<p>thanks,    <\/p>\n<p>Naveen<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609112074090,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/211503\/modulenotfounderror-no-module-named-azureml",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":26.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"ModuleNotFoundError: No module named 'azureml'",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":190,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>This is now solved. Thanks!<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-1.9,
        "Solution_reading_time":0.44,
        "Solution_score_count":29.0,
        "Solution_sentence_count":1,
        "Solution_word_count":5,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433746746023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":22140.0,
        "Answerer_view_count":1710.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been trying to install a machine learning package that I can use in my R script.<\/p>\n\n<p>I have done placed the tarball of the installer inside a zip file and am doing <\/p>\n\n<pre><code>install.packages(\"src\/packagename_2.0-3.tar.gz\", repos = NULL, type=\"source\") \n<\/code><\/pre>\n\n<p>from within the R script. However, the progress indicator just circles indefinitely, and it's not installed in environment.<\/p>\n\n<p>How can I install this package?<\/p>\n\n<p><code>ada<\/code> is the package I'm trying to install and <code>ada_2.0-3.tar.gz<\/code> is the file I'm using.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1443064053390,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1443071013852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32752659",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":7.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"unable to install R library in azure ml",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":982.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426899441168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2024.0,
        "Poster_view_count":298.0,
        "Solution_body":"<p>You cannot use the tarball packages. If you are on windows you need to do the following:<\/p>\n\n<p>Once you install a package (+ it's dependencies) it will download the packages in a directory <\/p>\n\n<blockquote>\n  <p>C:\\Users\\xxxxx\\AppData\\Local\\Temp\\some directory\n  name\\downloaded_packages<\/p>\n<\/blockquote>\n\n<p>These will be in a zip format. These are the packages you need. <\/p>\n\n<p>Or download the windows binaries from cran.<\/p>\n\n<p>Next you need to put all the needed packages in one total zip-file and upload this to AzureML as a new dataset.<\/p>\n\n<p>in AzureML load the data package connected to a r-script<\/p>\n\n<pre><code>install.packages(\"src\/ada.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(ada, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>Be sure to check that all dependent packages are available in Azure. Rpart is available.<\/p>\n\n<p>For a complete overview, look at this <a href=\"http:\/\/blogs.msdn.com\/b\/benjguin\/archive\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning.aspx\" rel=\"nofollow\">msdn blog<\/a> explaining it a bit better with some visuals.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":9.3,
        "Solution_reading_time":13.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":11,
        "Solution_word_count":135,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1294730361590,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY",
        "Answerer_reputation_count":57082.0,
        "Answerer_view_count":4597.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to list conda dependencies using a .yaml file for an AzureML <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-an-inference-configuration\" rel=\"nofollow noreferrer\">environment<\/a>. I do not want to use a custom docker image just for a few variations. I wonder if there is a way to instruct the build to run python commands using the .yaml file. Here are excerpts of what I have tried as of now:<\/p>\n<pre><code>name: classifer_environment\ndependencies:\n- python=3.6.2\n\n- pip:\n  - azureml-defaults&gt;=1.0.45\n  - nltk==3.4.5\n  - spacy\n\n- command: \n  - bash -c &quot;python -m nltk.downloader stopwords&quot;\n  - bash -c &quot;python -m spacy download en_core_web_sm&quot;\n<\/code><\/pre>\n<p>I also tried this:<\/p>\n<pre><code>name: classifer_environment\ndependencies:\n- python=3.6.2\n\n- pip:\n  - azureml-defaults&gt;=1.0.45\n  - nltk==3.4.5\n  - spacy\n\n- python: \n  - nltk.downloader stopwords\n  - spacy download en_core_web_sm\n<\/code><\/pre>\n<p>I do not have much clarity about yaml specifications. Both the specifications fail with the following messages respectively in the build logs: <br>\n&quot;Unable to install package for command.&quot; <br>\n&quot;Unable to install package for python.&quot;<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599654539997,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63811726",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":16.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"How to execute python commands from a conda .yaml specification file?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":408.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1555424023328,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bhubaneswar, Odisha, India",
        "Poster_reputation_count":328.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>This might be a neat feature to have, but for now it's not a thing - at least not directly in the YAML like this.<\/p>\n<p>Instead, the unit of computation in Conda is the <em>package<\/em>. That is, if you need to run additional scripts or commands at environment creation, it can be achieved by building a custom package and including this package in the YAML as a dependency. The package itself could be pretty much empty, but whatever code one needs to run would be included via <a href=\"https:\/\/docs.conda.io\/projects\/conda-build\/en\/latest\/resources\/link-scripts.html\" rel=\"nofollow noreferrer\">some installation scripts<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.5,
        "Solution_reading_time":7.96,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5,
        "Solution_word_count":92,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using SageMaker JupyterLab, but I found pandas is out of date, what's the process of updating it?<\/p>\n<p>I tried this:\nIn terminal:<\/p>\n<pre><code>cd SageMaker\nconda update pandas\n<\/code><\/pre>\n<p>The package has been updated to 1.0.5\nbut when I use this command in SageMaker instance:<\/p>\n<pre><code>import pandas\nprint(pandas,__version__)\n\nreturn:\n0.24.2\n<\/code><\/pre>\n<p>It didn't work at all, can someone help me? Thanks.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593894159730,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1593895091990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62734059",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":6.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to update pandas version in SageMaker notebook terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1158.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>If you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":17.4,
        "Solution_reading_time":3.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2,
        "Solution_word_count":30,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <br \/>\nI'm trying to setup an integration between a GITHub repository and my Jupyter Lab but I'm struggling to find the GIT options in my Jupyter Lab application.  <\/p>\n<p>I was expecting to see a Git clone button, a Git option on the toolbar and also the same option on the left pane but there is nothing GIT related.  <\/p>\n<p>I've already installed successfully the following:  <\/p>\n<p>pip install jupyterlab-git  <br \/>\npip install --upgrade python-gitlab  <\/p>\n<p>But nothing happens.  <br \/>\nWhen I try to clone a GIT repository, I get the folders\/files but then I can't interact with it. It's just copying it into my space but then I can't push\/pull anything.  <\/p>\n<p>Can you help me on this?  <\/p>\n<p>Thank you,  <br \/>\nCarla  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594373340610,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/45187\/azure-machine-learning-jupyter-lab-git-options",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":9.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Jupyter Lab Git options",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":132,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I found the answer to my question by following the steps here:  <\/p>\n<p><a href=\"https:\/\/www.oreilly.com\/library\/view\/jupyterlab-quick-start\/9781789805543\/94288841-0158-4a98-8151-4a90ea9bf2da.xhtml\">https:\/\/www.oreilly.com\/library\/view\/jupyterlab-quick-start\/9781789805543\/94288841-0158-4a98-8151-4a90ea9bf2da.xhtml<\/a> <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":21.3,
        "Solution_reading_time":4.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1,
        "Solution_word_count":16,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Started up a STANDARD_D11_V2 cluster to run some notebooks on.<\/p>\n<p>Wanted to use json_normalize from pandas: <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.json_normalize.html\">https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.json_normalize.html<\/a> and I get the below error:<\/p>\n<pre><code>AttributeError: module 'pandas' has no attribute 'json_normalize'\n<\/code><\/pre>\n<p>Pandas seems to be out of date. Checked the installed version of pandas:<\/p>\n<pre><code>$ python\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import pandas\n\n&gt;&gt;&gt; pandas.__version__\n\n'0.23.4\n<\/code><\/pre>\n<p>Pandas is indeed out of date, the latest version is v1.1.1. Fired up a terminal to run:<\/p>\n<pre><code>conda update --all\n<\/code><\/pre>\n<p>On the azureml_py36 environment which I had selected to run the notebook on. It hangs on:<\/p>\n<pre><code>Solving environment: \/\n<\/code><\/pre>\n<p>Went to update conda to see if that would help:<\/p>\n<pre><code>conda update conda\n<\/code><\/pre>\n<p>But I get this error:<\/p>\n<pre><code>PackageNotInstalledError: Package is not installed in prefix.\n  prefix: \/anaconda\/envs\/azureml_py36\n  package name: conda\n<\/code><\/pre>\n<p>Which leads me to think this is not a typical installation of conda.<\/p>\n<p>Would like to run the most up to date packages on Azure Machine Learning to replicate the local environment I have setup. Does anyone know how to do this?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598608952077,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/80212\/update-out-of-date-packages-on-azure-machine-learn",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":21.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Update out of date packages on Azure Machine Learning Compute instance",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":200,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@philmariusnew-9791 I have tried the above steps and the installation completed successfully for conda. But when we upgrade pandas azureml package has a dependency  so it cannot use version v1.1.1     <\/p>\n<p>I have went ahead and upgrade the pandas version but there is a warning as seen below:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/21216-image.png?platform=QnA\" alt=\"21216-image.png\" \/>    <\/p>\n<p>We would recommend to use the package that is compatible with azureml so your environment setup works as expected.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":12.8,
        "Solution_reading_time":6.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":73,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"I was getting an error when azuremllogonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. So, I installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below.\r\n\r\ngrep executes but now the error says \"Dataset with name 'mnist_opendataset' is not found\".\r\n\r\nAny help troubleshooting this error will be appreciated, I am trying to demo this to a customer. next week.\r\n\r\n**TEXT of the OUTPUT when error is encountered:**\r\n\r\n\r\nInstalling amlarc-compute K8s extension was successful.\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nLibrary configuration succeeded\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\n\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nfound compute target: ARC-ml\r\n\"\r\n Training model:\r\n                               \r\n            .....                                             .....\r\n         .........                                           .........\r\n        .........                 (((((((((##                 .........\r\n       .....                      (((((((####                      .....\r\n      ......                      #((########                      ......\r\n     ....... .............        ###########        ............. .......\r\n     ......................       ###########       ......................\r\n    .................*.....       ###########       ....,*.................\r\n    .........*******......       (((((((((((         ......*******.........\r\n         ............          (((((((((((     (.         ............\r\n                            .(((((((((((     (((((\/\r\n                          ((((((((((((     #(((((((##\r\n                        \/\/\/\/(((((((*     ##############\r\n                      \/\/\/\/\/\/(((((.         ,#############.\r\n                   ,**\/\/\/\/\/\/\/((               #############\/\r\n                    *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%##########\r\n                    \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######(\r\n                     \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%####\r\n                     .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#.\r\n\r\n\"\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nWARNING: Command group 'ml job' is experimental and under development. Reference and support levels: https:\/\/aka.ms\/CLI_refstatus\r\nUploading src:   0%|                                                                                                                                | 0.00\/3.08k [00:00<?, ?B\/s]\r\n\r\n**ERROR: Code: UserError**\r\n**Message: Dataset with name 'mnist_opendataset' is not found.**\r\n**You cannot call a method on a null-valued expression.**\r\n**At C:\\Temp\\AzureMLLogonScript.ps1:267 char:4**\r\n**+    $RunId = ($Job | grep '\\\"name\\\":').Split('\\\"')[3]**\r\n**+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**\r\n    **+ CategoryInfo          : InvalidOperation: (:) [], RuntimeException**\r\n    **+ FullyQualifiedErrorId : InvokeMethodOnNull**\r\n\r\n**RunId:**\r\n**Training model, hold tight...**\r\n**ERROR: argument --name\/-n: expected one argument**_****\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n",
        "Challenge_closed_time":1631711,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631563330000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/758",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":10.4,
        "Challenge_reading_time":56.26,
        "Challenge_repo_contributor_count":62.0,
        "Challenge_repo_fork_count":369.0,
        "Challenge_repo_issue_count":1562.0,
        "Challenge_repo_star_count":527.0,
        "Challenge_repo_watch_count":26.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":null,
        "Challenge_title":"error when installing AZURE ML training model piece",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":477,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @arturoqu77 - thanks for reaching out. We tried to repro this issue but couldn't.\r\n\r\nThis [line of code](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/AzureMLLogonScript.ps1#L266) leverages grep to parse the file name. `grep` should have been installed as part of the [bootstrap](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73). If  `grep` wasn't installed, this implies something must have interrupted the install before it got there.\r\n\r\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working. \r\n\r\nAre you seeing Postman installed - this happens [after `grep`](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73)? If not, this is probably what happened.\r\n\r\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above. Hello,\n\nThank you for your reply. I may have logged on before the bootstrap completed, I re-started the deployment to a new RG and seems to be working now.\n\nThanks for the help.\n\nRegards\n\n***@***.***\nArturo Quiroga\nSr. Cloud Solutions Architect (CSA)\nAzure Applications & Infrastructure\n***@***.******@***.***>\n[MSFT_logo_Gray DE sized SIG1.png]\n\n\nFrom: Raki ***@***.***>\nDate: Tuesday, September 14, 2021 at 6:22 PM\nTo: microsoft\/azure_arc ***@***.***>\nCc: Arturo Quiroga ***@***.***>, Mention ***@***.***>\nSubject: Re: [microsoft\/azure_arc] error when installing AZURE ML training model piece (#758)\n\nHi @arturoqu77<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Farturoqu77&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666347896%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=r1kAuKxYlYhONjoSTk83SERggUvNcbP1Hr4vmNh29io%3D&reserved=0> - thanks for reaching out. We tried to repro this issue but couldn't.\n\nThis line of code<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FAzureMLLogonScript.ps1%23L266&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=oAjL%2BfBBF4QXfnwN9gcM9UqEB4OA0ZZrzMuKilatz5A%3D&reserved=0> leverages grep to parse the file name. grep should have been installed as part of the bootstrap<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=V8dzJxj3W5a6IL8T%2BvB0mijBm5Ng4G46bb%2Fcdo2uvz4%3D&reserved=0>. If grep wasn't installed, this implies something must have interrupted the install before it got there.\n\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working.\n\nAre you seeing Postman installed - this happens after grep<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=XvpeFo2T7Kjr4qrIZYKO7eM0khlOddES9O3DGaw1yZ4%3D&reserved=0>? If not, this is probably what happened.\n\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fissues%2F758%23issuecomment-919554382&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ZBGNkrDGFcqvrdWHXy5iEGluQiq2Ph%2BZnfosqC3qTTU%3D&reserved=0>, or unsubscribe<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAHV4QUFA72NR7CEJ3UPS5NLUB7DLDANCNFSM5D6SSBHA&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ctEevpiqzC%2FQnTc6ho2hfr2PVGA%2FqwGJzj1pPUCEylY%3D&reserved=0>.\nTriage notifications on the go with GitHub Mobile for iOS<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=cMCZqYPB6q8c9n%2BgPTk9f3MCQr%2BlV4GsOW9iPFSZtgE%3D&reserved=0> or Android<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666387876%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=6B5T09q%2Bx2Q2rWftui6b32lD1VLrCRMPiLSrTUS7xnI%3D&reserved=0>.\n Great!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":11,
        "Solution_readability":21.6,
        "Solution_reading_time":96.07,
        "Solution_score_count":null,
        "Solution_sentence_count":39,
        "Solution_word_count":416,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1565215898703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to deploy an AML Notebook VM via an ARM template? If so, is there an example or documentation somewhere?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565189508713,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1565217649487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57397150",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":2.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploy Notebook VM via ARM Template?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":71.0,
        "Challenge_word_count":27,
        "Platform":"Stack Overflow",
        "Poster_created_time":1529439461716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":392.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>Unfortunately this is not supported today, but ARM support is in our roadmap<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.4,
        "Solution_reading_time":1.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1,
        "Solution_word_count":13,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have done quite a few google searches but have not found a clear answer to the following use case. Basically, I would rather use cloud 9 (most of the time) as my IDE rather than Jupyter. What I am confused\/not sure about is, how I could executed long running jobs like (Bayesian) hyper parameter optimisation from there. Can I use Sagemaker capabilities? Should I use docker and deploy to ECR (looking for the cheapest-ish option)? Any pointers w.r.t. to this particular issue would be very much appreciated. Thanks.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641644416603,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1641679883347,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70632239",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"cloud 9 and sagemaker - hyper parameter optimisation",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>You could use whatever IDE you choose (including your laptop).<br \/>\nSaegMaker tuning job (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex.html\" rel=\"nofollow noreferrer\">example<\/a>) is <strong>asynchronous<\/strong>, so you can safely close your IDE after launching it. You can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-monitor.html\" rel=\"nofollow noreferrer\">monitor the job the AWS web console,<\/a> or with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeHyperParameterTuningJob.html\" rel=\"nofollow noreferrer\">DescribeHyperParameterTuningJob API call<\/a>.<\/p>\n<p>You can launch TensorFlow, PyTorch, XGBoost, Scikit-learn, and other <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/frameworks.html\" rel=\"nofollow noreferrer\">popular ML frameworks<\/a>, using one of the built-in framework containers, avoiding the extra work of bringing your own container.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":20.2,
        "Solution_reading_time":13.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":81,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been working on a project for some months on AML Studio. I recently wanted to utilize the VSCode integration, so I opened my workspace via the extension. I had my notebook open in the vscode version, and ran a couple of cells. I shut down the compute, and left it there. <\/p>\n<p>Now I've come back to this and my notebook has been wiped. Completely, every cell. This is the case on the AML Studio proper, and on the vscode integration version. There doesn't seem to be any kind of backup tool I can use. I've looked in the filestore and there's no snapshot, no nothing. I looked at the last experiment run log, but it has no code. <\/p>\n<p>Ofc when I work elsewhere, I use git. But there's no git integration in AML studio, and the terminal is so unusably slow that I haven't touched it in a long time. <\/p>\n<p>I don't know what to do. This is literally months of work, wiped out. No idea what happened or how to rectify. <\/p>\n<p>Any help much appreciated. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1679589627240,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1192605\/connecting-to-vscode-wiped-my-notebook-completely",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.1,
        "Challenge_reading_time":12.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Connecting to VSCode wiped my notebook, completely",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":188,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=83376df1-3fe7-4f8e-aeae-798027e4c31b\">@Daniel Goldwater  <\/a>If you have your notebook setup under your user directory the files that are created and saved should be available in your default storage account i.e the storage account connected to your workspace under the fileshares of your storage account. For example, here is a screen shot of my notebooks under my username from the studio.<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/2e273099-db36-47d4-a82d-d1c43365bab9?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>By default, all these files are mounted to your compute instance when you start the instance, and this enables you to connect to git through terminal and use an external source control mechanism. Since you have not added this the files that are added previously should be available here even if you have deleted your compute. The files should be visible under your user account even when no compute is available. <\/p>\n<p>The file share that is mounted is from your storage account as seen in the screen shot below:<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/a149337f-0fb2-4873-99d6-337cf73a946d?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>This file share should be visible under your storage account file share tab and you should be able to download the data from storage explorer till the point the data was last saved. Ideally, notebook data is saved every 30seconds or whenever they are saved or when a checkpoint is created. Here is a screen shot of my storage account from storage browser to help you check the same on your account.<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/b527ad6a-403d-4aaa-b75b-d27131005d16?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>I hope this is helpful!! Thanks!!<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":10.5,
        "Solution_reading_time":23.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13,
        "Solution_word_count":242,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1415906440767,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kyiv",
        "Answerer_reputation_count":12948.0,
        "Answerer_view_count":363.0,
        "Challenge_adjusted_solved_time":260.2198386111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We have a notebook instance within Sagemaker which contains many Jupyter Python scripts. I'd like to write a program which downloads these various scripts each day (i.e. so that I could back them up). Unfortunately I don't see any reference to this in the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/index.html\" rel=\"nofollow noreferrer\">AWS CLI API<\/a>.<\/p>\n\n<p>Is this achievable? <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1541188759393,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53125108",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I download files within a Sagemaker notebook instance programatically?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":8552.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1299752760990,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2563.0,
        "Poster_view_count":167.0,
        "Solution_body":"<p>It's not exactly that you want, but looks like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Version_control\" rel=\"nofollow noreferrer\">VCS<\/a> can fit your needs. You can use Github(if you already use it) or CodeCommit(free privat repos) Details and additional ways like <code>sync<\/code> target <code>dir<\/code> with <code>S3<\/code> bucket - <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1542125550812,
        "Solution_link_count":3,
        "Solution_readability":20.6,
        "Solution_reading_time":8.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":44,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Challenge_closed_time":1586730,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571075742000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":16.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":91.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Can not compile SageMaker examples",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":19,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is reported by user and the problem is kubeflow pipeline has some breaking changes on parameters but we always install latest KFP pipeline which is not compatible. \r\n\r\nShort term. use lower kfp version\r\n```\r\n!pip install https:\/\/storage.googleapis.com\/ml-pipeline\/release\/0.1.29\/kfp.tar.gz --upgrade\r\n```\r\n\r\nLong term, update examples and make sure it leverages latest features of KFP.  Will check on the [SageMaker example](https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/01438d181f502504056eac89bfc0eb091733e9a8\/notebooks\/05_Kubeflow_Pipeline\/05_04_Pipeline_SageMaker.ipynb) and file a PR to make it leverage the latest features of KFP. And the master example of [SageMaker Kubeflow Pipeline](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/samples\/contrib\/aws-samples\/mnist-kmeans-sagemaker), will try to use [master yaml file](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/aws\/sagemaker). After so, will try to use latest version 2.05 of kfp to make it compatible. Potential SageMaker example issues with users: [1st](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1401) and [2nd](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1642). But the issue description is not that informative. Will talk with users if necessary. Let's not put time on this one. I will ask SM team to fix Op issue and we can concentrate on others. Since the updated SageMaker example has been merged, let's close this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6,
        "Solution_readability":10.4,
        "Solution_reading_time":18.52,
        "Solution_score_count":null,
        "Solution_sentence_count":17,
        "Solution_word_count":157,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello community,     <br \/>\nI'm facing a problem, my ACR in my resource group was deleted and I couldn't create any instance. I created again and now I can create instances but i'm having problems to run the dataset profile. It's failing to pull the image docker.    <\/p>\n<p>This is the output    <\/p>\n<pre><code>AzureMLCompute job failed.  \nFailedPullingImage: Unable to pull docker image  \n\timageName: 19acd0cdf57549bcace363c924cf045b.azurecr.io\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f  \n\terror: Run docker command to pull public image failed with error: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.  \n.  \n\tReason: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.  \n  \n\tInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.  \n<\/code><\/pre>\n<p>The ML Studio has the following permissions on the ACR permissions    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/132698-unbenannt.png?platform=QnA\" alt=\"132698-unbenannt.png\" \/>    <\/p>\n<p>The docker image appears in the repositories of the ACR    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/132781-unbenannt2.png?platform=QnA\" alt=\"132781-unbenannt2.png\" \/>    <\/p>\n<p>Any hint how can i solve this problem?    <\/p>\n<p>Thanks in advance    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631798842910,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/555024\/not-able-to-pull-docker-image-from-container-regis",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":23.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Not able to pull docker image from Container Registry",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":175,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=5565f700-af3b-41a9-b47f-9b8a6276d8fd\">@Moresi, Marco  <\/a> Does this container registry have the admin account enabled? A requirement while creating a workspace with an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace-cli?tabs=bringexistingresources1%2Cvnetpleconfigurationsv1cli#create-a-workspace\">existing container registry<\/a> is to have the admin account enabled.     <\/p>\n<p>If you have already enabled it then a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace-cli?tabs=bringexistingresources1%2Cvnetpleconfigurationsv1cli#sync-keys-for-dependent-resources\">re-sync of keys<\/a> might be required for your workspace.    <\/p>\n<pre><code>az ml workspace sync-keys -w &lt;workspace-name&gt; -g &lt;resource-group-name&gt;  \n<\/code><\/pre>\n<p>Deleting the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace?tabs=python#deleting-the-azure-container-registry\">default container registry<\/a> used by the workspace can also cause the workspace to break.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":18.3,
        "Solution_reading_time":14.97,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7,
        "Solution_word_count":79,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"## Description\r\nThe conda environment for python3.6 in notebooks cannot find `pandas.CSVDataSet`\r\n\r\n## Context\r\nI'm wanting to use sagemaker as my development environment. However, I cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## Steps to Reproduce\r\n\r\n0. Startup a Sagemaker instance with defaults\r\n\r\nTerminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => Success!\r\n\r\nNotebook fail:\r\n1. Create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> The environments for the terminal and notebooks are separate by design in Sagemaker\r\n2. Load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> Note that I've started to use the code below; Without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `Path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # Check it exists first. For some reason this is not an idempotent operation?\r\n    current_dir = Path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. Run `context.catalog.list()`\r\n\r\n## Expected Result\r\nThe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## Actual Result\r\n```\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\nFull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not None)\r\n    418         except StopIteration:\r\n\r\nStopIteration: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except StopIteration:\r\n--> 419             raise DataSetError(\"Class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\nDataSetError: Class `pandas.CSVDataSet` not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = AbstractDataSet.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise DataSetError(\r\n    153                 \"An exception occurred when parsing config \"\r\n--> 154                 \"for DataSet `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\nDataSetError: An exception occurred when parsing config for DataSet `example_iris_data`:\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\n## Investigations so far\r\n\r\n### `CSVLocalDataSet`\r\nUpon changing the yaml type for iris.csv from `pandas.CSVDataSet` to `CSVLocalDataSet`, we get success on both the terminal and the notebook. However, this is not my desired outcome; The transition to using `pandas.CSVDataSet` makes it easier, for me at least, to use both S3 and local datasets.\r\n\r\n### `pip install kedro` output from notebook\r\n```\r\nCollecting kedro\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/6f\/4faaa0e58728a318aeabc490271a636f87f6b9165245ce1d3adc764240cf\/kedro-0.15.8-py3-none-any.whl (12.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.5MB 4.1MB\/s eta 0:00:01\r\nRequirement already satisfied: xlsxwriter<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.0.4)\r\nCollecting azure-storage-file<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c9\/33\/6c611563412ffc409b2413ac50e3a063133ea235b86c137759774c77f3ad\/azure_storage_file-1.4.0-py2.py3-none-any.whl\r\nCollecting fsspec<1.0,>=0.5.1 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6e\/2b\/63420d49d5e5f885451429e9e0f40ad1787eed0d32b1aedd6b10f9c2719a\/fsspec-0.7.1-py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 33.5MB\/s ta 0:00:01\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (0.24.2)\r\nCollecting s3fs<1.0,>=0.3.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b8\/e4\/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675\/s3fs-0.4.2-py3-none-any.whl\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nCollecting tables<3.6,>=3.4.4 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/f7\/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a\/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 10.2MB\/s ta 0:00:01\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (2.20.0)\r\nCollecting toposort<2.0,>=1.5 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e9\/8a\/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4\/toposort-1.5-py2.py3-none-any.whl\r\nRequirement already satisfied: click<8.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (6.7)\r\nCollecting azure-storage-queue<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/72\/94\/4db044f1c155b40c5ebc037bfd9d1c24562845692c06798fbe869fe160e6\/azure_storage_queue-1.4.0-py2.py3-none-any.whl\r\nCollecting cookiecutter<2.0,>=1.6.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/86\/c9\/7184edfb0e89abedc37211743d1420810f6b49ae4fa695dfc443c273470d\/cookiecutter-1.7.0-py2.py3-none-any.whl (40kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 24.6MB\/s ta 0:00:01\r\nCollecting pandas-gbq<1.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c3\/74\/126408f6bdb7b2cb1dcb8c6e4bd69a511a7f85792d686d1237d9825e6194\/pandas_gbq-0.13.1-py3-none-any.whl\r\nCollecting pip-tools<5.0.0,>=4.0.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/94\/8f\/59495d651f3ced9b06b69545756a27296861a6edd6c5709fbe1265ed9032\/pip_tools-4.5.1-py2.py3-none-any.whl (41kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 27.5MB\/s ta 0:00:01\r\nCollecting azure-storage-blob<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/25\/f4\/a307ed89014e9abb5c5cfc8ca7f8f797d12f619f17a6059a6fd4b153b5d0\/azure_storage_blob-1.5.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 35.2MB\/s ta 0:00:01\r\nCollecting pyarrow<1.0.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/10\/93fad5849418eade4a4cd581f8cd27be1bbe51e18968ba1492140c887f3f\/pyarrow-0.16.0-cp36-cp36m-manylinux1_x86_64.whl (62.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.9MB 779kB\/s eta 0:00:01    40% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 25.7MB 56.1MB\/s eta 0:00:01\r\nRequirement already satisfied: SQLAlchemy<2.0,>=1.2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.2.11)\r\nRequirement already satisfied: xlrd<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.1.0)\r\nCollecting python-json-logger<1.0,>=0.1.9 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/80\/9d\/1c3393a6067716e04e6fcef95104c8426d262b4adaf18d7aa2470eab028d\/python-json-logger-0.1.11.tar.gz\r\nCollecting anyconfig<1.0,>=0.9.7 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4c\/00\/cc525eb0240b6ef196b98300d505114339bbb7ddd68e3155483f1eb32050\/anyconfig-0.9.10.tar.gz (103kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 34.4MB\/s ta 0:00:01\r\nCollecting azure-storage-common~=1.4 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/6c\/b2285bf3687768dbf61b6bc085b0c1be2893b6e2757a9d023263764177f3\/azure_storage_common-1.4.2-py2.py3-none-any.whl (47kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 25.9MB\/s ta 0:00:01\r\nCollecting azure-common>=1.1.5 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e5\/4d\/d000fc3c5af601d00d55750b71da5c231fcb128f42ac95b208ed1091c2c1\/azure_common-1.1.25-py2.py3-none-any.whl\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.7.3)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.14.3)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2018.4)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (4.0.1)\r\nRequirement already satisfied: numexpr>=2.6.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (2.6.5)\r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.11.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.23)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.6)\r\nCollecting whichcraft>=0.4.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b5\/a2\/81887a0dae2e4d2adc70d9a3557fdda969f863ced51cd3c47b587d25bce5\/whichcraft-0.6.1-py2.py3-none-any.whl\r\nCollecting future>=0.15.2 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/45\/0b\/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9\/future-0.18.2.tar.gz (829kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 27.8MB\/s ta 0:00:01\r\nCollecting poyo>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/42\/50\/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc\/poyo-0.5.0-py2.py3-none-any.whl\r\nCollecting binaryornot>=0.2.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/7e\/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2\/binaryornot-0.4.4-py2.py3-none-any.whl\r\nCollecting jinja2-time>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6a\/a1\/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4\/jinja2_time-0.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.10)\r\nCollecting google-auth-oauthlib (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7b\/b8\/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b\/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\r\nCollecting google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/b0\/cc391ebf8ebf7855cdcfe0a9a4cdc8dcd90287c90e1ac22651d104ac6481\/google_auth-1.12.0-py2.py3-none-any.whl (83kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 35.5MB\/s ta 0:00:01\r\nCollecting pydata-google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/ed\/9c9f410c032645632de787b8c285a78496bd89590c777385b921eb89433d\/pydata_google_auth-0.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (39.1.0)\r\nCollecting google-cloud-bigquery>=1.11.1 (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/8f\/f7\/b6f55e144da37f38a79552a06103f2df4a9569e2dfc6d741a7e2a63d3592\/google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 39.2MB\/s ta 0:00:01\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.14)\r\nCollecting arrow (from jinja2-time>=0.1.0->cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/fa\/f84896dede5decf284e6922134bf03fd26c90870bbf8015f4e8ee2a07bcc\/arrow-0.15.5-py2.py3-none-any.whl (46kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 26.3MB\/s ta 0:00:01\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.0)\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a3\/12\/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379\/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/95\/de\/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d\/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 32.5MB\/s ta 0:00:01\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/08\/6a\/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425\/cachetools-4.0.0-py3-none-any.whl\r\nCollecting google-api-core<2.0dev,>=1.15.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/63\/7e\/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77\/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 29.9MB\/s ta 0:00:01\r\nCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/89\/3c\/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97\/google_cloud_core-1.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.6.1)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/35\/9e\/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098\/google_resumable_media-0.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.11.5)\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/57\/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704\/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 42.0MB\/s ta 0:00:01\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/46\/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf\/googleapis-common-protos-1.51.0.tar.gz\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.18)\r\nBuilding wheels for collected packages: python-json-logger, anyconfig, future, googleapis-common-protos\r\n  Running setup.py bdist_wheel for python-json-logger ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\r\n  Running setup.py bdist_wheel for anyconfig ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\r\n  Running setup.py bdist_wheel for future ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\r\n  Running setup.py bdist_wheel for googleapis-common-protos ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\r\nSuccessfully built python-json-logger anyconfig future googleapis-common-protos\r\ncookiecutter 1.7.0 has requirement click>=7.0, but you'll have click 6.7 which is incompatible.\r\ngoogle-auth 1.12.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\r\ngoogle-cloud-bigquery 1.24.0 has requirement six<2.0.0dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\r\npip-tools 4.5.1 has requirement click>=7, but you'll have click 6.7 which is incompatible.\r\nInstalling collected packages: azure-common, azure-storage-common, azure-storage-file, fsspec, s3fs, tables, toposort, azure-storage-queue, whichcraft, future, poyo, binaryornot, arrow, jinja2-time, cookiecutter, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, pandas-gbq, pip-tools, azure-storage-blob, pyarrow, python-json-logger, anyconfig, kedro\r\n  Found existing installation: s3fs 0.1.5\r\n    Uninstalling s3fs-0.1.5:\r\n      Successfully uninstalled s3fs-0.1.5\r\n  Found existing installation: tables 3.4.3\r\n    Uninstalling tables-3.4.3:\r\n      Successfully uninstalled tables-3.4.3\r\nSuccessfully installed anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 cookiecutter-1.7.0 fsspec-0.7.1 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 oauthlib-3.1.0 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 s3fs-0.4.2 tables-3.5.2 toposort-1.5 whichcraft-0.6.1\r\n```\r\n\r\n### `pip install kedro` output from terminal\r\n```\r\nCollecting kedro\r\n  Using cached kedro-0.15.8-py3-none-any.whl (12.5 MB)\r\nCollecting pandas<1.0,>=0.24.0\r\n  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4 MB 9.6 MB\/s \r\nCollecting azure-storage-file<2.0,>=1.1.0\r\n  Using cached azure_storage_file-1.4.0-py2.py3-none-any.whl (30 kB)\r\nCollecting click<8.0\r\n  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 1.7 MB\/s \r\nCollecting cookiecutter<2.0,>=1.6.0\r\n  Using cached cookiecutter-1.7.0-py2.py3-none-any.whl (40 kB)\r\nCollecting SQLAlchemy<2.0,>=1.2.0\r\n  Downloading SQLAlchemy-1.3.15.tar.gz (6.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.1 MB 49.2 MB\/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting tables<3.6,>=3.4.4\r\n  Using cached tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\/python_json_logger-0.1.11-py2.py3-none-any.whl\r\nCollecting azure-storage-blob<2.0,>=1.1.0\r\n  Using cached azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\r\nCollecting pandas-gbq<1.0,>=0.12.0\r\n  Using cached pandas_gbq-0.13.1-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: fsspec<1.0,>=0.5.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.6.3)\r\nCollecting xlsxwriter<2.0,>=1.0.0\r\n  Downloading XlsxWriter-1.2.8-py2.py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 65.9 MB\/s \r\nCollecting pip-tools<5.0.0,>=4.0.0\r\n  Using cached pip_tools-4.5.1-py2.py3-none-any.whl (41 kB)\r\nCollecting pyarrow<1.0.0,>=0.12.0\r\n  Downloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63.1 MB 25 kB\/s \r\nCollecting xlrd<2.0,>=1.0.0\r\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 66.5 MB\/s \r\nRequirement already satisfied: s3fs<1.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.4.0)\r\nCollecting azure-storage-queue<2.0,>=1.1.0\r\n  Using cached azure_storage_queue-1.4.0-py2.py3-none-any.whl (23 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\/anyconfig-0.9.10-py2.py3-none-any.whl\r\nCollecting toposort<2.0,>=1.5\r\n  Using cached toposort-1.5-py2.py3-none-any.whl (7.6 kB)\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (2.23.0)\r\nRequirement already satisfied: pytz>=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2019.3)\r\nRequirement already satisfied: numpy>=1.13.3 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.18.1)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.8.1)\r\nCollecting azure-common>=1.1.5\r\n  Using cached azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\r\nCollecting azure-storage-common~=1.4\r\n  Using cached azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\r\nCollecting poyo>=0.1.0\r\n  Using cached poyo-0.5.0-py2.py3-none-any.whl (10 kB)\r\nCollecting jinja2-time>=0.1.0\r\n  Using cached jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\r\nCollecting whichcraft>=0.4.0\r\n  Using cached whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\r\nCollecting binaryornot>=0.2.0\r\n  Using cached binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.11.1)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\/future-0.18.2-cp36-none-any.whl\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (3.0.5)\r\nCollecting numexpr>=2.6.2\r\n  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 66.7 MB\/s \r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.14.0)\r\nCollecting pydata-google-auth\r\n  Using cached pydata_google_auth-0.3.0-py2.py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting google-cloud-bigquery>=1.11.1\r\n  Using cached google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\r\nCollecting google-auth\r\n  Using cached google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (46.1.1.post20200323)\r\nRequirement already satisfied: boto3>=1.9.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.12.27)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: idna<3,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nCollecting arrow\r\n  Using cached arrow-0.15.5-py2.py3-none-any.whl (46 kB)\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.1.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0\r\n  Using cached google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\r\nCollecting google-cloud-core<2.0dev,>=1.1.0\r\n  Using cached google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.11.3)\r\nCollecting google-api-core<2.0dev,>=1.15.0\r\n  Using cached google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.3.3)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.15.2)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.14.0)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\/googleapis_common_protos-1.51.0-cp36-none-any.whl\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.20)\r\nBuilding wheels for collected packages: SQLAlchemy\r\n  Building wheel for SQLAlchemy (PEP 517) ... done\r\n  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.15-cp36-cp36m-linux_x86_64.whl size=1215829 sha256=112167e02a19acada7f367d8aca55bbd1e0c655de9edfabebae5e9d055d9a9a6\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/4a\/1b\/3a\/c73044d7be48baeb47cbee343334f7803726ca1e9ba7b29095\r\nSuccessfully built SQLAlchemy\r\nInstalling collected packages: pandas, azure-common, azure-storage-common, azure-storage-file, click, poyo, arrow, jinja2-time, whichcraft, binaryornot, future, cookiecutter, SQLAlchemy, numexpr, tables, python-json-logger, azure-storage-blob, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery, pandas-gbq, xlsxwriter, pip-tools, pyarrow, xlrd, azure-storage-queue, anyconfig, toposort, kedro\r\n  Attempting uninstall: pandas\r\n    Found existing installation: pandas 0.22.0\r\n    Uninstalling pandas-0.22.0:\r\n      Successfully uninstalled pandas-0.22.0\r\nSuccessfully installed SQLAlchemy-1.3.15 anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 click-7.1.1 cookiecutter-1.7.0 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 numexpr-2.7.1 oauthlib-3.1.0 pandas-0.25.3 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 tables-3.5.2 toposort-1.5 whichcraft-0.6.1 xlrd-1.2.0 xlsxwriter-1.2.8\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n|environment | terminal | notebook|\r\n|----|----|----|\r\n|`kedro -V` | kedro, version 0.15.8 | kedro, version 0.15.8|\r\n|`python -V` | Python 3.6.10 :: Anaconda, Inc. | Python 3.6.5 :: Anaconda, Inc.|\r\n|os |  `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"` | `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"`|\r\n|`pip freeze` | anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==1.3.0<br>attrs==19.3.0<br>autovizwidget==0.12.9<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>backcall==0.1.0<br>bcrypt==3.1.7<br>binaryornot==0.4.4<br>bleach==3.1.0<br>boto3==1.12.27<br>botocore==1.15.27<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.14.0<br>chardet==3.0.4<br>click==7.1.1<br>colorama==0.4.3<br>cookiecutter==1.7.0<br>cryptography==2.8<br>decorator==4.4.2<br>defusedxml==0.6.0<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.15.2<br>entrypoints==0.3<br>environment-kernels==1.1.1<br>fsspec==0.6.3<br>future==0.18.2<br>gitdb==4.0.2<br>GitPython==3.1.0<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>hdijupyterutils==0.12.9<br>idna==2.9<br>importlib-metadata==1.5.0<br>ipykernel==5.1.4<br>ipython==7.13.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.5.1<br>jedi==0.16.0<br>Jinja2==2.11.1<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>json5==0.9.3<br>jsonschema==3.2.0<br>jupyter==1.0.0<br>jupyter-client==6.0.0<br>jupyter-console==6.1.0<br>jupyter-core==4.6.1<br>jupyterlab==1.2.7<br>jupyterlab-git==0.9.0<br>jupyterlab-server==1.0.7<br>kedro==0.15.8<br>MarkupSafe==1.1.1<br>mistune==0.8.4<br>mock==3.0.5<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.3<br>nbconvert==5.6.1<br>nbdime==2.0.0<br>nbexamples==0.0.0<br>nbformat==5.0.4<br>nbserverproxy==0.3.2<br>nose==1.3.7<br>notebook==5.7.8<br>numexpr==2.7.1<br>numpy==1.18.1<br>oauthlib==3.1.0<br>packaging==20.3<br>pandas==0.25.3<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.6.2<br>pexpect==4.8.0<br>pickleshare==0.7.5<br>pid==3.0.0<br>pip-tools==4.5.1<br>plotly==4.5.4<br>poyo==0.5.0<br>prometheus-client==0.7.1<br>prompt-toolkit==3.0.3<br>protobuf==3.11.3<br>protobuf3-to-dict==0.1.5<br>psutil==5.7.0<br>psycopg2==2.8.4<br>ptyprocess==0.6.0<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycparser==2.20<br>pydata-google-auth==0.3.0<br>pygal==2.4.0<br>Pygments==2.6.1<br>pykerberos==1.1.14<br>PyNaCl==1.3.0<br>pyOpenSSL==19.1.0<br>pyparsing==2.4.6<br>pyrsistent==0.15.7<br>PySocks==1.7.1<br>pyspark==2.3.2<br>python-dateutil==2.8.1<br>python-json-logger==0.1.11<br>pytz==2019.3<br>PyYAML==5.3.1<br>pyzmq==18.1.1<br>qtconsole==4.7.1<br>QtPy==1.9.0<br>requests==2.23.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rsa==3.4.2<br>s3fs==0.4.0<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-experiments==0.1.10<br>sagemaker-nbi-agent==1.0<br>sagemaker-pyspark==1.2.8<br>scipy==1.4.1<br>Send2Trash==1.5.0<br>six==1.14.0<br>smdebug-rulesconfig==0.1.2<br>smmap==3.0.1<br>sparkmagic==0.15.0<br>SQLAlchemy==1.3.15<br>tables==3.5.2<br>terminado==0.8.3<br>testpath==0.4.4<br>texttable==1.6.2<br>toposort==1.5<br>tornado==6.0.4<br>traitlets==4.3.3<br>urllib3==1.22<br>wcwidth==0.1.8<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>whichcraft==0.6.1<br>widgetsnbextension==3.5.1<br>xlrd==1.2.0<br>XlsxWriter==1.2.8<br>zipp==2.2.0 | alabaster==0.7.10<br>anaconda-client==1.6.14<br>anaconda-project==0.8.2<br>anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==0.24.0<br>astroid==1.6.3<br>astropy==3.0.2<br>attrs==18.1.0<br>Automat==0.3.0<br>autovizwidget==0.15.0<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>Babel==2.5.3<br>backcall==0.1.0<br>backports.shutil-get-terminal-size==1.0.0<br>bcrypt==3.1.7<br>beautifulsoup4==4.6.0<br>binaryornot==0.4.4<br>bitarray==0.8.1<br>bkcharts==0.2<br>blaze==0.11.3<br>bleach==2.1.3<br>bokeh==1.0.4<br>boto==2.48.0<br>boto3==1.12.27<br>botocore==1.15.27<br>Bottleneck==1.2.1<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.11.5<br>characteristic==14.3.0<br>chardet==3.0.4<br>click==6.7<br>cloudpickle==0.5.3<br>clyent==1.2.2<br>colorama==0.3.9<br>contextlib2==0.5.5<br>cookiecutter==1.7.0<br>cryptography==2.8<br>cycler==0.10.0<br>Cython==0.28.4<br>cytoolz==0.9.0.1<br>dask==0.17.5<br>datashape==0.5.4<br>decorator==4.3.0<br>defusedxml==0.6.0<br>distributed==1.21.8<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.14<br>entrypoints==0.2.3<br>enum34==1.1.9<br>environment-kernels==1.1.1<br>et-xmlfile==1.0.1<br>fastcache==1.0.2<br>filelock==3.0.4<br>Flask==1.0.2<br>Flask-Cors==3.0.4<br>fsspec==0.7.1<br>future==0.18.2<br>gevent==1.3.0<br>glob2==0.6<br>gmpy2==2.0.8<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>greenlet==0.4.13<br>h5py==2.8.0<br>hdijupyterutils==0.15.0<br>heapdict==1.0.0<br>html5lib==1.0.1<br>idna==2.6<br>imageio==2.3.0<br>imagesize==1.0.0<br>importlib-metadata==1.5.0<br>ipykernel==4.8.2<br>ipyparallel==6.2.2<br>ipython==6.4.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.4.0<br>isort==4.3.4<br>itsdangerous==0.24<br>jdcal==1.4<br>jedi==0.12.0<br>Jinja2==2.10<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>jsonschema==2.6.0<br>jupyter==1.0.0<br>jupyter-client==5.2.3<br>jupyter-console==5.2.0<br>jupyter-core==4.4.0<br>jupyterlab==0.32.1<br>jupyterlab-launcher==0.10.5<br>kedro==0.15.8<br>kiwisolver==1.0.1<br>lazy-object-proxy==1.3.1<br>llvmlite==0.23.1<br>locket==0.2.0<br>lxml==4.2.1<br>MarkupSafe==1.0<br>matplotlib==3.0.3<br>mccabe==0.6.1<br>mistune==0.8.3<br>mkl-fft==1.0.0<br>mkl-random==1.0.1<br>mock==4.0.1<br>more-itertools==4.1.0<br>mpmath==1.0.0<br>msgpack==0.6.0<br>msgpack-python==0.5.6<br>multipledispatch==0.5.0<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.2<br>nbconvert==5.4.1<br>nbformat==4.4.0<br>networkx==2.1<br>nltk==3.3<br>nose==1.3.7<br>notebook==5.5.0<br>numba==0.38.0<br>numexpr==2.6.5<br>numpy==1.14.3<br>numpydoc==0.8.0<br>oauthlib==3.1.0<br>odo==0.5.1<br>olefile==0.45.1<br>opencv-python==3.4.2.17<br>openpyxl==2.5.3<br>packaging==20.1<br>pandas==0.24.2<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.2.0<br>partd==0.3.8<br>path.py==11.0.1<br>pathlib2==2.3.2<br>patsy==0.5.0<br>pep8==1.7.1<br>pexpect==4.5.0<br>pickleshare==0.7.4<br>Pillow==5.1.0<br>pip-tools==4.5.1<br>pkginfo==1.4.2<br>plotly==4.5.2<br>pluggy==0.6.0<br>ply==3.11<br>poyo==0.5.0<br>prompt-toolkit==1.0.15<br>protobuf==3.6.1<br>protobuf3-to-dict==0.1.5<br>psutil==5.4.5<br>psycopg2==2.7.5<br>ptyprocess==0.5.2<br>py==1.5.3<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycodestyle==2.4.0<br>pycosat==0.6.3<br>pycparser==2.18<br>pycrypto==2.6.1<br>pycurl==7.43.0.1<br>pydata-google-auth==0.3.0<br>pyflakes==1.6.0<br>pygal==2.4.0<br>Pygments==2.2.0<br>pykerberos==1.2.1<br>pylint==1.8.4<br>PyNaCl==1.3.0<br>pyodbc==4.0.23<br>pyOpenSSL==18.0.0<br>pyparsing==2.2.0<br>PySocks==1.6.8<br>pyspark==2.3.2<br>pytest==3.5.1<br>pytest-arraydiff==0.2<br>pytest-astropy==0.3.0<br>pytest-doctestplus==0.1.3<br>pytest-openfiles==0.3.0<br>pytest-remotedata==0.2.1<br>python-dateutil==2.7.3<br>python-json-logger==0.1.11<br>pytz==2018.4<br>PyWavelets==0.5.2<br>PyYAML==5.3.1<br>pyzmq==17.0.0<br>QtAwesome==0.4.4<br>qtconsole==4.3.1<br>QtPy==1.4.1<br>requests==2.20.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rope==0.10.7<br>rsa==3.4.2<br>ruamel-yaml==0.15.35<br>s3fs==0.4.2<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-pyspark==1.2.8<br>scikit-image==0.13.1<br>scikit-learn==0.20.3<br>scipy==1.1.0<br>seaborn==0.8.1<br>Send2Trash==1.5.0<br>simplegeneric==0.8.1<br>singledispatch==3.4.0.3<br>six==1.11.0<br>smdebug-rulesconfig==0.1.2<br>snowballstemmer==1.2.1<br>sortedcollections==0.6.1<br>sortedcontainers==1.5.10<br>sparkmagic==0.12.5<br>Sphinx==1.7.4<br>sphinxcontrib-websupport==1.0.1<br>spyder==3.2.8<br>SQLAlchemy==1.2.11<br>statsmodels==0.9.0<br>sympy==1.1.1<br>tables==3.5.2<br>TBB==0.1<br>tblib==1.3.2<br>terminado==0.8.1<br>testpath==0.3.1<br>texttable==1.6.2<br>toolz==0.9.0<br>toposort==1.5<br>tornado==5.0.2<br>traitlets==4.3.2<br>typing==3.6.4<br>unicodecsv==0.14.1<br>urllib3==1.23<br>wcwidth==0.1.7<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>Werkzeug==0.14.1<br>whichcraft==0.6.1<br>widgetsnbextension==3.4.2<br>wrapt==1.10.11<br>xlrd==1.1.0<br>XlsxWriter==1.0.4<br>xlwt==1.3.0<br>zict==0.1.3<br>zipp==3.0.0|\r\n",
        "Challenge_closed_time":1585791,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585713762000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro\/issues\/308",
        "Challenge_link_count":35,
        "Challenge_participation_count":5,
        "Challenge_readability":22.7,
        "Challenge_reading_time":580.3,
        "Challenge_repo_contributor_count":164.0,
        "Challenge_repo_fork_count":740.0,
        "Challenge_repo_issue_count":1942.0,
        "Challenge_repo_star_count":7884.0,
        "Challenge_repo_watch_count":102.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":434,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker notebooks raise error for `pandas.CSVDataSet`",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1973,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Kedro aside there are a couple of things that you can do to ensure that your environments match from the terminal vs notebook.  I am not familiar with the new `pandas.CSVDataSet` as I am just now starting with my first `0.15.8` myself.  We have struggled to get package installs correct through our notebooks, I make sure my team is all using their own environment, created from the terminal.\r\n\r\n## activate python3 from the terminal before install\r\n\r\nNote that the file browser on the left hand side of a SageMaker notebook is really mounted at `~\/SageMaker`.\r\n\r\n``` bash\r\nsource activate python3\r\n# may also be - conda activate python3\r\n# unrelated on windows it was - activate python 3\r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n## install ipykernel in your terminal env\r\n\r\nFor conda environments to show up in the notebook dropdown selection you will need `ipykernel` installed. see [docs](https:\/\/ipython.readthedocs.io\/en\/stable\/install\/kernel_install.html)\r\n\r\n```\r\nconda create -n testing python=3.6\r\npip install ipykernel\r\n# I typically don't have to go this far, but installing ipykernel is recommended by the docs\r\nipykernel install --user \r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n\r\n\r\nDo note that if you shut down your SageMaker notebook you will loose your packages and environments by default.\r\n\r\nI also noticed that you have a difference between pandas.  I have no idea if that changes things, but might be a simple fix. Your second idea worked @WaylonWalker. I slightly adapted it as it didn't work straight up:\r\n```\r\nconda create --yes --name kedroenv python=3.6 ipykernel\r\nsource activate kedroenv\r\npython -m ipykernel install --user --name kedroenv --display-name \"Kedro py3.6\"\r\n\r\ncd ~\/Sagemaker\r\nkedro new # Name testing and example pipeline\r\ncd testing\/\r\nkedro run\r\n```\r\nWith a reasonable solution, I'll call this issue closed. Massive thank you @WaylonWalker for pointing me in the right direction.\r\n\r\nCheers,\r\nTom @tjcuddihy We're working with the AWS team to produce a knowledge document on using Kedro and Sagemaker. Would we be able to talk to you about how you used them together? I'd be keen on learning more about how to make Sagemaker play nicely with kedro so I can still access everything I need from my kedro context. @yetudada I have an alpha version of a kedro plugin that plays nicely with sagemaker and allows you to run processing jobs. @uwaisiqbal then you might be interested in this knowledge article that was just published on AWS: https:\/\/aws.amazon.com\/blogs\/opensource\/using-kedro-pipelines-to-train-amazon-sagemaker-models\/ \ud83d\ude80 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":11.1,
        "Solution_reading_time":32.51,
        "Solution_score_count":null,
        "Solution_sentence_count":20,
        "Solution_word_count":397,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I use visual studio code with jupyter notebook, I have an &quot;outline&quot; tab in the left panels that display the Markdown section of my notebook for quick access.<\/p>\n<p>But in Sagemaker studio I don't have this and I would like to add it.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652353604940,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72214443",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":3.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Get notebook outline in sagemaker studio like in Visual Studio Code",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576136255052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>The 'Outline' tab (Table of Contents extension in Jupyter) is not available for Studio yet.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"nofollow noreferrer\">SageMaker notebook instances<\/a> come with the extension prebuilt.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.8,
        "Solution_reading_time":3.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":26,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1317052342823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Franklin, TN",
        "Answerer_reputation_count":8183.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":0.9585202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When trying to upload a custom R module to Azure Machine Learning Studio what causes the following error.<\/p>\n\n<blockquote>\n  <p>[ModuleOutput]<\/p>\n<\/blockquote>\n\n<pre><code>\"ErrorId\":\"BuildCustomModuleFailed\",\"ErrorCode\":\"0114\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 0114: Custom module build failed with error(s): An item with the same key has already been added.\"}} [ModuleOutput] Error: Error 0114: Custom module build failed with error(s): An item with the same key has already been added. \n<\/code><\/pre>\n\n<p>I have tried renaming the module so a name that does not exists.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496847821797,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44416344",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":9.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio Custom Module Upload Error 0114 : An item with the same key has already been added",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":75.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1317052342823,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Franklin, TN",
        "Poster_reputation_count":8183.0,
        "Poster_view_count":727.0,
        "Solution_body":"<p>The duplicate key exception is a red herring. <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn962112.aspx\" rel=\"nofollow noreferrer\" title=\"MSDN Module Error Code 0114\">Build error 0114<\/a> is a general error that occurs if there is a system exception while building the custom module. The real issue my module was compressed using the built in compress folder option in the Mac Finder. To fix this compress the file using the command line interface for <code>zip<\/code> in Terminal in the following very specific manner.<\/p>\n\n<blockquote>\n  <p>The following example:<\/p>\n<\/blockquote>\n\n<pre><code>cd ScoredDatasetMetadata\/\nzip ScoredDatasetMetadata *\nmv ScoredDatasetMetadata.zip ..\/\n<\/code><\/pre>\n\n<p>Builds a zip file with the correct file structure.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1496851272470,
        "Solution_link_count":1,
        "Solution_readability":11.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":96,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Created a new compute instance in Azure ML and trained a model with out any issue. I wanted to draw a pairplot using <code>seaborn<\/code> but I keep getting the error <code>&quot;ImportError: No module named seaborn&quot;<\/code><\/p>\n<p>I ran <code>!conda list<\/code> and I can see seaborn in the list<\/p>\n<pre><code># packages in environment at \/anaconda:\n#\n# Name                    Version                   Build  Channel\n_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0  \nalabaster                 0.7.12                   py37_0  \nanaconda                  2018.12                  py37_0  \nanaconda-client           1.7.2                    py37_0  \nanaconda-navigator        1.9.6                    py37_0  \nanaconda-project          0.8.2                    py37_0  \napplicationinsights       0.11.9                    &lt;pip&gt;\nasn1crypto                0.24.0                   py37_0  \nastroid                   2.1.0                    py37_0  \nastropy                   3.1              py37h7b6447c_0  \natomicwrites              1.2.1                    py37_0  \nattrs                     18.2.0           py37h28b3542_0  \nbabel                     2.6.0                    py37_0  \nbackcall                  0.1.0                    py37_0  \nbackports                 1.0                      py37_1  \nbackports.os              0.1.1                    py37_0  \nbackports.shutil_get_terminal_size 1.0.0                    py37_2  \nbeautifulsoup4            4.6.3                    py37_0  \nbitarray                  0.8.3            py37h14c3975_0  \nbkcharts                  0.2                      py37_0  \nblas                      1.0                         mkl  \nblaze                     0.11.3                   py37_0  \nbleach                    3.0.2                    py37_0  \nblosc                     1.14.4               hdbcaa40_0  \nbokeh                     1.0.2                    py37_0  \nboto                      2.49.0                   py37_0  \nbottleneck                1.2.1            py37h035aef0_1  \nbzip2                     1.0.6                h14c3975_5  \nca-certificates           2020.7.22                     0    anaconda\ncairo                     1.14.12              h8948797_3  \ncertifi                   2020.6.20                py37_0    anaconda\ncffi                      1.11.5           py37he75722e_1  \nchardet                   3.0.4                    py37_1  \nclick                     7.0                      py37_0  \ncloudpickle               0.6.1                    py37_0  \nclyent                    1.2.2                    py37_1  \ncolorama                  0.4.1                    py37_0  \nconda                     4.5.12                   py37_0  \nconda-build               3.17.6                   py37_0  \nconda-env                 2.6.0                         1  \nconda-verify              3.1.1                    py37_0  \ncontextlib2               0.5.5                    py37_0  \ncryptography              2.4.2            py37h1ba5d50_0  \ncurl                      7.63.0            hbc83047_1000  \ncycler                    0.10.0                   py37_0    anaconda\ncython                    0.29.2           py37he6710b0_0  \ncytoolz                   0.9.0.1          py37h14c3975_1  \ndask                      1.0.0                    py37_0  \ndask-core                 1.0.0                    py37_0  \ndatashape                 0.5.4                    py37_1  \ndbus                      1.13.12              h746ee38_0    anaconda\ndecorator                 4.3.0                    py37_0  \ndefusedxml                0.5.0                    py37_1  \ndistributed               1.25.1                   py37_0  \ndocutils                  0.14                     py37_0  \nentrypoints               0.2.3                    py37_2  \net_xmlfile                1.0.1                    py37_0  \nexpat                     2.2.9                he6710b0_2    anaconda\nfastcache                 1.0.2            py37h14c3975_2  \nfilelock                  3.0.10                   py37_0  \nflask                     1.0.2                    py37_1  \nflask-cors                3.0.7                    py37_0  \nfontconfig                2.13.0               h9420a91_0    anaconda\nfreetype                  2.10.2               h5ab3b9f_0    anaconda\nfribidi                   1.0.5                h7b6447c_0  \nfuture                    0.17.1                   py37_0  \nget_terminal_size         1.0.0                haa9412d_0  \ngevent                    1.3.7            py37h7b6447c_1  \nglib                      2.56.2               hd408876_0    anaconda\nglob2                     0.6                      py37_1  \ngmp                       6.1.2                h6c8ec71_1  \ngmpy2                     2.0.8            py37h10f8cd9_2  \ngraphite2                 1.3.12               h23475e2_2  \ngreenlet                  0.4.15           py37h7b6447c_0  \ngst-plugins-base          1.14.0               hbbd80ab_1    anaconda\ngstreamer                 1.14.0               hb453b48_1    anaconda\nh5py                      2.8.0            py37h989c5e5_3  \nharfbuzz                  1.8.8                hffaf4a1_0  \nhdf5                      1.10.2               hba1933b_1  \nheapdict                  1.0.0                    py37_2  \nhtml5lib                  1.0.1                    py37_0  \nicu                       58.2                 he6710b0_3    anaconda\nidna                      2.8                      py37_0  \nimageio                   2.4.1                    py37_0  \nimagesize                 1.1.0                    py37_0  \nimportlib_metadata        0.6                      py37_0  \nintel-openmp              2019.1                      144  \nipykernel                 5.1.0            py37h39e3cac_0  \nipython                   7.2.0            py37h39e3cac_0  \nipython_genutils          0.2.0                    py37_0  \nipywidgets                7.4.2                    py37_0  \nisort                     4.3.4                    py37_0  \nitsdangerous              1.1.0                    py37_0  \njbig                      2.1                  hdba287a_0  \njdcal                     1.4                      py37_0  \njedi                      0.13.2                   py37_0  \njeepney                   0.4                      py37_0  \njinja2                    2.10                     py37_0  \njpeg                      9b                   habf39ab_1    anaconda\njsonschema                2.6.0                    py37_0  \njupyter                   1.0.0                    py37_7  \njupyter_client            5.2.4                    py37_0  \njupyter_console           6.0.0                    py37_0  \njupyter_core              4.4.0                    py37_0  \njupyterlab                0.35.3                   py37_0  \njupyterlab_server         0.2.0                    py37_0  \nkeyring                   17.0.0                   py37_0  \nkiwisolver                1.2.0            py37hfd86e86_0    anaconda\nkrb5                      1.16.1               h173b8e3_7  \nlazy-object-proxy         1.3.1            py37h14c3975_2  \nlcms2                     2.11                 h396b838_0    anaconda\nld_impl_linux-64          2.33.1               h53a641e_7    anaconda\nlibarchive                3.3.3                h5d8350f_5  \nlibcurl                   7.63.0            h20c2e04_1000  \nlibedit                   3.1.20191231         h14c3975_1    anaconda\nlibffi                    3.3                  he6710b0_2    anaconda\nlibgcc-ng                 9.1.0                hdf63c60_0    anaconda\nlibgfortran-ng            7.3.0                hdf63c60_0  \nliblief                   0.9.0                h7725739_1  \nlibpng                    1.6.37               hbc83047_0    anaconda\nlibsodium                 1.0.16               h1bed415_0  \nlibssh2                   1.8.0                h1ba5d50_4  \nlibstdcxx-ng              8.2.0                hdf63c60_1  \nlibtiff                   4.1.0                h2733197_1    anaconda\nlibtool                   2.4.6                h7b6447c_5  \nlibuuid                   1.0.3                h1bed415_2    anaconda\nlibxcb                    1.14                 h7b6447c_0    anaconda\nlibxml2                   2.9.10               he19cac6_1    anaconda\nlibxslt                   1.1.32               h1312cb7_0  \nllvmlite                  0.26.0           py37hd408876_0  \nlocket                    0.2.0                    py37_1  \nlxml                      4.2.5            py37hefd8a0e_0  \nlz4-c                     1.9.2                he6710b0_1    anaconda\nlzo                       2.10                 h49e0be7_2  \nmarkupsafe                1.1.0            py37h7b6447c_0  \nmatplotlib                3.3.1                         0    anaconda\nmatplotlib-base           3.3.1            py37h817c723_0    anaconda\nmccabe                    0.6.1                    py37_1  \nmistune                   0.8.4            py37h7b6447c_0  \nmkl                       2019.1                      144  \nmkl-service               1.1.2            py37he904b0f_5  \nmkl_fft                   1.0.10           py37ha843d7b_0    anaconda\nmkl_random                1.0.2            py37hd81dba3_0    anaconda\nmore-itertools            4.3.0                    py37_0  \nmpc                       1.1.0                h10f8cd9_1  \nmpfr                      4.0.1                hdf1c602_3  \nmpmath                    1.1.0                    py37_0  \nmsgpack-python            0.5.6            py37h6bb024c_1  \nmultipledispatch          0.6.0                    py37_0  \nnavigator-updater         0.2.1                    py37_0  \nnbconvert                 5.4.0                    py37_1  \nnbformat                  4.4.0                    py37_0  \nncurses                   6.2                  he6710b0_1    anaconda\nnetworkx                  2.2                      py37_1  \nnltk                      3.4                      py37_1  \nnose                      1.3.7                    py37_2  \nnotebook                  5.7.4                    py37_0  \nnumba                     0.41.0           py37h962f231_0  \nnumexpr                   2.6.8            py37h9e4a6bb_0  \nnumpy                     1.16.2           py37h7e9f1db_0    anaconda\nnumpy-base                1.16.2           py37hde5b4d6_0    anaconda\nnumpydoc                  0.8.0                    py37_0  \nodo                       0.5.1                    py37_0  \nolefile                   0.46                       py_0    anaconda\nopenpyxl                  2.5.12                   py37_0  \nopenssl                   1.1.1g               h7b6447c_0    anaconda\npackaging                 18.0                     py37_0  \npandas                    1.1.1            py37he6710b0_0    anaconda\npandoc                    1.19.2.1             hea2e7c5_1  \npandocfilters             1.4.2                    py37_1  \npango                     1.42.4               h049681c_0  \nparso                     0.3.1                    py37_0  \npartd                     0.3.9                    py37_0  \npatchelf                  0.9                  he6710b0_3  \npath.py                   11.5.0                   py37_0  \npathlib2                  2.3.3                    py37_0  \npatsy                     0.5.1                    py37_0  \npcre                      8.44                 he6710b0_0    anaconda\npep8                      1.7.1                    py37_0  \npexpect                   4.6.0                    py37_0  \npickleshare               0.7.5                    py37_0  \npillow                    7.2.0            py37hb39fc2d_0    anaconda\npip                       20.2.2                   py37_0    anaconda\npixman                    0.34.0               hceecf20_3  \npkginfo                   1.4.2                    py37_1  \npluggy                    0.8.0                    py37_0  \nply                       3.11                     py37_0  \nprometheus_client         0.5.0                    py37_0  \nprompt_toolkit            2.0.7                    py37_0  \npsutil                    5.4.8            py37h7b6447c_0  \nptyprocess                0.6.0                    py37_0  \npy                        1.7.0                    py37_0  \npy-lief                   0.9.0            py37h7725739_1  \npycodestyle               2.4.0                    py37_0  \npycosat                   0.6.3            py37h14c3975_0  \npycparser                 2.19                     py37_0  \npycrypto                  2.6.1            py37h14c3975_9  \npycurl                    7.43.0.2         py37h1ba5d50_0  \npyflakes                  2.0.0                    py37_0  \npygments                  2.3.1                    py37_0  \npylint                    2.2.2                    py37_0  \npyodbc                    4.0.25           py37he6710b0_0  \npyopenssl                 18.0.0                   py37_0  \npyparsing                 2.4.7                      py_0    anaconda\npyqt                      5.9.2            py37h22d08a2_1    anaconda\npysocks                   1.6.8                    py37_0  \npytables                  3.4.4            py37ha205bf6_0  \npytest                    4.0.2                    py37_0  \npytest-arraydiff          0.3              py37h39e3cac_0  \npytest-astropy            0.5.0                    py37_0  \npytest-doctestplus        0.2.0                    py37_0  \npytest-openfiles          0.3.1                    py37_0  \npytest-remotedata         0.3.1                    py37_0  \npython                    3.7.9                h7579374_0    anaconda\npython-dateutil           2.8.1                      py_0    anaconda\npython-libarchive-c       2.8                      py37_6  \npytz                      2020.1                     py_0    anaconda\npywavelets                1.0.1            py37hdd07704_0  \npyyaml                    3.13             py37h14c3975_0  \npyzmq                     17.1.2           py37h14c3975_0  \nqt                        5.9.7                h5867ecd_1    anaconda\nqtawesome                 0.5.3                    py37_0  \nqtconsole                 4.4.3                    py37_0  \nqtpy                      1.5.2                    py37_0  \nreadline                  8.0                  h7b6447c_0    anaconda\nrequests                  2.21.0                   py37_0  \nrope                      0.11.0                   py37_0  \nruamel_yaml               0.15.46          py37h14c3975_0  \nscikit-image              0.14.1           py37he6710b0_0  \nscikit-learn              0.20.1           py37hd81dba3_0  \nscipy                     1.2.1            py37h7c811a0_0    anaconda\nseaborn                   0.10.1                     py_0    anaconda\nsecretstorage             3.1.0                    py37_0  \nsend2trash                1.5.0                    py37_0  \nsetuptools                49.6.0                   py37_0    anaconda\nsimplegeneric             0.8.1                    py37_2  \nsingledispatch            3.4.0.3                  py37_0  \nsip                       4.19.24          py37he6710b0_0    anaconda\nsix                       1.15.0                     py_0    anaconda\nsnappy                    1.1.7                hbae5bb6_3  \nsnowballstemmer           1.2.1                    py37_0  \nsortedcollections         1.0.1                    py37_0  \nsortedcontainers          2.1.0                    py37_0  \nsphinx                    1.8.2                    py37_0  \nsphinxcontrib             1.0                      py37_1  \nsphinxcontrib-websupport  1.1.0                    py37_1  \nspyder                    3.3.2                    py37_0  \nspyder-kernels            0.3.0                    py37_0  \nsqlalchemy                1.2.15           py37h7b6447c_0  \nsqlite                    3.33.0               h62c20be_0    anaconda\nstatsmodels               0.9.0            py37h035aef0_0  \nsympy                     1.3                      py37_0  \ntblib                     1.3.2                    py37_0  \nterminado                 0.8.1                    py37_1  \ntestpath                  0.4.2                    py37_0  \ntk                        8.6.10               hbc83047_0    anaconda\ntoolz                     0.9.0                    py37_0  \ntornado                   6.0.4            py37h7b6447c_1    anaconda\ntqdm                      4.28.1           py37h28b3542_0  \ntraitlets                 4.3.2                    py37_0  \nunicodecsv                0.14.1                   py37_0  \nunixodbc                  2.3.7                h14c3975_0  \nurllib3                   1.24.1                   py37_0  \nwcwidth                   0.1.7                    py37_0  \nwebencodings              0.5.1                    py37_1  \nwerkzeug                  0.14.1                   py37_0  \nwheel                     0.35.1                     py_0    anaconda\nwidgetsnbextension        3.4.2                    py37_0  \nwrapt                     1.10.11          py37h14c3975_2  \nwurlitzer                 1.0.2                    py37_0  \nxlrd                      1.2.0                    py37_0  \nxlsxwriter                1.1.2                    py37_0  \nxlwt                      1.3.0                    py37_0  \nxz                        5.2.5                h7b6447c_0    anaconda\nyaml                      0.1.7                had09818_2  \nzeromq                    4.2.5                hf484d3e_1  \nzict                      0.1.3                    py37_0  \nzlib                      1.2.11               h7b6447c_3    anaconda\nzstd                      1.4.4                h0b5b093_3    anaconda\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599439961223,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1603385843288,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63770171",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.4,
        "Challenge_reading_time":102.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":282,
        "Challenge_solved_time":null,
        "Challenge_title":"\"ImportError: No module named seaborn\" in Azure ML",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1167.0,
        "Challenge_word_count":958,
        "Platform":"Stack Overflow",
        "Poster_created_time":1245726715288,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cumming, GA",
        "Poster_reputation_count":77230.0,
        "Poster_view_count":6359.0,
        "Solution_body":"<p>I just did the following and wasn't able to reproduce your error:<\/p>\n<ol>\n<li>make a new compute instance<\/li>\n<li>open it up using JupyterLab<\/li>\n<li>open a new terminal<\/li>\n<li><code>conda activate azureml_py36<\/code><\/li>\n<li><code>conda install seaborn -y<\/code><\/li>\n<li>open a new notebook and run <code>import seaborn as sns<\/code><\/li>\n<\/ol>\n<h3>Spitballing<\/h3>\n<ol>\n<li>Are you using the kernel, <code>Python 3.6 - AzureML<\/code> (i.e. the <code>azureml_py36<\/code> conda env)?<\/li>\n<li>Have you tried restarting the kernel and\/or creating a new compute instance?<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kPbLH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kPbLH.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":10.3,
        "Solution_reading_time":9.92,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7,
        "Solution_word_count":83,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\nA SageMaker Notebook-v3 workspace that was working fine on Friday today appears with the status as \"Unknown\". \r\nWhen clicking on connect the new window pop up but is empty, and when going back to the SWB page, we see the message, \"We have a problem! Something went wrong\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to 'Workspaces'\r\n2. Look for the workspace that was expected to be \"Stoped\"\r\n2. Click on 'connect'\r\n4. See error\r\n\r\n**Expected behavior**\r\nThat the workspace was \"Stopped\" and when clicking on Connect we can access to the workspace. \r\n\r\n**Screenshots**\r\n![Screen Shot 2021-09-13 at 1 27 57 PM](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**Versions (please complete the following information):**\r\nRelease Version installed: 3.3.1\r\n\r\n**Additional context**\r\nThe workspace was working fine all previous week, autostop and connect without any issue. Unknown status found today.",
        "Challenge_closed_time":1633460,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631554276000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/708",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] SageMaker Notebook-v3 Workspace changed to \"Unknown\" status and cannot connect anymore",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":148,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think there's a good chance this instance was autostopped, but that information was not propagated to DDB correctly.\r\n\r\nCan you log onto the hosting account for that Sagemaker instance and check if it's currently in the `Stopped` state. If yes, the latest code fixes that issue.\r\nhttps:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/8cb199b8093f5e799d2d87c228930a4929ebebb7 Hi @nguyen102 yes, I can confirm that the Sagemaker instance is  in the Stopped sate. So then, the latest code that you mention should fix the issue. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":8.0,
        "Solution_reading_time":6.64,
        "Solution_score_count":null,
        "Solution_sentence_count":5,
        "Solution_word_count":75,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello. I created a Notebook Job Definition, scheduled to run every hour. According to the status it is \"Active\". Whenever I manually trigger the job with the \"Run Job\" button, it creates a new Notebook Job and runs successfully. However, the notebook never runs on the schedule. It should execute every hour, but instead only executes when manually triggered.\n\nIs there anything I need to do for the notebook to obey the schedule? Thanks,",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675370723349,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1675718147630,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUjxD8Cf8lTc2Tknbt4MAzFQ\/sagemaker-notebook-doesn-t-run-on-job-schedule",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker notebook doesn't run on Job schedule",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":90.0,
        "Challenge_word_count":80,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Are there any problems with IAM role settings, etc.?\n\nPlease check this document for reference only.\nhttps:\/\/aws.amazon.com\/jp\/blogs\/machine-learning\/operationalize-your-amazon-sagemaker-studio-notebooks-as-scheduled-notebook-jobs\/",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1675405386807,
        "Solution_link_count":1,
        "Solution_readability":21.9,
        "Solution_reading_time":3.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":17,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I made a classifier in Python that uses a lot of libraries. I have uploaded the model to Amazon S3 as a pickle (my_model.pkl). Ideally, every time someone uploads a file to a specific S3 bucket, it should trigger an AWS Lambda that would load the classifier, return predictions and save a few files on an Amazon S3 bucket.<\/p>\n<p>I want to know if it is possible to use a Lambda to execute a Jupyter Notebook in AWS SageMaker. This way I would not have to worry about the dependencies and would generally make the classification more straight forward.<\/p>\n<p>So, is there a way to use an AWS Lambda to execute a Jupyter Notebook?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602969610527,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64407452",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.0,
        "Challenge_reading_time":8.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Use AWS Lambda to execute a jupyter notebook on AWS Sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2656.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526405779360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mexico City, CDMX, Mexico",
        "Poster_reputation_count":4713.0,
        "Poster_view_count":507.0,
        "Solution_body":"<p>Scheduling notebook execution is a bit of a SageMaker anti-pattern, because (1) you would need to manage data I\/O (training set, trained model) yourself, (2) you would need to manage metadata tracking yourself, (3) you cannot run on distributed hardware and (4) you cannot use Spot. Instead, it is recommended for scheduled task to leverage the various SageMaker long-running, background job APIs: SageMaker Training, SageMaker Processing or SageMaker Batch Transform (in the case of a batch inference).<\/p>\n<p>That being said, if you still want to schedule a notebook to run, you can do it in a variety of ways:<\/p>\n<ul>\n<li>in the <a href=\"https:\/\/www.youtube.com\/watch?v=6EYEoAqihPg\" rel=\"nofollow noreferrer\">SageMaker CICD Reinvent 2018 Video<\/a>, Notebooks are launched as Cloudformation templates, and their execution is automated via a SageMaker lifecycle configuration.<\/li>\n<li>AWS released <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/scheduling-jupyter-notebooks-on-sagemaker-ephemeral-instances\/\" rel=\"nofollow noreferrer\">this blog post<\/a> to document how to launch Notebooks from within Processing jobs<\/li>\n<\/ul>\n<p>But again, my recommendation for scheduled tasks would be to remove them from Jupyter, turn them into scripts and run them in SageMaker Training<\/p>\n<p>No matter your choices, all those tasks can be launched as API calls from within a Lambda function, as long as the function role has appropriate permissions<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":17.2,
        "Solution_reading_time":18.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":196,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Challenge_closed_time":1650643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647959057000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.23,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":null,
        "Challenge_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":227,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"It appears that the bug has been fixed by https:\/\/github.com\/triton-inference-server\/server\/pull\/3828 and I am not able to reproduce it using the model example for the plugin. Can you try the plugin from the latest codebase?\r\n```\r\npython `pwd`\/mlflow-triton-plugin\/scripts\/publish_model_to_mlflow.py \\\r\n    --model_name onnx_float32_int32_int32 \\\r\n    --model_directory `pwd`\/mlflow-triton-plugin\/examples\/onnx_float32_int32_int32\/ \\\r\n    --flavor triton\r\n```\r\nreturns:\r\n```\r\nRegistered model 'onnx_float32_int32_int32' already exists. Creating a new version of this model...\r\n2022\/04\/07 23:03:53 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: onnx_float32_int32_int32, version 3\r\nCreated version '3' of model 'onnx_float32_int32_int32'.\r\n.\/mlruns\/0\/945d5c5d6806470d889248cfc7f10b69\/artifacts\r\n``` Closing due to in-activity.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":12.9,
        "Solution_reading_time":11.49,
        "Solution_score_count":null,
        "Solution_sentence_count":9,
        "Solution_word_count":86,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1356359759000,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dubai - United Arab Emirates",
        "Answerer_reputation_count":436.0,
        "Answerer_view_count":73.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The code below works fine in a sagemaker notebook in the cloud. Locally I also have aws credentials created via the aws cli. Personally, I do not like notebooks (unless I do some EDA or the like). So I wonder if one can also fire this code up from a local machine (e.g. in visual studio code) as it only tells sagemaker what to do anyway? I guess it is only a question of authenticating and getting the session object? Thanks!<\/p>\n<pre><code>import boto3\nimport os\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker import image_uris\n\nregion_name = boto3.session.Session().region_name\ns3_bucket_name = 'bucket_name'\n# this image cannot be used below !!! there must be an issue with sagemaker ?\ntraining_image_name = image_uris.retrieve(framework='xgboost', region=region_name, version='latest')\nrole = get_execution_role()\ns3_prefix = 'my_model'\ntrain_file_name = 'sagemaker_train.csv'\nval_file_name = 'sagemaker_val.csv'\nsagemaker_session = sagemaker.Session()\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;10&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649999496223,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651138032863,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71880336",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":28.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"can one run sagemaker notebook code locally in visual studio code",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":425.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>On a local machine,<\/p>\n<ul>\n<li>Make sure to install AWS CLI: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html<\/a><\/li>\n<li>Create an access key id and secret access key to access Sagemaker services locally: <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html<\/a><\/li>\n<li>Set these credentials using <code>aws configure<\/code> command locally.<\/li>\n<li>The code should work fine except getting the execution role. You can either hard code the Sagemaker role in the code (not best practice) or store it in the Parameter store and access it from there.<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":15.9,
        "Solution_reading_time":11.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":78,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\n    Warning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n    \u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\n    Making 'packages.html' ... done\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\n    yum install r-cran-rjava\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527798496000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668616977192,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sagemaker-that-receive-a-non-zero-exit-status",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":744.0,
        "Challenge_word_count":189,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n`$ yum install r-cran-rjava`\n\nyou can try:\n\n`$ sudo yum install r-cran-rjava`\n\nwhich will allow you to impersonate the superuser (ie. root) for that command [1]\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results [2])\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n`$ sudo yum install -y R-java-devel.x86_64`\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n`$ sudo yum install -y libxml2-devel` [3]\n\nAfter which you can then open R (either as root user...)\n\n`$ sudo R`\n\nor personal\/local user\n\n`$ R`\n\nand execute the package installation:\n\n`> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")`\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n`$ sudo find \/ -iname libgomp.spec`\n\nwhich should be located at **\/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec** -- if so, you can manually create symlinks to fix this:\n\n```\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n```\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n`$ ls \/usr\/lib64\/libgomp*`\n\nOnce confirmed, you can run the install.package('rJava') command.\n\n[1]:https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/install-software.html\n[2]:https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/find-software.html\n[3]:https:\/\/stackoverflow.com\/questions\/29014062\/unable-to-install-xml-package-in-r-on-centos#29014363",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925576348,
        "Solution_link_count":4,
        "Solution_readability":14.1,
        "Solution_reading_time":23.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12,
        "Solution_word_count":223,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Greetings! I'm a data scientist working in SageMaker notebooks. I'd appreciate an explanation about when should I use Glue Interactive and not SageMaker Processing jobs. \nTo my understanding, they are very similar and I can't differentiate them.\n\nThank you!",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663171688837,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668623995118,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU7J5WaZe3Qzi2giJaOmBFDQ\/glue-interactive-vs-sagemaker-processing",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.0,
        "Challenge_reading_time":3.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Glue Interactive vs SageMaker Processing?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":638.0,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I would suggest that you use Sagemaker processing for the data cleansing and preparation. I have led projects where all the data cleansing, preparation and model build and testing have been done in Sagemaker and the data scientists love the tool.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1663248095278,
        "Solution_link_count":0,
        "Solution_readability":10.1,
        "Solution_reading_time":3.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":41,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1401187183867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sweden",
        "Answerer_reputation_count":1709.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm setting up a conda environment on Windows 10 Pro x64 using Miniconda 4.5.12 and have done a pip install of azureml-sdk inside the environment but get a ModuleNotFoundError when attempting to execute the following code:<\/p>\n\n<pre><code>import azureml.core\nazureml.core.VERSION\n<\/code><\/pre>\n\n<p>This is the output:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\n  File \"D:\\Projects\\style-transfer\\azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\nModuleNotFoundError: No module named 'azureml.core'; 'azureml' is not a package\n<\/code><\/pre>\n\n<p>The code above has been run from the conda prompt, with the test environment active as well as in vscode with the same environment selected.<\/p>\n\n<p>I setup the conda environment as per the following:<\/p>\n\n<ol>\n<li>Created the conda environment <code>conda create -n test<\/code>.<\/li>\n<li>Activated the environment <code>activate test<\/code>.<\/li>\n<li>Installed pip <code>conda install pip<\/code>.<\/li>\n<li>Installed azureml-sdk <code>pip install azureml-sdk<\/code>.<\/li>\n<\/ol>\n\n<p>This results in the following packages being installed in the environment as per <code>conda list<\/code>:<\/p>\n\n<pre><code>adal                      1.2.0                     &lt;pip&gt;\nantlr4-python3-runtime    4.7.2                     &lt;pip&gt;\napplicationinsights       0.11.7                    &lt;pip&gt;\nargcomplete               1.9.4                     &lt;pip&gt;\nasn1crypto                0.24.0                    &lt;pip&gt;\nazure-cli-command-modules-nspkg 2.0.2                     &lt;pip&gt;\nazure-cli-core            2.0.54                    &lt;pip&gt;\nazure-cli-nspkg           3.0.3                     &lt;pip&gt;\nazure-cli-profile         2.1.2                     &lt;pip&gt;\nazure-cli-telemetry       1.0.0                     &lt;pip&gt;\nazure-common              1.1.16                    &lt;pip&gt;\nazure-graphrbac           0.53.0                    &lt;pip&gt;\nazure-mgmt-authorization  0.51.1                    &lt;pip&gt;\nazure-mgmt-containerregistry 2.5.0                     &lt;pip&gt;\nazure-mgmt-keyvault       1.1.0                     &lt;pip&gt;\nazure-mgmt-nspkg          3.0.2                     &lt;pip&gt;\nazure-mgmt-resource       2.0.0                     &lt;pip&gt;\nazure-mgmt-storage        3.1.0                     &lt;pip&gt;\nazure-nspkg               3.0.2                     &lt;pip&gt;\nazure-storage-blob        1.4.0                     &lt;pip&gt;\nazure-storage-common      1.4.0                     &lt;pip&gt;\nazure-storage-nspkg       3.1.0                     &lt;pip&gt;\nazureml-core              1.0.6                     &lt;pip&gt;\nazureml-pipeline          1.0.6                     &lt;pip&gt;\nazureml-pipeline-core     1.0.6                     &lt;pip&gt;\nazureml-pipeline-steps    1.0.6                     &lt;pip&gt;\nazureml-sdk               1.0.6                     &lt;pip&gt;\nazureml-telemetry         1.0.6                     &lt;pip&gt;\nazureml-train             1.0.6                     &lt;pip&gt;\nazureml-train-core        1.0.6                     &lt;pip&gt;\nazureml-train-restclients-hyperdrive 1.0.6                     &lt;pip&gt;\nbackports.tempfile        1.0                       &lt;pip&gt;\nbackports.weakref         1.0.post1                 &lt;pip&gt;\nbcrypt                    3.1.5                     &lt;pip&gt;\nca-certificates           2018.03.07                    0\ncertifi                   2018.11.29               py37_0\ncffi                      1.11.5                    &lt;pip&gt;\nchardet                   3.0.4                     &lt;pip&gt;\ncolorama                  0.4.1                     &lt;pip&gt;\ncontextlib2               0.5.5                     &lt;pip&gt;\ncryptography              2.4.2                     &lt;pip&gt;\ndocker                    3.6.0                     &lt;pip&gt;\ndocker-pycreds            0.4.0                     &lt;pip&gt;\nfutures                   3.1.1                     &lt;pip&gt;\nhumanfriendly             4.17                      &lt;pip&gt;\nidna                      2.8                       &lt;pip&gt;\nisodate                   0.6.0                     &lt;pip&gt;\njmespath                  0.9.3                     &lt;pip&gt;\njsonpickle                1.0                       &lt;pip&gt;\nknack                     0.5.1                     &lt;pip&gt;\nmsrest                    0.6.2                     &lt;pip&gt;\nmsrestazure               0.6.0                     &lt;pip&gt;\nndg-httpsclient           0.5.1                     &lt;pip&gt;\noauthlib                  2.1.0                     &lt;pip&gt;\nopenssl                   1.1.1a               he774522_0\nparamiko                  2.4.2                     &lt;pip&gt;\npathspec                  0.5.9                     &lt;pip&gt;\npip                       18.1                     py37_0\nportalocker               1.2.1                     &lt;pip&gt;\npyasn1                    0.4.4                     &lt;pip&gt;\npycparser                 2.19                      &lt;pip&gt;\nPygments                  2.3.1                     &lt;pip&gt;\nPyJWT                     1.7.1                     &lt;pip&gt;\nPyNaCl                    1.3.0                     &lt;pip&gt;\npyOpenSSL                 18.0.0                    &lt;pip&gt;\npypiwin32                 223                       &lt;pip&gt;\npyreadline                2.1                       &lt;pip&gt;\npython                    3.7.1                h8c8aaf0_6\npython-dateutil           2.7.5                     &lt;pip&gt;\npytz                      2018.7                    &lt;pip&gt;\npywin32                   224                       &lt;pip&gt;\nPyYAML                    3.13                      &lt;pip&gt;\nrequests                  2.21.0                    &lt;pip&gt;\nrequests-oauthlib         1.0.0                     &lt;pip&gt;\nruamel.yaml               0.15.51                   &lt;pip&gt;\nSecretStorage             2.3.1                     &lt;pip&gt;\nsetuptools                40.6.3                   py37_0\nsix                       1.12.0                    &lt;pip&gt;\nsqlite                    3.26.0               he774522_0\ntabulate                  0.8.2                     &lt;pip&gt;\nurllib3                   1.23                      &lt;pip&gt;\nvc                        14.1                 h0510ff6_4\nvs2015_runtime            14.15.26706          h3a45250_0\nwebsocket-client          0.54.0                    &lt;pip&gt;\nwheel                     0.32.3                   py37_0\nwheel                     0.30.0                    &lt;pip&gt;\nwincertstore              0.2                      py37_0\n<\/code><\/pre>\n\n<p>If I run <code>which pip<\/code>, I get the following output, which confirms that I used the pip inside the environment to install azureml-sdk, I think:<\/p>\n\n<pre><code>\/c\/Users\/allan\/Miniconda3\/envs\/test\/Scripts\/pip\n<\/code><\/pre>\n\n<p>I can also see that the azureml packages do in fact exist within the environment folder structure.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1545617082117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53908529",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.5,
        "Challenge_reading_time":56.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":99,
        "Challenge_solved_time":null,
        "Challenge_title":"How to fix ModuleNotFoundError in azureml-sdk when installed inside conda environment",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":6343.0,
        "Challenge_word_count":466,
        "Platform":"Stack Overflow",
        "Poster_created_time":1460456204196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Australia",
        "Poster_reputation_count":140.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>It's probably because the name if your python file is the same as a module name you are trying import. In this case, rename the file to something other than <code>azureml.py<\/code>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.8,
        "Solution_reading_time":2.32,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2,
        "Solution_word_count":31,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":326.3584541667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried to follow the normal (non-studio) documentation on mounting an EFS file system, as can be found <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/mount-an-efs-file-system-to-an-amazon-sagemaker-notebook-with-lifecycle-configurations\/\" rel=\"nofollow noreferrer\">here<\/a>, however, these steps don't work in a studio notebook. Specifically, the <code>sudo mount -t nfs ...<\/code> does not work in both the Image terminal and the system terminal.<\/p>\n<p>How do I mount an EFS file system that already exists to amazon Sagemaker, so I can access the data\/ datasets I stored in them?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596816839977,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63305569",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":8.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to mount an EFS volume on AWS Sagemaker Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2331.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>Update: I spoke to an AWS Solutions Architect, and he confirms that EFS is not supported on Sagemaker Studio.<\/p>\n<hr \/>\n<p><strong>Workaround:<\/strong><\/p>\n<p>Instead of mounting your old EFS, you can mount the SageMaker studio EFS onto an EC2 instance, and copy over the data manually. You would need the correct EFS storage volume id, and you'll find your newly copied data available in Sagemaker Studio. <em>I have not actually done this though.<\/em><\/p>\n<p>To find the EFS id, look at the section &quot;Manage your storage volume&quot; <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks.html#manage-your-storage-volume\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1597991730412,
        "Solution_link_count":1,
        "Solution_readability":10.2,
        "Solution_reading_time":8.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":88,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI want to use `awswrangler` package in my Jupyter Notebook instance of SageMaker.\n\nI understand that we have to use **Lifecycle configuration**. I tried to do it using the following script:\n\n    #!\/bin\/bash\n    \n    pip install awswrangler==0.2.2\n\nBut when I import that package into my Notebook:\n\n    import boto3                                      # For executing native S3 APIs\n    import pandas as pd                               # For munging tabulara data\n    import numpy as np                                # For doing some calculation\n    import awswrangler as wr\n    import io\n    from io import StringIO\n\nI still get the following error:\n\n    ---------------------------------------------------------------------------\n    ModuleNotFoundError                       Traceback (most recent call last)\n    <ipython-input-1-f3d85c7dd0f6> in <module>()\n          2 import pandas as pd                               # For munging tabulara data\n          3 import numpy as np                                # For doing some calculation\n    ----> 4 import awswrangler as wr\n          5 import io\n          6 from io import StringIO\n    \n    ModuleNotFoundError: No module named 'awswrangler'\n\nAny documentation or reference on how to install certain package for Jupyter Notebook in SageMaker?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592823369000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668557190488,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU4pvReJNZS6eDLxhd4pK-tQ\/how-to-install-phyton-package-in-jupyter-notebook-instance-in-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to install Phyton package in Jupyter Notebook instance in SageMaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1292.0,
        "Challenge_word_count":154,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\n example how to use lifecycle config to install python package in one environment : https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-single-environment\/on-start.sh\n\nand to all conda env - https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-all-environments\/on-start.sh",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925572268,
        "Solution_link_count":2,
        "Solution_readability":40.4,
        "Solution_reading_time":6.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":21,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"\r\n<img width=\"1430\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/5203025\/123860354-63399680-d958-11eb-9dc8-dc0a52d67cc2.png\">\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 109d9284-e234-5086-5da6-4155291361c8\r\n* Version Independent ID: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d\r\n* Content: [azureml.core.ScriptRunConfig class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.scriptrunconfig?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1626388,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624997173000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1534",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":25.6,
        "Challenge_reading_time":13.49,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Broken link in AML doc to azureml.core.runconfig.MpiConfiguration",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for submitting the issue. I am fixing this broken link now.  The links should be fixed on next SDK release on Aug 3rd",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":2.9,
        "Solution_reading_time":1.47,
        "Solution_score_count":null,
        "Solution_sentence_count":3,
        "Solution_word_count":24,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1636044845360,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":241.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":2833.1668325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook.<\/p>\n<p>After having prepared the correct Dockerfile and the URI where to push the image that contains my train.py script, I try to push the image directly in a notebook cell.<\/p>\n<p>The exact command I try to execute is: <code>!docker build .\/ -t $IMAGE_URI<\/code>, where IMAGE_URI is the environmental variable previously defined. However I try to run this command I get the error: <code>\/bin\/bash: docker: command not found<\/code>. I also tried to execute it with the magic cell %%bash, importing the subprocess library and also execute the command stored in a .sh file.<\/p>\n<p>Unfortunately none of the above solutions work, they all return the same <strong>command not found<\/strong> error with code 127.<\/p>\n<p>If instead I run the command from a bash present in the Jupyterlab it works fine as expected.<\/p>\n<p>Is there any workaround to make the push execute inside the jupyter notebook? I was trying to keep the whole custom training process inside the same notebook.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651142988007,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1651151719750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72042363",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1648140384823,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>If you follow this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/create-user-managed-notebooks-instance-console-quickstart\" rel=\"nofollow noreferrer\">guide<\/a> to create a user-managed notebook from Vertex AI workbench and select Python 3, then it comes with Docker available.<\/p>\n<p>So you will be able to use Docker commands such as <code>! docker build .<\/code> inside the user-managed notebook.<\/p>\n<p>Example:\n<a href=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" alt=\"Vertex AI Managed Notebook\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661351120347,
        "Solution_link_count":3,
        "Solution_readability":12.3,
        "Solution_reading_time":8.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7,
        "Solution_word_count":57,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1623222646127,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have been struggling to run this code snippet in my Jupyter notebook in Sagemaker studio when I can run it locally. I previously imported Sagemaker. I can import Sagemaker smoothly and I pip installed it previously.<\/p>\n<pre><code>from sagemaker.workflow.lambda_step import LambdaStep\n<\/code><\/pre>\n<p>Here's the error thrown when I attempted to run it:<\/p>\n<pre><code>ModuleNotFoundError: No module named 'sagemaker.workflow.lambda_step'\n<\/code><\/pre>\n<p>I also tried different ways of importing such as 'from sagemaker.workflow import lambda_step' or 'sagemaker.workflow.lambda_step.LambdaStep' straight away when calling the function. Those ways also did not help me.<\/p>\n<p>I'm also not sure why I can't import LambdaStep when I can run the code snippet below smoothly in Sagemaker studio:<\/p>\n<pre><code>from sagemaker.workflow.pipeline import Pipeline\n<\/code><\/pre>\n<p>I am not sure how to solve this problem and would greatly appreciate if someone can help me out!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1628752144690,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68753045",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":13.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Why is it that I can import LambdaStep when I run my notebook locally but not when I run it in Sagemaker studio?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":153,
        "Platform":"Stack Overflow",
        "Poster_created_time":1623222646127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Strangely, my issue is solved after I installed sagemaker again but by using this code snippet<\/p>\n<pre><code>!pip install -U sagemaker\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.6,
        "Solution_reading_time":1.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":21,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.    <\/p>\n<p>Features in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python<\/a> are neat, but that of having API docs\/swagger for the webservice seems missing.    <\/p>\n<p>Having some documentation is useful especially if the model takes in input several features of different type.    <\/p>\n<p>To overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.     <\/p>\n<p>Can I do something equivalent to this with AKS in AzureML instead? If so, how?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600897231890,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":12.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=9ced4628-b03a-4169-99b4-e42b0955c045\">@Davide Fiocco  <\/a> The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.akswebservice?view=azure-ml-py\">here<\/a>. You can print your <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service\">swagger_uri<\/a> of the web service and check if it confirms with the specifications you are creating currently.     <\/p>\n<p>If the above response helps, please accept the response as answer. Thanks!!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.9,
        "Solution_reading_time":8.84,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5,
        "Solution_word_count":63,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Challenge_closed_time":1652640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652380533000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":185,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Closed by #313 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.18,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":3,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1587281590603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":473.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<h2>The problem<\/h2>\n<p>I have a jupyter notebook with an <code>ipydatagrid<\/code> widget that displays a dataframe. This notebook works correctly when run locally, but not when run in AWS SageMaker Studio. When run in SageMaker Studio, instead of showing the widget it simply shows the text <code>Loading widget...<\/code><\/p>\n<p>How does one use a <code>ipydatagrid<\/code> widget in the SageMaker Studio environment?<\/p>\n<h2>Details<\/h2>\n<p>Python version:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ python --version\nPython 3.7.10\n<\/code><\/pre>\n<p>Run at start:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip install -r requirements.txt\n$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n$ jupyter nbextension install --py --symlink --sys-prefix ipydatagrid\n$ jupyter nbextension enable --py --sys-prefix ipydatagrid\n<\/code><\/pre>\n<p>File <code>requirements.txt<\/code>:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>ipydatagrid==1.1.11\npandas==1.0.1\n<\/code><\/pre>\n<p>Notebook contents:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># %%\nimport pandas as pd\nfrom ipydatagrid import DataGrid\nfrom IPython.display import display\nimport ipywidgets\n\n# %%\ndata = [\n    (&quot;potato&quot;, 1.2, True),\n    (&quot;sweet potato&quot;, 0.8, False),\n    (&quot;french fries&quot;, 4.5, True),\n    (&quot;waffle fries&quot;, 4.9, True)\n]\ndf = pd.DataFrame(\n    data,\n    columns=[&quot;food&quot;, &quot;stars&quot;, &quot;is_available&quot;]\n)\n\n# %%\ngrid = DataGrid(df)\n\n# %%\ndisplay(grid)\n\n# %%\n# SANITY CHECK:\nbutton = ipywidgets.Button(\n    description=&quot;Button&quot;,\n    disabled=False\n)\ndef on_click(b):\n    print(&quot;CLICK&quot;)\nbutton.on_click(on_click)\ndisplay(button)\n<\/code><\/pre>\n<h3>Error messages<\/h3>\n<p>If I use the Google Chrome developer tools, I can see more logs in the browser that give some error messages, most of which are repeated:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>manager.js:305 Uncaught (in promise) Error: Module ipydatagrid, semver range ^1.1.11 is not registered as a widget module\n    at C.loadClass (manager.js:305:19)\n    at C.&lt;anonymous&gt; (manager-base.js:263:46)\n    at l (manager-base.js:44:23)\n    at Object.next (manager-base.js:25:53)\n    at manager-base.js:19:71\n    at new Promise (&lt;anonymous&gt;)\n    at Rtm6.k (manager-base.js:15:12)\n    at C.e._make_model (manager-base.js:257:16)\n    at C.&lt;anonymous&gt; (manager-base.js:246:45)\n    at l (manager-base.js:44:23)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>utils.js:119 Error: Could not create a model.\n    at n (utils.js:119:27)\n    at async C._handleCommOpen (manager.js:61:51)\n    at async v._handleCommOpen (default.js:994:100)\n    at async v._handleMessage (default.js:1100:43)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>manager-base.js:273 Could not instantiate widget\n<\/code><\/pre>\n<p>However, there is no overt error message that's immediately obvious to the user, including in the log where <code>print<\/code> statements send their output.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648246951157,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71623732",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":39.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":null,
        "Challenge_title":"ipydatagrid widget does not display in SageMaker Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":304,
        "Platform":"Stack Overflow",
        "Poster_created_time":1560606498248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Provo, UT, USA",
        "Poster_reputation_count":528.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab<\/em>), and per the <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid#Installation\" rel=\"nofollow noreferrer\">ipydatagrid installation instructions<\/a>, current\/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.<\/p>\n<p>I had a quick look at the past releases of <code>ipydatagrid<\/code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/0.2.16\" rel=\"nofollow noreferrer\">v0.2.16<\/a> and <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/1.0.1\" rel=\"nofollow noreferrer\">v1.0.1<\/a> (which are adjacent on GitHub).<\/p>\n<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package<\/code> and also note that versions &lt;1.0 don't seem to be present <a href=\"https:\/\/libraries.io\/pypi\/ipydatagrid\/versions\" rel=\"nofollow noreferrer\">on PyPI<\/a>.<\/p>\n<p>So unfortunately I think (unless\/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":10.3,
        "Solution_reading_time":18.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14,
        "Solution_word_count":168,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nAttempting to deploy the docker image build yaml file available under https:\/\/github.com\/polyaxon\/polyaxon-examples\/blob\/master\/in_cluster\/build_image\/build-ml.yaml using the polyaxon cli returns the following error:\n\n>polyaxon run -f build_docker_image.yaml -l\nPolyaxonfile is not valid.\nError message: The Polyaxonfile's version specified is not supported by your current CLI.Your CLI support Polyaxonfile versions between: 1.1 <= v <= 1.1.You can run `polyaxon upgrade` and check documentation for the specification..\n\nTo reproduce\n\nThe contents of build_docker_image.yaml\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: python:3.8.8-buster\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nExpected behavior\n\nA build operation should begin that results in an image being saved to the docker registry connected under destination: connection:\n\nEnvironment\n\nPolyaxon 1.8.0\nPolyaxon CLI 1.8.0",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618113122000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1303",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.9,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Issue building docker image using example build config",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":140,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Very strange, and I could not reproduce. Did you try other files in the same examples folder https:\/\/github.com\/polyaxon\/polyaxon-examples\/tree\/master\/in_cluster\/build_image ?\nNot sure if quoting all values would work on your system:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: \"polyaxon-examples:ml\"\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"python:3.8.8-buster\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - 'pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nFinally you may try with polyaxon -v ... to trigger some additional debug information.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.2,
        "Solution_reading_time":8.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":78,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\u00a0\n\nI'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.\u00a0 I have enabled\u00a0aiplatform.googleapis.com.\n\nCommand:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifier\n\nUsing endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.\n\nPlease suggest what am I missing.",
        "Challenge_closed_time":1661575,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661561100000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project\/m-p\/460565#M543",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":322.0,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.3,
        "Solution_reading_time":3.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":51,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1437915365627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":2665.0,
        "Answerer_view_count":722.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In one of the cells of Sagemaker notebook, I've set a variable<\/p>\n<pre><code>region=&quot;us-west-2&quot;\n<\/code><\/pre>\n<p>In subsequent cell, I run following 2 shell commands<\/p>\n<pre><code>!echo $region\n<\/code><\/pre>\n<p>Output<\/p>\n<pre><code>us-west-2\n<\/code><\/pre>\n<p>However, unable to run aws shell command using this variable<\/p>\n<pre><code>!aws ecr get-login-password --region $region\n<\/code><\/pre>\n<p><code>$ variable-name<\/code> doesn't help inside jupyter cell <code>! shell command<\/code><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592937308720,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62541587",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.0,
        "Challenge_reading_time":7.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to access python variables in Sagemaker Jupyter Notebook shell command",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1285.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437915365627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":2665.0,
        "Poster_view_count":722.0,
        "Solution_body":"<p>As answered here: <a href=\"https:\/\/stackoverflow.com\/a\/19674648\/5157515\">https:\/\/stackoverflow.com\/a\/19674648\/5157515<\/a><\/p>\n<p>There's no direct way to access python variables with <code>!<\/code> command.<\/p>\n<p>But with magic command <code>%%bash<\/code> it is possible<\/p>\n<pre><code>%%bash  -s &quot;$region&quot;\naws ecr get-login-password --region $1\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.1,
        "Solution_reading_time":4.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":33,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1511190340208,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":385.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I noticed my Sagemaker (Amazon aws) jupyter notebook has an outdated version of the sklearn library.<\/p>\n<p>when I run <code>! pip freeze<\/code> I get:<\/p>\n<pre><code>sklearn==0.0\n<\/code><\/pre>\n<p>and when I run (with python) <code>print(sklearn.__version__)<\/code> I get<\/p>\n<pre><code>0.24.1\n<\/code><\/pre>\n<p>I'm not sure which one is my real version but I need 1.0.0 in order to use the <code>from_predictions()<\/code> method.<\/p>\n<p>But when I am trying to run <code>! \/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/bin\/python -m pip install --upgrade sklearn<\/code> I am getting the following output:<\/p>\n<blockquote>\n<p>Requirement already satisfied: sklearn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(0.0) Requirement already satisfied: scikit-learn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from sklearn) (0.24.1) Requirement already satisfied: scipy&gt;=0.19.1\nin\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.5.3) Requirement already satisfied:\njoblib&gt;=0.11 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.0.1) Requirement already satisfied:\nthreadpoolctl&gt;=2.0.0 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (2.1.0) Requirement already satisfied:\nnumpy&gt;=1.13.3 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.19.5)<\/p>\n<\/blockquote>\n<p>This is a very pupular library so it's weird if sagemaker cant upgrade it. Anyone has an idea what am I doing wrong?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1637426688750,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1637429547623,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70047920",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.1,
        "Challenge_reading_time":23.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"How to upgrade the sklearn library in sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":634.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Poster_created_time":1381413304940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":593.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>I managed to update sklearn to version 0.24.2 via the following command:<\/p>\n<pre><code>!conda update scikit-learn --yes\n<\/code><\/pre>\n<p>To further update it, you probably have to also update Python, which is version 3.6 in the current conda_python3 kernel on Sagemaker.<\/p>\n<p>It also looks promising to create your custom conda environment, as explained here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":15.2,
        "Solution_reading_time":7.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":56,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1312912826288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":5843.0,
        "Answerer_view_count":153.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have deployed a build of mlflow to a pod in my kubernetes cluster. I'm able to port forward to the mlflow ui, and now I'm attempting to test it. To do this, I am running the following test on a jupyter notebook that is running on another pod in the same cluster.<\/p>\n<pre><code>import mlflow\n\nprint(&quot;Setting Tracking Server&quot;)\ntracking_uri = &quot;http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000&quot;\n\nmlflow.set_tracking_uri(tracking_uri)\n\nprint(&quot;Logging Artifact&quot;)\nmlflow.log_artifact('\/home\/test\/mlflow-example-artifact.png')\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p>When I run this though, I get<\/p>\n<pre><code>ConnectionError: HTTPConnectionPool(host='mlflow-tracking-server.default.svc.cluster.local', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get? (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>\n<p>The way I have deployed the mlflow pod is shown below in the yaml and docker:<\/p>\n<p>Yaml:<\/p>\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: &lt;ECR_IMAGE&gt;\n        ports:\n        - containerPort: 5000\n        env:\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;S3_BUCKET&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\n  annotations:\n    service.beta.kubernetes.io\/aws-load-balancer-type: nlb\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  selector:\n    app: mlflow-tracking-server\n  ports:\n    - name: http\n      port: 5000\n      targetPort: http\n<\/code><\/pre>\n<p>While the dockerfile calls a script that executes the mlflow server command: <code>mlflow server --default-artifact-root ${AWS_MLFLOW_BUCKET} --host 0.0.0.0 --port 5000<\/code>, I cannot connect to the service I have created using that mlflow pod.<\/p>\n<p>I have tried using the tracking uri <code>http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000<\/code>, I've tried using the service EXTERNAL-IP:5000, but everything I tried cannot connect and log using the service. Is there anything that I have missed in deploying my mlflow server pod to my kubernetes cluster?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1587495286493,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1599477403816,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61351024",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":14.0,
        "Challenge_reading_time":34.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Kubernetes MLflow Service Pod Connection",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":276,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417012835812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":945.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>Your <strong>mlflow-tracking-server<\/strong> service should have <em>ClusterIP<\/em> type, not <em>LoadBalancer<\/em>. <\/p>\n\n<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer<\/em> Service type.<\/p>\n\n<blockquote>\n  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that\u2019s outside of your cluster.\n  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.<\/p>\n  \n  <p>Type values and their behaviors are:<\/p>\n  \n  <ul>\n  <li><p><strong>ClusterIP<\/strong>: Exposes the Service on a cluster-internal IP. Choosing this\n  value makes the Service only reachable from within the cluster. This\n  is the default ServiceType. <\/p><\/li>\n  <li><p><strong>NodePort<\/strong>: Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll > be able to contact the NodePort Service, from outside the cluster, by\n  requesting :. <\/p><\/li>\n  <li><strong>LoadBalancer<\/strong>: Exposes the Service\n  externally using a cloud provider\u2019s load balancer. NodePort and\n  ClusterIP Services, to which the external load balancer routes, are\n  automatically created. <\/li>\n  <li><strong>ExternalName<\/strong>: Maps the Service to the contents\n  of the externalName field (e.g. foo.bar.example.com), by returning a\n  CNAME record with its value. No proxying of any kind is set up.<\/li>\n  <\/ul>\n  \n  <p><a href=\"https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#publishing-services-service-types\" rel=\"nofollow noreferrer\">kubernetes.io<\/a><\/p>\n<\/blockquote>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1587499353943,
        "Solution_link_count":1,
        "Solution_readability":10.1,
        "Solution_reading_time":21.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18,
        "Solution_word_count":206,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":12,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThe product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nDSVM, DB\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\neverything runs\r\n\r\n### Other Comments\r\nquestion to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?\r\n\r\n",
        "Challenge_closed_time":1555413,
        "Challenge_comment_count":0,
        "Challenge_created_time":1553860230000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/695",
        "Challenge_link_count":0,
        "Challenge_participation_count":12,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Remove contrib from azureml",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"azureml_hyperdrive_wide_and_deep notebook does**n't** use any azureml contrib modules. papermill PR in progress is using azureml.contrib.notebook. If it's not desired in the yaml file, I can install this package only inside the notebook. Will this work? mmm, yeah that would be a workaround.  Not in the Hyperdrive notebooks. since @jingyanwangms is the only one using it, can you take care of this issue in your PR? Do we want to require people to install things at the beginning of notebooks, though?  azureml.contrib.notebook is required for submitting notebook through aml. But if we don't want azureml.contrib to install as default in base yaml files, @heatherbshapiro what would you recommend doing here? @miguelgfierro Sure. I can do it in my PR. Hey guys\r\nI am getting an error while trying to run this line \"from azureml.contrib.notebook import NotebookRunConfig, AzureMLNotebookHandler\".\r\n**The error is ModuleNotFoundError: No module named 'azureml.contrib'** although I have installed azureml-contrib-notebook from pip. What should I do?\r\n HI @Raman1121 , which file in the code is this line in? I am trying to experiment with jupyter notebook on Azure through API calls by following the code snippet given here - \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-notebook\/azureml.contrib.notebook.azuremlnotebookhandler?view=azure-ml-py Well, that code is not related to the Recommenders GitHub repo. Most likely you have not configured your conda or python settings appropriately.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":7.2,
        "Solution_reading_time":18.91,
        "Solution_score_count":null,
        "Solution_sentence_count":21,
        "Solution_word_count":210,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can someone explain or help me install <a href=\"https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/tree\/master\/python\" rel=\"nofollow noreferrer\">vowpalwabbit<\/a> (I'm interested in the python bindings) on an Amazon linux machine, either EC2 or SageMaker?\nfor some reason it is very hard and I can't find anything about it online...<\/p>\n\n<p>a <code>pip install vowpalwabbit<\/code> returns a <\/p>\n\n<pre><code>Using cached https:\/\/files.pythonhosted.org\/packages\/d1\/5a\/9fcd64fd52ad22e2d1821b2ef871e8783c324b37e2103e7ddefa776c2ed7\/vowpalwabbit-8.8.0.tar.gz\nBuilding wheels for collected packages: vowpalwabbit\n  Building wheel for vowpalwabbit (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0]= '\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-x0j85ac_ --python-tag cp36\n       cwd: \/tmp\/pip-install-tvp1174t\/vowpalwabbit\/\n<\/code><\/pre>\n\n<p>lower in the error I can also see a:<\/p>\n\n<pre><code>CMake Error at \/usr\/lib64\/python3.6\/dist-packages\/cmake\/data\/share\/cmake-3.13\/Modules\/FindBoost.cmake:2100 (message):\n    Unable to find the requested Boost libraries.\n\n    Boost version: 1.53.0\n\n    Boost include path: \/usr\/include\n\n    Could not find the following Boost libraries:\n\n            boost_python3\n\n    Some (but not all) of the required Boost libraries were found.  You may\n    need to install these additional Boost libraries.  Alternatively, set\n    BOOST_LIBRARYDIR to the directory containing Boost libraries or BOOST_ROOT\n    to the location of Boost.\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1580567889550,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60017893",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":15.4,
        "Challenge_reading_time":25.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to install Vowpal Wabbit on Amazon EC2 or SageMaker? (amazon linux)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442180190107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3203.0,
        "Poster_view_count":400.0,
        "Solution_body":"<p>Tested again 1.5 years later, and a <code>pip install vowpalwabbit<\/code> works fine on notebook instance. In training job, adding vowpalwabbit in a <code>requirements.txt<\/code> send to an AWS-managed Scikit learn container (<code>141502667606.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3<\/code>) also installs successfully. Both tested with vowpalwabbit-8.11.0<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":38,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"On chart release v0.13.2 the default value for projectOperator.mlflow.image.tag is set to latest when it should be set to v0.13.2.\r\n\r\nCheck values.yml:\r\n\r\n```yaml\r\nprojectOperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullPolicy: IfNotPresent\r\n  mlflow:\r\n    image:\r\n      repository: konstellation\/mlflow\r\n      tag: latest\r\n      pullPolicy: IfNotPresent\r\n    volume:\r\n      storageClassName: standard\r\n      size: 1Gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullPolicy: IfNotPresent\r\n```",
        "Challenge_closed_time":1635871,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635429600000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/623",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":15.5,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Project operator mlflow image tag is set to \"latest\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1314313109232,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Technion, Israel",
        "Answerer_reputation_count":18777.0,
        "Answerer_view_count":2000.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>upon running <code>pip install trains<\/code> in my virtual env<\/p>\n<p>I am getting<\/p>\n<pre><code>    ERROR: Command errored out with exit status 1:\n     command: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying\n         cwd: \/tmp\/pip-install-owzh8lnl\/retrying\/\n    Complete output (10 lines):\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib\n    copying retrying.py -&gt; build\/lib\n    running install_lib\n    copying build\/lib\/retrying.py -&gt; \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\n    byte-compiling \/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/retrying.py to retrying.cpython-38.pyc\n    error: [Errno 13] Permission denied: '\/home\/epdadmin\/noam\/code\/venv_linux\/lib\/python3.8\/site-packages\/__pycache__\/retrying.cpython-38.pyc.139678407381360'\n    ----------------------------------------\nERROR: Command errored out with exit status 1: \/home\/epdadmin\/noam\/code\/venv_linux\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-owzh8lnl\/retrying\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-lxz5t8pu\/install-record.txt --single-version-externally-managed --compile --install-headers \/home\/epdadmin\/noam\/code\/venv_linux\/include\/site\/python3.8\/retrying Check the logs for full command output.\n<\/code><\/pre>\n<p>I know that <a href=\"https:\/\/stackoverflow.com\/questions\/15028648\/is-it-acceptable-and-safe-to-run-pip-install-under-sudo\">I am not supposed to run under sudo when using a venv<\/a>, so I don't really understand the problem<\/p>\n<p>running for example <code>pip install pandas<\/code> does work.<\/p>\n<p>Python 3.8<\/p>\n<p>How to install trains?<\/p>\n<hr \/>\n<p>EDIT:<\/p>\n<p>running <code>pip install trains --user<\/code> or <code>pip install --user trains<\/code> gives<\/p>\n<pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1602431071093,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1609353956110,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64305945",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":19.6,
        "Challenge_reading_time":39.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"pip install trains fails",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1031.0,
        "Challenge_word_count":187,
        "Platform":"Stack Overflow",
        "Poster_created_time":1314313109232,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Technion, Israel",
        "Poster_reputation_count":18777.0,
        "Poster_view_count":2000.0,
        "Solution_body":"<p>The problem was a permissions problem for the venv.\nAnother problem was trains required some packages that were not yet available with wheels on Python3.8, so I had to downgrade Python to 3.7<\/p>\n<p>That venv was created using Pycharm, and for some reason it was created with low permissions.<\/p>\n<p>There was probably a way to elevate its permissions, but instead I just deleted it and created another one using command line by<\/p>\n<pre><code>python -m virtualenv --python=\/usr\/bin\/python3.7 venv\n<\/code><\/pre>\n<p>And now <code>pip install trains<\/code> worked.<\/p>\n<p>Very annoying.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1604308183096,
        "Solution_link_count":0,
        "Solution_readability":6.9,
        "Solution_reading_time":7.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":86,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a locally trained RandomForest model into Azure Machine Learning Studio.<\/p>\n<p><strong>training code (whentrain.ipynb) :<\/strong><\/p>\n<pre><code>#import libs and packages\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom math import sqrt\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom azureml.core import Workspace, Dataset\n\n# get existing workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# get the datastore to upload prepared data\ndatastore = workspace.get_default_datastore()\n\n# load the dataset which is placed in the data folder\ndataset = Dataset.Tabular.from_delimited_files(datastore.path('UI\/12-23-2021_023530_UTC\/prepped_data101121.csv'))\ndataset = dataset.to_pandas_dataframe()\n\n# Create the outputs directories to save the model and images\nos.makedirs('outputs\/model', exist_ok=True)\nos.makedirs('outputs\/output', exist_ok=True)\ndataset['Date'] = pd.to_datetime(dataset['Date'])\ndataset = dataset.set_index('Date')\n###\nscaler = MinMaxScaler()\n\n#inputs\nX = dataset.iloc[:, 1:]\n#output\ny = dataset.iloc[:, :1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42, shuffle=True)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n###\n\nmodel1 = RandomForestRegressor(n_estimators = 6,\n                                   max_depth = 10,\n                                   min_samples_leaf= 1,\n                                   oob_score = 'True',\n                                   random_state=42)\nmodel1.fit(X_train, y_train.values.ravel())\n\ny_pred2 = model1.predict(X_test)\n<\/code><\/pre>\n<p><strong>And here is the code on the estimator part (estimator.ipynb):<\/strong><\/p>\n<pre><code>from azureml.core import Experiment\nfrom azureml.core import Workspace\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.train.dnn import TensorFlow\nfrom azureml.widgets import RunDetails\n\nimport os\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\nexp = Experiment(workspace=workspace, name='azure-exp')\ncluster_name = &quot;gpucluster&quot;\n\ntry:\n    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2',\n                                                           max_nodes=1)\n\n    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)  # , min_node_count=None, timeout_in_minutes=20)\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\nfrom azureml.core import ScriptRunConfig\nsource_directory = os.getcwd()\n\nfrom azureml.core import Environment\n\nmyenv = Environment(&quot;user-managed-env&quot;)\nmyenv.python.user_managed_dependencies =True\nfrom azureml.core import Dataset\ntest_data_ds = Dataset.get_by_name(workspace, name='prepped_data101121')\n\nsrc = ScriptRunConfig(source_directory=source_directory,\n                      script='whentrain.ipynb',\n\n                      arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],\n                      compute_target=compute_target,\n                      environment=myenv)\nrun = exp.submit(src)\nRunDetails(run).show()\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>The error that happens in <strong>run.wait_for_completion<\/strong> states :<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 107, in &lt;module&gt;\n[stderr]    &quot;notebookHasBeenCompleted&quot;: true\n[stderr]NameError: name 'true' is not defined\n[stderr]\n<\/code><\/pre>\n<p>As you can see in my whentrain.ipynb, it does not even reach line 107, and I could not find where this error come from. So how do I fix it?<\/p>\n<p>I'm running the Notebook on Python 3.<\/p>\n<p><strong>UPDATE:<\/strong><\/p>\n<p>Okay, after a little adjustment that should not affect the whole code (I just removed some extra columns, added model save code in whentrain.ipynb making use of import os) it's now giving me somewhat the same error.<\/p>\n<pre><code>[stderr]Traceback (most recent call last):\n[stderr]  File &quot;whentrain.ipynb&quot;, line 115, in &lt;module&gt;\n[stderr]    &quot;source_hidden&quot;: false,\n[stderr]NameError: name 'false' is not defined\n[stderr]\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1640331391010,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/674712\/nameerror-when-trying-to-run-an-scriptrunconfig-in",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":61.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":null,
        "Challenge_title":"NameError when trying to run an ScriptRunConfig in Azure Machine Learning",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":413,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b72317d8-d212-487c-8510-7d965e8d135f\">@Ash  <\/a> Ok, I think the issue is here.     <\/p>\n<pre><code>src = ScriptRunConfig(source_directory=source_directory,  \n                       script='whentrain.ipynb',  \n                            \n                       arguments=['--input-data', test_data_ds.as_named_input('prepped_data101121')],  \n                       compute_target=compute_target,  \n                       environment=myenv)  \n<\/code><\/pre>\n<p>The script parameter is set to the notebook &quot;whentrain.ipynb&quot;, This should be a python script *.py which can train your model. Since you are using the notebook filename the entire source of jupyter notebook is loaded and it fails with these errors. You can lookup samples on azure ml notebook github repo for <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-amlcompute\/train.py\">reference<\/a>. I think if you can convert your whentrain.ipynb file to a python script whentrain.py and save it the current folder structure you should be able to use it in this step.    <\/p>\n",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":9.4,
        "Solution_reading_time":12.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11,
        "Solution_word_count":106,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1608747030727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":105.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I'm a beginner in docker and containers and I've been getting this error for days now.\nI get this error when my lambda function runs a sagemaker processing job.\nMy core python file resides in an s3 bucket.\nMy docker image resides in ECR.\nBut I dont understand why I dont get a similar error when I run the same processing job with a python docker image.\nPFB the python docker file that didnt throw any errors.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM python:latest\n#installing dependencies\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\nRUN pip3 install matplotlib\n<\/code><\/pre>\n<p>I only get this error when i run this with a an ubunutu docker image with python3 installed.\nPFB the dockerfile which throws the error mentioned.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM ubuntu:20.04\n\nRUN apt-get update -y\nRUN apt-get install -y python3\nRUN apt-get install -y python3-pip\n\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy==1.19.1\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\n\nENTRYPOINT [ &quot;python3&quot; ]\n<\/code><\/pre>\n<p>How do I fix this?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622233871527,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1622239825870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67745141",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":17.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Facing this error : container_linux.go:367: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1123.0,
        "Challenge_word_count":205,
        "Platform":"Stack Overflow",
        "Poster_created_time":1608747030727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Fixed this error by changing the entry point to<\/p>\n<p><strong>ENTRYPOINT [ &quot;\/usr\/bin\/python3.8&quot;]<\/strong><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.1,
        "Solution_reading_time":1.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1,
        "Solution_word_count":11,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi everyone, I am using wandb with Huggingface in a AWS Sagemaker notebook and I am refering to the tutorial here: <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface\" class=\"inline-onebox\">Hugging Face Transformers | Weights &amp; Biases Documentation<\/a>.<\/p>\n<p>I tried to set the <code>WANDB_PROJECT<\/code> environment variable before setting up the <code>huggingface_estimator<\/code>, which will call <code>train.py<\/code>.<\/p>\n<p><code>train.py<\/code> is where I initialize the <code>Trainer<\/code>. The above tutorial mentions to make sure to set the project name before initializing the <code>Trainer<\/code>, and I think I am doing this correctly here.<\/p>\n<p>Here are some useful snippets of my code.<\/p>\n<pre><code class=\"lang-auto\">import wandb\nwandb.login()\n\nWANDB_PROJECT=my_project_name\n\n...\n\nhuggingface_estimator = HuggingFace(\n  image_uri=image_uri,\n  entry_point='train.py',\n  source_dir='.\/scripts',\n  instance_type='ml.g4dn.xlarge',\n  instance_count=1,\n  role=role,\n  py_version='py39',\n  hyperparameters=hyperparameters,\n)\n<\/code><\/pre>\n<p>train.py<\/p>\n<pre><code class=\"lang-auto\">    training_args = TrainingArguments(\n        output_dir=args.output_dir,\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        num_train_epochs=args.epochs,\n        learning_rate=args.learning_rate,\n        save_strategy=\"epoch\",\n        logging_strategy='epoch',\n        report_to=\"wandb\",\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        data_collator=collate_fn,\n        tokenizer=image_processor,\n    )\n\n    trainer.train()\n<\/code><\/pre>\n<p>I would greatly appreciate any guidance or advice on how to resolve this issue. Thank you very much in advance for your help!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678765360341,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/set-the-wandb-project-environment-variable-cannot-name-the-project-properly\/4055",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":15.9,
        "Challenge_reading_time":23.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Set the WANDB_PROJECT environment variable cannot name the project properly",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":156,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/oschan77\">@oschan77<\/a> ,<\/p>\n<p>You can set the project name in your script like so:<\/p>\n<pre><code class=\"lang-auto\">import os\nos.environ[\"WANDB_PROJECT\"] = \"sentiment-analysis\"\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.3,
        "Solution_reading_time":3.03,
        "Solution_score_count":null,
        "Solution_sentence_count":2,
        "Solution_word_count":22,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nThank you for creating such a helpful tool!\r\nThe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the MLFlow experiment artifacts dir. I think the issue is with inconsistent naming for the saved png for certain plot types.\r\nThank you for your help!\r\n<!--\r\n-->\r\n\r\n**To Reproduce**\r\n<!--\r\nAdd a Minimal, Complete, and Verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nIf the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=False, \r\n      silent=True,\r\n      verbose=False,\r\n      # for MLFlow logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = True, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**Expected behavior**\r\n<!--\r\n-->\r\nI expect ALL of the plot types to be logged under the MLFlow artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nHowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\r\nI think the issue is with inconsistent naming of the file. Here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:INFO:Saving 'calibration.png'\r\n2021-10-11 19:03:20,064:INFO:Visual Rendered Successfully\r\n2021-10-11 19:03:20,213:INFO:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:WARNING:[Errno 2] No such file or directory: 'Calibration Curve.png'\r\n```\r\nSo you can see that it is looking for 'Calibration Curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**Versions**\r\nPython 3.8.11\r\n\r\n<!--\r\nPlease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\nPycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1635405,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634052175000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1674",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":27.37,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] some types plot types are not getting saved to the MLFlow experiment artifacts dir",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":268,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@ejohnson-amerilife Thank you so much for bringing this up. Would you like to submit a PR for this? ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.2,
        "Solution_score_count":null,
        "Solution_sentence_count":2,
        "Solution_word_count":18,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1614882423070,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For some reason the jupyter notebooks on my VM are in the wrong environment (ie stuck in <code>(base)<\/code>). Furthermore, I can change the environment in the terminal but not in the notebook. Here is what happens when I attempt <code>!conda activate desired_env<\/code> in the notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n# conda environments:\n#\nbase                  *  \/anaconda\nazureml_py36             \/anaconda\/envs\/azureml_py36\nazureml_py38             \/anaconda\/envs\/azureml_py38\nazureml_py38_pytorch     \/anaconda\/envs\/azureml_py38_pytorch\nazureml_py38_tensorflow     \/anaconda\/envs\/azureml_py38_tensorflow\n<\/code><\/pre>\n<p>I tried the answers <a href=\"https:\/\/stackoverflow.com\/questions\/61915607\/commandnotfounderror-your-shell-has-not-been-properly-configured-to-use-conda\">here<\/a> (e.g., first running <code>!source \/anaconda\/etc\/profile.d\/conda.sh<\/code>).<\/p>\n<p>I also tried activating the environment using <code>source<\/code> rather than 'conda activate': <code>!source \/anaconda\/envs\/desired_env\/bin\/activate<\/code>. This runs but doesn't actually do anything when I see the current environment in <code>conda env list<\/code><\/p>\n<p>Edit: also adding that if I install a package in the <code>(base)<\/code> environment in the terminal, I still don't have access to it in jupyter notebook.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636646694700,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1636647028003,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69931411",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't change virtual environment within Azure ML notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":672.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586979502910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":361.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>I'm the PM that released AzureML Notebooks, you can't activate a Conda env from a cell, you have to create a new kernel will the Conda Env. Here are the instructions: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels<\/a><\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":16.1,
        "Solution_reading_time":5.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3,
        "Solution_word_count":35,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie to aws sagemaker.\nI am trying to setup a model in aws sagemaker using keras with GPU support.\nThe docker base image used to infer the model is given below<\/p>\n\n<pre><code>FROM tensorflow\/tensorflow:1.10.0-gpu-py3\n\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx curl\n...\n<\/code><\/pre>\n\n<p>This is the keras code I'm using to check if a GPU is identified by keras in flask.<\/p>\n\n<pre><code>import keras\n@app.route('\/ping', methods=['GET'])\ndef ping():\n\n    keras.backend.tensorflow_backend._get_available_gpus()\n\n    return flask.Response(response='\\n', status=200,mimetype='application\/json')\n<\/code><\/pre>\n\n<p>When I spin up a notebook instance in a sagemaker using the GPU the keras code shows available GPUs.\nSo, in order to access GPU in the inference phase(model) do I need to install any additional libraries in the docker file apart from the tensorflow GPU base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544505483170,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53717800",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":12.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Configuring GPU in aws sagemaker with keras and tensorflow as backend",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1958.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544503799112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":57.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>You shouldn't need to install anything else. Keras relies on TensorFlow for GPU detection and configuration.<\/p>\n\n<p>The only thing worth noting is how to use multiple GPUs during training. I'd recommend passing 'gpu_count' as an hyper parameter, and setting things up like so:<\/p>\n\n<pre><code>from keras.utils import multi_gpu_model\nmodel = Sequential()\nmodel.add(...)\n...\nif gpu_count &gt; 1:\n    model = multi_gpu_model(model, gpus=gpu_count)\nmodel.compile(...)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.8,
        "Solution_reading_time":6.08,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6,
        "Solution_word_count":59,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"Seems like recent upgrade to V1.33 for Azure ML SDK has changed how identity based access worked? Previously if you had a datastore (ex. SQL) with no credentials and then tried to register a dataset, it would prompt you to login to get your AAD auth token to see if you had permission to get access to the underlying data source. Seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab AD auth token:\r\n**_Getting data access token with Assigned Identity (client_id=clientid)._**\r\n\r\n\r\nI have verified the underlying datastore does not have Managed Identity on and V1.32 SDK Prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. Has any changes been made to the identity based access feature from on V1.33 SDK? According to the SDK docs, running the TabularDataset.to_pandas_dataframe() command should prompt an AD login if using no credentialed datastore into dataset creation. FYI currently using Azure SQL DB as datastore, any clarifications would be appreciated!\r\nazureml.core.Datastore class - Azure Machine Learning Python | Microsoft Docs\r\n",
        "Challenge_closed_time":1632248,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630006433000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1584",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":8.6,
        "Challenge_reading_time":15.54,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Identity Based Access No longer works (with Azure SQL DB datastore) in V1.33 of Azure ML SDK",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":204,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"[70_driver_log.txt](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/7062339\/70_driver_log.txt)\r\n\r\nError generated in new compute that uses the V1.33 SDK Any updates to this? I had created another AML Workspace and issue disappeared but for some other subscriptions it still doesnt work and errors with the same thing as in the logs. Everything works perfectly fine in V1.32 of the SDK so not sure if new update changed some sort of Identity SDK used in Azure? The driver log had error message \"Compute has no identity provisioned.\" Try updating the compute to enable managed identity, and grant managed identity access to the data storage. Ah ok I was under the impression only the compute clusters had MI and not the compute instance. I'll take a look at the docs and will also re-configure the datastore which might be issue. @rudizhou428 we had some new feature for Compute Instance, which can use your identity in the CI, but, you need to re-create the CI as it won't automatically update the existing one. @chunyli0328 Ah ok cool, I ended up creating a new Azure ML Workspace and moved all my files over and since you need to recreate the CI and clusters, I'm guessing thats why it started to work again. Closing this issue, thanks for the help everyone!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":8.9,
        "Solution_reading_time":15.53,
        "Solution_score_count":null,
        "Solution_sentence_count":12,
        "Solution_word_count":208,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nUsers get the error \"null is not an object\" when pop-ups are enabled in SWB (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620))\r\nThis error is illegible to the user and causes confusion. Can we make the error message more clear such as:\r\n\"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Diable pop-ups \r\n2. Connect to a workspace\r\n\r\n**Expected behavior**\r\nIf the workspace is unable to open, a more legible error message should be shown, such as \"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v4.3.1 and v5.0.0]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671209,
        "Challenge_comment_count":0,
        "Challenge_created_time":1670601495000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1081",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.6,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] More descriptive error message for \"null is not an object\" while trying to connect to Sagemaker notebook. ",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @simranmakwana, thank you for creating this issue. There are currently no plans to enrich the error messages in the UI; the recommendation is for you to customize the error messages within your installation of SWB as you see fit. Please reply back if there are any concerns with this approach. Thank you! ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.0,
        "Solution_reading_time":3.75,
        "Solution_score_count":null,
        "Solution_sentence_count":3,
        "Solution_word_count":53,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1619204963587,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1442.0,
        "Answerer_view_count":879.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>There is a gcloud command to create a user-managed notebook instance.<\/p>\n<pre><code>gcloud notebooks instances create \n<\/code><\/pre>\n<p>Is is possible to create a managed notebook with gcloud?<\/p>\n<p>It looks to be possible in the <a href=\"https:\/\/stackoverflow.com\/questions\/70223161\/how-to-create-a-new-workbench-managed-notebook-using-rest-api\">API<\/a>. I can't find a gcloud reference.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1659584245230,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73230096",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.8,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to create a new VertexAI Workbench Managed Notebook using gcloud",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1359519204743,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Melbourne, Australia",
        "Poster_reputation_count":3943.0,
        "Poster_view_count":339.0,
        "Solution_body":"<p>As mentioned by @gogasca, the gcloud SDK for creating managed notebook is currently under work. Meanwhile you can try client libraries and REST API to create the same.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.4,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":28,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1589205020747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I constantly run into problems when working on Azure Compute Instances and trying to connect from the Jupyter Lab to the workspace.<\/p>\n<p>With InteractiveLoginAuthentication I get the following message:<\/p>\n<pre><code>AuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<p>With a Service Principal this one (SP is owner in the ML Workspace):<\/p>\n<pre><code>WorkspaceException: WorkspaceException:\n    Message: No workspaces found with name=xxx in all the subscriptions that you have access to.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;No workspaces found with name=xxx in all the subscriptions that you have access to.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I had another workspace in a different subscription where I could resolve it by giving the tennant as an extra input to the InteractiveLoginAuthentication. This time, no chance.<\/p>\n<p>The funny thing is, though, that I can login to the workspace via InteractiveLoginAuthentication when doing it from my local computer.<\/p>\n<p>I supsected that some old tokens are cached somewhere so I tried to use the &quot;Private browsing&quot; function of my browser. Furthermore, I deleted <code>\/home\/azureuser\/.azure\/accessTokens.json<\/code> but no effect.<\/p>\n<p>Maybe some of you had this problem before and have an idea?<\/p>\n<p>For reference some sites I checked:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/4618\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/4618<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/6147\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/6147<\/a><\/li>\n<\/ul>\n<h1>Update<\/h1>\n<p>When I run this code:<\/p>\n<pre><code>from azureml.core.authentication import InteractiveLoginAuthentication\ninteractive_auth = InteractiveLoginAuthentication(tenant_id='xxx')\n\nws = Workspace.get(name='xxx',\n                   subscription_id='xxx',\n                   resource_group='xxx',\n                   auth=interactive_auth)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>---------------------------------------------------------------------------\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    288                     module_logger.debug(&quot;{} acquired lock in {} s.&quot;.format(type(self).__name__, duration))\n--&gt; 289                 return test_function(self, *args, **kwargs)\n    290             except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n\nDuring handling of the above exception, another exception occurred:\n\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n&lt;ipython-input-2-fd1276999d15&gt; in &lt;module&gt;\n      5                    subscription_id='00c983e5-d766-480b-be75-abf95d1a46c3',\n      6                    resource_group='BusinessIntelligence',\n----&gt; 7                    auth=interactive_auth)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in get(name, auth, subscription_id, resource_group)\n    547 \n    548         result_dict = Workspace.list(\n--&gt; 549             subscription_id, auth=auth, resource_group=resource_group)\n    550         result_dict = {k.lower(): v for k, v in result_dict.items()}\n    551         name = name.lower()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in list(subscription_id, auth, resource_group)\n    637         elif subscription_id and resource_group:\n    638             workspaces_list = Workspace._list_legacy(\n--&gt; 639                 auth, subscription_id=subscription_id, resource_group_name=resource_group)\n    640 \n    641             Workspace._process_autorest_workspace_list(\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1373                 return None\n   1374             else:\n-&gt; 1375                 raise e\n   1376 \n   1377     @staticmethod\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1367             # azureml._base_sdk_common.workspace.models.workspace.Workspace\n   1368             workspace_autorest_list = _commands.list_workspace(\n-&gt; 1369                 auth, subscription_id=subscription_id, resource_group_name=resource_group_name)\n   1370             return workspace_autorest_list\n   1371         except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_project\/_commands.py in list_workspace(auth, subscription_id, resource_group_name)\n    386         if resource_group_name:\n    387             list_object = WorkspacesOperations.list_by_resource_group(\n--&gt; 388                 auth._get_service_client(AzureMachineLearningWorkspaces, subscription_id).workspaces,\n    389                 resource_group_name)\n    390             workspace_list = list_object.value\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_service_client(self, client_class, subscription_id, subscription_bound, base_url)\n    155         # in the multi-tenant case, which causes confusion.\n    156         if subscription_id:\n--&gt; 157             all_subscription_list, tenant_id = self._get_all_subscription_ids()\n    158             self._check_if_subscription_exists(subscription_id, all_subscription_list, tenant_id)\n    159 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_all_subscription_ids(self)\n    497         :rtype: list, str\n    498         &quot;&quot;&quot;\n--&gt; 499         arm_token = self._get_arm_token()\n    500         return self._get_all_subscription_ids_internal(arm_token)\n    501 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    293                     InteractiveLoginAuthentication(force=True, tenant_id=self._tenant_id)\n    294                     # Try one more time\n--&gt; 295                     return test_function(self, *args, **kwargs)\n    296                 else:\n    297                     raise e\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    473             return self._ambient_auth._get_arm_token()\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n    477     @_login_on_failure_decorator(_interactive_auth_lock)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    588         profile_object = Profile(async_persist=False, cloud_type=cloud_type)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n    592         if self._tenant_id and fetch_tenantid_from_aad_token(arm_token) != self._tenant_id:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    324             while True:\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n    328                     if &quot;Connection aborted.&quot; in str(e) and attempt &lt;= retries:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1828         if not token_about_to_expire:\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n   1832     try:\n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<ul>\n<li><code>azureml-sdk<\/code> is on version 1.9.0<\/li>\n<li>I can connect an authenticate from my local machine. Problems only occur when I want to work on a compute instance.<\/li>\n<\/ul>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591616299803,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1594308517016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62261222",
        "Challenge_link_count":8,
        "Challenge_participation_count":3,
        "Challenge_readability":18.4,
        "Challenge_reading_time":220.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":139,
        "Challenge_solved_time":null,
        "Challenge_title":"Workspace Authentication: More than one token matches the criteria",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2737.0,
        "Challenge_word_count":1188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589205020747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":163.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Okay, here is the answer:<\/p>\n<ul>\n<li>You work for company A which is on Azure.<\/li>\n<li>You get access to company B's subscription.<\/li>\n<li>Problem is: You are associated to A's AAD in ML-Studio.<\/li>\n<li>You need to specify the tenant ID in the <code>InteractiveLoginAuthentication<\/code> like so:<\/li>\n<\/ul>\n<pre><code>interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)\n\nworkspace = Workspace.get(name=workspace_name,\n                          subscription_id=subscription_id,\n                          resource_group=resource_group,\n                          auth=interactive_auth)\n<\/code><\/pre>\n<ul>\n<li>Now the <strong>important<\/strong> part: You need to use company B's <code>tenant_id<\/code> (I used company A's all the time since I thought that was my authentication point)<\/li>\n<li>Of course, this is obvious while you read it...as it is to me now :)<\/li>\n<\/ul>\n<p>Hope this helps you. Took me some time but learned a lot ;)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.7,
        "Solution_reading_time":11.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7,
        "Solution_word_count":109,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1622117284230,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":316.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running this notebook in my managed notebooks environment on Google Cloud and I'm getting the following error when trying to install the packages: &quot;WARNING: The script google-oauthlib-tool is installed in '\/home\/jupyter\/.local\/bin' which is not on PATH.\nConsider adding this directory to PATH.&quot;<\/p>\n<p>Here is the python code that I'm trying to run for reference. <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb<\/a><\/p>\n<p>Any suggestions on how I can update the package installation so it is on path and resolve the error? I'm currently working on GCP user-managed notebooks on a Mac.<\/p>\n<p>Thanks so much for any tips!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658518447040,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73085293",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Library is not installed on PATH - How can I install on path?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1621620820567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Open up your shell config file (likely .zshrc because the default shell on Mac is now zsh and that's the name of the zsh config file) located at your home directory in a text editor (TextEdit, etc) and add the path to the  executable.\nLike this:\nOpen the file:\n<code>open -e ~\/.zshrc<\/code>\nEdit the file:\nAdd this line at the top (may vary, check the documentation):\n<code>export PATH=&quot;\/home\/jupyter\/.local\/bin&quot;<\/code>\nThat may not work, try this:\n<code>export PATH=&quot;$PATH:\/home\/jupyter\/.local\/bin&quot;<\/code>\nYour best bet is to read the package documentation.<\/p>\n<p>After saving the config file, run <code>source ~\/.zshrc<\/code> and replace .zshrc with the config file name if it's different OR open a new terminal tab.<\/p>\n<p>What this does is tells the shell that the command exists and where to find it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.8,
        "Solution_reading_time":10.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10,
        "Solution_word_count":126,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello, receiving the following error in an Azure Notebook VM while trying to import the ML library - \r\n\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\n# error here!!!\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b8d543bb7111> in <module>\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n----> 5 from azureml.train.automl import AutoMLConfig\r\n      6 from sklearn.externals import joblib\r\n      7 from azureml.core.model import Model\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/__init__.py in <module>\r\n     23     # Suppress the warnings at the import phase.\r\n     24     warnings.simplefilter(\"ignore\")\r\n---> 25     from ._automl import fit_pipeline\r\n     26     from .automlconfig import AutoMLConfig\r\n     27     from .automl_step import AutoMLStep, AutoMLStepRun\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_automl.py in <module>\r\n     17 from automl.client.core.runtime.cache_store import CacheStore\r\n     18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities\r\n---> 19 from azureml.automl.core import data_transformation, fit_pipeline as fit_pipeline_helper\r\n     20 from azureml.automl.core.automl_pipeline import AutoMLPipeline\r\n     21 from azureml.automl.core.data_context import RawDataContext, TransformedDataContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/fit_pipeline.py in <module>\r\n     18 from automl.client.core.common.limit_function_call_exceptions import TimeoutException\r\n     19 from automl.client.core.runtime.datasets import DatasetBase\r\n---> 20 from . import package_utilities, pipeline_run_helper, training_utilities\r\n     21 from .automl_base_settings import AutoMLBaseSettings\r\n     22 from .automl_pipeline import AutoMLPipeline\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/pipeline_run_helper.py in <module>\r\n     18 from automl.client.core.common.exceptions import ClientException\r\n     19 from automl.client.core.runtime import metrics\r\n---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module\r\n     21 from automl.client.core.runtime.datasets import DatasetBase\r\n     22 from automl.client.core.runtime.execution_context import ExecutionContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in <module>\r\n     21 \r\n     22 from automl.client.core.common import constants\r\n---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers\r\n     24 from automl.client.core.runtime.nimbus_wrappers import AveragedPerceptronBinaryClassifier, \\\r\n     25     AveragedPerceptronMulticlassClassifier, NimbusMlClassifierMixin, NimbusMlRegressorMixin\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in <module>\r\n     34 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n     35 if tf_found:\r\n---> 36     tf.logging.set_verbosity(tf.logging.ERROR)\r\n     37 \r\n     38     OPTIMIZERS = {\r\n \r\nAttributeError: module 'tensorflow' has no attribute 'logging'\r\n",
        "Challenge_closed_time":1587086,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572995244000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/644",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":45.44,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":null,
        "Challenge_title":"Error trying to load azureml.train.automl",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":272,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Do you know which version of tensorflow you are using? \r\n\r\nThis SO question may be applicable: https:\/\/stackoverflow.com\/questions\/55318626\/module-tensorflow-has-no-attribute-logging Hello, Not sure about tensorflow.  This is a \"stock\" Notebook VM that was created last week, so no changes were made to the libraries. Hello,\r\n\r\nSorry for the inconvenience. This issue has been fixed since v1.0.72 but, it's related to the fact that tf==2.0. is installed by default on the notebook instance. It broke other things too as TF2.0 has many changes in its API. Your two options are to upgrade to v1.0.72+ or use the following code to downgrade tensorflow.\r\n\r\npip install -U tensorflow-gpu==1.14.0 \r\ntensorflow==estimator==1.14.0 \r\n\r\nThat should fix it for you.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":5.2,
        "Solution_reading_time":9.24,
        "Solution_score_count":null,
        "Solution_sentence_count":14,
        "Solution_word_count":109,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1318047784840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":6665.0,
        "Answerer_view_count":926.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to install the following library in my Azure ML instance:<\/p>\n<p><a href=\"https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI<\/a><\/p>\n<p>My Dockerfile looks like this:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04:20210615.v1\n\nENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.7\n\n# Create conda environment\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n    python=3.7 \\\n    pip=20.2.4 \\\n    pytorch=1.7.1 \\\n    torchvision=0.8.2 \\\n    torchaudio=0.7.2 \\\n    cudatoolkit=11.0 \\\n    nvidia-apex=0.1.0 \\\n    -c anaconda -c pytorch -c conda-forge\n\n# Prepend path to AzureML conda environment\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                'psutil&gt;=5.8,&lt;5.9' \\\n                'tqdm&gt;=4.59,&lt;4.60' \\\n                'pandas&gt;=1.1,&lt;1.2' \\\n                'scipy&gt;=1.5,&lt;1.6' \\\n                'numpy&gt;=1.10,&lt;1.20' \\\n                'azureml-core==1.31.0' \\\n                'azureml-defaults==1.31.0' \\\n                'azureml-mlflow==1.31.0' \\\n                'azureml-telemetry==1.31.0' \\\n                'tensorboard==2.4.0' \\\n                'tensorflow-gpu==2.4.1' \\\n                'onnxruntime-gpu&gt;=1.7,&lt;1.8' \\\n                'horovod[pytorch]==0.21.3' \\\n                'future==0.17.1' \\\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n\n# This is needed for mpi to locate libpython\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n<\/code><\/pre>\n<p>An error is thrown when the library is being installed:<\/p>\n<pre><code>  Cloning https:\/\/github.com\/philferriere\/cocoapi.git to \/tmp\/pip-install-_i3sjryy\/pycocotools\n[91m    ERROR: Command errored out with exit status 1:\n     command: \/azureml-envs\/pytorch-1.7\/bin\/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base \/tmp\/pip-pip-egg-info-o68by1_q\n         cwd: \/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\n    Complete output (5 lines):\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py&quot;, line 2, in &lt;module&gt;\n        from Cython.Build import cythonize\n    ModuleNotFoundError: No module named 'Cython'\n<\/code><\/pre>\n<p>I've tried adding Cython as a dependecy in both the pip section and as part of the conda environment but the error is still thrown.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625322434170,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68237132",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":20.7,
        "Challenge_reading_time":39.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"No module named 'Cython' setting up Azure ML docker instance",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":209.0,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Poster_created_time":1318047784840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":6665.0,
        "Poster_view_count":926.0,
        "Solution_body":"<p>Solution was to add the following to the Dockerfile:<\/p>\n<pre><code># Install Cython\nRUN pip3 install Cython\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                ...\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":22.9,
        "Solution_reading_time":4.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1,
        "Solution_word_count":26,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When attempting to select the Dependencies file in the AML GUI - the file selector window that opens has the wrong file extension selected (*.py) - it should be either <em>.<\/em> or potentially <em>.yml\/<\/em>.yaml - thanks    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/272512-capture3.png?platform=QnA\" alt=\"272512-capture3.png\" \/>    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1671543586710,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1136021\/selecting-the-dependencies-file-in-the-studio-gui",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.4,
        "Challenge_reading_time":6.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Selecting the Dependencies file in the Studio GUI for deploying to an endpoint is bugged - wrong file format specified",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=0ec06fb6-513e-4f5c-9aff-281bc5e44e22\">@Neil McAlister  <\/a>     <\/p>\n<p>Thanks for your waiting due to the holiday season and sorry for the confusion. This buttion is actually for adding <strong>code dependencies<\/strong>, i.e., my scoring script requires dependency.py. For package dependencies, those need to be included in the environment to use for the deployment.    <\/p>\n<p>The button name &quot;Add Dependencies&quot; is confused here, and product team is working on changing the name or add more explanation for this button.     <\/p>\n<p>In the UI here, you must have already created an environment to do a deployment. Providing a conda.yml file in the endpoints create dialog will not register a new environment for you.  The environment must be selected in the bottom section of the page as below -    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/274589-microsoftteams-image-12.png?platform=QnA\" alt=\"274589-microsoftteams-image-12.png\" \/>    <\/p>\n<p>If your environment doesn't exist in this list, or in the custom environments, then you'll need to go to the environments tab on the left side of the portal and create an enviornment.    <\/p>\n<p>I am sorry for the confusion and product team will fix it in the near future.    <\/p>\n<p>I hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly acceept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.2,
        "Solution_reading_time":18.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14,
        "Solution_word_count":204,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Vertex AI managed notebook in <code>europe-west1-d<\/code>. I try both (locally and in the notebook):<\/p>\n<pre><code>gcloud notebooks instances list --location=europe-west1-d\ngcloud compute instances list --filter=&quot;my-notebook-name&quot; --format &quot;[box]&quot;\n<\/code><\/pre>\n<p>and both return nothing. What am I missing?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653555893293,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72389357",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":5.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Why does gcloud not see my managed notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>When creating a Vertex AI managed notebook, it is expected that the instance won't appear on your project. By design, the notebook instance is created in a Google managed project and is not visible to the end user.<\/p>\n<p>But if you want to see instance details of your managed notebooks, you can use the Notebooks API to send a request to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/reference\/rest\/v1\/projects.locations.runtimes\/list\" rel=\"nofollow noreferrer\">runtimes.list<\/a>. See example request:<\/p>\n<pre><code>project=your-project-here\nlocation=us-central1 #adjust based on your location\n\ncurl -X GET -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/notebooks.googleapis.com\/v1\/projects\/${project}\/locations\/${location}\/runtimes&quot;\n<\/code><\/pre>\n<p>Response output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NFjJW.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":16.5,
        "Solution_reading_time":14.21,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":102,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained and deployed a custom vision model via an Azure ML Notebook, following the guide: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/quickstarts\/image-classification?tabs=visual-studio&amp;pivots=programming-language-python\">https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/quickstarts\/image-classification?tabs=visual-studio&amp;pivots=programming-language-python<\/a>  <\/p>\n<p>I'm now trying to send images to the service via the SDK Notebook code: <\/p>\n<pre><code class=\"lang-python\">test_images_folder = '5point2\/frames'\noutput_images_folder = '5point2\/output'\n\n# Load the Arial font from the Matplotlib font library\nfont_path = fm.findfont(fm.FontProperties(family='Arial'))\n\n# Create a PIL ImageFont object using the Arial font\nfont_size = 16\nfont = ImageFont.truetype(font_path, font_size)\n\nfor filename in os.listdir(test_images_folder):\n    if filename.endswith(&quot;.jpg&quot;):\n        image_path = os.path.join(test_images_folder, filename)\n\n        with open(image_path, &quot;rb&quot;) as image_contents:\n            predictor1 = CustomVisionPredictionClient(end_point, pred_key)\n            headers = {'Prediction-Key': pred_key, 'Content-Type': 'application\/octet-stream'}\n            results = predictor1.classify_image(project.id, pub_iter_name, image_contents.read(), headers=headers)\n\n            # Load the image and create a drawing context.\n            im = Image.open(image_path)\n            draw = ImageDraw.Draw(im)\n\n            # Draw the class labels and their probabilities on the image.\n            for prediction in results.predictions:\n                label = prediction.tag_name + &quot;: {0:.2f}%&quot;.format(prediction.probability * 100)\n                draw.text((10, 10 + 20 * results.predictions.index(prediction)), label, fill=&quot;white&quot;, font=font)\n\n            # Save the output image.\n            output_image_path = os.path.join(output_images_folder, filename)\n            im.save(output_image_path)\n\n            # Print the predictions.\n            print(&quot;Predictions for&quot;, filename)\n            for prediction in results.predictions:\n                print(&quot;\\t&quot; + prediction.tag_name + &quot;: {0:.2f}%&quot;.format(prediction.probability * 100))\n\n        # Delay before sending the next image.\n        time.sleep(1)\n\n<\/code><\/pre>\n<p>But I get the following error: <\/p>\n<pre><code>AttributeError                            Traceback (most recent call last)\nInput In [114], in &lt;cell line: 18&gt;()\n     23 predictor1 = CustomVisionPredictionClient(end_point, pred_key)\n     24 headers = {'Prediction-Key': pred_key, 'Content-Type': 'application\/octet-stream'}\n---&gt; 25 results = predictor1.classify_image(project.id, pub_iter_name, image_contents.read(), headers=headers)\n     27 # Load the image and create a drawing context.\n     28 im = Image.open(image_path)\n\nFile \/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/azure\/cognitiveservices\/vision\/customvision\/prediction\/operations\/_custom_vision_prediction_client_operations.py:73, in CustomVisionPredictionClientOperationsMixin.classify_image(self, project_id, published_name, image_data, application, custom_headers, raw, **operation_config)\n     71 # Construct and send request\n     72 request = self._client.post(url, query_parameters, header_parameters, form_content=form_data_content)\n---&gt; 73 response = self._client.send(request, stream=False, **operation_config)\n     75 if response.status_code not in [200]:\n     76     raise models.CustomVisionErrorException(self._deserialize, response)\n\nFile \/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/msrest\/service_client.py:336, in ServiceClient.send(self, request, headers, content, **kwargs)\n    334 kwargs.setdefault('stream', True)\n    335 try:\n--&gt; 336     pipeline_response = self.config.pipeline.run(request, **kwargs)\n    337     # There is too much thing that expects this method to return a &quot;requests.Response&quot;\n    338     # to break it in a compatible release.\n    339     # Also, to be pragmatic in the &quot;sync&quot; world &quot;requests&quot; rules anyway.\n    340     # However, attach the Universal HTTP response\n    341     # to get the streaming generator.\n    342     response = pipeline_response.http_response.internal_response\n\nFile \/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/msrest\/pipeline\/__init__.py:197, in Pipeline.run(self, request, **kwargs)\n    195 pipeline_request = Request(request, context)  # type: Request[HTTPRequestType]\n    196 first_node = self._impl_policies[0] if self._impl_policies else self._sender\n--&gt; 197 return first_node.send(pipeline_request, **kwargs)\n\nFile \/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/msrest\/pipeline\/__init__.py:150, in _SansIOHTTPPolicyRunner.send(self, request, **kwargs)\n    148 self._policy.on_request(request, **kwargs)\n    149 try:\n--&gt; 150     response = self.next.send(request, **kwargs)\n    151 except Exception:\n    152     if not self._policy.on_exception(request, **kwargs):\n\nFile \/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/msrest\/pipeline\/requests.py:65, in RequestsCredentialsPolicy.send(self, request, **kwargs)\n     63 session = request.context.session\n     64 try:\n---&gt; 65     self._creds.signed_session(session)\n     66 except TypeError: # Credentials does not support session injection\n     67     _LOGGER.warning(&quot;Your credentials class does not support session injection. Performance will not be at the maximum.&quot;)\n\nAttributeError: 'str' object has no attribute 'signed_session'\n<\/code><\/pre>\n<p>I have checked all my credentials and they are all correct.   <br \/>\nI am unable to use  azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionEndpoint as I am using Python.   <\/p>\n<p>I assume this is an SDK issue, please assist.   <\/p>\n<p>Thank you. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1680780634250,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1199848\/attributeerror-str-object-has-no-attribute-signed",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":16.6,
        "Challenge_reading_time":74.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":null,
        "Challenge_title":"AttributeError: 'str' object has no attribute 'signed_session' for Python Notebook request to CV API.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":463,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=43306268-160b-4903-9156-e7ca00e7f352\">@Shane Dzartov  <\/a> I think this error is from the <code>msrest<\/code> package rather than the custom vision library. I see that you have not used or imported <code>ApiKeyCredentials<\/code> from <code>msrest<\/code> and instead the keys are directly passed to prediction client which seems to fail the request.<\/p>\n<p>Could you add the following in imports section:<\/p>\n<p><code>from msrest.authentication import ApiKeyCredentials<\/code><\/p>\n<p>And pass your key to <code>ApiKeyCredentials<\/code> and then to the prediction client?<\/p>\n<pre><code class=\"lang-python\">python prediction_credentials = ApiKeyCredentials(in_headers={&quot;Prediction-key&quot;: prediction_key})  \npredictor1 = CustomVisionPredictionClient(end_point, prediction_credentials) \n#Comment the headers declaration since SDK should take care of Content-Type header \nresults = predictor1.classify_image(project.id, pub_iter_name, image_contents.read()) \n\n<\/code><\/pre>\n<p>If this answers your query, do click <code>Accept Answer<\/code> and <code>Yes<\/code> for was this answer helpful. And, if you have any further query do let us know.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.8,
        "Solution_reading_time":15.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":123,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1531231343652,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"St. Louis, MO, USA",
        "Answerer_reputation_count":676.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an AWS endpoint using a Docker container (I followed <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers.html\" rel=\"nofollow noreferrer\">this<\/a>).<\/p>\n<p>Everything is working perfectly but now I need to put it in production and define an auto scaling strategy.<\/p>\n<p>I tried 2 things:<\/p>\n<ol>\n<li><p>AWS console but the auto scaling button is greyed\nout.<\/p>\n<\/li>\n<li><p>The method described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-code-apply.html\" rel=\"nofollow noreferrer\">here<\/a>. My endpoint name\nis <code>EmbeddingEndpoint<\/code> and my variant name is <code>SimpleVariant<\/code>. So my\nfinal command is<\/p>\n<\/li>\n<\/ol>\n<pre><code>aws application-autoscaling put-scaling-policy \\\n--policy-name scalable_policy_for_embedding \\\n--policy-type TargetTrackingScaling \\\n--resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant \\\n--service-namespace sagemaker \\\n--scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n--target-tracking-scaling-policy-configuration file:\/\/policy_config.json\n<\/code><\/pre>\n<p>but I get this result :<\/p>\n<pre><code>An error occurred (ObjectNotFoundException) when calling the PutScalingPolicy operation: \nNo scalable target registered for service namespace: sagemaker, resource ID: \nendpoint\/EmbeddingEndpoint\/variant\/SimpleVariant, scalable dimension: \nsagemaker:variant:DesiredInstanceCount\n<\/code><\/pre>\n<p>Does someone have another solution, or is it that I didn't set the variable well ?\nThank you in advance !<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625051653240,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1630503140507,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68193708",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":19.2,
        "Challenge_reading_time":21.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to Define Auto Scaling for SageMaker Endpoint",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570620624976,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Your <code>sagemaker<\/code> service-namespace does not have any registered scaling targets. You need to first run <code>register-scalable-target<\/code> before running <code>put-scaling-policy<\/code>.<\/p>\n<pre><code>aws application-autoscaling register-scalable-target \\\n    --service-namespace sagemaker \\\n    --scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n    --resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":29.5,
        "Solution_reading_time":6.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":29,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618566783027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1618585069787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":25.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1216.0,
        "Challenge_word_count":251,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369257942212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":70285.0,
        "Poster_view_count":13121.0,
        "Solution_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.1,
        "Solution_reading_time":10.19,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7,
        "Solution_word_count":97,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1468845236196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":55.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>so I've been poking around with sacred a little bit and it seems great. unfortunately I did not find any multiple files use-cases examples like I am trying to implement.<\/p>\n\n<p>so i have this file called configuration.py, it is intended to contain different variables which will (using sacred) be plugged in to the rest of the code (laying in different files):<\/p>\n\n<pre><code>from sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.config\ndef configure_analysis_default():\n    \"\"\"Initializes default  \"\"\"\n    generic_name = \"C:\\\\basic_config.cfg\" # configuration filename\n    message = \"This is my generic name: %s!\" % generic_name\n    print(message)\n\n@ex.automain #automain function needs to be at the end of the file. Otherwise everything below it is not defined yet\n#  when the experiment is run.\ndef my_main(message):\n    print(message)\n<\/code><\/pre>\n\n<p>This by itself works great. sacred is working as expected. However, when I'm trying to introduce a second file named Analysis.py:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n\n@ex.capture\ndef what_is_love(generic_name):\n    message = \" I don't know\"\n    print(message)\n    print(generic_name)\n\n@ex.automain\ndef my_main1():\n    what_is_love()\n<\/code><\/pre>\n\n<p>running Analysis.py yields:<\/p>\n\n<p><strong>Error:<\/strong><\/p>\n\n<blockquote>\n  <p>TypeError: what_is_love is missing value(s) for ['generic_name']<\/p>\n<\/blockquote>\n\n<p>I expected that the 'import configuration' statement to include the configuration.py file, thus importing everything that was configured in there including configure_analysis_default() alongside its decorator @ex.config and then inject it to what_is_love(generic_name).\nWhat am I doing wrong? how can i fix this?<\/p>\n\n<p>Appreciate it!<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513160560930,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1513161714983,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47790619",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":22.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"sacred, python - ex.config in one file and ex",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1446.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468845236196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>So, pretty dumb, but I'll post it here in favour of whoever will have similar issue...<\/p>\n\n<p>My issue is that I have created a different instance of experiment. I needed simply to import my experiment from the configuration file.<\/p>\n\n<p>replacing this:<\/p>\n\n<pre><code>import configuration\nfrom sacred import Experiment\nex = Experiment('Analysis')\n<\/code><\/pre>\n\n<p>with this:<\/p>\n\n<pre><code>import configuration\nex = configuration.ex\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.6,
        "Solution_reading_time":5.76,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":57,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":1594906911350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41528.0,
        "Answerer_view_count":6164.0,
        "Challenge_adjusted_solved_time":138.9138230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are using AWS Sagemaker feature, bring your own docker, where we have inference model written in R. As I understood, batch transform job runs container in a following way:<\/p>\n<pre><code>docker run image serve\n<\/code><\/pre>\n<p>Also, on docker we have a logic to determine which function to invoke:<\/p>\n<pre><code>args &lt;- commandArgs()\nif (any(grepl('train', args))) {\n    train()}\nif (any(grepl('serve', args))) {\n    serve()}\n<\/code><\/pre>\n<p>Is there a way, to override default container invocation so we can pass some additional parameters?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1599220820873,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63740792",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":16.6,
        "Challenge_reading_time":7.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Provide additional input to docker container running inference model",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":702.0,
        "Challenge_word_count":84,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473837223712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgrade",
        "Poster_reputation_count":353.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>As you said, and is indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, Sagemaker will run your container with the following command:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>docker run image serve\n<\/code><\/pre>\n<p>By issuing this command Sagemaker will overwrite any <code>CMD<\/code> that you provide in your container Dockerfile, so you cannot use <code>CMD<\/code> to provide dynamic arguments to your program.<\/p>\n<p>We can think in use the Dockerfile <code>ENTRYPOINT<\/code> to consume some environment variables, but the documentation of AWS dictates that it is preferable use the <code>exec<\/code> form of the <code>ENTRYPOINT<\/code>. Somethink like:<\/p>\n<pre><code>ENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;]\n<\/code><\/pre>\n<p>I think that, for analogy with <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">model training<\/a>, they need this kind of container execution to enable the container to receive termination signals:<\/p>\n<blockquote>\n<p>The exec form of the <code>ENTRYPOINT<\/code> instruction starts the executable directly, not as a child of <code>\/bin\/sh<\/code>. This enables it to receive signals like <code>SIGTERM<\/code> and <code>SIGKILL<\/code> from SageMaker APIs.<\/p>\n<\/blockquote>\n<p>To allow variable expansion, we need to use the <code>ENTRYPOINT<\/code> <code>shell<\/code> form. Imagine:<\/p>\n<pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;, &quot;$ENV_VAR1&quot;]\n<\/code><\/pre>\n<p>If you try to do the same with the <code>exec<\/code> form the variables provided will be treated as a literal and will not be sustituited for their actual values.<\/p>\n<p>Please, see the approved answer of <a href=\"https:\/\/stackoverflow.com\/questions\/37904682\/how-do-i-use-docker-environment-variable-in-entrypoint-array\">this<\/a> stackoverflow question for a great explanation of this subject.<\/p>\n<p>But, one thing you can do is obtain the value of these variables in your R code, similar as when you process <code>commandArgs<\/code>:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>ENV_VAR1 &lt;- Sys.getenv(&quot;ENV_VAR1&quot;)\n<\/code><\/pre>\n<p>To pass environment variables to the container, as indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\"><code>CreateModel<\/code><\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\"><code>CreateTransformJob<\/code><\/a> requests on your container.<\/p>\n<p>You probably will need to include in your Dockerfile <code>ENV<\/code> definitions for every required environment variable on your container, and provide for these definitions default values with <code>ARG<\/code>:<\/p>\n<pre><code>ARG ENV_VAR1_DEFAULT_VALUE=VAL1\nENV_VAR1=$ENV_VAR1_DEFAULT_VALUE\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1599720910636,
        "Solution_link_count":6,
        "Solution_readability":17.2,
        "Solution_reading_time":43.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18,
        "Solution_word_count":313,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":1835.4266388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using SageMaker for distributed TensorFlow model training and serving.  I am trying to get the shape of the pre-processed datasets from the ScriptProcessor so I can provide it to the TensorFlow Environment.<\/p>\n<pre><code>script_processor = ScriptProcessor(command=['python3'],\n                image_uri=preprocess_img_uri,\n                role=role,\n                instance_count=1,\n                sagemaker_session=sm_session,\n                instance_type=preprocess_instance_type)\n\nscript_processor.run(code=preprocess_script_uri,\n                inputs=[ProcessingInput(\n                        source=source_dir + username + '\/' + dataset_name,\n                        destination='\/opt\/ml\/processing\/input')],\n                outputs=[\n                        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n                        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;)\n                ],\n\n                arguments = ['--filepath', dataset_name, '--labels', 'labels', '--test_size', '0.2', '--shuffle', 'False', '--lookback', '5',])\n\npreprocessing_job_description = script_processor.jobs[-1].describe()\n\noutput_config = preprocessing_job_description[&quot;ProcessingOutputConfig&quot;]\nfor output in output_config[&quot;Outputs&quot;]:\n    if output[&quot;OutputName&quot;] == &quot;train_data&quot;:\n        preprocessed_training_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]\n    if output[&quot;OutputName&quot;] == &quot;test_data&quot;:\n        preprocessed_test_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]\n<\/code><\/pre>\n<p>I would like to get the following data:<\/p>\n<pre><code>pre_processed_train_data_shape = script_processor.train_data_shape?\n<\/code><\/pre>\n<p>I am just not sure how to get the value out of the docker container.  I have reviewed the documentation here:<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647581865580,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71522857",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":23.4,
        "Challenge_reading_time":25.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Get Variable from SageMaker Script Processor",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":334.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1478923885800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>There are a few options:<\/p>\n<ol>\n<li><p>Write some data to a text file at <code>\/opt\/ml\/output\/message<\/code>, then call <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/describe-processing-job.html\" rel=\"nofollow noreferrer\">DescribeProcessingJob<\/a> (using Boto3 or the AWS CLI or API) and retrieve the <code>ExitMessage<\/code> value<\/p>\n<pre><code>aws sagemaker describe-processing-job \\\n  --processing-job-name foo \\\n  --output text \\\n  --query ExitMessage\n<\/code><\/pre>\n<\/li>\n<li><p>Add a new output to your processing job and send data there<\/p>\n<\/li>\n<li><p>If your <code>train_data<\/code> is in CSV, JSON, or Parquet then use an <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/s3api\/select-object-content.html\" rel=\"nofollow noreferrer\">S3 Select query<\/a> on <code>train_data<\/code> for it's # of rows\/columns<\/p>\n<pre><code>aws s3api select-object-content \\\n  --bucket foo \\\n  --key 'path\/to\/train_data.csv' \\\n  --expression &quot;SELECT count(*) FROM s3object&quot; \\\n  --expression-type 'SQL' \\\n  --input-serialization '{&quot;CSV&quot;: {}}' \\\n  --output-serialization '{&quot;CSV&quot;: {}}' \/dev\/stdout\n<\/code><\/pre>\n<\/li>\n<\/ol>\n<p>Set <code>expression<\/code> to <code>select * from s3object limit 1<\/code> to get the columns<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1654189401480,
        "Solution_link_count":2,
        "Solution_readability":21.7,
        "Solution_reading_time":16.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4,
        "Solution_word_count":116,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1518706063680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When AzureML creates a python environment and runs <code>pip install<\/code>, I'd like it to use additional non-public indices. Is there a way to do that?<\/p>\n\n<p>I'm running my python script on an AzureML compute. The environment is created from pip requirements as per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-use-environments#conda-and-pip-specification-files\" rel=\"nofollow noreferrer\">docs<\/a>. The script now references a package in a private index. To run the script on a local or build machine I just specify <code>PIP_EXTRA_INDEX_URL<\/code> environment variable with credentials to the index before running <code>pip install -c ...<\/code>. How to enable same functionality on AzureML environment prep process?<\/p>\n\n<p>AzureML docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-use-environments#private-wheel-files\" rel=\"nofollow noreferrer\">suggest<\/a> that I directly supply wheel files instead of package names. That means I have to manually do all the work that pip is built for: identify private packages among other requirements, choose right versions and platform, download them.<\/p>\n\n<p>Ideally, I would have to just write something like this:<\/p>\n\n<pre><code>myenv = Environment.from_pip_requirements(\n    name = \"myenv\",\n    file_path = \"path-to-pip-requirements-file\",\n    extra-index-url = [\"url1\", \"url2\"])\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572610849127,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58659160",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":19.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to specify pip extra-index-url when creating an azureml environment?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":5362.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518706063680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>It appears, there is a <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py#set-pip-option-pip-option-\" rel=\"nofollow noreferrer\"><code>set_pip_option<\/code> method<\/a> in the SDK which sorts out the problem with one single extra-index-url, e.g.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.environment import CondaDependencies\ndep = CondaDependencies.create(pip_packages=[\"pyyaml\", \"param\"])\ndep.set_pip_option(\"--extra-index-url https:\/\/user:password@extra.index\/url\")\n<\/code><\/pre>\n\n<p>Unfortunately, second call to this function replaces the first value with the new one. For the <code>--extra-index-url<\/code> option this logic should be changed in order to support search in more than 2 indices (one public, one private).<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":15.6,
        "Solution_reading_time":11.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":72,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426046529436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":180.0,
        "Answerer_view_count":20.0,
        "Challenge_adjusted_solved_time":207.9795563889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use XGBoost on Sagemaker notebook.<\/p>\n\n<p>I am using <code>conda_python3<\/code> kernel, and the following packages are installed:<\/p>\n\n<ul>\n<li>py-xgboost-mutex<\/li>\n<li>libxgboost<\/li>\n<li>py-xgboost<\/li>\n<li>py-xgboost-gpu<\/li>\n<\/ul>\n\n<p>But once I am trying to import xgboost it fails on import:<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-5-5943d1bfe3f1&gt; in &lt;module&gt;()\n----&gt; 1 import xgboost as xgb\n\nModuleNotFoundError: No module named 'xgboost'\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1591825043587,
        "Challenge_favorite_count":3.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62313532",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":15.1,
        "Challenge_reading_time":7.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"xgboost on Sagemaker notebook import fails",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1532.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1391632571720,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv-Yafo, Israel",
        "Poster_reputation_count":1248.0,
        "Poster_view_count":137.0,
        "Solution_body":"<p>In Sagemaker notebooks  use the below steps <\/p>\n\n<h3>a) If in Notebook<\/h3>\n\n<p>i)  <code>!type python3<\/code><\/p>\n\n<p>ii) Say the above is \/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 for you <\/p>\n\n<p>iii) <code>!\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 -m pip install  xgboost<\/code><\/p>\n\n<p>iv)  <code>import xgboost<\/code><\/p>\n\n<hr>\n\n<h3>b) If using Terminal<\/h3>\n\n<p>i) <code>conda activate conda_python3<\/code><br>\nii) <code>pip install xgboost<\/code><\/p>\n\n<p>Disclaimer :  sometimes the installation would fail with gcc version ,in that case  update pip version before running install<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1592573769990,
        "Solution_link_count":0,
        "Solution_readability":14.0,
        "Solution_reading_time":7.84,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3,
        "Solution_word_count":64,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1357263005087,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Granada Hills, Los Angeles, CA, United States",
        "Answerer_reputation_count":6262.0,
        "Answerer_view_count":428.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm on a Jupyter notebook using Python3 and trying to plot a tree with code like this:<\/p>\n\n<pre><code>import xgboost as xgb\nfrom xgboost import plot_tree\n\nplot_tree(model, num_trees=4)\n<\/code><\/pre>\n\n<p>On the last line I get:<\/p>\n\n<pre><code>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/xgboost\/plotting.py in to_graphviz(booster, fmap, num_trees, rankdir, yes_color, no_color, **kwargs)\n196         from graphviz import Digraph\n197     except ImportError:\n--&gt; 198         raise ImportError('You must install graphviz to plot tree')\n199 \n200     if not isinstance(booster, (Booster, XGBModel)):\n\nImportError: You must install graphviz to plot tree\n<\/code><\/pre>\n\n<p>How do I install graphviz so I can see the plot_tree?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1552345636840,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55112494",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":9.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Install graphiz on AWS Sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":572.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Poster_created_time":1357263005087,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Granada Hills, Los Angeles, CA, United States",
        "Poster_reputation_count":6262.0,
        "Poster_view_count":428.0,
        "Solution_body":"<p>I was finally able to learn that Conda has a package which can install it for you. I was able to get it installed by running the command:<\/p>\n\n<pre><code>!conda install python-graphviz --yes\n<\/code><\/pre>\n\n<p>Note the <code>--yes<\/code> is only needed if the installation needs to verify adding\/changing other packages since the Jupyter notebook is not interactive once it is running.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.8,
        "Solution_reading_time":4.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3,
        "Solution_word_count":59,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1639907005592,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":323.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":1.51305,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Looking at <a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/r_examples\/r_in_sagemaker_processing\/r_in_sagemaker_processing.html\" rel=\"nofollow noreferrer\">this page<\/a> and this piece of code in particular:<\/p>\n<pre><code>import boto3\n\naccount_id = boto3.client(&quot;sts&quot;).get_caller_identity().get(&quot;Account&quot;)\nregion = boto3.session.Session().region_name\n\necr_repository = &quot;r-in-sagemaker-processing&quot;\ntag = &quot;:latest&quot;\n\nuri_suffix = &quot;amazonaws.com&quot;\nprocessing_repository_uri = &quot;{}.dkr.ecr.{}.{}\/{}&quot;.format(\n    account_id, region, uri_suffix, ecr_repository + tag\n)\n\n# Create ECR repository and push Docker image\n!docker build -t $ecr_repository docker\n!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n!aws ecr create-repository --repository-name $ecr_repository\n!docker tag {ecr_repository + tag} $processing_repository_uri\n!docker push $processing_repository_uri\n<\/code><\/pre>\n<p>This is not pure Python obviously? Are these AWS CLI commands? I have used docker previously but I find this example very confusing. Is anyone aware of an end-2-end example of simply running some R job in AWS using sage maker\/docker? Thanks.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639919443300,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70411715",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":16.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"running r script in AWS",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":113,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>This is Python code mixed with shell script magic calls (the <code>!commands<\/code>).<\/p>\n<p>Magic commands aren't unique to this platform, you can use them in <a href=\"https:\/\/ipython.readthedocs.io\/en\/stable\/interactive\/magics.html\" rel=\"nofollow noreferrer\">Jupyter<\/a>, but this particular code is meant to be run on their platform. In what seems like a fairly convoluted way of running R scripts as processing jobs.<\/p>\n<p>However, the only thing you really need to focus on is the R script, and the final two cell blocks. The instruction at the top (don't change this line) creates a file (preprocessing.R) which gets executed later, and then you can see the results.<\/p>\n<p>Just run all the code cells in that order, with your own custom R code in the first cell. Note the line <code>plot_key = &quot;census_plot.png&quot;<\/code> in the last cell. This refers to the image being created in the R code. As for other output types (eg text) you'll have to look up the necessary Python package (PIL is an image manipulation package) and adapt accordingly.<\/p>\n<p>Try this to get the CSV file that the R script is also generating (this code is not validated, so you might need to fix any problems that arise):<\/p>\n<pre><code>import csv\n\ncsv_key = &quot;plot_data.csv&quot;\ncsv_in_s3 = &quot;{}\/{}&quot;.format(preprocessed_csv_data, csv_key)\n!aws s3 cp {csv_in_s3} .\n\nfile = open(csv_key)\ndat = csv.reader(file)\n\ndisplay(dat)\n<\/code><\/pre>\n<p>So now you should have an idea of how two different output types the R script example generates are being handled, and from there you can try and adapt your own R code based on what it outputs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1639924890280,
        "Solution_link_count":1,
        "Solution_readability":7.8,
        "Solution_reading_time":20.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":17,
        "Solution_word_count":247,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"TrainingPipeline needs to be updated to accommodate the `sagemaker.tensorflow.serving.Model` from Tensorflow package.\r\n\r\nRelated Thread: https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1201",
        "Challenge_closed_time":1579559,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578691549000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/aws-step-functions-data-science-sdk-python\/issues\/17",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":20.6,
        "Challenge_reading_time":3.48,
        "Challenge_repo_contributor_count":21.0,
        "Challenge_repo_fork_count":78.0,
        "Challenge_repo_issue_count":190.0,
        "Challenge_repo_star_count":244.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Support Tensorflow with the new sagemaker.tensorflow.serving.Model",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":20,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This PR https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/1252 fixes the issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":18.6,
        "Solution_reading_time":1.07,
        "Solution_score_count":null,
        "Solution_sentence_count":2,
        "Solution_word_count":6,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"### Summary\r\n\r\n[<!-- Summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb)\r\n### Steps to Reproduce it\r\n\r\nUsed Binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         mlflow.whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (Optional). If not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is None:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is None:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise RuntimeError(\"Session is already closed. Cannot create more loggers\")\r\n    175 \r\n    176         # Explicitly set the default timezone to utc if none was provided. Helps with equality testing\r\n\r\nRuntimeError: Session is already closed. Cannot create more loggers\r\n```\r\n### Example\r\n\r\n",
        "Challenge_closed_time":1655127,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641941774000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/411",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":14.3,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow example: close session error",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":192,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The example closes the default session, and then later the mlflow.whylogs wrapper is using that closed session to create loggers. Need to update that example's initial session creation to something like:\r\n```\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session()\r\nsummary = session.profile_dataframe(train, \"training-data\").flat_summary()['summary']\r\n\r\nsummary\r\n``` Still need to update the example to work in Binder better:\r\n* install dependencies\r\n* coinfigure mlflow writer, currently the default session will just write to local disk Part of the reason is that it picks up the default YAML file with default list of writers - and they don't contain mlflow (for obvious reason): https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs.yaml\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/26821974\/149250188-154d5b19-348e-44ea-b64a-0c4724b3c0cd.png)\r\n\r\nHere's my fix in the notebook\r\n\r\nNow this poses interesting quesiton:\r\n* Should mlflow writer be allowed if you don't run mlflow? My instinct is to say yes, but you get a big warning. Or exception?\r\n* If you specify a config without mlflow writer, should we implicitly add mlflow writer? Maybe yes.\r\n\r\nHowever so far I'm not a fan of implicit behaviors because it's freaking hard for us to reason about (see this issue - took a bit of debugging to find out that it's config related). My vote is to throw exception with an option to disable that exception if user chooses the path of ignorance. Drop in the code of the two cells:\r\n\r\n```\r\nconfig = \"\"\"\r\nproject: example-project\r\npipeline: example-pipeline\r\nverbose: false\r\nwriters:\r\n# Save to mlflow\r\n- formats:\r\n    - protobuf\r\n  output_path: mlflow\r\n  type: mlflow\r\n\"\"\"\r\ncfg_file = \"mlflow_config.yaml\"\r\n\r\n!echo \"{config}\" > {cfg_file}\r\n\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session(cfg_file)\r\n\r\nassert whylogs.__version__ >= \"0.1.13\" # we need 0.1.13 or later for MLflow integration\r\nwhylogs.enable_mlflow(session)\r\n``` This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":11.3,
        "Solution_reading_time":25.46,
        "Solution_score_count":null,
        "Solution_sentence_count":15,
        "Solution_word_count":261,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The sample notebooks for <strong>SageMaker Distributed training<\/strong>, like here:\u00a0https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/distributed_tensorflow_mask_rcnn\/mask-rcnn-scriptmode-s3.ipynb rely on the\u00a0<code>docker build .<\/code> and\u00a0<code>docker push .<\/code> commands, which are not available or installable in <a href=\"https:\/\/aws.amazon.com\/sagemaker\/studio\/\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio<\/a>.<\/p>\n<p>Are there alternatives of these notebooks that are compatible with the SageMaker Studio?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662868156347,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73676684",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":17.0,
        "Challenge_reading_time":8.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to run SageMaker Distributed training from SageMaker Studio?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":16.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1389887039672,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":5854.0,
        "Poster_view_count":794.0,
        "Solution_body":"<p>SageMaker Studio does not support Docker, since the Studio apps are containers themselves. You can use the SageMaker Docker Build tool to build docker images from Studio (uses CodeBuild in the backend). See the blog <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks\/\" rel=\"nofollow noreferrer\">Using the Amazon SageMaker Studio Image Build CLI to build container images from your Studio notebooks<\/a> and the <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">Github repo<\/a> for details.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":16.5,
        "Solution_reading_time":8.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":63,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":12.0333325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to construct an <code>Explanation<\/code> object for a unit test, but can't seem to get it to work. Here's what I'm trying:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.compat.types.explanation_v1.Explanation(\n    attributions=aiplatform.compat.types.explanation_v1.Attribution(\n        {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n    )\n)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>&quot;.venv\/lib\/python3.7\/site-packages\/proto\/message.py&quot;, line 521, in __init__\n    super().__setattr__(&quot;_pb&quot;, self._meta.pb(**params))\nTypeError: Value must be iterable\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/413#issuecomment-872094378\" rel=\"nofollow noreferrer\">this<\/a> on github, but I'm not sure how to apply that workaround here.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644344024883,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71038823",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Cannot construct an Explanation object",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519933306780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3576.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>As the error mentioned value to be passed at <code>attributions<\/code> should be <strong>iterable<\/strong>. See <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.Explanation\" rel=\"nofollow noreferrer\">Explanation attributes documentation<\/a>.<\/p>\n<p>I tried your code and placed the <code>Attribution<\/code> object in a list and the error is gone. I assigned your objects in variables just so the code is readable.<\/p>\n<p>See code and testing below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\ntest = {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n\nattributions=aiplatform.compat.types.explanation_v1.Attribution(test)\nx  = aiplatform.compat.types.explanation_v1.Explanation(\n    attributions=[attributions]\n)\nprint(x)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>attributions {\n  baseline_output_value: 0.9280818700790405\n  instance_output_value: 0.6717480421066284\n  feature_attributions {\n    struct_value {\n      fields {\n        key: &quot;feature_1&quot;\n        value {\n          number_value: -0.0410824716091156\n        }\n      }\n      fields {\n        key: &quot;feature_2&quot;\n        value {\n          number_value: 0.01155053575833639\n        }\n      }\n    }\n  }\n  output_index: 0\n  output_display_name: &quot;true&quot;\n  approximation_error: 0.010399332817679649\n  output_name: &quot;scores&quot;\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1644387344880,
        "Solution_link_count":1,
        "Solution_readability":17.4,
        "Solution_reading_time":22.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19,
        "Solution_word_count":111,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation.  I have some initialization to do at creation time (clone a github repo, etc.).  That all works fine.  The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a `Stopped` state.  A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\n```\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n```\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige.  I would have thought it would let me do the same in the lifecycle config.  Unfortunately, SageMaker still has the notebook in the `Pending` state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539775195000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668613505334,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloudformation-with-sagemaker-lifecycleconfig-without-leaving-the-instance-running",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":479.0,
        "Challenge_word_count":208,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"One solutions will be to create a [CFN custom resource](https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/template-custom-resources-lambda.html) backed by lambda.\nYou can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925548143,
        "Solution_link_count":1,
        "Solution_readability":11.9,
        "Solution_reading_time":4.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":40,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/I8c93.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I8c93.png\" alt=\"enter image description here\"><\/a>I am attempting to install Gluonnlp to a sagemaker jupyter notebook. Im using the command <code>!sudo pip3 install gluonnlp<\/code> to install.  Which is successful.  However on import I get <code>ModuleNotFoundError: No module named 'gluonnlp'<\/code><\/p>\n\n<p>I got the same issue when attempting to install mxnet with pip in the same notebook.  It was resolved when I conda installed mxnet instead.  However conda install has not been working for gluonnlp as it cannot find the package.  I can't seem to find a way to conda install gluonnlp.  Any suggestions would be highly appreciated.<\/p>\n\n<p>Here are some of the commands I have tried<\/p>\n\n<p><code>!sudo pip3 install gluonnlp<\/code><\/p>\n\n<p><code>!conda install gluonnlp<\/code> --> Anaconda cant find the package in any channels<\/p>\n\n<pre><code>!conda install pip --y\n!sudo pip3 install gluonnlp\n\n!sudo pip3 install gluonnlp\n\n!conda install -c conda-forge gluonnlp --y\n<\/code><\/pre>\n\n<p>All these commands on my import \nimport warnings<\/p>\n\n<pre><code>warnings.filterwarnings('ignore')\n\nimport io\nimport random\nimport numpy as np\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom bert import data, model\n<\/code><\/pre>\n\n<p>result in the error<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'gluonnlp'\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1564111030610,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1564413127092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57212696",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":9.6,
        "Challenge_reading_time":18.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Gluonnlp installation not found on Sagemaker jupyter notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2015.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>this is as simple as creating a Jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing:<\/p>\n\n<pre><code>!pip install gluonnlp\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.6,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":22,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"### Contact Details [Optional]\n\n_No response_\n\n### System Information\n\nZenml == 0.10.0\n\n### What happened?\n\nZenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### Reproduction steps\n\n1. Create a SageMaker pipeline.\r\n2. Create a s3 artifact store.\r\n3. Run the pipeline\r\n\n\n### Relevant log output\n\n```shell\nCreating run for pipeline: mnist_pipeline\r\nCache enabled for pipeline mnist_pipeline\r\nUsing stack sagemaker_stack to run pipeline mnist_pipeline...\r\nStep importer has started.\r\nUsing cached version of importer.\r\nStep importer has finished in 0.045s.\r\nStep trainer has started.\r\nINFO:botocore.credentials:Found credentials in shared credentials file: ~\/.aws\/credentials\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:752 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    749 \u2502   \u2502   \u2502   \u2502   \u2502   params[\"CreateBucketConfiguration\"] = {          \u2502\r\n\u2502    750 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \"LocationConstraint\": region_name            \u2502\r\n\u2502    751 \u2502   \u2502   \u2502   \u2502   \u2502   }                                                \u2502\r\n\u2502 >  752 \u2502   \u2502   \u2502   \u2502   await self._call_s3(\"create_bucket\", **params)       \u2502\r\n\u2502    753 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(\"\")                            \u2502\r\n\u2502    754 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(bucket)                        \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:302 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    299 \u2502   \u2502   \u2502   except Exception as e:                                   \u2502\r\n\u2502    300 \u2502   \u2502   \u2502   \u2502   err = e                                              \u2502\r\n\u2502    301 \u2502   \u2502   err = translate_boto_error(err)                              \u2502\r\n\u2502 >  302 \u2502   \u2502   raise err                                                    \u2502\r\n\u2502    303 \u2502                                                                    \u2502\r\n\u2502    304 \u2502   call_s3 = sync_wrapper(_call_s3)                                 \u2502\r\n\u2502    305                                                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:282 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    279 \u2502   \u2502   additional_kwargs = self._get_s3_method_kwargs(method, *akwa \u2502\r\n\u2502    280 \u2502   \u2502   for i in range(self.retries):                                \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   try:                                                     \u2502\r\n\u2502 >  282 \u2502   \u2502   \u2502   \u2502   out = await method(**additional_kwargs)              \u2502\r\n\u2502    283 \u2502   \u2502   \u2502   \u2502   return out                                           \u2502\r\n\u2502    284 \u2502   \u2502   \u2502   except S3_RETRYABLE_ERRORS as e:                         \u2502\r\n\u2502    285 \u2502   \u2502   \u2502   \u2502   logger.debug(\"Retryable error: %s\", e)               \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:198 in _make_api_call                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   195 \u2502   \u2502   \u2502   'has_streaming_input': operation_model.has_streaming_inpu \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   'auth_type': operation_model.auth_type,                   \u2502\r\n\u2502   197 \u2502   \u2502   }                                                             \u2502\r\n\u2502 > 198 \u2502   \u2502   request_dict = await self._convert_to_request_dict(           \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   api_params, operation_model, context=request_context)     \u2502\r\n\u2502   200 \u2502   \u2502   resolve_checksum_context(request_dict, operation_model, api_p \u2502\r\n\u2502   201                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:246 in _convert_to_request_dict                            \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   243 \u2502                                                                     \u2502\r\n\u2502   244 \u2502   async def _convert_to_request_dict(self, api_params, operation_mo \u2502\r\n\u2502   245 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      context=None):                 \u2502\r\n\u2502 > 246 \u2502   \u2502   api_params = await self._emit_api_params(                     \u2502\r\n\u2502   247 \u2502   \u2502   \u2502   api_params, operation_model, context)                     \u2502\r\n\u2502   248 \u2502   \u2502   request_dict = self._serializer.serialize_to_request(         \u2502\r\n\u2502   249 \u2502   \u2502   \u2502   api_params, operation_model)                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:275 in _emit_api_params                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502                                                                 \u2502\r\n\u2502   273 \u2502   \u2502   event_name = (                                                \u2502\r\n\u2502   274 \u2502   \u2502   \u2502   'before-parameter-build.{service_id}.{operation_name}')   \u2502\r\n\u2502 > 275 \u2502   \u2502   await self.meta.events.emit(                                  \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   event_name.format(                                        \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   service_id=service_id,                                \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   \u2502   operation_name=operation_name),                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\hooks.py:29 in _emit                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   26 \u2502   \u2502   \u2502   if asyncio.iscoroutinefunction(handler):                   \u2502\r\n\u2502   27 \u2502   \u2502   \u2502   \u2502   response = await handler(**kwargs)                     \u2502\r\n\u2502   28 \u2502   \u2502   \u2502   else:                                                      \u2502\r\n\u2502 > 29 \u2502   \u2502   \u2502   \u2502   response = handler(**kwargs)                           \u2502\r\n\u2502   30 \u2502   \u2502   \u2502                                                              \u2502\r\n\u2502   31 \u2502   \u2502   \u2502   responses.append((handler, response))                      \u2502\r\n\u2502   32 \u2502   \u2502   \u2502   if stop_on_response and response is not None:              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\botoc \u2502\r\n\u2502 ore\\handlers.py:243 in validate_bucket_name                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    240 \u2502   \u2502   \u2502   'Invalid bucket name \"%s\": Bucket name must match '      \u2502\r\n\u2502    241 \u2502   \u2502   \u2502   'the regex \"%s\" or be an ARN matching the regex \"%s\"' %  \u2502\r\n\u2502    242 \u2502   \u2502   \u2502   \u2502   bucket, VALID_BUCKET.pattern, VALID_S3_ARN.pattern)) \u2502\r\n\u2502 >  243 \u2502   \u2502   raise ParamValidationError(report=error_msg)                 \u2502\r\n\u2502    244                                                                      \u2502\r\n\u2502    245                                                                      \u2502\r\n\u2502    246 def sse_md5(params, **kwargs):                                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nParamValidationError: Parameter validation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\run-sagemaker.py:87 in       \u2502\r\n\u2502 <module>                                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   84 \u2502   \u2502   trainer=trainer(),                                             \u2502\r\n\u2502   85 \u2502   \u2502   evaluator=evaluator(),                                         \u2502\r\n\u2502   86 \u2502   )                                                                  \u2502\r\n\u2502 > 87 \u2502   pipeline.run()                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\pipelines\\base_pipeline.py:489 in run                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   486 \u2502   \u2502   self._reset_step_flags()                                      \u2502\r\n\u2502   487 \u2502   \u2502   self.validate_stack(stack)                                    \u2502\r\n\u2502   488 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 489 \u2502   \u2502   return stack.deploy_pipeline(                                 \u2502\r\n\u2502   490 \u2502   \u2502   \u2502   self, runtime_configuration=runtime_configuration         \u2502\r\n\u2502   491 \u2502   \u2502   )                                                             \u2502\r\n\u2502   492                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\stack\\stack.py:595 in deploy_pipeline                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   pipeline=pipeline, runtime_configuration=runtime_configur \u2502\r\n\u2502   593 \u2502   \u2502   )                                                             \u2502\r\n\u2502   594 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 595 \u2502   \u2502   return_value = self.orchestrator.run(                         \u2502\r\n\u2502   596 \u2502   \u2502   \u2502   pipeline, stack=self, runtime_configuration=runtime_confi \u2502\r\n\u2502   597 \u2502   \u2502   )                                                             \u2502\r\n\u2502   598                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:212 in run                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   pipeline=pipeline, pb2_pipeline=pb2_pipeline              \u2502\r\n\u2502   210 \u2502   \u2502   )                                                             \u2502\r\n\u2502   211 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 212 \u2502   \u2502   result = self.prepare_or_run_pipeline(                        \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   sorted_steps=sorted_steps,                                \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   pipeline=pipeline,                                        \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                                \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\local\\local_orchestrator.py:68 in prepare_or_run_pipeline    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   65 \u2502   \u2502                                                                  \u2502\r\n\u2502   66 \u2502   \u2502   # Run each step                                                \u2502\r\n\u2502   67 \u2502   \u2502   for step in sorted_steps:                                      \u2502\r\n\u2502 > 68 \u2502   \u2502   \u2502   self.run_step(                                             \u2502\r\n\u2502   69 \u2502   \u2502   \u2502   \u2502   step=step,                                             \u2502\r\n\u2502   70 \u2502   \u2502   \u2502   \u2502   run_name=runtime_configuration.run_name,               \u2502\r\n\u2502   71 \u2502   \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:316 in run_step                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   313 \u2502   \u2502   # This is where the step actually gets executed using the     \u2502\r\n\u2502   314 \u2502   \u2502   # component_launcher                                          \u2502\r\n\u2502   315 \u2502   \u2502   repo.active_stack.prepare_step_run()                          \u2502\r\n\u2502 > 316 \u2502   \u2502   execution_info = self._execute_step(component_launcher)       \u2502\r\n\u2502   317 \u2502   \u2502   repo.active_stack.cleanup_step_run()                          \u2502\r\n\u2502   318 \u2502   \u2502                                                                 \u2502\r\n\u2502   319 \u2502   \u2502   return execution_info                                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:340 in _execute_step                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   337 \u2502   \u2502   start_time = time.time()                                      \u2502\r\n\u2502   338 \u2502   \u2502   logger.info(f\"Step `{pipeline_step_name}` has started.\")      \u2502\r\n\u2502   339 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 > 340 \u2502   \u2502   \u2502   execution_info = tfx_launcher.launch()                    \u2502\r\n\u2502   341 \u2502   \u2502   \u2502   if execution_info and get_cache_status(execution_info):   \u2502\r\n\u2502   342 \u2502   \u2502   \u2502   \u2502   logger.info(f\"Using cached version of `{pipeline_step \u2502\r\n\u2502   343 \u2502   \u2502   except RuntimeError as e:                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:528 in launch                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   525 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self._pipeline_runtime_spe \u2502\r\n\u2502   526 \u2502                                                                     \u2502\r\n\u2502   527 \u2502   # Runs as a normal node.                                          \u2502\r\n\u2502 > 528 \u2502   execution_preparation_result = self._prepare_execution()          \u2502\r\n\u2502   529 \u2502   (execution_info, contexts,                                        \u2502\r\n\u2502   530 \u2502    is_execution_needed) = (execution_preparation_result.execution_i \u2502\r\n\u2502   531 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    execution_preparation_result.contexts,   \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:388 in _prepare_execution                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   385 \u2502   \u2502   \u2502     output_dict=output_artifacts,                           \u2502\r\n\u2502   386 \u2502   \u2502   \u2502     exec_properties=exec_properties,                        \u2502\r\n\u2502   387 \u2502   \u2502   \u2502     execution_output_uri=(                                  \u2502\r\n\u2502 > 388 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_executor_output_uri(execu \u2502\r\n\u2502   389 \u2502   \u2502   \u2502     stateful_working_dir=(                                  \u2502\r\n\u2502   390 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_stateful_working_director \u2502\r\n\u2502   391 \u2502   \u2502   \u2502     tmp_dir=self._output_resolver.make_tmp_dir(execution.id \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\outputs_utils.py:172 in get_executor_output_uri       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   169 \u2502   \"\"\"Generates executor output uri given execution_id.\"\"\"           \u2502\r\n\u2502   170 \u2502   execution_dir = os.path.join(self._node_dir, _SYSTEM, _EXECUTOR_E \u2502\r\n\u2502   171 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    str(execution_id))                   \u2502\r\n\u2502 > 172 \u2502   fileio.makedirs(execution_dir)                                    \u2502\r\n\u2502   173 \u2502   return os.path.join(execution_dir, _EXECUTOR_OUTPUT_FILE)         \u2502\r\n\u2502   174                                                                       \u2502\r\n\u2502   175   def get_driver_output_uri(self) -> str:                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\d \u2502\r\n\u2502 sl\\io\\fileio.py:80 in makedirs                                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    77                                                                       \u2502\r\n\u2502    78 def makedirs(path: PathType) -> None:                                 \u2502\r\n\u2502    79   \"\"\"Make a directory at the given path, recursively creating parents \u2502\r\n\u2502 >  80   _get_filesystem(path).makedirs(path)                                \u2502\r\n\u2502    81                                                                       \u2502\r\n\u2502    82                                                                       \u2502\r\n\u2502    83 def mkdir(path: PathType) -> None:                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\integrations\\s3\\artifact_stores\\s3_artifact_store.py:275 in makedirs       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502   Args:                                                         \u2502\r\n\u2502   273 \u2502   \u2502   \u2502   path: The path to create.                                 \u2502\r\n\u2502   274 \u2502   \u2502   \"\"\"                                                           \u2502\r\n\u2502 > 275 \u2502   \u2502   self.filesystem.makedirs(path=path, exist_ok=True)            \u2502\r\n\u2502   276 \u2502                                                                     \u2502\r\n\u2502   277 \u2502   def mkdir(self, path: PathType) -> None:                          \u2502\r\n\u2502   278 \u2502   \u2502   \"\"\"Create a directory at the given path.                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:85 in wrapper                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    82 \u2502   @functools.wraps(func)                                            \u2502\r\n\u2502    83 \u2502   def wrapper(*args, **kwargs):                                     \u2502\r\n\u2502    84 \u2502   \u2502   self = obj or args[0]                                         \u2502\r\n\u2502 >  85 \u2502   \u2502   return sync(self.loop, func, *args, **kwargs)                 \u2502\r\n\u2502    86 \u2502                                                                     \u2502\r\n\u2502    87 \u2502   return wrapper                                                    \u2502\r\n\u2502    88                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:65 in sync                                                        \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    62 \u2502   \u2502   # suppress asyncio.TimeoutError, raise FSTimeoutError         \u2502\r\n\u2502    63 \u2502   \u2502   raise FSTimeoutError from return_result                       \u2502\r\n\u2502    64 \u2502   elif isinstance(return_result, BaseException):                    \u2502\r\n\u2502 >  65 \u2502   \u2502   raise return_result                                           \u2502\r\n\u2502    66 \u2502   else:                                                             \u2502\r\n\u2502    67 \u2502   \u2502   return return_result                                          \u2502\r\n\u2502    68                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:25 in _runner                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    22 \u2502   if timeout is not None:                                           \u2502\r\n\u2502    23 \u2502   \u2502   coro = asyncio.wait_for(coro, timeout=timeout)                \u2502\r\n\u2502    24 \u2502   try:                                                              \u2502\r\n\u2502 >  25 \u2502   \u2502   result[0] = await coro                                        \u2502\r\n\u2502    26 \u2502   except Exception as ex:                                           \u2502\r\n\u2502    27 \u2502   \u2502   result[0] = ex                                                \u2502\r\n\u2502    28 \u2502   finally:                                                          \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:767 in _makedirs                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    764 \u2502                                                                    \u2502\r\n\u2502    765 \u2502   async def _makedirs(self, path, exist_ok=False):                 \u2502\r\n\u2502    766 \u2502   \u2502   try:                                                         \u2502\r\n\u2502 >  767 \u2502   \u2502   \u2502   await self._mkdir(path, create_parents=True)             \u2502\r\n\u2502    768 \u2502   \u2502   except FileExistsError:                                      \u2502\r\n\u2502    769 \u2502   \u2502   \u2502   if exist_ok:                                             \u2502\r\n\u2502    770 \u2502   \u2502   \u2502   \u2502   pass                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:758 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502    756 \u2502   \u2502   \u2502   \u2502   raise translate_boto_error(e)                        \u2502\r\n\u2502    757 \u2502   \u2502   \u2502   except ParamValidationError as e:                        \u2502\r\n\u2502 >  758 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"Bucket create failed %r: %s\" % (bu \u2502\r\n\u2502    759 \u2502   \u2502   else:                                                        \u2502\r\n\u2502    760 \u2502   \u2502   \u2502   # raises if bucket doesn't exist and doesn't get create  \u2502\r\n\u2502    761 \u2502   \u2502   \u2502   await self._ls(bucket)                                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nValueError: Bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': Parameter \r\nvalidation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1657782,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657726485000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":154.35,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":246.0,
        "Challenge_repo_issue_count":1160.0,
        "Challenge_repo_star_count":2570.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG]: SageMaker + S3 artifact store fails trying to create a new bucket",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":829,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @danguitavinas,\r\n\r\nI'm guessing from the stack trace that you're running on windows with the local orchestrator? If that's the case, my guess is that this issue should be fixed by #735.\r\n\r\nIf you're interested in trying this, you could install ZenML from that branch using the command `pip install git+https:\/\/github.com\/zenml-io\/zenml.git@bugfix\/windows-source-utils` @schustmi Thank you so much, that worked! Im closing the issue!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":6.9,
        "Solution_reading_time":5.41,
        "Solution_score_count":null,
        "Solution_sentence_count":5,
        "Solution_word_count":62,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,  <\/p>\n<p>I am not able to use my Jupyterlab and Jupyter Notebook. It cannot connect to a Kernel.  <\/p>\n<p>Will this problem be solved when I uninstall Anaconda and install Microsoft Machine Learning Server.  <\/p>\n<p>Thanks,  <\/p>\n<p>Naveen<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610574178773,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/229665\/machine-learning-server",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.1,
        "Challenge_reading_time":3.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"machine learning server",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, I'm assuming you're using your own development environment. I found some <a href=\"https:\/\/jupyter-notebook.readthedocs.io\/en\/stable\/troubleshooting.html\">troubleshooting steps<\/a> that may be helpful. Furthermore, <a href=\"https:\/\/learn.microsoft.com\/en-us\/machine-learning-server\/what-is-machine-learning-server\">Azure Machine Learning Server<\/a> is an enterprise software that provides the tools for performing data science tasks. You can review Azure ML Server and other <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment#local\">options<\/a> to determine which option best suits your data science scenario. Hope this helps.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":13.4,
        "Solution_reading_time":9.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":62,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"**SSL Connection to remote Neptune not working**\r\nI am unable to figure out how can I specify the correct certificate SFSRootCAG2.pem when running queries against SSL-enabled Neptune.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. I set up SSH tunnel via bastion to the Neptune cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -N -L 8182:yourneptuneendpoint:8182_'\r\n2. I start graph-notebook as '_jupyter notebook notebook\/destination_neptune_'. This gives me the output _Jupyter Notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_\r\n3. I open my notebook and run the following magic commands \r\n_'%%graph_notebook_config\r\n{\r\n  \"host\": \"localhost\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"iam_credentials_provider_type\": \"ROLE\",\r\n  \"load_from_s3_arn\": \"\",\r\n  \"aws_region\": <myregion>,\r\n  **\"ssl\": true**\r\n}'_\r\n4. I run the command \r\n_%%sparql        \r\nSELECT * WHERE {?s ?p ?o} LIMIT 1_\r\n\r\n5. It gives me the error\r\n**{'error': SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'localhost\\', port=8182): Max retries exceeded with url: \/sparql (Caused by SSLError(SSLCertVerificationError(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............**\r\n\r\n**Expected behavior**\r\nI expect to be able to connect to a remote neptune that has ssl enabled.\r\n\r\n**Screenshots**\r\nNone\r\n\r\n**Desktop (please complete the following information):**\r\n - macOS 10.15.7 Catalina\r\n - Browser Chrome\r\n - Version 86.0.4240.198 (Official Build) (x86_64)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.None",
        "Challenge_closed_time":1607104,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606948478000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/40",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":20.85,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] No documentation on how to connect local notebook to remote Neptune SSL",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":192,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Looks like we have a missing piece in our walkthrough for connecting to Neptune:\r\n\r\nhttps:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune\r\n\r\nCould you set your hostname from localhost to your Neptune endpoint:\r\n\r\n```\r\n%graph_notebook_host <your endpoint here>\r\n```\r\n\r\nAnd give it a try? I  updated \/etc\/hosts on my Mac and added an alias for localhost as\r\n\r\n> _27.0.0.1       localhost    yourneptuneendpoint_\r\n\r\nFlushed DNS cache.\r\nSet the hostname in Jupyter notebook graph_notebook_config command to **yourneptuneendpoint**.\r\nRan sparql query and it successfully completed.\r\n\r\n\r\n Glad it worked! I have filed an issue for us to expand our documentation to cover the steps you had to take. Closing this but feel free to open or submit a new issue if you need further assistance",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":6.9,
        "Solution_reading_time":9.58,
        "Solution_score_count":null,
        "Solution_sentence_count":9,
        "Solution_word_count":110,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>According to the adoption plan, we need to rebuild everything, there is no quick way to push <a href=\"https:\/\/github.com\/Azure\/Azure-Machine-Learning-Adoption-Framework\">https:\/\/github.com\/Azure\/Azure-Machine-Learning-Adoption-Framework<\/a>    <\/p>\n<p>Am I correct?     <\/p>\n<p>It\u2019s not user friendly if I am not wrong.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656615790997,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/909961\/azure-machine-learning-adoption-framework",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure-Machine-Learning-Adoption-Framework",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ce84de18-8973-44f3-8101-f191f9216b1f\">@Alexandre  <\/a>    <\/p>\n<p>Thanks for reaching out to us, I have answered this question as well in your other post. I think you are talking about move from Studio classc to Designer, please refer to below document:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview<\/a>    <\/p>\n<p>Basically yes for your other thread, you need to rebuild the whole pipeline since we can not copy - paste your orignal structure to Designer.    <\/p>\n<p>I am sorry for the inconveniences since the new studio has a disfferent structure to make this migration not that easy. Please let me know if you have any question during this process, we will provide help.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.3,
        "Solution_reading_time":12.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7,
        "Solution_word_count":126,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nError Building SageMaker Types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerFile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.DeepCopy undefined (type Tag has no field or method DeepCopy)\r\nmake: *** [vet] Error 2**\r\n\r\n**What you expected to happen**:\r\nPackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nImport of sagemaker types in Go Client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n- Operator version (controller image tag): v1.1.0\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Challenge_closed_time":1599678,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592335014000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/122",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":21.73,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Error Building SageMaker Types due to missing types in common\/manual_deepcopy",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":180,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I believe this may be caused due to the generated deepcopy code not being checked in to the v1.1.0 branch. I attempted to backport this code previously but I don't think the go modules ever picked this up for some reason. I might suggest attempting this pinning to the `master` branch rather than `v1.1.0`. The APIs are backwardly compatible (while `master` is still pointed at a `v1.X`).",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.5,
        "Solution_reading_time":4.73,
        "Solution_score_count":null,
        "Solution_sentence_count":4,
        "Solution_word_count":67,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"**Describe the bug**\r\nService Workbench appears to be unable to launch SageMaker notebook instances at all, due to a missing permission for `sagemaker:AddTags`. This seems to also be the case when custom tags aren't included in the workspace configuration.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Install Service Workbench from the latest version.\r\n2. Create a workspace configuration for a SageMaker notebook.\r\n3. Launch a workspace using the new configuration.\r\n4. Wait a few minutes and observe the error.\r\n\r\n**Expected behavior**\r\nExpected the notebook to launch :)\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png)\r\n```\r\nError provisioning environment TestNotebook1. Reason: Errors from CloudFormation: [{LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : The following resource(s) failed to create: [BasicNotebookInstance]. Rollback requested by user.}, {LogicalResourceId : BasicNotebookInstance, ResourceType : AWS::SageMaker::NotebookInstance, StatusReason : User: arn:aws:sts::XXXXXXXXXXXX:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:XXXXXXXXXXXX:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: adee97b7-1c89-47e2-8ca7-5aa374a80004; Proxy: null)}, {LogicalResourceId : IAMRole, ResourceType : AWS::IAM::Role, StatusReason : Resource creation Initiated}, {LogicalResourceId : SecurityGroup, ResourceType : AWS::EC2::SecurityGroup, StatusReason : Resource creation Initiated}, {LogicalResourceId : InstanceRolePermissionBoundary, ResourceType : AWS::IAM::ManagedPolicy, StatusReason : Resource creation Initiated}, {LogicalResourceId : BasicNotebookInstanceLifecycleConfig, ResourceType : AWS::SageMaker::NotebookInstanceLifecycleConfig, StatusReason : Resource creation Initiated}, {LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : User Initiated}]\r\n```\r\n\r\n**Versions (please complete the following information):**\r\n5.2.0\r\n(also replicated on an older 5.0.0 install)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1659686,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658897296000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1018",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":19.7,
        "Challenge_reading_time":33.11,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] SageMaker instances can't be launched due to missing tags permission",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":223,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Further context - this only started happening approx. 32 hours ago. If I had to guess... maybe it should have never worked and something just happened to get 'fixed' in the IAM API yesterday? \ud83d\ude01  Hi @tdmalone is this still an issue? Hi @kcadette, it is, yes:\r\n\r\n```\r\nUser: arn:aws:sts::xxxxxxxxxxxx:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:xxxxxxxxxxxx:notebook-instance\/basicnotebookinstance-lqerepcrnmaw because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: 4f72193d-aa17-41b9-8ed0-ee381686cb5b; Proxy: null)}\r\n```\r\n\r\nIt should be a one-line fix, so I've submitted a PR: https:\/\/github.com\/awslabs\/service-workbench-on-aws\/pull\/1021",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.8,
        "Solution_reading_time":11.05,
        "Solution_score_count":null,
        "Solution_sentence_count":6,
        "Solution_word_count":89,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to run sagemaker locally for tensorflow in script mode. It seems like I cannot pull the docker container. I have ran the code below from a sagemaker notebook instance and everything ran fine. But when running it on my machine it doesn't work.<\/p>\n\n<p>How can I download the container, so I can debug things locally?<\/p>\n\n<pre><code>import os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\n\nhyperparameters = {}\nrole = 'arn:aws:iam::xxxxxxxx:role\/yyyyyyy'\nestimator = TensorFlow(\n    entry_point='train.py',\n    source_dir='.',\n    train_instance_type='local',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>I get this output<\/p>\n\n<pre><code>INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-\nscriptmode-2019-01-28-18-51-57-787\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n\nsubprocess.CalledProcessError: Command 'docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3' returned non-zero exit status 1.\n<\/code><\/pre>\n\n<p>The warning looks like the output you get when using the docker login stuff <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/Registries.html\" rel=\"nofollow noreferrer\">here<\/a>. If I follow these steps to register to the directory with tensorflow container it says login success<\/p>\n\n<pre><code>Invoke-Expression -Command (aws ecr get-login --no-include-email --registry-ids 520713654638 --region eu-west-2)\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\n<\/code><\/pre>\n\n<p>But then I still cannot pull it<\/p>\n\n<pre><code>docker pull 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.11.0-cpu-py3\nError response from daemon: pull access denied for 520713654638.dkr.ecr.eu-west-2.amazonaws.com\/sagemaker-tensorflow-scriptmode, repository does not exist or may require 'docker login'\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1548702401737,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54408673",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.1,
        "Challenge_reading_time":29.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting sagemaker container locally",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2235.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Poster_created_time":1352206833663,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":893.0,
        "Poster_view_count":185.0,
        "Solution_body":"<p>the same sequence works for me locally : 'aws ecr get-login', 'docker login', 'docker pull'. <\/p>\n\n<p>Does your local IAM user have sufficient credentials to pull from ECR? The 'AmazonEC2ContainerRegistryReadOnly' policy should be enough: <a href=\"https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/ecr_managed_policies.html<\/a><\/p>\n\n<p>Alternatively, you can grab the container from Github and build it: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":19.7,
        "Solution_reading_time":9.06,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6,
        "Solution_word_count":52,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"## \ud83d\udc1b Bug Description\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nwhen I do the example:\r\nqrun qrun benchmarks\\GATs\\workflow_config_gats_Alpha158.yaml\r\n\r\nI got the error info:\r\n\r\n\r\n\r\n(py38) D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples>qrun benchmarks\\GATs\\workflow_config_gats_Alpha158_full02.yaml\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [config.py:413] - default_conf: client.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.workflow - [expm.py:31] - experiment manager uri is at file:D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples\\mlruns\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:\/Users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [expm.py:316] - <mlflow.tracking.client.MlflowClient object at 0x0000017B5D406F40>\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [exp.py:260] - Experiment 3 starts running ...\r\n[7724:MainThread](2022-10-14 07:53:34,124) INFO - qlib.workflow - [recorder.py:339] - Recorder 41d40d173e614811bad721127a3204b8 starts running under Experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,140) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,158) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,164) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff --cached`\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"Param value\", MAX_PARAM_VAL_LENGTH, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nThe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nIncorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    mlflow.log_param(\"depth\", 3)\r\n    mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will throw an MlflowException for overwriting a\r\nlogged parameter.\r\n\r\nCorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 3)\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:MainThread](2022-10-14 07:53:35,515) INFO - qlib.GATs - [pytorch_gats_ts.py:81] - GATs pytorch version...\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:100] - GATs parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : LSTM\r\nmodel_path : None\r\nvisible_GPU : 0\r\nuse_GPU : True\r\nseed : None\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:146] - model:\r\nGATModel(\r\n  (rnn): LSTM(158, 64, num_layers=2, batch_first=True, dropout=0.7)\r\n  (transformation): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc_out): Linear(in_features=64, out_features=1, bias=True)\r\n  (leaky_relu): LeakyReLU(negative_slope=0.01)\r\n  (softmax): Softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nThen the program re-run again.\r\nI am wondering how to fix it.\r\nThanks a lot.\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - Qlib version:\r\n - 0.8.6.99'\r\n - Python version:\r\n - 3.8.5\r\n - OS (`Windows`, `Linux`, `MacOS`):\r\n - windows 10\r\n - Commit number (optional, please provide it if you are using the dev version):\r\n\r\n## Additional Notes\r\n\r\n<!-- Add any other information about the problem here. -->\r\n",
        "Challenge_closed_time":1667718,
        "Challenge_comment_count":0,
        "Challenge_created_time":1665708717000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1317",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.5,
        "Challenge_reading_time":92.22,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":null,
        "Challenge_title":"on qrun:\"mlflow.exceptions.MlflowException: Param value .... had length 780, which exceeded length limit of 500 \"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":583,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I had the same problem TT Same for all the example in `benchmarks\/LightGBM`. This is because mlflow limits the length of params since 1.28.0.\r\nWhile waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution. > This is because mlflow limits the length of params since 1.28.0. While waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution.\r\n\r\nThank you for help. Wish you have a good day.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.8,
        "Solution_reading_time":6.36,
        "Solution_score_count":null,
        "Solution_sentence_count":9,
        "Solution_word_count":89,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was following a guide on mounting EFS in SageMaker studio, but when using the following as a notebook cell:<\/p>\n<pre><code>%%sh \n\nsudo mount -t nfs \\\n    -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\\n    172.31.5.227:\/ \\\n    ..\/efs\n\nsudo chmod go+rw ..\/efs\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>sh: 2: sudo: not found\nsh: 7: sudo: not found\n<\/code><\/pre>\n<p>Even in the terminal ('image terminal'), sudo is not found: <code># sudo \/bin\/sh: 1: sudo: not found<\/code><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596811147697,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1596823164336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63304005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"sudo: not found on AWS Sagemaker Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":980.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>I managed to get sudo working in the &quot;System Terminal&quot; instead. The image terminals don't seem to have access to sudo.<\/p>\n<p>Unrelated: But then when I tried to mount EFS onto the SageMaker studio app, it simply failed, saying mount target is not a directory. Looks like I'm not using Sagemaker Studio this year.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1596815281143,
        "Solution_link_count":0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":54,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1431537216680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2243.0,
        "Answerer_view_count":231.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to interact with Azure Batch with python API, in the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azure.batch import BatchServiceClient\nbatch = BatchServiceClient('&lt;mycredential&gt;','https:\/\/&lt;mybatchaccount&gt;.&lt;region&gt;.batch.azure.com')\nnext(batch.job.list())\n<\/code><\/pre>\n<p>This is run in a ML Studio notebook.<\/p>\n<p>However the following error appears: <code>AttributeError: 'str' object has no attribute 'signed_session'<\/code>.<br \/>\nI am taking the url and credentials from my batch console UI:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lc9n4m.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lc9n4m.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As a credential I tried both Primary and Secondary access keys amd &quot;URL&quot; as batch url.<br \/>\nAm I doing anything wrong?<br \/>\nThanks<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622712274873,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1622714028187,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67818831",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":12.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Batch API rising 'AttributeError' in ML notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436006890427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Paesi Bassi",
        "Poster_reputation_count":959.0,
        "Poster_view_count":204.0,
        "Solution_body":"<p><code>&lt;mycredential&gt;<\/code> should not be your bare auth key string. You need to create a shared auth key object.<\/p>\n<pre><code>credentials = batchauth.SharedKeyCredentials(BATCH_ACCOUNT_NAME, BATCH_ACCOUNT_KEY)\nbatch_client = batch.BatchServiceClient(credentials, base_url=BATCH_ACCOUNT_URL)\n<\/code><\/pre>\n<p>Please see the <a href=\"https:\/\/docs.microsoft.com\/azure\/batch\/tutorial-parallel-python\" rel=\"nofollow noreferrer\">Azure Batch Python tutorial<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":17.4,
        "Solution_reading_time":6.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":35,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1408356046196,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bonn, Deutschland",
        "Answerer_reputation_count":594.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to test my service and to do so I deploy it locally and until now everything worked fine. However, for some unrelated reason I was forced to delete all my docker images and since then I'm unable to deploy the service locally. Upon deployment I receive the following error:<\/p>\n\n<blockquote>\n  <p>404 Client Error: Not Found for url:\n  http+docker:\/\/localnpipe\/v1.39\/images\/471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\/json<\/p>\n<\/blockquote>\n\n<p>And also:<\/p>\n\n<blockquote>\n  <p>ImageNotFound: 404 Client Error: Not Found (\"no such image: \n  471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814: No\n  such image:\n  sha256:471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\")<\/p>\n<\/blockquote>\n\n<p>What I did to deploy the model:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.webservice import LocalWebservice\nfrom azureml.core.model import InferenceConfig\n\nws = Workspace.from_config(\"config.json\")\n\ndeployment_config = LocalWebservice.deploy_configuration(port=8890)\n\ninference_config = InferenceConfig(runtime= \"python\", \n                               entry_script=\"score.py\",\n                               conda_file=\"env.yml\")\n\nmodel_box = Model(ws, \"box\")\nmodel_view = Model(ws, \"view_crop\")\nmodel_damage = Model(ws, \"damage_crop\")\n\nservice = Model.deploy(ws, \"test-service\", [model_box, model_view, model_damage], inference_config, deployment_config)\n\nservice.wait_for_deployment(True)\n<\/code><\/pre>\n\n<p>I understand why there is no image present, but I would expect that it is downloaded in that case.<\/p>\n\n<p>Is there a way to force the build process to re-download the docker base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559042269360,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56341012",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":22.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Docker image not found during local deployment (\"no such image\")",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":3685.0,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408356046196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bonn, Deutschland",
        "Poster_reputation_count":594.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>I just found the problem and corresponding solution:<\/p>\n\n<p>I deleted all images but there where still some containers based on deleted images present. Deleting the corresponding container had the desired effect that the docker image is reloaded from the server.<\/p>\n\n<p>You can delete all containers with <code>docker kill $(docker ps -q)<\/code>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.9,
        "Solution_reading_time":4.45,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3,
        "Solution_word_count":51,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"In Azure Machine Learning Service, when we deploy a Model as an AKS Webservice Endpoint, how can we raise exceptions to let the end-user get proper feedback if their API call is unsuccessful? Azure mentions using `azureml.exceptions.WebserviceException` in their documentation. However, how do we use this class to raise exceptions in case the API call cannot be processed properly and the end-user is responsible for it?",
        "Challenge_closed_time":1617344,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617254943000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1413",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":6.6,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to effectively use azureml.exceptions.WebserviceException for efficient REST API Error Management?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@anirbansaha96 can you link to the doc you mention? The only doc I'm aware of about authoring errors is here: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#binary-data\r\n \r\nWhich mentions using AMLResponse in the scoring file.\r\n\r\nAMLResponse will allow the writer of the scoring file to set a custom error message and api response code. Yes `return AMLResponse(\"Message\", status-code)` is what I was looking for. The documentation I was referring to: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.webserviceexception?view=azure-ml-py",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":13.4,
        "Solution_reading_time":7.89,
        "Solution_score_count":null,
        "Solution_sentence_count":6,
        "Solution_word_count":65,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1583491811220,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Baku, Azerbaijan",
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633602279323,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69479488",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":44.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":null,
        "Challenge_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":418.0,
        "Challenge_word_count":296,
        "Platform":"Stack Overflow",
        "Poster_created_time":1583491811220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Baku, Azerbaijan",
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":6.6,
        "Solution_reading_time":11.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13,
        "Solution_word_count":112,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Context\n\nLet's say I have following structure\n\nroot\n\u251c\u2500\u2500model_files\n\u251c\u2500\u2500polyaxonfiles\n\u2502      \u251c\u2500\u2500build.yml\n\u2502      \u2514\u2500\u2500run.yml\n\u251c\u2500\u2500utils\n\u2514\u2500\u2500dockerfiles\n         \u251c\u2500\u2500Dockerfile.0\n         \u2514\u2500\u2500Dockerfile.1\n\n\nmy build looks as simple as:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  context:\n    value: \"{{ globals.run_artifacts_path }}\/uploads\"\n  destination:\n    connection: docker-registry\n    value: machine-learning\/polyaxon-tutorial:1\n\nhubRef: kaniko\n\nRunning this with command\n\npolyaxon run -f polyaxonfiles\/build.yml -u\n\nGives me very much expected error:\n\nError: error resolving dockerfile path: please provide a valid path to a Dockerfile within the build context with --dockerfile\n\nSo the Kaniko expects Dockerfile to exist in root of the context by default. Does the polyaxonfile specification exposes a way to overwrite the default as suggested with error message?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620659662000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1315",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":11.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"(How) Can I pass path to Dockerfile using Kaniko?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @captainCapitalism, next release we will be pushing a new set of guides, including guides for building containers and how to pass a custom dockerfile context.\n\nWe will update this thread as soon as we start updating the documentation's guides section.",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.3,
        "Solution_reading_time":3.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2,
        "Solution_word_count":41,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"I am running a lightly edited version of this pipeline example: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-azureml\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.`\r\n\r\nI am also getting this same warning in other pipelines I make and I cannot figure out what is causing it.\r\n\r\nHere is a slightly reduced MWE for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication\r\nfrom azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\r\nfrom azureml.core.conda_dependencies import CondaDependencies\r\nfrom azureml.core.compute import ComputeTarget, AmlCompute\r\nfrom azureml.core.compute_target import ComputeTargetException\r\nfrom azureml.data import OutputFileDatasetConfig\r\nfrom azureml.pipeline.steps import PythonScriptStep\r\nfrom azureml.pipeline.core import Pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\r\n                                    service_principal_id=os.environ['AML_PRINCIPAL_ID'],\r\n                                    service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = Workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = PythonScriptStep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=RunConfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom azureml.core import Run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = Pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = Experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=True)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nExpected a StepRun object but received <class 'azureml.core.run.Run'> instead.\r\nThis usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\r\nPlease check for package conflicts in your python environment\r\n```\r\n",
        "Challenge_closed_time":1626719,
        "Challenge_comment_count":0,
        "Challenge_created_time":1624292044000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1517",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":15.5,
        "Challenge_reading_time":36.56,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML Pipelines: Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":249,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@afogarty85 can you share the version of SDK you are using? ```\r\nimport azureml\r\nprint(azureml.core.__version__)\r\n1.31.0\r\n``` @afogarty85, I'm unable to reproduce the error you are seeing. Is the pipeline running despite the error\/warning? It is running\/working anyways and indeed -- on a different workspace, I too cannot reproduce it. I am not sure why it is a symptom of the one I am on. I am opening a bug for investigation and will update you when I have a response.  I am also running into this issue with code that was working previously. Had a weekly pipeline scheduled to run at the start of every Monday. It usually took around a couple of minutes  to finish but looking back at some logs it seems like after June 13  runs were taking 100+ hours and most timed out. I tried to manually run the pipeline and hit the exact same issue with Expecting StepRun object, not sure if there was some sort of update around the middle of June to the SDK?\r\n\r\n\r\n***EDIT Had to update the Azure ML SDK along with the azureml-automl-core, azureml-pipeline-core, and azureml-pipeline packages*** I'm sharing the investigation from engineering below. Since this is expected behavior, we will not be fixing it. Hope this helps. \r\n\r\nThis bug is activated if the user has a package version conflict in their local python environment, the PipelineRun.wait_for_completion() method may fail with an error 'Unexpected keyword argument timeout_seconds'. This is because the run rehydration fails and we receive a run object with the wrong type, which doesn't have this argument.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.0,
        "Solution_reading_time":18.91,
        "Solution_score_count":null,
        "Solution_sentence_count":17,
        "Solution_word_count":257,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"Hi, I am trying to install some libraries in Studio Lab which requires root privileges. \r\n\r\nBelow I have run `whoami` to check if I am root user. (I am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nBelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\nI followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\nOn running `su -` , It asks for the password, but we don't have any password for Studio Lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\nCan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nPlease let me know if my query is not clear. ",
        "Challenge_closed_time":1655933,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654778335000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/118",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.06,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get root access in SageMaker Studio Lab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":122,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you for trying Studio Lab. Now Studio Lab does not allow `sudo` (similar issue: https:\/\/github.com\/aws\/studio-lab-examples\/issues\/40#issuecomment-1005305538). We can use `pip` and `conda` instead. Please refer the following issue.\r\n\r\nWhat software do you try to install? Some libraries will be available in `conda-forge` . Here is the sample of search.\r\n\r\nhttps:\/\/anaconda.org\/search?q=gym Thanks for the reply, I will try to explain the issue.\r\n I am trying to run bipedal robot from Open-ai gym. \r\n```\r\n!pip install gym\r\n!apt-get update\r\n!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> \/dev\/null\r\n!apt-get install xvfb\r\n!pip install pyvirtualdisplay \r\n!pip -q install pyglet\r\n!pip -q install pyopengl\r\n!apt-get install swig\r\n!pip install box2d box2d-kengz\r\n!pip install pybullet\r\n```\r\nThese are the libraries which I need to install for the code to work. \r\nIt works fine on google-colab: (screenshot below)\r\n![colab_gym_ss](https:\/\/user-images.githubusercontent.com\/91401599\/173034039-1973ee00-6c0b-4f49-adad-9bb324c59b8c.png)\r\n\r\nBut it throws error when I run it on SageMaker Studio Lab: (screenshot below)\r\n![sagemaker1](https:\/\/user-images.githubusercontent.com\/91401599\/173034679-f49340dc-de46-42bb-be1b-7c7e3616dac4.png)\r\n\r\nError Log (I have made gym_install.sh file which installs everything described above, I am running it below.): \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ sh gym_install.sh\r\nRequirement already satisfied: gym in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (0.24.1)\r\nRequirement already satisfied: gym-notices>=0.0.4 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (0.0.7)\r\nRequirement already satisfied: numpy>=1.18.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (1.22.4)\r\nRequirement already satisfied: cloudpickle>=1.2.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (2.1.0)\r\nReading package lists... Done\r\nE: List directory \/var\/lib\/apt\/lists\/partial is missing. - Acquire (13: Permission denied)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nRequirement already satisfied: pyvirtualdisplay in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.0)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nCollecting box2d\r\n  Using cached Box2D-2.3.2.tar.gz (427 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting box2d-kengz\r\n  Using cached Box2D-kengz-2.3.3.tar.gz (425 kB)\r\n  Preparing metadata (setup.py) ... done\r\nBuilding wheels for collected packages: box2d, box2d-kengz\r\n  Building wheel for box2d (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d\r\n  Running setup.py clean for box2d\r\n  Building wheel for box2d-kengz (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d-kengz\r\n  Running setup.py clean for box2d-kengz\r\nFailed to build box2d box2d-kengz\r\nInstalling collected packages: box2d-kengz, box2d\r\n  Running setup.py install for box2d-kengz ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Running setup.py install for box2d-kengz did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [18 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running install\r\n      \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n        warnings.warn(\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: legacy-install-failure\r\n\r\n\u00d7 Encountered error while trying to install package.\r\n\u2570\u2500> box2d-kengz\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for output from the failure.\r\nRequirement already satisfied: pybullet in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.2.5)\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\n### To be specific I am getting error in this line:\r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ apt-get install xvfb\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\n```\r\nSo as mentioned by you I tried to find xvfb in conda-forge, but couldn't find it. \r\nThere are some wrapper xvfb on conda-forge but installing them didn't help with the error. \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ python check.py\r\nTraceback (most recent call last):\r\n  File \"\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/GM_\/ARS_src\/check.py\", line 9, in <module>\r\n    display = Display(visible=0, size=(1024, 768))\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/display.py\", line 54, in __init__\r\n    self._obj = cls(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/xvfb.py\", line 44, in __init__\r\n    AbstractDisplay.__init__(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/abstractdisplay.py\", line 85, in __init__\r\n    helptext = get_helptext(program)\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/util.py\", line 13, in get_helptext\r\n    p = subprocess.Popen(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 966, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 1842, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\nTo reproduce above error run below code (check.py) :->\r\n```\r\nimport os\r\nimport numpy as np\r\nimport gym\r\nfrom gym import wrappers\r\nimport pyvirtualdisplay\r\nfrom pyvirtualdisplay import Display\r\n\r\nif __name__ == \"__main__\":\r\n    display = Display(visible=0, size=(1024, 768))\r\n```\r\n Thank you for sharing the error message. Studio Lab does not allow `apt install` so that the `gym_install.sh` does not work straightly. We have to prepare the environment by `conda` and `pip`.\r\n\r\nAt first we can install `swig` from `conda`. We can not install `xvfb` from `conda`, is this necessary to run the code?  Basically I have to capture the video output using `Display` of `pyvirtualdisplay` library. Everything else works. \r\nAs you can see `display = Display(visible=0, size=(1024, 768))` this line throws error  `FileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'` , so I am trying to install `Xvfb`.\r\n\r\nThanks, @ar8372 Sorry for the late reply. I raised the #124 to work OpenAI Gym in Studio Lab. Please comment to #124 if you have additional information. We need your insight to solve the issue. If you do not mind, please close this issue to suppress the duplication of issues.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":11.2,
        "Solution_reading_time":134.31,
        "Solution_score_count":null,
        "Solution_sentence_count":129,
        "Solution_word_count":1049,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Working on deployment of 170 ml models using ML studio and azure Kubernetes service which is referred on the  below doc  link &quot;https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-deploy-azure-kubernetes-service.md&quot;.     <\/p>\n<p>We are training the model using python script with the custom environment and we are registering the ml model on the  Azure ML services. Once we register the mode we are deploying it on the AKS by using the container images.     <\/p>\n<p>While deploying the ML model we are able to deploy up 10 to 11 models per pods for each Node in AKS. When we try to deploy the model on the same node we are getting deployment timeout error and we are getting the below error message.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/129300-image-2021-09-04t13-25-12-512z.png?platform=QnA\" alt=\"129300-image-2021-09-04t13-25-12-512z.png\" \/>    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630746472547,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/540001\/how-many-models-can-be-deployed-in-single-node-in",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"how many models can be deployed in single node in azure kubernetes service?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=f023f08d-7d4a-4ac5-ba62-e9d37f7e7c70\">@suvedharan  <\/a>     <\/p>\n<p>The number of models to be deployed is limited to 1,000 models per deployment (per container).    <\/p>\n<p>Autoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">more details<\/a>    <\/p>\n<p>If the Answer is helpful, please click <code>Accept Answer<\/code> and <strong>up-vote<\/strong>, so that it can help others in the community looking for help on similar topics.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.2,
        "Solution_reading_time":9.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":86,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1261400320736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Antwerp, Belgium",
        "Answerer_reputation_count":7876.0,
        "Answerer_view_count":924.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I run <code>mlflow ui<\/code> the following error occurred:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\gunicorn.exe\\__main__.py\", line 5, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\wsgiapp.py\", line 9, in &lt;module&gt;\n    from gunicorn.app.base import Application\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\base.py\", line 12, in &lt;module&gt;\n    from gunicorn import util\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\util.py\", line 9, in &lt;module&gt;\n    import fcntl\nModuleNotFoundError: No module named 'fcntl'\nTraceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\cli.py\", line 131, in ui\n    mlflow.server._run_server(file_store, file_store, host, port, 1)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 48, in _run_server\n    env=env_map, stream_output=True)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\utils\\process.py\", line 38, in exec_cmd\n    raise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\nmlflow.utils.process.ShellCommandException: Non-zero exitcode: 1\n<\/code><\/pre>\n\n<p>I used anaconda + python 3.6.5 and I installed git and set path with <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> and <code>C:\\Program Files\\Git\\cmd<\/code>.<\/p>\n\n<p>I installed <code>mlflow<\/code> whit <code>pip install mlflow<\/code> and its version is 0.2.1.<\/p>\n\n<p>I set a variable with name <code>GIT_PYTHON_GIT_EXECUTABLE<\/code> and value <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> in Environment Variables. <\/p>\n\n<p>How can I solve this?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1531546446273,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1531837117032,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51335594",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.0,
        "Challenge_reading_time":34.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":null,
        "Challenge_title":"Error with \"mlflow ui\" when trying to run it on MS Windows",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":4688.0,
        "Challenge_word_count":235,
        "Platform":"Stack Overflow",
        "Poster_created_time":1308552848512,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1177.0,
        "Poster_view_count":144.0,
        "Solution_body":"<p><a href=\"https:\/\/github.com\/databricks\/mlflow\" rel=\"nofollow noreferrer\">mlflow documentation<\/a> already says that <\/p>\n\n<blockquote>\n  <p>Note 2: We <strong>do not currently support running MLflow on Windows<\/strong>.\n  Despite this, we would appreciate any contributions to make MLflow\n  work better on Windows.<\/p>\n<\/blockquote>\n\n<p>You're hitting <code>fcntl<\/code> problem: it's not available on MS Windows platform because it's a \"wrapper\" around the <a href=\"http:\/\/man7.org\/linux\/man-pages\/man2\/fcntl.2.html\" rel=\"nofollow noreferrer\">fcntl function<\/a> that's available on POSIX-compatible systems. (See <a href=\"https:\/\/stackoverflow.com\/a\/1422436\/236007\">https:\/\/stackoverflow.com\/a\/1422436\/236007<\/a> for more details.)<\/p>\n\n<p>Solving this requires modifying the source code of mlflow accordingly. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":12.5,
        "Solution_reading_time":10.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7,
        "Solution_word_count":80,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Since the latest Azure ML release, we have been unable to submit any job using a private docker registry. Same jobs were working before the new release.  <br \/>\nWe configure the job as follows (all of this is automated and the code has not changed):<\/p>\n<p>base_image_name = 'REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_1601582281'<\/p>\n<pre><code># Set the container registry information  \n\nmyenv = Environment(name=&amp;#34;lb&amp;#34;)  \n\nmyenv.docker.enabled = True  \nmyenv.docker.base_image = base_image_name  \nmyenv.docker.base_image_registry.address = &amp;#39;REDACTED.azurecr.io\/lb\/&amp;#39;  \nmyenv.docker.base_image_registry.username, myenv.docker.base_image_registry.password = get_docker_secrets()  \nmyenv.python.user_managed_dependencies = True  \n\nmyenv.python.interpreter_path = &amp;#34;\/opt\/miniconda\/bin\/python&amp;#34;  \n<\/code><\/pre>\n<p>Instead of successful job submission, we are instead getting:  <br \/>\n{  <br \/>\n&quot;error&quot;: {  <br \/>\n&quot;message&quot;: &quot;Activity Failed:\\n{\\n \\&quot;error\\&quot;: {\\n \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n \\&quot;message\\&quot;: \\&quot;Unable to get image details : Specified base docker image REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_16\\&quot;,\\n \\&quot;details\\&quot;: []\\n },\\n \\&quot;correlation\\&quot;: {\\n \\&quot;operation\\&quot;: null,\\n \\&quot;request\\&quot;: \\&quot;c41448d429f9c80b\\&quot;\\n },\\n \\&quot;environment\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;location\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;time\\&quot;: \\&quot;2020-11-09T21:40:39.699533Z\\&quot;,\\n \\&quot;componentName\\&quot;: \\&quot;execution-worker\\&quot;\\n}&quot;  <br \/>\n}  <br \/>\n}  <br \/>\nThe image has not changed (we tried a few different ones from prior successful jobs) and the use of the SDK has not changed.  <br \/>\nHas anybody else encountered a similar problem since the Nov 5 upgrade (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes<\/a>)?  <br \/>\nThis is a major block as we cannot proceed with any project that depend on Azure ML at this time.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":4,
        "Challenge_created_time":1604958506767,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/157021\/unable-to-use-private-docker-registry-with-latest",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":14.5,
        "Challenge_reading_time":29.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to use private docker registry with latest Azure ML release",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":191,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d02d6aeb-d51f-460e-9be5-e8da649952cc\">@Fabien Campagne  <\/a>  Thanks for the details, with fully qualified base image name you do not need to specify container registry address. container registry address itself should be just a host name.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.3,
        "Solution_reading_time":3.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":34,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>I have a Kubernetes Service attached as an inference cluster to an azure machine learning workspace. I have deployed multiple models to that the AKS service, each with their own endpoints. I plan to configure this such that I just need to send the request to one main endpoint, which after applying some conditions, will redirect the request to one of the endpoints (e.g. redirect the request to the appropriate model). Are there any best practices to approach this problem?    <\/p>\n<p>There seems to be an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router\">Azure ML router using azureml-fe<\/a> that does something similar, but I cannot find any documentation about it.     <\/p>\n<p>Thanks,    <br \/>\nLawrence    <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1601043259400,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/107990\/best-practices-for-routing-requests-within-inferen",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":8.4,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Best Practices for Routing Requests within Inference Clusters",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":118,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=888ae66b-34ae-4a26-b896-abb53c4a8a32\">@Lawrence Wong  <\/a> ,    <br \/>\nWe do have a solution for this in private preview (called Many Models solution accelerator).    <br \/>\nPlease send your email id to AzCommunity[at]microsoft[dot]com). Include title and link to this thread in the email (and reply here once you do for faster response) and we can take the conversation from there.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.8,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":56,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1536318818623,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"\u0130zmit, Kocaeli, T\u00fcrkiye",
        "Answerer_reputation_count":1033.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute the Azure ml sdk from the local system using the Jupyter notebook. When I run the below code i am getting an error.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\n\nModuleNotFoundError: No module named 'ruamel' \n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622646368700,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67807756",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":4.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"ModuleNotFoundError: No module named 'ruamel' when excuting from azureml.core",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Poster_created_time":1599816833352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New Delhi, Delhi, India",
        "Poster_reputation_count":329.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>You have to add pip 20.1.1<\/p>\n<p>Conda ruamel needs higher version of pip<\/p>\n<pre><code>conda install pip=20.1.1\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":17,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1306431192840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Rafael, CA, USA",
        "Answerer_reputation_count":5369.0,
        "Answerer_view_count":637.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Thanks for getting back to me.<\/p>\n\n<p>Basically I subscribed to a Cluster API service (cortana analytics). This is the sample application as per Microsoft Machine Learning site<\/p>\n\n<p><a href=\"http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx\" rel=\"nofollow\">http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx<\/a><\/p>\n\n<p>As you could see there are 2 arguments to be passed on<\/p>\n\n<p>Input<\/p>\n\n<p>K<\/p>\n\n<p>Where input could be 10;5;2,18;1;6,7;5;5,22;3;4,12;2;1,10;3;4 (each row is separated by semi colon)<\/p>\n\n<p>And K is cluster number: 5 (for example)<\/p>\n\n<p>So to consume this API I use PowerBI Edit Query, <\/p>\n\n<p>So go to Get Data > More > Azure > Microsoft Data MarketPlace, I can see the list of APIs I subscribed to, one of them is the one I referred to in the link above.<\/p>\n\n<p>So I load that as Function lets called it \"Score\"<\/p>\n\n<p>Then I got energy table which I loaded in from a csv file, I want to cluster energy consumption into 5 clusters.<\/p>\n\n<p>So my data layout is<\/p>\n\n<p>Year   Energy<\/p>\n\n<p>2001   6.28213<\/p>\n\n<p>2002  14.12845<\/p>\n\n<p>2003   5.55851<\/p>\n\n<p>and so on, lets say I got 100 rows of the data.<\/p>\n\n<p>So I tried to pass \"6.28213;14.12845;5.55851\", \"5\" to Score function but I dont know how to <\/p>\n\n<ol>\n<li><p>Convert my table into records<\/p><\/li>\n<li><p>pass 2 argument records and constant value 5 as K.<\/p><\/li>\n<\/ol>\n\n<p>Hope this makes sense.<\/p>\n\n<p>Please help! :)<\/p>\n\n<p>Thank you in advance.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476795601683,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40108999",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":19.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Consume Microsoft Cluster API using PowerBI",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":137.0,
        "Challenge_word_count":215,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427899203630,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":403.0,
        "Poster_view_count":200.0,
        "Solution_body":"<p>To convert a column of numbers into a semicolon delimited text, do this to your table:<\/p>\n\n<ol>\n<li>Convert your Energy column is type text.<\/li>\n<li>Add <code>[Energy]<\/code> after the name of your table, which gives you a list of the numbers.<\/li>\n<li>Use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/mt253358.aspx\" rel=\"nofollow\"><code>Text.Combine<\/code><\/a> to turn the list into a text value seperated by <code>;<\/code><\/li>\n<\/ol>\n\n<p>Here's a mashup that does that:<\/p>\n\n<pre><code>let\n    Source = Table.FromRows(Json.Document(Binary.Decompress(Binary.FromText(\"NcjBCQAgDAPAXfKWYqKR7iLdfw1F8J63N9Q70bBCKQ5Ue6VbnEHl9L9xz2GniaoD\", BinaryEncoding.Base64), Compression.Deflate)), let _t = ((type text) meta [Serialized.Text = true]) in type table [Year = _t, Energy = _t]),\n    #\"Changed Type\" = Table.TransformColumnTypes(Source,{{\"Year\", Int64.Type}, {\"Energy\", type text}}),\n    Custom1 = #\"Changed Type\"[Energy],\n    Custom2 = Text.Combine(Custom1, \";\")\nin\n    Custom2\n<\/code><\/pre>\n\n<hr>\n\n<p>Once you have a function, you'll invoke it like <code>YourFunction(Custum2, 5)<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":13.3,
        "Solution_reading_time":14.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":108,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1659144422092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":166.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>l'am running a ipynb file on sagemaker, however the error of occurs.\nl have used 'pip install tqdm' in terminals to install the tqdm so l've no idea what's happening. Is it running in a different environment?\nThanks for any answer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bO6zS.png\" rel=\"nofollow noreferrer\">error report from my ipynb file <\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5tP1J.png\" rel=\"nofollow noreferrer\">what l've done in terminal<\/a><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659151516027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1659151556683,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73172644",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":6.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Running ipynb file, but can't import a certain module",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1658300099523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>There is a possibility you may be executing &quot;pip&quot; in a different environment.<\/p>\n<p>Try executing &quot;!pip install tqdm&quot; or &quot;!pip3 install tqdm&quot; as a code cell in the Sagemaker document itself.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.9,
        "Solution_reading_time":2.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":31,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1459625723203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Grenoble, France",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My final purpose is to run experiment from  an Api.<\/p>\n\n<p>the experiment come from :\n<a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2<\/a>\nbut export the file in my custom git where I clone it, in the image below -><\/p>\n\n<p>I have 2 images in my docker compose :\ntree project : <\/p>\n\n<pre><code>|_app\/\n| |_Dockerfile\n|\n|_mlflow\/\n| |_Dockerfile\n|\n|_docker-compose.yml\n\n<\/code><\/pre>\n\n<p>app\/Dockerfile<\/p>\n\n<pre><code>FROM continuumio\/anaconda3\n\nENV APP_HOME .\/\nWORKDIR ${APP_HOME}\nRUN conda config --append channels conda-forge\nRUN conda install --quiet --yes \\\n    'mlflow' \\\n    'psycopg2' \\\n    'tensorflow'\nRUN pip install pylint\nRUN pwd;ls \\\n&amp;&amp; git clone https:\/\/github.com\/MChrys\/QuickSign.git \nRUN pwd;ls \\\n    &amp;&amp; cd QuickSign \\\n    &amp;&amp; pwd;ls\n\nCOPY . .\n\n#RUN conda install jupyter \n#CMD jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root --no-browser\nCMD cd QuickSign &amp;&amp; mlflow run .\n<\/code><\/pre>\n\n<p>mlflow\/Dockerfile<\/p>\n\n<pre><code>FROM python:3.7.0\n\nRUN pip install mlflow\n\nRUN mkdir \/mlflow\/\n\nCMD mlflow server \\\n    --backend-store-uri \/mlflow \\\n    --host 0.0.0.0\n<\/code><\/pre>\n\n<p>docker-compose.yml<\/p>\n\n<pre><code>version: '3'\nservices:\n  notebook:\n    build:\n      context: .\/app\n    ports:\n      - \"8888:8888\"\n    depends_on: \n      - mlflow\n    environment: \n      MLFLOW_TRACKING_URI: 'http:\/\/mlflow:5000'\n  mlflow:\n    build:\n      context: .\/mlflow\n    expose: \n      - \"5000\"\n    ports:\n      - \"5000:5000\"\n<\/code><\/pre>\n\n<p>when I <code>docker-compose up<\/code> the image I obtain  :<\/p>\n\n<pre><code>notebook_1_74059cdc20ce |     response = requests.request(**kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/api.py\", line 60, in request\nnotebook_1_74059cdc20ce |     return session.request(method=method, url=url, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\nnotebook_1_74059cdc20ce |     resp = self.send(prep, **send_kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\nnotebook_1_74059cdc20ce |     r = adapter.send(request, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\nnotebook_1_74059cdc20ce |     raise ConnectionError(e, request=request)\nnotebook_1_74059cdc20ce | requests.exceptions.ConnectionError: HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/create (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fd5db4edc50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\n<\/code><\/pre>\n\n<p>The problem look like that I run a project which is not found in the server images, as I run it in the app image, but I don't know how figure it out I have to trigger  the experiment from a futur flask app <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578475698630,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1578477326563,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59642900",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":11.0,
        "Challenge_reading_time":38.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to connect Mlflow server to my mlflow project image",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2937.0,
        "Challenge_word_count":289,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459625723203,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Grenoble, France",
        "Poster_reputation_count":56.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>The problem came from  docker for windows, I was unable to make working docker compose on it but there are no problem to build it when I run it on virtual machine with ubuntu.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":14.2,
        "Solution_reading_time":2.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1,
        "Solution_word_count":34,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nHi, I\u2019m trying to deploy Polyaxon CE with helm\/argocd and the polyaxon-api keeps logging in loop:\n\nSystem check identified some issues:\n\nPreparing...\n\n\nand the pod never becomes ready.\nAny idea what could cause that?\n\nMore\n\nIt seems the polyaxon helm chart installation always fails with the message:\n\nError: timed out waiting for the condition\nThis is what I see:\n\npolyaxon-polyaxon-api-d8d7c8b5f-spsb2           1\/1     Running   0          3d5h\npolyaxon-polyaxon-api-dbb84b79c-6jqm9           0\/1     Running   2          13m\n\n\nThe polyaxon api pods take a long time to become ready. It fails liveness probe:\n\nEvents:\n  Type     Reason     Age                   From               Message\n  ----     ------     ----                  ----               -------\n  Normal   Scheduled  15m                   default-scheduler  Successfully assigned polyaxon\/polyaxon-polyaxon-api-dbb84b79c-6jqm9 to ip-192-168-165-30.us-east-2.compute.internal\n  Normal   Pulling    14m                   kubelet            Pulling image \"polyaxon\/polyaxon-api:xx\"\n  Normal   Pulled     14m                   kubelet            Successfully pulled image \"polyaxon\/polyaxon-api:xx\" in 33.387487209s\n  Normal   Created    14m                   kubelet            Created container polyaxon-api\n  Normal   Started    14m                   kubelet            Started container polyaxon-api\n  Normal   Killing    8m49s                 kubelet            Container polyaxon-api failed liveness probe, will be restarted\n  Warning  Unhealthy  8m37s (x10 over 13m)  kubelet            Readiness probe failed: Get \"[http:\/\/192.168.184.124:80\/healthz\/](http:\/\/192.168.184.124\/healthz\/)\": dial tcp 192.168.184.124:80: connect: connection refused\n  Normal   Pulled     8m19s                 kubelet            Container image \"polyaxon\/polyaxon-api:1.9.5\" already present on machine\n  Warning  Unhealthy  4m49s (x16 over 13m)  kubelet            Liveness probe failed: Get \"[http:\/\/192.168.184.124:80\/healthz\/](http:\/\/192.168.184.124\/healthz\/)\": dial tcp 192.168.184.124:80: connect: connection refused",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649328918000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1473",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":10.4,
        "Challenge_reading_time":22.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Polyaxon CE fresh deployment never finishes",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":196,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The firts step is to cehck if you are using the --wait flag with Helm, if yes the deployment will not pass because there's helm hook.\n\nThe reason of the deadlock: helm waits forever for API to be healthy and the API is waiting for the hook to initialize the database.\n\nMore info about the helm deadlock issue: helm\/helm#5118\n\nFor ArgoCD, the flag is set to true automatically: argoproj\/argo-cd#6880\n\nFor the Terraform provider for helm release, the flag is set to true by default: https:\/\/registry.terraform.io\/providers\/hashicorp\/helm\/latest\/docs\/resources\/release#wait and should be set it to false.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.1,
        "Solution_reading_time":7.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":92,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1545360696800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Earth",
        "Answerer_reputation_count":1011.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":6002.4414652778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I started with SageMaker recently, and I'm loving it. However, I've been installing the same libraries over and over again to one of the in-built conda environments, and I want to create a life cycle configuration to do that automatically on startup. based on <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">the bottom of this<\/a>:<\/p>\n<blockquote>\n<p>notebook instance lifecycle configurations are available when you create a new notebook instance.<\/p>\n<\/blockquote>\n<p>the trouble is, I already have a notebook I've been working in for a while. Is there any way to apply a life cycle configuration on startup to an already existing notebook?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597107545867,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1597107925232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63350039",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Add lifecycle configuration to existing notebook in SageMaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":395.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>You need to shut the instance down, then you can edit it. Then, if you use your eyes (which I neglected to do) you can see the &quot;Additional Configurations&quot; section contains lifecycle configurations<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1618716714507,
        "Solution_link_count":0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2,
        "Solution_word_count":33,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"According to the doc ( https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/smd_model_parallel_general.html ), there are different parameters depending on the version of `smdistributed-modelparallel` module \/ package. However, I am unable to find a way to check the version (e.g. via sagemaker python SDK) or just from the training container documentation (e.g. https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md#huggingface-training-containers ).\n\nAny idea?\n\nThanks!",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660805972479,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668434749756,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUsfpWY8CuRsiyHg_x7qyJzw\/how-to-check-smdistributed-modelparallel-version",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":20.2,
        "Challenge_reading_time":7.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to check smdistributed-modelparallel version?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Have not yet found a programmatic way to check the version.\n\nHowever, for each DLC (Deep Learning Container) available at \nhttps:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md , we can look at the corresponding docker build files.\n\nE.g. for `PyTorch 1.10.2 with HuggingFace transformers` DLC, the corresponding dockerfile is here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/huggingface\/pytorch\/training\/docker\/1.10\/py3\/cu113\/Dockerfile.gpu\n\nAnd we can see that the version: `smdistributed_modelparallel-1.8.1-cp38-cp38-linux_x86_64.whl`.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1660807962126,
        "Solution_link_count":2,
        "Solution_readability":16.1,
        "Solution_reading_time":7.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":52,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use WanDB but when i use wandb.init() there is nothing.!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am waiting a lot of time.<\/p>\n<p>However, there is nothing in window.<\/p>\n<p>This is working well in Kernel.<\/p>\n<p>please.. help me guys<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655016960480,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72590067",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":5.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"jupyterLab Wandb does not iterative",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1609152494583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"South Korea",
        "Poster_reputation_count":72.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I work at Weights &amp; Biases. If you're in a notebook the quickest thing you can do to get going with <code>wandb<\/code> is simply:<\/p>\n<pre><code>wandb.init(project=MY_PROJECT, entity=MY_ENTITY)\n<\/code><\/pre>\n<p>No <code>!wandb login<\/code>, <code>wandb.login()<\/code> or <code>%%wandb<\/code> needed. If you're not already logged in then <code>wandb.init<\/code> will ask you for you API key.<\/p>\n<p>(curious where you found <code>%%wandb<\/code> by the way?)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.2,
        "Solution_reading_time":6.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8,
        "Solution_word_count":57,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a function I can use to get the instance type of my SageMaker instance.<\/p>\n<p>I basically want to do something like this<\/p>\n<pre><code>region = boto3.Session().region_name\n<\/code><\/pre>\n<p>but for the instance type.<\/p>\n<p>I know I can find it manually, but I want to automate it so that my script can work on any instance.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658254476363,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73041737",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":4.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Print SageMaker instance type",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Poster_created_time":1657050754840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeNotebookInstance.html\" rel=\"nofollow noreferrer\">DescribeNotebookInstance<\/a> API to get the instance size.<\/p>\n<pre><code>sm_client = boto3.client(&quot;sagemaker&quot;)\nsm.describe_notebook_instance(\n    NotebookInstanceName=&lt;nb-name&gt;\n)['InstanceType']\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":24.0,
        "Solution_reading_time":5.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":19,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1572208191120,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":45.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm currently using Sagemaker notebook instance (not from Sagemaker Studio), and I want to run a notebook that is expected to take around 8 hours to finish. I want to leave it overnight, and see the output from each cell, the output is a combination of print statements and plots.<\/p>\n<p>Howevever, when I start running the notebook and make sure the initial cells run, I close the Jupyterlab tab in my browser, and some minutes after, I open it again to see how is it going, but the notebook is stopped.<\/p>\n<p>Is there any way where I can still use my notebook as it is, see the output from each cell (prints and plots) and do not have to keep the Jupyterlab tab open (turn my laptop off, etc)?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1646922683383,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71425842",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":10.5,
        "Challenge_reading_time":9.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Run Sagemaker notebook instance and be able to close tab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1154.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572208191120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":139.0,
        "Poster_view_count":45.0,
        "Solution_body":"<p>Answering my own question.<\/p>\n<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-run-notebook\" rel=\"nofollow noreferrer\">Sagemaker run notebook<\/a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.<\/p>\n<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.<\/p>\n<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":8.3,
        "Solution_reading_time":9.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8,
        "Solution_word_count":126,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Would like to upload Jupyter notebooks from different sources like GitHub into my workspace either directly or through my local machine (download locally first and then upload) but I would like to do it programmatically. Either with the AzureML SDK or azure cli  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651101620807,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/829311\/how-could-i-upload-notebooks-to-my-azureml-workspa",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How could I upload notebooks to my AzureML workspace programatically",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. You can use compute instance <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal\">terminal<\/a> in AML notebooks to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/samples-notebooks#get-samples-on-azure-machine-learning-compute-instance\">clone<\/a> the GitHub repo. There's currently no option to upload notebooks to your workspace programmatically using sdk or cli.<\/p>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":14.5,
        "Solution_reading_time":7.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":45,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1294730361590,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY",
        "Answerer_reputation_count":57082.0,
        "Answerer_view_count":4597.0,
        "Challenge_adjusted_solved_time":4.4930697222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm quite new using Kedro and after installing kedro in my conda environment, I'm getting the following error when trying to list my catalog:<\/p>\n<p>Command performed: <code>kedro catalog list<\/code><\/p>\n<p>Error:<\/p>\n<blockquote>\n<p>kedro.io.core.DataSetError: An exception occurred when parsing config\nfor DataSet <code>df_medinfo_raw<\/code>: Object <code>ParquetDataSet<\/code> cannot be loaded\nfrom <code>kedro.extras.datasets.pandas<\/code>. Please see the documentation on\nhow to install relevant dependencies for\nkedro.extras.datasets.pandas.ParquetDataSet:<\/p>\n<\/blockquote>\n<p>I installed kedro trough conda-forge: <code>conda install -c conda-forge &quot;kedro[pandas]&quot;<\/code>. As far as I understand, this way to install kedro also installs the pandas dependencies.<\/p>\n<p>I tried to read the kedro documentation for dependencies, but it's not really clear how to solve this kind of issue.<\/p>\n<p>My kedro version is <strong>0.17.6<\/strong>.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642224358493,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1642264400916,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70719080",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":13.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"AttributeError: Object ParquetDataSet cannot be loaded from kedro.extras.datasets.pandas",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":863.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492098397316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":135.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Kedro uses Pandas to load <code>ParquetDataSet<\/code> objects, and Pandas requires additional dependencies to accomplish this (see <a href=\"https:\/\/pandas.pydata.org\/docs\/getting_started\/install.html#other-data-sources\" rel=\"nofollow noreferrer\">&quot;Installation: Other data sources&quot;<\/a>). That is, in addition to Pandas, one must also install either <code>fastparquet<\/code> or <code>pyarrow<\/code>.<\/p>\n<p>For Conda you either want:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>## use pyarrow for parquet\nconda install -c conda-forge kedro pandas pyarrow\n<\/code><\/pre>\n<p>or<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>## or use fastparquet for parquet\nconda install -c conda-forge kedro pandas fastparquet\n<\/code><\/pre>\n<p>Note that the syntax used in the question <code>kedro[pandas]<\/code> is meaningless to Conda (i.e., it ultimately parses to just <code>kedro<\/code>). Conda package specification uses <a href=\"https:\/\/stackoverflow.com\/a\/57734390\/570918\">a custom grammar called <code>MatchSpec<\/code><\/a>, where anything inside a <code>[...]<\/code> is parsed for a <code>[key1=value1;key2=value2;...]<\/code> syntax. Essentially, the <code>[pandas]<\/code> is treated as an unknown key, which is ignored.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1642280575967,
        "Solution_link_count":2,
        "Solution_readability":13.5,
        "Solution_reading_time":16.42,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9,
        "Solution_word_count":127,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1317676233236,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":2348.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554750635030,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55580232",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":2.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Update SageMaker Jupyterlab environment",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1720.0,
        "Challenge_word_count":22,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>Hi and thank you for using SageMaker!<\/p>\n\n<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait<\/code>.<\/p>\n\n<p>Best,\nKevin<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.1,
        "Solution_reading_time":2.95,
        "Solution_score_count":8.0,
        "Solution_sentence_count":3,
        "Solution_word_count":29,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running CI\/CD pipelines in Azure Dev Ops in order to deploy my machine learning workloads on components of a Azure Machine Learning Workspace (e.g. AzureML pipelines, endpoints, etc). I recently came across an issue in my pipelines and wanted to validate that the versions of the Azure CLI and Azure CLI ml extension didn't cause this issue.<\/p>\n<p>Because the Azure CLI is a tool that is developed in python, it is relatively easy to install an older version of the cli. I can just run <code>pip install azure-cli==2.44.1<\/code>. However, I'm having trouble figuring out how to install a specific version of the ml extension. According to this documentation, all extensions are packaged as python wheels, so theoretically if I had the URL of the build wheel, I could just target that. But I'm having trouble finding where the ml extension code is hosted. I found the pypi package for <a href=\"https:\/\/pypi.org\/project\/azure-cli-ml\/\">v1 of the extension<\/a>. Does anyone know where v2 is hosted?<\/p>\n<p>The version of the extension I would like to install is <code>2.13.0<\/code>.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1677177834880,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1183753\/how-do-i-install-an-older-version-of-the-azure-cli",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":7.7,
        "Challenge_reading_time":14.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I install an older version of the Azure CLI ml extension?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":186,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=1837dca2-1954-4413-9ba8-ffcc3afb6ce4\">Claire Salling<\/a> You can install a specific version of <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-cli?tabs=private\">Azure ML cli extension<\/a> using these commands.<\/p>\n<p>Remove any existing extensions of azure-cli-ml(v1) or ml(v2)<\/p>\n<p><code>az extension remove -n azure-cli-ml <\/code>  <br \/>\n<code>az extension remove -n ml<\/code><\/p>\n<p>Add the required version:<\/p>\n<p><code>az extension add --name ml --version 2.13.0<\/code><\/p>\n<p>List the extension to confirm the version.<\/p>\n<p><code>az extension list<\/code><\/p>\n<pre><code> {\n    &quot;experimental&quot;: false,\n    &quot;extensionType&quot;: &quot;whl&quot;,\n    &quot;name&quot;: &quot;ml&quot;,\n    &quot;path&quot;: &quot;C:\\\\Users\\\\user\\\\.azure\\\\cliextensions\\\\ml&quot;,\n    &quot;preview&quot;: false,\n    &quot;version&quot;: &quot;2.13.0&quot;\n  }\n<\/code><\/pre>\n<p>If this answers your query, do click <code>Accept Answer<\/code> and <code>Yes<\/code> for was this answer helpful. And, if you have any further query do let us know.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.9,
        "Solution_reading_time":14.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":100,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The Python package <a href=\"https:\/\/pypi.org\/project\/azureml-train\/\">azureml-train<\/a> is deprecated and seems basically a wrapper around <a href=\"https:\/\/pypi.org\/project\/azureml-train-core\/\">azureml-train-core<\/a>. When installing <code>azureml-train<\/code>, if I'm correct it tries to install the <code>azureml-train-core<\/code> package with the same version number. However, on the 24th of August 2021 the <code>azureml-train<\/code> package was <a href=\"https:\/\/pypi.org\/project\/azureml-train\/#history\">updated<\/a> to version 1.33.1 whereas <code>azureml-train-core<\/code> <a href=\"https:\/\/pypi.org\/project\/azureml-train-core\/#history\">wasn't updated<\/a>. This causes the installation of <code>azureml-train<\/code> to fail.   <\/p>\n<p>I would suggest to remove version 1.33.1 of <code>azureml-train<\/code> such that it still can be installed.  <br \/>\nOtherwise, I'm curious why this situation is the case.  <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629973035930,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/528959\/(bug)-azureml-train-python-package-deprecated-but",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] azureml-train Python package deprecated but did receive an update which is not in line with azureml-train-core",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=b3ae76bb-30c9-45f0-9c89-d61eac6d87f6\">@SjoerdGn  <\/a> Yes, this is a bug in the release cycle that was pushed to pypi, This also caused other packages to get updated.    <\/p>\n<p><a href=\"https:\/\/pypi.org\/project\/azureml-train-automl-runtime\/1.33.1.post1\/\">https:\/\/pypi.org\/project\/azureml-train-automl-runtime\/1.33.1.post1\/<\/a>    <br \/>\n<a href=\"https:\/\/pypi.org\/project\/azureml-train-automl\/1.33.1\/\">https:\/\/pypi.org\/project\/azureml-train-automl\/1.33.1\/<\/a>    <\/p>\n<p><a href=\"https:\/\/pypi.org\/project\/azureml-sdk\/#history\">azureml-sdk 1.33.0.post1<\/a> is released now to ensure the correct versions are installed with the SDK. As you mentioned above azureml-train 1.33.1 is not required and can be removed.     <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":15.0,
        "Solution_reading_time":9.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7,
        "Solution_word_count":62,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1524670343368,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Z\u00fcrich, Schweiz",
        "Answerer_reputation_count":460.0,
        "Answerer_view_count":52.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to run a test script on an existing compute instance of Azure using the Azure Machine Learning extension to the Azure CLI:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg\n<\/code><\/pre>\n<p>I get a Service Error with the following error message:<\/p>\n<pre><code>Unable to run conda package manager. AzureML uses conda to provision python\\nenvironments from a dependency specification. To manage the python environment\\nmanually instead, set userManagedDependencies to True in the python environment\\nconfiguration. To use system managed python environments, install conda from:\\nhttps:\/\/conda.io\/miniconda.html\n<\/code><\/pre>\n<p>But when I connect to the compute instance through the Azure portal and select the default Python kernel, <code>conda --version<\/code> prints 4.5.12. So conda is effectively already installed on the compute instance. This is why I do not understand the error message.<\/p>\n<p>Further information on the azure versions:<\/p>\n<pre><code>  &quot;azure-cli&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-core&quot;: &quot;2.12.1&quot;,\n  &quot;azure-cli-telemetry&quot;: &quot;1.0.6&quot;,\n  &quot;extensions&quot;: {\n    &quot;azure-cli-ml&quot;: &quot;1.15.0&quot;\n  }\n<\/code><\/pre>\n<p>The image I use is:<\/p>\n<pre><code>mcr.microsoft.com\/azure-cli:latest\n<\/code><\/pre>\n<p>Can somebody please explain as to why I am getting this error and help me resolve the error? Thank you!<\/p>\n<p>EDIT: I tried to update the environment in which the <code>az ml run<\/code>-command is run.\nEssentially this is my GitLab job. The installation of miniconda is a bit complicated as the azure-cli uses an alpine Linux image (reference: <a href=\"https:\/\/stackoverflow.com\/questions\/47177538\/installing-miniconda-on-alpine-linux-fails\">Installing miniconda on alpine linux fails<\/a>). I replaced some names with ... and cut out some irrelevant pieces of code.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>test:\n  image: 'mcr.microsoft.com\/azure-cli:latest'\n  script:\n    - echo &quot;Download conda&quot;\n    - apk --update add bash curl wget ca-certificates libstdc++ glib\n    - wget -q -O \/etc\/apk\/keys\/sgerrand.rsa.pub https:\/\/raw.githubusercontent.com\/sgerrand\/alpine-pkg-node-bower\/master\/sgerrand.rsa.pub\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-2.23-r3.apk&quot; -o glibc.apk\n    - apk del libc6-compat\n    - apk add glibc.apk\n    - curl -L &quot;https:\/\/github.com\/sgerrand\/alpine-pkg-glibc\/releases\/download\/2.23-r3\/glibc-bin-2.23-r3.apk&quot; -o glibc-bin.apk \n    - apk add glibc-bin.apk \n    - curl -L &quot;https:\/\/github.com\/andyshinn\/alpine-pkg-glibc\/releases\/download\/2.25-r0\/glibc-i18n-2.25-r0.apk&quot; -o glibc-i18n.apk\n    - apk add --allow-untrusted glibc-i18n.apk \n    - \/usr\/glibc-compat\/bin\/localedef -i en_US -f UTF-8 en_US.UTF-8 \n    - \/usr\/glibc-compat\/sbin\/ldconfig \/lib \/usr\/glibc\/usr\/lib\n    - rm -rf glibc*apk \/var\/cache\/apk\/*\n    - echo &quot;yes&quot; | curl -sSL https:\/\/repo.continuum.io\/miniconda\/Miniconda3-latest-Linux-x86_64.sh -o miniconda.sh\n    - echo &quot;Install conda&quot;\n    - (echo -e &quot;\\n&quot;; echo &quot;yes&quot;; echo -e &quot;\\n&quot;; echo &quot;yes&quot;) | bash -bfp miniconda.sh\n    - echo &quot;Installing Azure Machine Learning Extension&quot;\n    - az extension add -n azure-cli-ml\n    - echo &quot;Azure Login&quot;\n    - az login\n    - az account set --subscription ...\n    - az configure --defaults group=...\n    - az ml folder attach -w ... \n    - az ml run submit-script test.py --target ... --experiment-name hello_world --resource-group ...\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1601897074350,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1601970763932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64207678",
        "Challenge_link_count":7,
        "Challenge_participation_count":9,
        "Challenge_readability":11.6,
        "Challenge_reading_time":48.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":null,
        "Challenge_title":"How to avoid error \"conda --version: conda not found\" in az ml run --submit-script command?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1089.0,
        "Challenge_word_count":373,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524670343368,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Z\u00fcrich, Schweiz",
        "Poster_reputation_count":460.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>One needs to pass the <code>--workspace-name<\/code> argument to be able to run it on Azure's compute target and not on the local compute target:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az ml run submit-script test.py --target compute-instance-test --experiment-name test_example --resource-group ex-test-rg --workspace-name test-ws\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":14.6,
        "Solution_reading_time":4.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":40,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1306085731476,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cleveland, TN",
        "Answerer_reputation_count":7737.0,
        "Answerer_view_count":454.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to follow a Python tutorial and I have been able to execute almost everything, until the point of Deploying an endpoint to Azure with python.<\/p>\n\n<p>In order to give some context I have uploaded the scripts to my git account:\n<a href=\"https:\/\/github.com\/levalencia\/MLTutorial\" rel=\"nofollow noreferrer\">https:\/\/github.com\/levalencia\/MLTutorial<\/a><\/p>\n\n<p>File 1 and 2 Work perfectly fine<\/p>\n\n<p>However the following section in File 3 fails:<\/p>\n\n<pre><code>%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\n\ninference_config = InferenceConfig(runtime= \"python\", \n                                   entry_script=\"score.py\",\n                                   conda_file=\"myenv.yml\")\n\nservice = Model.deploy(workspace=ws, \n                       name='keras-mnist-svc2', \n                       models=[amlModel], \n                       inference_config=inference_config, \n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>with below error:<\/p>\n\n<pre><code>ERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n\nERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n<\/code><\/pre>\n\n<p>When I download the logs, I got this:<\/p>\n\n<pre><code>wheel-0.34.2         | 24 KB     |            |   0% [0m[91m\nwheel-0.34.2         | 24 KB     | ########## | 100% [0m\nDownloading and Extracting Packages\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... failed\n[91m\nERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'.\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\nAttempting to roll back.\n\n[0mRolling back transaction: ...working... done\n[91m\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\n\n\n[0mThe command '\/bin\/sh -c ldconfig \/usr\/local\/cuda\/lib64\/stubs &amp;&amp; conda env create -p \/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56 -f azureml-environment-setup\/mutated_conda_dependencies.yml &amp;&amp; rm -rf \"$HOME\/.cache\/pip\" &amp;&amp; conda clean -aqy &amp;&amp; CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; rm -rf \"$CONDA_ROOT_DIR\/pkgs\" &amp;&amp; find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + &amp;&amp; ldconfig' returned a non-zero code: 1\n2020\/03\/11 12:28:11 Container failed during run: acb_step_0. No retries remaining.\nfailed to run step ID: acb_step_0: exit status 1\n\nRun ID: cb3 failed after 2m21s. Error: failed during run, err: exit status 1\n<\/code><\/pre>\n\n<p>Update 1:<\/p>\n\n<p>I tried to run:\nconda list    --name base  conda<\/p>\n\n<p>inside the notebook and I got this:<\/p>\n\n<pre><code> # packages in environment at \/anaconda:\n    #\n    # Name                    Version                   Build  Channel\n    _anaconda_depends         2019.03                  py37_0  \n    anaconda                  custom                   py37_1  \n    anaconda-client           1.7.2                    py37_0  \n    anaconda-navigator        1.9.6                    py37_0  \n    anaconda-project          0.8.4                      py_0  \n    conda                     4.8.2                    py37_0  \n    conda-build               3.17.6                   py37_0  \n    conda-env                 2.6.0                         1  \n    conda-package-handling    1.6.0            py37h7b6447c_0  \n    conda-verify              3.1.1                    py37_0  \n\n    Note: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n\n<p>However in the deployment log I got this:<\/p>\n\n<pre><code>Solving environment: ...working... \ndone\n[91m\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.5.11\n  latest version: 4.8.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1583931341127,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1584001089836,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60636558",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":12.9,
        "Challenge_reading_time":59.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":null,
        "Challenge_title":"ERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":5656.0,
        "Challenge_word_count":421,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>Unfortunately there seems to an issue with this version of Conda (4.5.11). To complete this task in the tutorial, you can just update the dependency for Tensorflow and Keras to be from <code>pip<\/code> and not <code>conda<\/code>. There are reasons why this is less than ideal for a production environment. The Azure ML <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py\" rel=\"nofollow noreferrer\">documentation states<\/a>:<\/p>\n\n<blockquote>\n  <p>\"If your dependency is available through both Conda and pip (from\n  PyPi), use the Conda version, as Conda packages typically come with\n  pre-built binaries that make installation more reliable.\"<\/p>\n<\/blockquote>\n\n<p>In this case though, if you update the following code block:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"tensorflow\")\nmyenv.add_conda_package(\"keras\")\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n\n# Review environment file\nwith open(\"myenv.yml\",\"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>To be the following:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_pip_package(\"tensorflow==2.0.0\")\nmyenv.add_pip_package(\"azureml-defaults\")\nmyenv.add_pip_package(\"keras\")\n\nwith open(\"myenv.yml\", \"w\") as f:\n    f.write(myenv.serialize_to_string())\n\nwith open(\"myenv.yml\", \"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>The tutorial should be able to be completed.  Let me know if any of this does not work for you once this update has been made.<\/p>\n\n<p>I have also reported this issue to Microsoft (in regards to the Conda version).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":11.7,
        "Solution_reading_time":22.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":20,
        "Solution_word_count":187,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"My customer is looking to use Glue dev endpoints along with a SageMaker notebook. What I've noticed is that in Glue, a package, in this case scipy, will be listed as 1.4.1, but this will or won't match what you get in a sagemaker notebook dependent on kernel. \n\nconda_python3:\n\n    !pip show scipy\n    Name: scipy\n    Version: 1.1.0\n    Summary: SciPy: Scientific Library for Python\n    Home-page: https:\/\/www.scipy.org\n    Author: None\n    Author-email: None\n    License: BSD\n    Location: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\n    Requires: \n    Required-by: seaborn, scikit-learn, sagemaker\n\nconda_tensorflow_p36:\n\n    !pip show scipy\n    Name: scipy\n    Version: 1.4.1\n    Summary: SciPy: Scientific Library for Python\n    Home-page: https:\/\/www.scipy.org\n    Author: None\n    Author-email: None\n    License: BSD\n    Location: \/home\/ec2-user\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\n    Requires: numpy\n    Required-by: seaborn, scikit-learn, sagemaker, Keras\n\nIs there some sort of best practice to use a kernel that corresponds directly to what's installed on Glue?\n\nSeparate not very useful question. I wasn't able activate the venv that Jupyter notebooks do via shell. Is it using a venv? How come I can't find the right activate script?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591210245000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668452450079,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QULN3fro-LQ1umpDN831VlZg\/glue-sagemaker-pip-packages",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":15.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Glue + SageMaker Pip Packages",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":151.0,
        "Challenge_word_count":161,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"conda_python3 and conda_tensorflow_p36 are local kernels on the SageMaker notebook instance while the Spark kernels execute remotely in the Glue Spark environment.\n\nHence you are seeing different versions. The Glue Spark environment comes with 1.4.1 version of scipy. So when you use the PySpark (python) or Spark (scala) kernels and you will get the 1.4.1 version of scipy.\n\nIf you use the default LifeCycle script that Glue SageMaker notebooks already come with, the connectivity to the Glue Dev endpoint should be in place. Note that the Glue SageMaker notebooks has a tag called 'aws-glue-dev-endpoint' that is used to identify which Glue Dev endpoint that particular notebook instance communicates with.\n\nThe Spark kernels cannot be replicated via the python shell. Those kernels relay Spark commands via the Livy service to Spark on the Glue Dev endpoint using a Jupyter module called Sparkmagic.\n\nRef: https:\/\/github.com\/jupyter-incubator\/sparkmagic",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925566052,
        "Solution_link_count":1,
        "Solution_readability":8.9,
        "Solution_reading_time":11.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10,
        "Solution_word_count":143,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1326780552283,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a prediction web service to Azure using ML Workbench process using cluster mode in this tutorial (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3#prepare-to-operationalize-locally<\/a>)<\/p>\n\n<p>The model gets sent to the manifest, the scoring script and schema <\/p>\n\n<blockquote>\n  <p>Creating\n  service..........................................................Error\n  occurred: {'Error': {'Code': 'KubernetesDeploymentFailed', 'Details':\n  [{'Message': 'Back-off 40s restarting failed container=...pod=...',\n  'Code': 'CrashLoopBackOff'}], 'StatusCode': 400, 'Message':\n  'Kubernetes Deployment failed'}, 'OperationType': 'Service',\n  'State':'Failed', 'Id': '...', 'ResourceLocation':\n  '\/api\/subscriptions\/...', 'CreatedTime':\n  '2017-10-26T20:30:49.77362Z','EndTime': '2017-10-26T20:36:40.186369Z'}<\/p>\n<\/blockquote>\n\n<p>Here is the result of checking the ml service realtime logs <\/p>\n\n<pre><code>C:\\Users\\userguy\\Documents\\azure_ml_workbench\\projecto&gt;az ml service logs realtime -i projecto\n2017-10-26 20:47:16,118 CRIT Supervisor running as root (no user in config file)\n2017-10-26 20:47:16,120 INFO supervisord started with pid 1\n2017-10-26 20:47:17,123 INFO spawned: 'rsyslog' with pid 9\n2017-10-26 20:47:17,124 INFO spawned: 'program_exit' with pid 10\n2017-10-26 20:47:17,124 INFO spawned: 'nginx' with pid 11\n2017-10-26 20:47:17,125 INFO spawned: 'gunicorn' with pid 12\n2017-10-26 20:47:18,160 INFO success: rsyslog entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:18,160 INFO success: program_exit entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2017-10-26 20:47:22,164 INFO success: nginx entered RUNNING state, process has stayed up for &gt; than 5 seconds (startsecs)\n2017-10-26T20:47:22.519159Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting gunicorn 19.6.0\n2017-10-26T20:47:22.520097Z, INFO, 00000000-0000-0000-0000-000000000000, , Listening at: http:\/\/127.0.0.1:9090 (12)\n2017-10-26T20:47:22.520375Z, INFO, 00000000-0000-0000-0000-000000000000, , Using worker: sync\n2017-10-26T20:47:22.521757Z, INFO, 00000000-0000-0000-0000-000000000000, , worker timeout is set to 300\n2017-10-26T20:47:22.522646Z, INFO, 00000000-0000-0000-0000-000000000000, , Booting worker with pid: 22\n2017-10-26 20:47:27,669 WARN received SIGTERM indicating exit request\n2017-10-26 20:47:27,669 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26T20:47:27.669556Z, INFO, 00000000-0000-0000-0000-000000000000, , Handling signal: term\n2017-10-26 20:47:30,673 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:33,675 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\nInitializing logger\n2017-10-26T20:47:36.564469Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insights client\n2017-10-26T20:47:36.564991Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up request id generator\n2017-10-26T20:47:36.565316Z, INFO, 00000000-0000-0000-0000-000000000000, , Starting up app insight hooks\n2017-10-26T20:47:36.565642Z, INFO, 00000000-0000-0000-0000-000000000000, , Invoking user's init function\n2017-10-26 20:47:36.715933: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36,716 INFO waiting for nginx, gunicorn, rsyslog, program_exit to die\n2017-10-26 20:47:36.716376: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instruc\ntions, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716542: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructio\nns, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716703: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructi\nons, but these are available on your machine and could speed up CPU computations.\n2017-10-26 20:47:36.716860: W tensorflow\/core\/platform\/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructio\nns, but these are available on your machine and could speed up CPU computations.\nthis is the init\n2017-10-26T20:47:37.551940Z, INFO, 00000000-0000-0000-0000-000000000000, , Users's init has completed successfully\nUsing TensorFlow backend.\n2017-10-26T20:47:37.553751Z, INFO, 00000000-0000-0000-0000-000000000000, , Worker exiting (pid: 22)\n2017-10-26T20:47:37.885303Z, INFO, 00000000-0000-0000-0000-000000000000, , Shutting down: Master\n2017-10-26 20:47:37,885 WARN killing 'gunicorn' (12) with SIGKILL\n2017-10-26 20:47:37,886 INFO stopped: gunicorn (terminated by SIGKILL)\n2017-10-26 20:47:37,889 INFO stopped: nginx (exit status 0)\n2017-10-26 20:47:37,890 INFO stopped: program_exit (terminated by SIGTERM)\n2017-10-26 20:47:37,891 INFO stopped: rsyslog (exit status 0)\n\nReceived 41 lines of log\n<\/code><\/pre>\n\n<p>My best guess is theres something silent happening to cause \"WARN received SIGTERM indicating exit request\". The rest of the scoring.py script seems to kick off - see tensorflow get initiated and the \"this is the init\" print statement.<\/p>\n\n<p><a href=\"http:\/\/127.0.0.1:63437\" rel=\"nofollow noreferrer\">http:\/\/127.0.0.1:63437<\/a> is accessible from my local machine, but the ui endpoint is blank.<\/p>\n\n<p>Any ideas on how to get this up and running in an Azure cluster? I'm not very familiar with how Kubernetes works, so any basic debugging guidance would be appreciated.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1509052128353,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1511310430992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46963846",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":77.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Workbench Kubernetes Deployment Failed",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":447.0,
        "Challenge_word_count":620,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421081882987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":588.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>We discovered a bug in our system that could have caused this. The fix was deployed last night. Can you please try again and let us know if you still encounter this issue?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.14,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3,
        "Solution_word_count":33,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a private Azure Container Registry, and pushed a docker image to that registry. I was trying to understand the correct way to access that registry in my pipeline, and my understanding was that I needed to set the following info in the run configuration:<\/p>\n\n<pre><code>        run_config.environment.docker.base_image = \"myprivateacr.azurecr.io\/mydockerimage:0.0.1\"\n        run_config.environment.docker.base_image_registry.username = \"MyPrivateACR\"\n        run_config.environment.docker.base_image_registry.password = \"&lt;the password for the registry&gt;\"\n<\/code><\/pre>\n\n<p>Let's assume that I correctly provided the username and password. Any idea why this didn't work? Or: is there an example of a pipeline notebook that uses a docker image that's in a private docker registry, and thus deals with this type of authentication issue?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566428573660,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57600154",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":11.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to correctly specify a private ACR Docker image in an Azure ML Pipeline?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":278.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1491928725063,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA, United States",
        "Poster_reputation_count":45.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>There's a separate address property for a custom image registry. Try specifying it this way:<\/p>\n\n<pre><code>run_config.environment.docker.base_image = \"mydockerimage:0.0.1\"\nrun_config.environment.docker.base_image_registry.address = \"myprivateacr.azurecr.io\"\nrun_config.environment.docker.base_image_registry.username = \"MyPrivateACR\"\nrun_config.environment.docker.base_image_registry.password = \"&lt;the password for the registry&gt;\"\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":27.3,
        "Solution_reading_time":6.18,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4,
        "Solution_word_count":28,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1483548930012,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1875.0,
        "Answerer_view_count":146.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>[UPDATED] We are currently working on creating a Multi-Arm Bandit model for sign up optimization using the Build Your Own workflow that can be found here (basically substituting the model for our own):<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/p>\n<p>Our project directory is set up as:\n<a href=\"https:\/\/i.stack.imgur.com\/QwaIQ.png\" rel=\"nofollow noreferrer\">Project Directory<\/a><\/p>\n<p>The issue is that I added some code including the dataclasses library that is only available since Python 3.7, and our project seems to keep using 3.6, causing a failure when running the Cloud Formation set up. The error in our Cloudwatch Logs is:<\/p>\n<pre><code>2021-03-31T11:04:11.077-05:00 Copy\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import predictor as myapp\n  File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt;\n    from model_contents.model import MultiArmBandit, BanditParameters\n  File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt;\n    from dataclasses import dataclass, field, asdict\nTraceback (most recent call last): File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker worker.init_process() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process self.load_wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi self.wsgi = self.app.wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi self.callable = self.load() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load return self.load_wsgiapp() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp return util.import_app(self.app_uri) File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app mod = importlib.import_module(module) File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt; import predictor as myapp File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt; from model_contents.model import MultiArmBandit, BanditParameters File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt; from dataclasses import dataclass, field, asdict\n\n    2021-03-31T11:04:11.077-05:00\n\nCopy\nModuleNotFoundError: No module named 'dataclasses'\nModuleNotFoundError: No module named 'dataclasses'\n<\/code><\/pre>\n<p>Our updated Dockerfile is:<\/p>\n<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:18.04\n\n# Retrieves information about what packages can be installed\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python3-pip \\\n         python3.8 \\\n         python3-setuptools \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Set python 3.8 as default\nRUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n\n# Here we get all python packages.\nRUN pip --no-cache-dir install numpy boto3 flask gunicorn\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\nENV PYTHONPATH \/model_contents\n\n# Set up the program in the image\nCOPY bandit\/ \/opt\/program\/\nWORKDIR \/opt\/program\/\n\nRUN chmod +x \/opt\/program\/serve &amp;&amp; chmod +x \/opt\/program\/train\nLABEL git_tag=$GIT_TAG\n<\/code><\/pre>\n<p>I'm not sure if the nginx.conf file defaults to Py 3.6 so I want to make sure that it's not a big hassle to upgrade to Py 3.7 or 3.8 without many changes.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617309858560,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1617383727407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66911321",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.5,
        "Challenge_reading_time":88.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":77,
        "Challenge_solved_time":null,
        "Challenge_title":"Upgrading Python version for running and creating custom container for Sagemaker Endpoint",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1398.0,
        "Challenge_word_count":615,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526288719836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation_count":33.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>You can update the Dockerfile after it install Python3.8 using <code>apt-get<\/code> with the following <code>RUN<\/code> commands<\/p>\n<pre><code>RUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n<\/code><\/pre>\n<p>The first <code>RUN<\/code> command will link <code>\/usr\/bin\/python<\/code> to <code>\/usr\/bin\/python3.8<\/code> and the second one will link <code>\/usr\/bin\/python3<\/code> to <code>\/usr\/bin\/python3.8<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1617312029390,
        "Solution_link_count":0,
        "Solution_readability":10.7,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5,
        "Solution_word_count":49,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342713532568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seoul, South Korea",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In DVC one may define pipelines.  In Unix, one typically does not work at the root level.  Further, DVC expects files to be inside the git repository.<\/p>\n<p>So, this seems like a typical problem.<\/p>\n<p>Suppose I have the following:<\/p>\n<pre><code>\/home\/user\/project\/content-folder\/data\/data-type\/cfg.json\n\/home\/user\/project\/content-folder\/app\/foo.py\n<\/code><\/pre>\n<p>Git starts at <code>\/home\/user\/project\/<\/code><\/p>\n<pre><code>cd ~\/project\/content-folder\/data\/data-type\n..\/..\/app\/foo.py do-this --with cfg.json --dest $(pwd) \n<\/code><\/pre>\n<p>Seems reasonable to me: the script takes a configuration, which is stored in a particular location, runs it against some encapsulated functionality, and outputs it to the destination using an absolute path.<\/p>\n<p>The default behavior of <code>--dest<\/code> is to output to the current working directory.  This seems like another reasonable default.<\/p>\n<hr \/>\n<p>Next, I go to configure the <code>params.yaml<\/code> file for <code>dvc<\/code>, and I am immediately confusing and unsure what is going to happen.  I write:<\/p>\n<pre><code>foodoo:\n  params: do-this --with ????\/cfg.json --dest ????\n<\/code><\/pre>\n<p>What I want to write (and would in a shell script):<\/p>\n<pre><code>#!\/usr\/bin\/env bash\norigin:=$(git rev-parse --show-toplevel)\n\nverb=do-this\nparams=--with $(origin)\/content-folder\/data\/data-type\/cfg.json --dest $(origin)\/content-folder\/data\/data-type\n<\/code><\/pre>\n<hr \/>\n<p>But, in DVC, the pathing seems to be implicit, and I do not know where to start as either:<\/p>\n<ol>\n<li>DVC will calculate the path to my script locally<\/li>\n<li>Not calculate the path to my script locally<\/li>\n<\/ol>\n<p>Which is fine -- I can discover that.  But I am reasonably sure that DVC will absolutely not prefix the directory and file params in my params.yaml with the path to my project.<\/p>\n<hr \/>\n<p>How does one achieve path control that does not assume a fixed project location, like I would in BASH?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1608672730887,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65416056",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":25.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Data Version Control: Absolute Paths and Project Paths in the Pipeline Parameters?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":448.0,
        "Challenge_word_count":262,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405262190020,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Atlanta, GA",
        "Poster_reputation_count":26244.0,
        "Poster_view_count":1383.0,
        "Solution_body":"<p>By default, DVC will run your stage command from the same directory as the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files#dvcyaml-file\" rel=\"nofollow noreferrer\">dvc.yaml<\/a> file. If you need to run the command from a different location, you can specify an alternate working directory via <code>wdir<\/code>, which should be a path relative to <code>dvc.yaml<\/code>'s location.<\/p>\n<p>Paths for everything else in your stage (like <code>params.yaml<\/code>) should be specified as relative to <code>wdir<\/code> (or relative to <code>dvc.yaml<\/code> if <code>wdir<\/code> is not provided).<\/p>\n<p>Looking at your example, there also seems to be a bit of confusion on parameters in DVC. In a DVC stage, <code>params<\/code> is for specifying <a href=\"https:\/\/dvc.org\/doc\/command-reference\/params\" rel=\"nofollow noreferrer\">parameter dependencies<\/a>, not used for specifying command-line flags. The full command including flags\/options should be included  the <code>cmd<\/code> section for your stage. If you wanted to make sure that your stage was rerun every time certain values in <code>cfg.json<\/code> have changed, your stage's <code>params<\/code> section would look something like:<\/p>\n<pre><code>params:\n  &lt;relpath from dvc.yaml&gt;\/cfg.json:\n    - param1\n    - param2\n    ...\n<\/code><\/pre>\n<p>So your example <code>dvc.yaml<\/code> would look something like:<\/p>\n<pre><code>stages:\n  foodoo:\n    cmd: &lt;relpath from dvc.yaml&gt;\/foo.py do-this --with &lt;relpath from dvc.yaml&gt;\/cfg.json --dest &lt;relpath from dvc.yaml&gt;\/...\n    deps:\n      &lt;relpath from dvc.yaml&gt;\/foo.py\n    params:\n      &lt;relpath from dvc.yaml&gt;\/cfg.json:\n        ...\n    ...\n<\/code><\/pre>\n<p>This would make the command <code>dvc repro<\/code> rerun your stage any time that the code in foo.py has changed, or the specified parameters in <code>cfg.json<\/code> have changed.<\/p>\n<p>You may also want to refer to the docs for <a href=\"https:\/\/dvc.org\/doc\/command-reference\/run#run\" rel=\"nofollow noreferrer\">dvc run<\/a>, which can be used to generate or update a <code>dvc.yaml<\/code> stage (rather than writing <code>dvc.yaml<\/code> by hand)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3,
        "Solution_readability":9.4,
        "Solution_reading_time":26.99,
        "Solution_score_count":2.0,
        "Solution_sentence_count":26,
        "Solution_word_count":248,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606945520000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668512453971,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sagemaker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1054.0,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Where do you launch the terminal from? If you use the launcher window, it would start on the t2.medium as you are experiencing.\n\nHowever, if you use the **launch terminal button** in the toolbar that is displayed at the top of your notebook, it will launch the image terminal on the new instance the notebook's kernel is running on (your m5.8xlarge instance).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1610011918846,
        "Solution_link_count":0,
        "Solution_readability":7.0,
        "Solution_reading_time":4.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":62,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426093220648,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, India",
        "Answerer_reputation_count":1861.0,
        "Answerer_view_count":294.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any way to access the kedro pipeline environment name? Actually below is my problem.<\/p>\n<p>I am loading the config paths as below<\/p>\n<pre><code>conf_paths = [&quot;conf\/base&quot;, &quot;conf\/local&quot;]  \nconf_loader = ConfigLoader(conf_paths)\nparameters = conf_loader.get(&quot;parameters*&quot;, &quot;parameters*\/**&quot;)\ncatalog = conf_loader.get(&quot;catalog*&quot;)\n\n<\/code><\/pre>\n<p>But  I have few environments like  <code>&quot;conf\/server&quot; <\/code>, <code>&quot;conf\/test&quot;<\/code> etc, So if I have env name available I can add it to conf_paths as <code>&quot;conf\/&lt;env_name&gt;&quot;<\/code>  so that kedro will read the files from the respective env folder.\nBut now if the env path is not added to conf_paths, the files are not being read by kedro even if i specify the env name while I  run kedro like    <code>kedro run --env=server <\/code>\nI searched for all the docs but was not able to find any solution.<\/p>\n<p>EDIT:\nElaborating more on the problem.\nI am using the above-given parameters and catalog dicts in the nodes. I only have keys that are common for all runs in <code>conf\/base\/parameters.yml<\/code> and the environment specific keys in <code>conf\/server\/parameters.yml<\/code> but when i do <code>kedro run --env=server<\/code> I am getting <code>keyerror<\/code> which means the keys in <code>conf\/server\/parameters.yml<\/code> is not available in the parameters dict. If I add  <code>conf\/server<\/code> to config_paths kedro is running well without keyerror.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":8,
        "Challenge_created_time":1639518159733,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1639559950496,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70355869",
        "Challenge_link_count":0,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":19.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How to access environment name in kedro pipeline",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":712.0,
        "Challenge_word_count":201,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You don't need to define config paths, config loader etc unless you are trying to override something.<\/p>\n<p>If you are using kedro 0.17.x, the hooks.py will look something like this.<\/p>\n<p>Kedro will pass, base, local and the env you specified during runtime in <code>conf_paths<\/code> into <code>ConfigLoader<\/code>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectHooks:\n    @hook_impl\n    def register_config_loader(\n        self, conf_paths: Iterable[str], env: str, extra_params: Dict[str, Any]\n    ) -&gt; ConfigLoader:\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n    def register_catalog(\n        self,\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n        credentials: Dict[str, Dict[str, Any]],\n        load_versions: Dict[str, str],\n        save_version: str,\n        journal: Journal,\n    ) -&gt; DataCatalog:\n        return DataCatalog.from_config(\n            catalog, credentials, load_versions, save_version, journal\n        )\n<\/code><\/pre>\n<p>In question, I can see you have defined <code>conf_paths<\/code> and <code>conf_loader<\/code> and the env path is not present. So kedro will ignore the env passed during runtime.<\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.7,
        "Solution_reading_time":13.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":121,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1409077455067,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Mountain View, CA",
        "Answerer_reputation_count":171.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to use the RStudio IDE to R on an Amazon SageMaker instance. What I have tried so far is to run the following docker command:<\/p>\n\n<pre><code>docker run --rm -p 8787:8787 rocker\/verse\n<\/code><\/pre>\n\n<p>which appears to work successfully. What I would then do when running that command from my local computer is go to <code>http:\/\/localhost:8787<\/code> where I would be able to login and find a fully functional RStudio IDE within my browser. <\/p>\n\n<p>However, this is obviously not possible from within SageMaker as there is no <code>localhost<\/code> to visit.<\/p>\n\n<p>Is there some way I can direct my browser to capture the output to port 8787 from the SageMaker instance? <\/p>\n\n<p>Thanks in advance. <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568043633847,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57857195",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to use RStudio IDE on an AWS SageMaker instance",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1935.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437507107363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":553.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>RStudio is now available as a managed service in SageMaker. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rstudio.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rstudio.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":30.8,
        "Solution_reading_time":3.16,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2,
        "Solution_word_count":14,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1544524371740,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cologne Germany",
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running into a ModuleNotFoundError for pandas while using the following code to orchestrate my Azure Machine Learning Pipeline:<\/p>\n<pre><code># Loading run config\nprint(&quot;Loading run config&quot;)\ntask_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \n\ntask_1_script_run_config = ScriptRunConfig(\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    run_config=task_1_run_config    \n)\n\ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name=task_1_script_run_config.script,\n    source_directory=task_1_script_run_config.source_directory,\n    compute_target=compute_target\n)\n\npipeline_run_config = Pipeline(workspace=workspace, steps=[task_1_py_script_step])#, task_2])\n\npipeline_run = Experiment(workspace, 'Test_Run_New_Pipeline').submit(pipeline_run_config)\npipeline_run.wait_for_completion()\n<\/code><\/pre>\n<p>The environment.yml<\/p>\n<pre><code>name: phinmo_pipeline_env\ndependencies:\n- python=3.8\n- pip:\n  - pandas\n  - azureml-core==1.43.0\n  - azureml-sdk\n  - scipy\n  - scikit-learn\n  - numpy\n  - pyyaml==6.0\n  - datetime\n  - azure\nchannels:\n  - conda-forge\n<\/code><\/pre>\n<p>The loaded RunConfiguration in T01_Test_Task.yml looks like this:<\/p>\n<pre><code># The script to run.\nscript: T01_Test_Task.py\n# The arguments to the script file.\narguments: [\n  &quot;--test&quot;, False,\n  &quot;--date&quot;, &quot;2022-07-26&quot;\n]\n# The name of the compute target to use for this run.\ncompute_target: phinmo-compute-cluster\n# Framework to execute inside. Allowed values are &quot;Python&quot;, &quot;PySpark&quot;, &quot;CNTK&quot;, &quot;TensorFlow&quot;, and &quot;PyTorch&quot;.\nframework: Python\n# Maximum allowed duration for the run.\nmaxRunDurationSeconds: 6000\n# Number of nodes to use for running job.\nnodeCount: 1\n\n#Environment details.\nenvironment:\n  # Environment name\n  name: phinmo_pipeline_env\n  # Environment version\n  version:\n  # Environment variables set for the run.\n  #environmentVariables:\n  #  EXAMPLE_ENV_VAR: EXAMPLE_VALUE\n  # Python details\n  python:\n    # user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.\n    userManagedDependencies: false\n    # The python interpreter path\n    interpreterPath: python\n    # Path to the conda dependencies file to use for this run. If a project\n    # contains multiple programs with different sets of dependencies, it may be\n    # convenient to manage those environments with separate files.\n    condaDependenciesFile: environment.yml\n    # The base conda environment used for incremental environment creation.\n    baseCondaEnvironment: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n  # Docker details\n  \n# History details.\nhistory:\n  # Enable history tracking -- this allows status, logs, metrics, and outputs\n  # to be collected for a run.\n  outputCollection: true\n  # Whether to take snapshots for history.\n  snapshotProject: true\n  # Directories to sync with FileWatcher.\n  directoriesToWatch:\n  - logs\n# data reference configuration details\ndataReferences: {}\n# The configuration details for data.\ndata: {}\n# Project share datastore reference.\nsourceDirectoryDataStore:\n<\/code><\/pre>\n<p>I already tried a few things like overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml. But I am at a loss at what else to try. the T01_Test_Task.py runs without issues on its own. But putting it into a pipeline just does not seem to work.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1658922798813,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1658924360076,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73137433",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":47.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":null,
        "Challenge_title":"ModuleNotFoundError while using AzureML pipeline with yml file based RunConfiguration and environment.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":366,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544524371740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cologne Germany",
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Okay I found the issue.\nI am unnecessarily using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. I was able to see that only in the Task description in the Azure Machine Learning Studio UI.<\/p>\n<p>I was able to just remove that part and now it works:<\/p>\n<pre><code>task_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name='T01_Test_Task.py',\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    runconfig=task_1_run_config, \n    compute_target=compute_target\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":64,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I am trying to create an Azure ML Environment using a Dockerfile but it contains the 'COPY' instruction.    <\/p>\n<p>From the documentation of Environment.from_dockerfile ( <a href=\"https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-\">https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-<\/a> ), I can not find a way to give it some files along with the Dockerfile itself.    <\/p>\n<p>So, how to pass context to enable using COPY in the Dockerfile ?    <\/p>\n<p>Thank you for your time !<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634739305317,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/597612\/(azure)(ml)(python-sdk)(environment)(docker)-docke",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"[Azure][ML][Python SDK][Environment][Docker] Docker copy missing context",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Docker context is not supported with AzureML Python SDK at the moment. Context support will added later this year<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":19,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I\u2019m using wandb version 0.14.0 in an ipynb file using vscode as part of assignment 1 of the \u2018Effective MLOps\u2019 course (logging dataset as artifact and visualising data with a table)<\/p>\n<p>When I execute <code>run.finish()<\/code> at the end of my file the cell hangs indefinitely with the message<\/p>\n<pre><code class=\"lang-console\">Waiting for W&amp;B process to finish... (success).\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678979712586,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/run-finish-hangs\/4069",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.8,
        "Challenge_reading_time":5.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Run.finish() hangs",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":159.0,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>How much data are you logging? It might still be uploading in the background. You can check one of the <code>debug.log<\/code> or <code>debug-internal.log<\/code> files in the <code>wandb<\/code> folder to see if there is any upload activity happening<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.4,
        "Solution_reading_time":3.22,
        "Solution_score_count":null,
        "Solution_sentence_count":5,
        "Solution_word_count":37,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1622632545867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a <strong>Jupyter Lab<\/strong> instance on <strong>AWS SageMaker<\/strong>.<\/p>\n<p>Kernel: <code>conda_mxnet_latest_p37<\/code>.<\/p>\n<p><code>url_lib<\/code> contains some false urls, that I exception handle.<\/p>\n<pre><code>['15', '259', '26', '58', 'https:\/\/imagepool.1und1-drillisch.de\/v2\/download\/nachhaltigkeitsbericht\/1&amp;1Drillisch_Sustainability_Report_EN_2018.pdf', 'https:\/\/imagepool.1und1-drillisch.de\/\/v2\/download\/nachhaltigkeitsbericht\/2018-04-06_1und1-Drillisch_Sustainability_Report_eng.pdf', '6', 'http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf', '80', 'https:\/\/multimedia.3m.com\/mws\/media\/1691941O\/2019-sustainability-report.PDF', 'https:\/\/s3-us-west-2.amazonaws.com\/ungc-production\/attachments\/cop_2020\/483648\/original\/GPIC_Sustainability_Report_2020__-_40_Years_of_Sustainable_Success.pdf?1583154650', 'https:\/\/drive.google.com\/open?id=1_dnBcfXWjexy9QoWRhOk_3gnOkWfYRCw', 'http:\/\/aepsustainability.com\/performance\/docs\/2020AEPGRIReport.pdf']  # sample\n<\/code><\/pre>\n<p>However, ones that are working URLs, throw this error:<\/p>\n<pre><code>[Errno 13] Permission denied: '\/data'\n<\/code><\/pre>\n<p>I don't have the directory opened, nor files since I they're not downloaded.<\/p>\n<p>I ran in <strong>Terminal<\/strong> without luck:<\/p>\n<pre><code>sh-4.2$ chmod 777 data\nsh-4.2$ chmod 777 data\/\nsh-4.2$ chmod 777 data\/gri\nsh-4.2$ chmod 777 data\/gri\/\n<\/code><\/pre>\n<p><strong>Code:<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport opendatasets as od\nimport urllib\nimport zipfile\nimport os\n\ncsr_df = pd.read_excel('data\/Company Sustainability Reports.xlsx', index_col=None)\nurl_list = csr_df['Report PDF Address'].tolist()\n\nfor url in url_list:\n    try:\n        download = od.download(url, '\/data\/gri\/')\n        filename = url.rsplit('\/', 1)[1]\n\n        path_extract = 'data\/gri\/' + filename\n        with zipfile.ZipFile('data\/gri\/' + filename + '.zip', 'r') as zip_ref:\n            zip_ref.extractall(path_extract)\n\n        os.remove(path_extract + 'readme.txt')\n\n        filenames = os.listdir(path_extract)\n        scans = []\n        for f in filenames:\n            with Image.open(path_extract + f) as img:\n                matrix = np.array(img)\n                scans.append(matrix)\n\n        # shutil.rmtree(path_extract)\n        os.remove(path_extract[:-1] + '.zip')\n\n    except (urllib.error.URLError, IOError, RuntimeError) as e:\n        print('Download PDFs', e)\n<\/code><\/pre>\n<p><strong>Output:<\/strong><\/p>\n<pre><code>Download PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs list index out of range\nDownload PDFs [Errno 13] Permission denied: '\/data'\n...\n<\/code><\/pre>\n<p>Please let me know if there is anything else I should clarify.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639402213260,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1639406637992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70335470",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":41.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"'[Errno 13] Permission denied' - Jupyter Labs on AWS SageMaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":684.0,
        "Challenge_word_count":264,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622632545867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p><code>download<\/code> has a forward-slash <code>\/<\/code> as first character of save directory (second parameter). I removed this:<\/p>\n<pre><code>download = od.download(url, 'data\/gri\/')\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>...\nDownloading http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf to data\/gri\/1556248045.pdf\n450560it [00:02, 207848.59it\/s]\n...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":12.7,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5,
        "Solution_word_count":30,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"This is my code.\n\n```python\nfrom datetime import datetime\nfrom sagemaker.multidatamodel import MultiDataModel\nmme = MultiDataModel(\n    name=\"LV-multi-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n    model_data_prefix=model_dir, # 2\uc5d0\uc11c \uad6c\ud55c \ubaa8\ub378\uc774 \ubaa8\uc5ec\uc788\ub294 \ud3f4\ub354(\uacbd\ub85c)!!,\n    model=sagemaker_model,  # \ubaa8\ub378 \uac1d\uccb4 1\uac1c \uc6b0\uc120 \ub123\uae30\n    sagemaker_session=sess\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.g4dn.xlarge\"\n)\n```\n\nAnd error message.\nHow can I find Ecr Image(within multi-models=true)?\n```\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Your Ecr Image 763104351884.dkr.ecr.ap-northeast-2.amazonaws.com\/pytorch-inference:1.8.1-gpu-py3 does not contain required com.amazonaws.sagemaker.capabilities.multi-models=true Docker label(s).\n```",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660122368052,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667925983239,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJQBp6A_dSQm1RJ3f8AYMmg\/how-can-make-multi-model-endpoint-with-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":10.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How can make multi model endpoint with SageMaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":82.0,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there - thanks for opening this thread. Multi-model endpoints are not supported on GPU instance types, see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-endpoint-instance\n\nIn order to host a multi-model endpoint, choose a CPU instance type instead. The ECR image for CPUs will contain the required  `com.amazonaws.sagemaker.capabilities.multi-models=true` label, see here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1660139884319,
        "Solution_link_count":2,
        "Solution_readability":19.9,
        "Solution_reading_time":7.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5,
        "Solution_word_count":46,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1370505440848,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I attempt to Upgrade <strong>Pandas<\/strong> above version <strong>1.1.5<\/strong> on my <strong>AWS Sagemaker<\/strong> provided <strong>JupyterLab<\/strong> notebook I receive the error <strong>No Matching Distribution Found<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\n!{sys.executable} -m pip install --pre --upgrade pandas==1.3.5\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>ERROR: Could not find a version that satisfies the requirement pandas==1.3.5 (from versions: 0.1, 0.2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.25.2, 0.25.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5)\nERROR: No matching distribution found for pandas==1.3.5\n<\/code><\/pre>\n<h2>Background<\/h2>\n<p>I created a Notebook instance from the AWS Console via <strong>AWS Sagemaker -&gt; Notebook instances -&gt; Create Notebook instance<\/strong>.<\/p>\n<p>I then selected the Kernel <strong>conda_Python3<\/strong>.<\/p>\n<p>I use <strong>sys.executable<\/strong> to show the Kernel's Python, Pip and Pandas version.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -version\nPython 3.6.13\n\n!{sys.executable} -m pip show pip\nName: pip\nVersion: 21.3.1\nSummary: The PyPA recommended tool for installing Python packages.\nHome-page: https:\/\/pip.pypa.io\/\nAuthor: The pip developers\nAuthor-email: distutils-sig@python.org\nLicense: MIT\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: \nRequired-by: \n\n!{sys.executable} -m pip show pandas\nName: pandas\nVersion: 1.1.5\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: https:\/\/pandas.pydata.org\nAuthor: \nAuthor-email: \nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: numpy, python-dateutil, pytz\nRequired-by: autovizwidget, awswrangler, hdijupyterutils, odo, sagemaker, seaborn, shap, smclarify, sparkmagic, statsmodels\n<\/code><\/pre>\n<p>I cannot upgrade <strong>Pandas<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -m pip install --pre --upgrade pandas\nRequirement already satisfied: pandas in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (1.1.5)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy&gt;=1.15.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (1.18.5)\nRequirement already satisfied: six&gt;=1.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0)\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654183154907,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72478572",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":43.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon Sagemaker JupiterLab Notebook - No matching distribution found for Pandas",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":315,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504724308867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington D.C., DC, United States",
        "Poster_reputation_count":97.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>see this - <a href=\"https:\/\/stackoverflow.com\/questions\/68750375\/no-matching-distribution-found-for-pandas-1-3-1\">No matching distribution found for pandas==1.3.1<\/a><\/p>\n<p>The latest version to support python 3.6 is 1.1.5.<\/p>\n<p>You can create a new conda environment with python version &gt;= 3.7 in your existing notebook, or move to notebooks with Amazon Linux 2 (see <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebook-instance-now-supports-amazon-linux-2\/\" rel=\"nofollow noreferrer\">blog post<\/a>). In the AL2 notebooks, <code>conda_python3<\/code> kernels come with Python 3.8.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":13.2,
        "Solution_reading_time":8.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8,
        "Solution_word_count":58,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1376680978667,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":1454.0,
        "Answerer_view_count":109.0,
        "Challenge_adjusted_solved_time":4809.9866886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am fairly new to kubernetes - I have developed web UI\/API that automates model deployment using Azure Machine Learning Services to Azure Kubernetes Services (AKS). As a hardening measure, I am tying to set up managed identity for deployed pods in AKS using <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-azure-ad-identity\" rel=\"nofollow noreferrer\">this documentation<\/a>. One of the step is to edit the deployment to add identity-feature label at <code>\/spec\/template\/metadata\/labels<\/code> for the deployment (see para starting like <code>Edit the deployment to add ...<\/code> in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-azure-ad-identity#assign-azure-identity-to-machine-learning-web-service\" rel=\"nofollow noreferrer\">this section<\/a>). <\/p>\n\n<p>I wish to automate this step using python kubernetes client (<a href=\"https:\/\/github.com\/kubernetes-client\/python\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kubernetes-client\/python<\/a>). Browsing the available API, I was wondering that perhaps <code>patch_namespaced_deployment<\/code> will allow me to edit deployment and add label at <code>\/spec\/template\/metadata\/labels<\/code>. I was looking for some example code using the python client for the same - any help to achieve above will be appreciated.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":5,
        "Challenge_created_time":1590392663393,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61997914",
        "Challenge_link_count":4,
        "Challenge_participation_count":6,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How to edit\/patch kubernetes deployment to add label using python",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":6338.0,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Poster_created_time":1281518863807,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mumbai, India",
        "Poster_reputation_count":45542.0,
        "Poster_view_count":3034.0,
        "Solution_body":"<p>Have a look at this example:<\/p>\n<p><a href=\"https:\/\/github.com\/kubernetes-client\/python\/blob\/master\/examples\/deployment_crud.py#L62-L70\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kubernetes-client\/python\/blob\/master\/examples\/deployment_crud.py#L62-L70<\/a><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def update_deployment(api_instance, deployment):\n    # Update container image\n    deployment.spec.template.spec.containers[0].image = &quot;nginx:1.16.0&quot;\n    # Update the deployment\n    api_response = api_instance.patch_namespaced_deployment(\n        name=DEPLOYMENT_NAME,\n        namespace=&quot;default&quot;,\n        body=deployment)\n    print(&quot;Deployment updated. status='%s'&quot; % str(api_response.status))\n<\/code><\/pre>\n<p>The Labels are on the deployment object, from the App v1 API,<\/p>\n<pre><code>kind: Deployment\nmetadata:\n  name: deployment-example\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app: nginx\n<\/code><\/pre>\n<p>which means you need to update the following:<\/p>\n<p><code>deployment.spec.template.metadata.labels.app = &quot;nginx&quot;<\/code><\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1607708615472,
        "Solution_link_count":2,
        "Solution_readability":24.4,
        "Solution_reading_time":14.66,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6,
        "Solution_word_count":71,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1375058329287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":171.1678108333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Summary<\/strong>: I am trying to define a <code>dvc<\/code> step using <code>dvc-run<\/code> where the command depends on some environment variables (for instance <code>$HOME<\/code>). The problem is that when I'm defining the step on machine A, then the variable is expanded when stored in the <code>.dvc<\/code> file. In this case, it won't be possible to reproduce the step on machine B. Did I hit a limitation of <code>dvc<\/code>? If that's not the case, what's the right approach?<\/p>\n\n<p><strong>More details<\/strong>: I faced the issue when trying to define a step for which the command is a <code>docker run<\/code>. Say that:<\/p>\n\n<ul>\n<li>on machine A <code>myrepo<\/code> is located at <code>\/Users\/user\/myrepo<\/code> and <\/li>\n<li>on machine B it is to be found at <code>\/home\/ubuntu\/myrepo<\/code>. <\/li>\n<\/ul>\n\n<p>Furthermore, assume I have a script <code>myrepo\/script.R<\/code> which processes a data file to be found at <code>myrepo\/data\/mydata.txt<\/code>. Lastly, assume that my step's command is something like: <\/p>\n\n<pre><code>docker run -v $HOME\/myrepo\/:\/prj\/ my_docker_image \/prj\/script.R \/prj\/data\/mydata.txt\n<\/code><\/pre>\n\n<p>If I'm running <code>dvc run -f step.dvc -d ... -d ... [cmd]<\/code> where <code>cmd<\/code> is the <code>docker<\/code> execution above, then in <code>step.dvc<\/code> the environment variable <code>$HOME<\/code> will be expanded. In this case, the step will be broken on machine B.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1563703426437,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57132106",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":8.6,
        "Challenge_reading_time":18.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Expanding environment variables in the command part of a dvc run",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300789717227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11410.0,
        "Poster_view_count":1782.0,
        "Solution_body":"<p>From <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Use single quotes ' instead of \" to wrap the command if there are environment variables in it, that you want to be evaluated dynamically. E.g. dvc run -d script.sh '.\/myscript.sh $MYENVVAR'<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1564319630556,
        "Solution_link_count":1,
        "Solution_readability":9.3,
        "Solution_reading_time":4.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":37,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1547478776530,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker does not update the package",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1546.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527781503483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Metz, France",
        "Poster_reputation_count":352.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2,
        "Solution_word_count":25,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am from R background where we can use Plumber kind tool which provide visualization\/graph as Image via end points so we can integrate in our Java application.<\/p>\n\n<p>Now I want to integrate my Python\/Juypter visualization graph with my Java application but not sure how to host it and make it as endpoint. Right now I using AWS sagemaker to host Juypter notebook<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525949293883,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50271174",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":5.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to make a Python Visualization as service | Integrate with website | specially sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. <\/p>\n\n<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn\u2019t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.3,
        "Solution_reading_time":9.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":123,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried connecting through Sagemaker notebook to RDS. However, to connect to RDS, my public IP needs to be allowed for security reasons. I can see when I run this command: &quot;curl ifconfig.me&quot; on Sagemaker Notebook instance that public IP keeps changing from time to time.<\/p>\n<p>What is the correct way to connect to RDS with notebook on sagemaker? Do I need to crawl the RDS with AWS Glue and then use Athena on crawled tables and then take the query results from S3 with Sagemaker notebook?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599481052090,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63777462",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Jupyter Notebook Cannot Connect to RDS",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":865.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509559597047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.<\/p>\n<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":7.8,
        "Solution_reading_time":7.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6,
        "Solution_word_count":105,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1569518464147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":432.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Pip installation is stuck in an infinite loop if there are unresolvable conflicts in dependencies. To reproduce, <code>pip==20.3.0<\/code> and:<\/p>\n<pre><code>pip install pyarrow==2.0.0 azureml-defaults==1.18.0\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":6,
        "Challenge_created_time":1606928045387,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1608739608112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65112585",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":11.3,
        "Challenge_reading_time":4.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":13,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Pip installation stuck in infinite loop if unresolvable conflicts in dependencies",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2146.0,
        "Challenge_word_count":34,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568658032912,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA, USA",
        "Poster_reputation_count":133.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Workarounds:<\/p>\n<p>Local environment:\nDowngrade pip to &lt; 20.3<\/p>\n<p>Conda environment created from yaml:\nThis will be seen only if conda-forge is highest priority channel, anaconda channel doesn't have pip 20.3 (as of now). To mitigate the issue please explicitly specify pip&lt;20.3 (!=20.3 or =20.2.4 pin to other version) as a conda dependency in the conda specification file<\/p>\n<p>AzureML experimentation:\nFollow the case above to make sure pinned pip resulted as a conda dependency in the environment object, either from yml file or programmatically<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":11.0,
        "Solution_reading_time":7.12,
        "Solution_score_count":12.0,
        "Solution_sentence_count":6,
        "Solution_word_count":83,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a conda environment called <code>Foo<\/code>. After activating this environment I installed Kedro with <code>pip<\/code>, since <code>conda<\/code> was giving me a conflict. Even though I'm inside the <code>Foo<\/code> environment, when I run:<\/p>\n<pre><code>kedro jupyter lab\n<\/code><\/pre>\n<p>It picks up the modules from my <code>base<\/code> environment, not the <code>Foo<\/code> environment. Any idea, why this is happening, and how I can change what modules my notebook detect?<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>By mangling with my code I found out that on the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> it was calling the python from the base environment, not the <code>Foo<\/code> environment. I changed it manually, but is there a mode automatic way of setting the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> to use the current environment I'm on?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":1,
        "Challenge_created_time":1652795556263,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1652825440368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72275283",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro using wrong conda environment",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":115.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423164285360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p>The custom Kedro kernel spec is a feature that I recently added to Kedro. When you run <code>kedro jupyter lab\/notebook<\/code> it should automatically pick up on the conda environment without you needing to manually edit the kernel.json file. I tested this myself to check that it worked so I'm very interested in understanding what's going on here!<\/p>\n<p>The function <a href=\"https:\/\/github.com\/kedro-org\/kedro\/blob\/58c57c384f5257b998edebb99d94bff46574ae1e\/kedro\/framework\/cli\/jupyter.py#L99\" rel=\"nofollow noreferrer\"><code>_create_kernel<\/code><\/a> is what makes the the Kedro kernel spec. The docstring for that explains what's going on, but in short we delegate to <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L92\" rel=\"nofollow noreferrer\"><code>ipykernel.kernelspec.install<\/code><\/a>. This generates a kernelspec that points towards the Python path given by <code>sys.executable<\/code> (see <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L27\" rel=\"nofollow noreferrer\"><code>make_ipkernel_cmd<\/code><\/a>). In theory this should already point towards the correct Python path, which takes account of the conda environment.<\/p>\n<p>It's worth checking <code>which kedro<\/code> to see which conda environment that points to, and if we need to debug further then please do raise an issue on our <a href=\"https:\/\/github.com\/kedro-org\/kedro\" rel=\"nofollow noreferrer\">Github repo<\/a>. I'd definitely like to get to the bottom of this and understand where the problem is.<\/p>\n<p>P.S. you can also do a plain <code>jupyter lab\/notebook<\/code> to launch a kernel with the right conda environment and then run <code>%load_ext kedro.extras.extensions.ipython<\/code> in the first cell. This is basically equivalent to using the Kedro kernelspec, which loads the Kedro IPython extension automatically.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4,
        "Solution_readability":11.8,
        "Solution_reading_time":25.37,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17,
        "Solution_word_count":217,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Assuming you don't add any custom lifecycle configuration scripts?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675651245777,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1675999192316,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqGiSmuPcS9WxsjybQq5d2w\/what-s-kind-of-the-average-launch-time-for-a-sagemaker-studio-notebook",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":1.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"What's kind of the average launch time for a SageMaker Studio notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":20,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Yann,\n\nTypically you should be using the fast launch instance types, which is designed to launch under 2 minutes. Generally it is 5-10 times faster than instance based notebooks.\n\nHere is to documentation which states so: [https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html](link).\n\nHope it helps!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1675676899059,
        "Solution_link_count":1,
        "Solution_readability":9.7,
        "Solution_reading_time":3.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":41,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1463756509236,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Uppsala, Sverige",
        "Answerer_reputation_count":400.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":3335.9770963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently working on machine learning project with Azure Machine Learning Services. But I found the problem that I can't update a new docker image to the existing web service (I want to same url as running we service). <\/p>\n\n<p>I have read the documentation but it doesn't really tell me how to update (documentation link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where<\/a>). \nThe documentation said that we have to use update() with image = new-image. <\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\n\nservice_name = 'aci-mnist-3\n\n# Retrieve existing service\nservice = Webservice(name = service_name, workspace = ws)\n\n# Update the image used by the service\nservice.update(image = new-image)\n\nprint(service.state)\n<\/code><\/pre>\n\n<p>But the <code>new-image<\/code> isn't described where it comes from. <\/p>\n\n<p>Does anyone know how to figure out this problem?<\/p>\n\n<p>Thank you<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544435800590,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1554674817316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53703191",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How to update the existing web service with a new docker image on Azure Machine Learning Services?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541599822383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangkok, \u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22",
        "Poster_reputation_count":23.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The documentation could be a little more clear on this part, I agree. The <code>new-image<\/code> is an image object that you should pass into the <code>update()<\/code> function. If you just created the image you might already have the object in a variable, then just pass it. If not, then you can obtain it from your workspace using<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.image.image import Image\nnew_image = Image(ws, image_name)\n<\/code><\/pre>\n\n<p>where <code>ws<\/code> is your workspace object and <code>image_name<\/code> is a string with the name of the image you want to obtain. Then you go on calling <code>update()<\/code> as<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.webservice import Webservice\n\nservice_name = 'aci-mnist-3'\n\n# Retrieve existing service\nservice = Webservice(name = service_name, workspace = ws)\n\n# Update the image used by the service\nservice.update(image = new_image) # Note that dash isn't supported in variable names\n\nprint(service.state)\n<\/code><\/pre>\n\n<p>You can find more information in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/intro?view=azure-ml-py\" rel=\"nofollow noreferrer\">SDK documentation<\/a><\/p>\n\n<p>EDIT:\nBoth the <code>Image<\/code> and the <code>Webservice<\/code> classes above are abstract parent classes.<\/p>\n\n<p>For the <code>Image<\/code> object, you should really use one of these classes, depending on your case:<\/p>\n\n<ul>\n<li><code>ContainerImage<\/code><\/li>\n<li><code>UnknownImage<\/code><\/li>\n<\/ul>\n\n<p>(see <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.image?view=azure-ml-py\" rel=\"nofollow noreferrer\">Image package<\/a> in the documentation).<\/p>\n\n<p>For the <code>Webservice<\/code> object, you should use one of these classes, depending on your case:<\/p>\n\n<ul>\n<li><code>AciWebservice<\/code><\/li>\n<li><code>AksWebservice<\/code><\/li>\n<li><code>UnknownWebservice<\/code><\/li>\n<\/ul>\n\n<p>(see <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice?view=azure-ml-py\" rel=\"nofollow noreferrer\">Webservice package<\/a> in the documentation).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1566684334863,
        "Solution_link_count":3,
        "Solution_readability":13.9,
        "Solution_reading_time":28.15,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14,
        "Solution_word_count":213,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I understand what the entry script\/scoring script is and does. See <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/deploying-models.html\" rel=\"nofollow noreferrer\">here<\/a> as an example. As I struggle to expose my deployed model via code as described <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">here<\/a> (see also <a href=\"https:\/\/stackoverflow.com\/questions\/67535014\/deploy-model-to-azure-machine-learning-via-azuremlsdk\">here<\/a>), I am trying to use the UI ml.azure.com instead. I am a bit puzzled by the mandatory dependency: conda dependencies file:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BWtsE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BWtsE.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have an R model but clearly this is a Python thing. What shall I use in this case?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621004713270,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67536581",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":13.8,
        "Challenge_reading_time":12.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"conda dependencies file r model in azure machine learning",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":609.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>conda is actually not just a Python thing, you might be thinking of pip?<\/p>\n<p>Conda is a package &amp; environment manager for nearly any kind of package, provided that it has been uploaded to anaconda. So you <em>can<\/em> use anaconda (and conda environment files) for R projects.<\/p>\n<p>The trouble is that the <code>azuremlsdk<\/code> CRAN package is not hosted as an anaconda package, but is probably needed for the scoring service. Worth using a file like below to see what it works.<\/p>\n<p>If it doesn't work, then I agree that this UI needs to generalized to better support R model deployment scenarios.<\/p>\n<p>It is also possible to add the <code>azuremlsdk<\/code> CRAN package to anaconda, but that requires <a href=\"https:\/\/stackoverflow.com\/a\/36653411\/3842610\">some extra work<\/a>, but ideally you shouldn't have to require this much manual effort.<\/p>\n<code>environment.yml<\/code>\n<p>Here's an example conda dependencies file for R.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: scoring_environment\nchannels:\n  - defaults\ndependencies:\n  - r-base=3.6.1\n  - r-essentials=3.6.0\n  # whatever other dependencies you have\n  - r-tidyverse=1.2.1\n  - r-caret=6.0_83\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":10.0,
        "Solution_reading_time":15.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11,
        "Solution_word_count":157,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We developed a Jupyter Notebook in a local machine to train models with the Python (V3) libraries <code>sklearn<\/code> and <code>gensim<\/code>.\nAs we set the <code>random_state<\/code> variable to a fixed integer, the results were always the same.<\/p>\n\n<p>After this, we tried moving the notebook to a workspace in Azure Machine Learning Studio (classic), but the results differ even if we leave the <code>random_state<\/code> the same.<\/p>\n\n<p>As suggested in the following links, we installed the same libraries versions and checked the <code>MKL<\/code> version was the same and the <code>MKL_CBWR<\/code> variable was set to <code>AUTO<\/code>.<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/46766714\/t-sne-generates-different-results-on-different-machines\">t-SNE generates different results on different machines<\/a><\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/38228088\/same-python-code-same-data-different-results-on-different-machines\">Same Python code, same data, different results on different machines<\/a><\/p>\n\n<p>Still, we are not able to get the same results.<\/p>\n\n<p>What else should we check or why is this happening?<\/p>\n\n<p><strong>Update<\/strong><\/p>\n\n<p>If we generate a <code>pkl<\/code> file in the local machine and import it in AML, the results are the same (as the intention of the pkl file is).<\/p>\n\n<p>Still, we are looking to get the same results (if possible) without importing the pkl file.<\/p>\n\n<p><strong>Library versions<\/strong><\/p>\n\n<pre><code>gensim 3.8.3.\nsklearn 0.19.2.\nmatplotlib 2.2.3.\nnumpy 1.17.2.\nscipy 1.1.0.\n<\/code><\/pre>\n\n<p><strong>Code<\/strong><\/p>\n\n<p>Full code can be found <a href=\"https:\/\/t.ly\/YlCi\" rel=\"nofollow noreferrer\">here<\/a>, sample data link inside.<\/p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nfrom gensim.models import KeyedVectors\n%matplotlib inline\n\nimport time\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\nwordvectors_file_vec = '..\/libraries\/embeddings-new_large-general_3B_fasttext.vec'\nwordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)\n\nmath_quests = # some transformations using wordvectors\n\ndf_subset = pd.DataFrame()\n\npca = PCA(n_components=3, random_state = 42)\npca_result = pca.fit_transform(mat_quests)\ndf_subset['pca-one'] = pca_result[:,0]\ndf_subset['pca-two'] = pca_result[:,1] \n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state = 42)\ntsne_results = tsne.fit_transform(mat_quests)\n\ndf_subset['tsne-2d-one'] = tsne_results[:,0]\ndf_subset['tsne-2d-two'] = tsne_results[:,1]\n\npca_50 = PCA(n_components=50, random_state = 42)\npca_result_50 = pca_50.fit_transform(mat_quests)\nprint('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))\n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, random_state = 42)\ntsne_pca_results = tsne.fit_transform(pca_result_50)\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":5,
        "Challenge_created_time":1591464199347,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1591489515656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62235365",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":12.2,
        "Challenge_reading_time":42.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"Models generate different results when moving to Azure Machine Learning Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":201.0,
        "Challenge_word_count":320,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585590244876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Definitely empathize with the issue you're having. Every data scientist has struggled with this at some point.<\/p>\n\n<p>The hard truth I have for you is that Azure ML Studio (classic) isn't really capable of  solving this \"works on my machine\" problem. However, the good news is that Azure ML Service is incredible at it. Studio classic doesn't let you define custom environments deterministically, only add and remove packages (and not so well even at that) <\/p>\n\n<p>Because ML Service's execution is built on top of <code>Docker<\/code> containers and <code>conda<\/code> environments, you can feel more confident in repeated results. I highly recommend you take the time to learn it (and I'm also happy to debug any issues that come up). Azure's <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\" rel=\"nofollow noreferrer\">MachineLearningNotebooks repo<\/a> has a lot of great tutorials for getting started.<\/p>\n\n<p>I spent two hours making <a href=\"https:\/\/github.com\/swanderz\/MachineLearningNotebooks\/blob\/SO_CPR\/how-to-use-azureml\/training\/train-on-amlcompute\/train-on-amlcompute.ipynb\" rel=\"nofollow noreferrer\">a proof of concept<\/a> that demonstrate how ML Service solves the problem you're having by synthesizing:<\/p>\n\n<ul>\n<li>your code sample (before you shared your notebook),<\/li>\n<li><a href=\"https:\/\/scikit-learn.org\/stable\/auto_examples\/manifold\/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py\" rel=\"nofollow noreferrer\">Jake Vanderplas's sklearn example<\/a>, and<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-amlcompute\/train-on-amlcompute.ipynb\" rel=\"nofollow noreferrer\">this Azure ML tutorial<\/a> on remote training.<\/li>\n<\/ul>\n\n<p>I'm no T-SNE expert, but from the screenshot below, you can see that the t-sne outputs are the same when I run the script locally and remotely. This might be possible with Studio classic, but it would be hard to guarantee that it will always work.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mhlg6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mhlg6.png\" alt=\"Azure ML Experiment Results Page\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6,
        "Solution_readability":13.5,
        "Solution_reading_time":28.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17,
        "Solution_word_count":242,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"Currently, the `silnlp.nmt.translate` script always creates a ClearML task. This should be optional. By default, it should just execute locally.",
        "Challenge_closed_time":1657980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641546207000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/120",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.7,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":147.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Execute translate script without creating ClearML task",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think that this error might be preventing me from using the Translate script locally.\r\n\r\nWhen I try I get the following error:\r\nInstalling the current project: silnlp (1.0.0)\r\n(silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate BT-English\/cba-en\/cba-en_cp01 --src-project cba --trg-iso en --books EXO --output-usfm BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT --checkpoint best\r\n2022-06-28 21:02:08,808 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-06-28 21:02:09,149 - silnlp.common.utils - INFO - Git commit: f46a63c3b3\r\nRetrying (Retry(total=239, connect=239, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4556fa0>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\nRetrying (Retry(total=238, connect=238, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569190>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n^CRetrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569340>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n\r\nprint(args):\r\nNamespace(books=['EXO'], checkpoint='best', clearml_queue=None, eager_execution=False, end_seq=None, experiment='BT-English\/cba-en\/cba-en_cp01', memory_growth=False, output_usfm='BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT', src=None, src_prefix=None, src_project='cbaNT', start_seq=None, trg=None, trg_iso='en', trg_prefix=None)\r\n Tested this for translating and it worked fine.   (silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate --checkpoint last --src-project tl-TCB --src \/home\/david\/disk2\/gutenberg\/Paratext\/projects\/TCB\/091SAtlASD15.SFM --trg-iso blx --output-usfm \/home\/david\/disk2\/gutenberg\/BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\/results\/091SAAMIU_last.sfm BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\r\n2022-07-16 15:03:40,452 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-07-16 15:03:40,828 - silnlp.common.utils - INFO - Git commit: 8cd5b9c649\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 231, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 225, in main\r\n    translator.translate_text_file(args.src, args.trg_iso, args.trg)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 151, in translate_text_file\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{os.path.basename(src_file_path)}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 79, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 9, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml_connection.py\", line 24, in __post_init__\r\n    self.name = self.name.replace(\"\\\\\", \"\/\")\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":15.1,
        "Solution_reading_time":46.38,
        "Solution_score_count":null,
        "Solution_sentence_count":31,
        "Solution_word_count":282,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1310893185208,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Thiruvananthapuram, Kerala, India",
        "Answerer_reputation_count":2763.0,
        "Answerer_view_count":851.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to store my model artifacts using mlflow to s3. In the API services, we use <code>MLFLOW_S3_ENDPOINT_URL<\/code> as the s3 bucket. In the mlflow service, we pass it as an environment variable. But, the mlflow container servicer fails with the below exception:<\/p>\n<pre><code>mflow_server  | botocore.exceptions.HTTPClientError: An HTTP Client raised an unhandled exception: Not supported URL scheme s3\n<\/code><\/pre>\n<p>docker-compose file as below:<\/p>\n<pre><code>version: &quot;3.3&quot;\nservices:\n  prisim-api:\n    image: prisim-api:latest\n    container_name: prisim-api\n    expose:\n      - &quot;8000&quot;\n    environment: \n    - S3_URL=s3:\/\/mlflow-automation-artifacts\/\n    - MLFLOW_SERVER=http:\/\/mlflow:5000\n    - AWS_ID=xyz+\n    - AWS_KEY=xyz\n\n    networks:\n      - prisim \n    depends_on:\n      - mlflow\n    links:\n            - mlflow\n    volumes:\n      - app_data:\/usr\/data\n  mlflow:\n    image: mlflow_server:latest\n    container_name: mflow_server\n    ports:\n      - &quot;5000:5000&quot;    \n    environment:\n      - AWS_ACCESS_KEY_ID=xyz+\n      - AWS_SECRET_ACCESS_KEY=xyz\n      - MLFLOW_S3_ENDPOINT_URL=s3:\/\/mlflow-automation-artifacts\/\n    healthcheck:\n      test: [&quot;CMD&quot;, &quot;echo&quot;, &quot;mlflow server is running&quot;]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n    networks:\n       - prisim \nnetworks:\n prisim:\nvolumes:\n  app_data:\n<\/code><\/pre>\n<p>Why the scheme s3 is not supported?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1640933189943,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70539698",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":17.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"MlFlow - Unable to run with S3 as default-artifact-root",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":932.0,
        "Challenge_word_count":132,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310893185208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Thiruvananthapuram, Kerala, India",
        "Poster_reputation_count":2763.0,
        "Poster_view_count":851.0,
        "Solution_body":"<p>I found the solution.<\/p>\n<p>I have added <code>[&quot;AWS_DEFAULT_REGION&quot;]<\/code> to the environment variables and it worked.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":9.8,
        "Solution_reading_time":1.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2,
        "Solution_word_count":15,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1489644560420,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Planet Earth",
        "Answerer_reputation_count":791.0,
        "Answerer_view_count":253.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to install python package cassandra driver in Azure Machine Learning studio. I am following this answer from <a href=\"https:\/\/stackoverflow.com\/questions\/44371692\/install-python-packages-in-azure-ml\">here<\/a>. Unfortunately i don't see any wheel file for cassandra-driver <a href=\"https:\/\/pypi.python.org\/pypi\/cassandra-driver\/\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/cassandra-driver\/<\/a> so i downloaded the .tar file and converted to zip.<\/p>\n<p>I included this .zip file as dataset and connected to python script<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" alt=\"jpg1\" \/><\/a><\/p>\n<p>But when i run it, it says No module named cassandra\n<a href=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" alt=\"jpg2\" \/><\/a><\/p>\n<p>Does this work only with wheel file? Any solution is much appreciated.<\/p>\n<p>I am using Python Version :  Anoconda 4.0\/Python 3.5<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1519110374087,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48879595",
        "Challenge_link_count":7,
        "Challenge_participation_count":3,
        "Challenge_readability":10.9,
        "Challenge_reading_time":14.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"ImportError: No module named cassandra in Azure Machine Learning Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":500.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489644560420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Planet Earth",
        "Poster_reputation_count":791.0,
        "Poster_view_count":253.0,
        "Solution_body":"<p>I got it working. Changed the folder inside .zip file to <code>\"cassandra\"<\/code> (just like cassandra package). <\/p>\n\n<p>And in the Python script, i added <\/p>\n\n<pre><code>from cassandra import *\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4,
        "Solution_word_count":29,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I have been using Sagemaker Studio Notebook and suddenly it started hanging.\nWhen this happens, the notebook freezes completely. Than I have to wait some seconds (the delay duration is not constant and is common to reach about 30 seconds) and then it just freezes again, making its usage impossible.\nI was using a temporary account provided by Udacity and after trying different approaches to find and solve the problem, I switched to a personal account but the problem persists.\nApproaches I have tried so far:\n- Shutdow and start kernel\n- Restart kernel\n- Restart kernel and clear outputs\n- Log out and Login (from Sagemaker)\n- Log out and Login (from AWS)\n- Change region\n- Trying a different browser (I tried Chrome and Firefox)\n- Trying using other account (personal)\n\nI also checked CloudWatch logs but didn't find anything that seemed unusual.",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657750167416,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1667926330662,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbUkR0L2-Q1CcAHtTbLYJmg\/sagemaker-notebook-keeps-hanging-freezing",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":10.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Notebook keeps hanging\/freezing",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":140,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The most likely cause of this from my experience is a **(very) large number of active git changes**.\n\nGiven your \"current\" working folder (the one you're navigated to in the folder sidebar menu), the jupyterlab-git integration regularly checks if you're inside a git repository and polls for changes in that repository if so.\n\nWhen this list is very large, I've sometimes seen it cause significant slowdowns in the overall UI because of the way the underlying (open-source) extension works. This has been discussed before for example [in this GitHub issue](https:\/\/github.com\/jupyterlab\/jupyterlab-git\/issues\/667) - which is now marked closed but I've still seen it happening.\n\nFor example, maybe you (like me \ud83d\ude05) forgot to [gitignore](https:\/\/git-scm.com\/docs\/gitignore) a data folder or node_modules and generated thousands of untracked files there: You might see a significant slowdown whenever you're navigated to a folder within the scope of that git repo.\n\nSuggested solution would be:\n\n- Use the folder sidebar to navigate anywhere other than the affected git repository (e.g. to your root folder?), and you should see the slowdown resolve pretty much immediately if this is the underlying cause\n- Now the tricky task of finding and clearing up the problemmatic folder(s) without navigating to them in the folder GUI:\n    - You could use a System Terminal, `cd` to the affected folder and run `git status` to see where the many changes are hiding, if you're not sure already\n    - Add a `.gitignore` file (or modify your existing one) to make git ignore those changes. Because it starts with a dot, `.gitignore` is hidden by default in the JupyterLab file browser anyway. I usually use a system terminal to e.g. `cp myrepo\/.gitignore gitignore.txt` to create a visible copy (somewhere other than the repository folder which you're trying to avoid navigating to!) and then `mv gitignore.txt myrepo\/.gitignore` to overwrite with my edited version\n\nAlternatively (if e.g. it's a folder full of new files that you no longer care about like `node_modules`) you could just slog through the slowness to delete the problemmatic folder in the UI - but of course the problem would return if you re-created them later without `.gitignore`.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1657857148683,
        "Solution_link_count":2,
        "Solution_readability":10.8,
        "Solution_reading_time":27.44,
        "Solution_score_count":1.0,
        "Solution_sentence_count":18,
        "Solution_word_count":346,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I want to submit a training job on sagemaker. I tried it on notebook and it works. When I try the following I get `ModuleNotFoundError: No module named 'nltk'`\n\nMy code is\n\n    import sagemaker  \n    from sagemaker.pytorch import PyTorch\n\n    JOB_PREFIX   = 'pyt-ic'\n    FRAMEWORK_VERSION = '1.3.1'\n\n    estimator = PyTorch(entry_point='finetune-T5.py',\n                       source_dir='..\/src',\n                       train_instance_type='ml.p2.xlarge' ,\n                       train_instance_count=1,\n                       role=sagemaker.get_execution_role(),\n                       framework_version=FRAMEWORK_VERSION, \n                       debugger_hook_config=False,  \n                       py_version='py3',\n                       base_job_name=JOB_PREFIX)\n\n    estimator.fit()\n\n\n\n\n`finetune-T5.py` have many other libraries that are not installed. How can I install the missing library? Or is there a better way to run the training job?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598912648000,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1668620734992,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUJMd_4s52RpWXDXITXFsQdw\/modulenotfounderror-when-starting-a-training-job-on-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.9,
        "Challenge_reading_time":10.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"ModuleNotFoundError when starting a training job on Sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":833.0,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Check out this [link][1] (Using third-party libraries section) on how to install third-party libraries for training jobs.  You need to create requirement.txt file in the same directory as your training script to install other dependencies at runtime.\n\n\n  [1]: https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#using-third-party-libraries",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925593038,
        "Solution_link_count":1,
        "Solution_readability":14.3,
        "Solution_reading_time":4.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3,
        "Solution_word_count":39,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":29.8171413889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I hope I am not missing something obvious here. I am using the new azure ml studio designer. I am able to use to create datasets, train models and use them just fine.<\/p>\n\n<p>azure ml studio allows creation of Jupyter notebooks (also) and use them to do machine learning. I am able to do that too. <\/p>\n\n<p>So, now, I am wondering, can I build my ML pipeline\/experiment in ML studio designer, and once it is in good shape, export it as a python and jupyter notebook? then, use it in the same designer provided notebook option or may be use it locally?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":4,
        "Challenge_created_time":1582134046887,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60306240",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"export azure ml studio designer project as jupyter notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":789.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442334437952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":2272.0,
        "Poster_view_count":516.0,
        "Solution_body":"<p>This is not currently supported, but I am 80% sure it is in the roadmap.\nAn alternative would be to use the SDK to create the same pipeline using <code>ModuleStep<\/code> where  I <em>believe<\/em> you can reference a Designer Module by its name to use it like a <code>PythonScriptStep<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1582241388596,
        "Solution_link_count":0,
        "Solution_readability":11.5,
        "Solution_reading_time":3.66,
        "Solution_score_count":6.0,
        "Solution_sentence_count":2,
        "Solution_word_count":48,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"logs: \r\n\r\n```\r\nRun papermill notebooks\/sklearn\/train-diabetes-mlproject.ipynb out.ipynb -k python\r\nInput Notebook:  notebooks\/sklearn\/train-diabetes-mlproject.ipynb\r\nOutput Notebook: out.ipynb\r\n\r\nExecuting:   0%|          | 0\/7 [00:00<?, ?cell\/s]Executing notebook with kernel: python\r\n\r\nExecuting:  14%|\u2588\u258d        | 1\/7 [00:01<00:07,  1.33s\/cell]\r\nExecuting:  29%|\u2588\u2588\u258a       | 2\/7 [00:02<00:07,  1.43s\/cell]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4\/7 [00:05<00:03,  1.32s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:07<00:01,  1.34s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:08<00:01,  1.40s\/cell]\r\nTraceback (most recent call last):\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/bin\/papermill\", line 8, in <module>\r\n    sys.exit(papermill())\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/cli.py\", line 240, in papermill\r\n    execute_notebook(\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 110, in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 222, in raise_for_execution_errors\r\n    raise error\r\npapermill.exceptions.PapermillExecutionError: \r\n---------------------------------------------------------------------------\r\nException encountered at \"In [5]\":\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-5-ef514d3992f5> in <module>\r\n----> 1 run = mlflow.projects.run(\r\n      2     uri=str(project_uri),\r\n      3     parameters=***\"alpha\": 0.3***,\r\n      4     backend=\"azureml\",\r\n      5     backend_config=backend_config,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in run(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\r\n    271     )\r\n    272 \r\n--> 273     submitted_run_obj = _run(\r\n    274         uri=uri,\r\n    275         experiment_id=experiment_id,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in _run(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, use_conda, storage_dir, synchronous)\r\n     98         backend = loader.load_backend(backend_name)\r\n     99         if backend:\r\n--> 100             submitted_run = backend.run(\r\n    101                 uri,\r\n    102                 entry_point,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/mlflow\/_internal\/projects.py in run(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id)\r\n    240         if compute and compute != _LOCAL and compute != _LOCAL.upper():\r\n    241             remote_environment = _load_remote_environment(mlproject)\r\n--> 242             remote_environment.register(workspace=workspace)\r\n    243             cpu_cluster = _load_compute_target(workspace, backend_config)\r\n    244             src.run_config.target = cpu_cluster.name\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/environment.py in register(self, workspace)\r\n    803         environment_client = EnvironmentClient(workspace.service_context)\r\n    804         environment_dict = Environment._serialize_to_dict(self)\r\n--> 805         response = environment_client._register_environment_definition(environment_dict)\r\n    806         env = Environment._deserialize_and_add_to_object(response)\r\n    807 \r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/_restclient\/environment_client.py in _register_environment_definition(self, environment_dict)\r\n     75             message = \"Error registering the environment definition. Code: ***\\n: ***\".format(response.status_code,\r\n     76                                                                                             response.text)\r\n---> 77             raise Exception(message)\r\n     78 \r\n     79     def _get_image_details(self, name, version=None):\r\n\r\nException: Error registering the environment definition. Code: 409\r\n: ***\r\n  \"error\": ***\r\n    \"code\": \"TransientError\",\r\n    \"severity\": null,\r\n    \"message\": \"Etag conflict on 0e149764-3720-4610-b0f3-3e3f974544ac\/8f54aa7d6c05b2722ba149d8ea3185c263ecf5310eb2d7271569d1918c736972 with etag .\",\r\n    \"messageFormat\": null,\r\n    \"messageParameters\": null,\r\n    \"referenceCode\": null,\r\n    \"detailsUri\": null,\r\n    \"target\": null,\r\n    \"details\": [],\r\n    \"innerError\": null,\r\n    \"debugInfo\": null\r\n  ***,\r\n  \"correlation\": ***\r\n    \"operation\": \"db22e6e6bfa07f499f1749f708b798c9\",\r\n    \"request\": \"f470e9430c5ed842\"\r\n  ***,\r\n  \"environment\": \"eastus\",\r\n  \"location\": \"eastus\",\r\n  \"time\": \"2020-10-01T20:17:52.8383774+00:00\",\r\n  \"componentName\": \"environment-management\"\r\n***\r\n\r\nError: Process completed with exit code 1.\r\n```",
        "Challenge_closed_time":1601658,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601583593000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1170",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":20.3,
        "Challenge_reading_time":69.38,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":null,
        "Challenge_title":"mlflow.projects.run failing consistently with etag error ",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":338,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1389049461896,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":455.0,
        "Answerer_view_count":108.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've so far seen people using tensorflow in Azure using in this <a href=\"http:\/\/www.mikelanzetta.com\/tensorflow-on-azure-using-docker.html\" rel=\"nofollow\">link<\/a>.\nAlso using the advantage of ubuntu in windows tensorflow can be run on\nwindows pc as well.Here is the <a href=\"http:\/\/www.hanselman.com\/blog\/PlayingWithTensorFlowOnWindows.aspx\" rel=\"nofollow\">link<\/a>.\nHowever during a conversation with Windows Azure engineer Hai Ning it came out\nthat \"Azure ML PaaS VMs use Windows OS; TensorFlow is not supported on Windows as of now.\"\nHence there is no direct way of running tensorflow in Azure ML.\nIs there any work around anyone figured out that allows running tensorflow in Azure ML.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1473271529687,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1473276447316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39376560",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to call Tensorflow in Azure ML",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1743.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1435075201580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":546.0,
        "Poster_view_count":59.0,
        "Solution_body":"<p>Quick update for you. As of TensorFlow r0.12 there is now a native TensorFlow package for Windows. I have it running successfully on my Windows 10 laptop. See this <a href=\"https:\/\/developers.googleblog.com\/2016\/11\/tensorflow-0-12-adds-support-for-windows.html\" rel=\"nofollow noreferrer\">blog post<\/a> from Google for more information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":9.4,
        "Solution_reading_time":4.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6,
        "Solution_word_count":39,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I was working on a sagemaker studio for ML work, I attached Lifecycle Configuration with it, which was creating problem. Then I deleted the lifecycle configuration without detaching it, and this problem is happening. Can't start sagemaker studio notebook and this is shown.\n\n![Enter image description here](\/media\/postImages\/original\/IMg3hralubRIO8ITzuV3La8Q)\n\nAny suggestion to fix this ?",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667756083758,
        "Challenge_favorite_count":1.0,
        "Challenge_last_edit_time":1668480117644,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU91ywEwTsRRqmHKZJ1yVrrA\/sagemaker-studio-is-not-opening-after-deleting-lifecycle-configuration",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Studio is not opening after deleting lifecycle configuration",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":169.0,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can try detaching the LCC script using the CLI. You can use the [CloudShell](https:\/\/aws.amazon.com\/cloudshell\/) from console, since your console role is able to perform updates on the domain. \n\nUse the [update-domain](https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/update-domain.html) CLI call, and provide an empty configuration for the default user settings, something like- \n```\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'\n```",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1667797891160,
        "Solution_link_count":2,
        "Solution_readability":13.6,
        "Solution_reading_time":7.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5,
        "Solution_word_count":56,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1430233500800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":212.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to start an EMR cluster every time a sagemaker notebook is started.\nHowever, I have find out that Lifecycle configuration scripts cannot run for longer than 5 minutes. Sadly my EMR cluster take more then 5 minutes to be up. This is a problem as I need to wait for the cluster to be up in order to retrieve the master ip address (that ip address is then used to configure the connection between sagemaker notebbok and the cluster).<\/p>\n\n<p>Below an extract of the code which is run into the lifecycle configuration script. <\/p>\n\n<p>Is there someone out there which have faced a similar problem and found a solution ? <\/p>\n\n<pre><code>job_flow_id = client.run_job_flow(**CLUSTER_CONFIG)['JobFlowId']\n\n...\n...\n\n# Retrieve private Ip of master node for later use\nmaster_instance = client.list_instances(ClusterId=job_flow_id, InstanceGroupTypes=['MASTER'])['Instances'][0]\nmaster_private_ip = master_instance['PrivateIpAddress']\n\n# Send to sagemaker the config file in order to tell him how to communicate with spark\ns3 = boto3.client('s3')\nfile_object = s3.get_object(Bucket='dataengine', Key='emr\/example_config.json')\ndata = json.loads(file_object['Body'].read().decode('utf-8'))\ndata['kernel_python_credentials']['url'] = 'http:\/\/{}:8998'.format(master_private_ip)\ndata['kernel_scala_credentials']['url'] = 'http:\/\/{}:8998'.format(master_private_ip)\ndata['kernel_r_credentials']['url'] = 'http:\/\/{}:8998'.format(master_private_ip)\n\nwith open('.\/sparkmagic\/config.json', 'w') as outfile:\n    json.dump(data, outfile)```\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1549984867947,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54653359",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.8,
        "Challenge_reading_time":21.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a way to use sagemaker lifecycle configuration to run an EMR cluster on notebook start",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1652.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408609820407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":111.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>You can also consider using <a href=\"https:\/\/en.wikipedia.org\/wiki\/Nohup\" rel=\"nofollow noreferrer\">nohup<\/a> in Lifecycle config to execute your script in background, so you don't get blocked by 5-min limit. <\/p>\n\n<p>Let us know if there's anything else you need assistance of.<\/p>\n\n<p>Thanks,<\/p>\n\n<p>Han<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":7.2,
        "Solution_reading_time":3.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4,
        "Solution_word_count":40,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in sagemaker\r\nhow long is this going to even take man",
        "Challenge_closed_time":1667627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667438077000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/155",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"we are experiencing elevated fault rate in start runtime API. The SageMaker Studio Lab team is working to restore the service.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":51,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"up Ah finally btw, I can run the CPU but not GPU today\r\n @saleemmalik10835 Sorry for the long inconvenience of Studio Lab. As you know, the service was back and we confirmed that we can say it to you. I will close this issue because the mentioned problem is solved.\r\n\r\nBut as @aozorahime said, the GPU instance is sometime unavailable because of another instance allocation issue. Of course, we deal with this problem now. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.7,
        "Solution_reading_time":5.05,
        "Solution_score_count":null,
        "Solution_sentence_count":5,
        "Solution_word_count":74,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to AWS and I'm trying out AWS Sagemaker. I'm currently doing my project which involves quite a long time to finish and I don't think I can finish it in a day. I'm worried if I close my JupyterLab of my notebook instance in SageMaker, my code will be gone. How do I save my code and cell run progress when using Sagemaker?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619084183027,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67210677",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.8,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"If I close my JupyterLab from notebook instance, would my code be gone?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":77,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605938672327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Poster_reputation_count":97.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>If you are training directly in the notebook the answer is yes.\nHowever the best practice is not to train directly with the notebook.\nUse instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!\nAs a quick example.. in my notebook I have this cell:<\/p>\n<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :\n0.01}\n\nmetrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n\ninputs = {'train': train_data, 'test': validation_data}\n\nmyJobName = 'myname'\n\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n<\/code><\/pre>\n<p>My training script training.py is something like this:<\/p>\n<pre><code>if __name__ =='__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # input data and model directories\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--learning-rate', type=float, default=0.0001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--epochs', type=int, default=1)\n....\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.4,
        "Solution_reading_time":32.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":29,
        "Solution_word_count":227,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1629385138956,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":395.0,
        "Answerer_view_count":38.0,
        "Challenge_adjusted_solved_time":26.6152038889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I would like to know what is the OPTIMAL way to store the result of a Google BigQuery table query, to Google Cloud storage. My code, which is currently being run in some Jupyter Notebook (in Vertex AI Workbench, same project than both the BigQuery data source, as well as the Cloud Storage destination), looks as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># CELL 1 OF 2\n\nfrom google.cloud import bigquery\nbqclient = bigquery.Client()\n\n# The query string can vary:\nquery_string = &quot;&quot;&quot;\n        SELECT *  \n        FROM `my_project-name.my_db.my_table` \n        LIMIT 2000000\n        &quot;&quot;&quot;\n\ndataframe = (\n    bqclient.query(query_string)\n    .result()\n    .to_dataframe(\n        create_bqstorage_client=True,\n    )\n)\nprint(&quot;Dataframe shape: &quot;, dataframe.shape)\n\n# CELL 2 OF 2:\n\nimport pandas as pd\ndataframe.to_csv('gs:\/\/my_bucket\/test_file.csv', index=False)\n<\/code><\/pre>\n<p>This code takes around 7.5 minutes to successfully complete.<\/p>\n<p><strong>Is there a more OPTIMAL way to achive what was done above?<\/strong> (It would mean <em>faster<\/em>, but maybe something else could be improved).<\/p>\n<p>Some additional notes:<\/p>\n<ol>\n<li>I want to run it &quot;via a Jupyter Notebook&quot; (in Vertex AI Workbench), because sometimes some data preprocessing, or special filtering must be done, which cannot be easily accomplished via SQL queries.<\/li>\n<li>For the first part of the code, I have discarded <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas.read_gbq<\/a>, as it was giving me some weird EOF errors, when (experimentally) &quot;storing as .CSV and reading back&quot;.<\/li>\n<li>Intuitively, I would focus the optimization efforts in the second half of the code (<code>CELL 2 OF 2<\/code>), as the first one was borrowed from <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/bigquery-storage-python-pandas\" rel=\"nofollow noreferrer\">the official Google documentation<\/a>. I have tried <a href=\"https:\/\/stackoverflow.com\/a\/57404119\/16706763\">this<\/a> but it does not work, however in the same thread <a href=\"https:\/\/stackoverflow.com\/a\/60644694\/16706763\">this<\/a> option worked OK.<\/li>\n<li>It is likley that this code will be included in some Docker image afterwards, so &quot;as little libraries as possible&quot; must be used.<\/li>\n<\/ol>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1651600671333,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72103557",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":30.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":null,
        "Challenge_title":"Save the result of a query in a BigQuery Table, in Cloud Storage",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1409.0,
        "Challenge_word_count":292,
        "Platform":"Stack Overflow",
        "Poster_created_time":1629385138956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":395.0,
        "Poster_view_count":38.0,
        "Solution_body":"<p>After some experiments, I think I have got to a solution for my original post. First, the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd  # Just one library is imported this time\n\n# This SQL query can vary, modify it to match your needs\nquery_string = &quot;&quot;&quot;\nSELECT *\nFROM `my_project.my_db.my_table`\nLIMIT 2000000\n&quot;&quot;&quot;\n\n# One liner to query BigQuery data.\ndownloaded_dataframe = pd.read_gbq(query_string, dialect='standard', use_bqstorage_api=True)\n\n# Data processing (OPTIONAL, modify it to match your needs)\n# I won't do anything this time, just upload the previously queried data\n\n# Data store in GCS\ndownloaded_dataframe.to_csv('gs:\/\/my_bucket\/uploaded_data.csv', index=False)\n<\/code><\/pre>\n<p>Some final notes:<\/p>\n<ol>\n<li>I have not done an &quot;in-depth research&quot; about the processing speed VS the number of rows existing in a BigQuery table, however I saw that the processing time with the updated code and the original query, now takes ~6 minutes; that's enough for the time being. <em>This answer might have some room for further improvements<\/em> therefore, but it's better than the original situation.<\/li>\n<li>The EOF error I mentioned in  my original post was: <code>ParserError: Error tokenizing data. C error: EOF inside string starting at row 70198<\/code>. In the end I got to realize that it did not have anything to do with <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas_gbq<\/a> function, but with &quot;how I was saving the data&quot;. See, <em>I was 'experimentally' storing the .csv file in the Vertex AI Workbench local storage, then downloading it to my local device, and when trying to open that data from my local device, I kept stumbling upon that error, however not getting the same when downloading the .csv data from Cloud Storage<\/em> ... Why? Well, it happens that if you download the .csv data &quot;very quickly&quot; after &quot;it gets generated&quot; (i.e., after few seconds), from Vertex AI Workbench local storage, the data is simply still incomplete, but it does not give any error or warning message: it will simply &quot;let you start with the download&quot;. For this reason, I think it is safer to export your data to Cloud Storage, and then download safely from there. This behaviour is more noticeable on large files (i.e. my own generated file, which had ~3.1GB in size).<\/li>\n<\/ol>\n<p>Hope this helps.<\/p>\n<p>Thank you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1651696486067,
        "Solution_link_count":1,
        "Solution_readability":9.6,
        "Solution_reading_time":31.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22,
        "Solution_word_count":357,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When reading the examples from Microsoft on azure ML CLI v2, they use the symbols:\n&quot;|&quot;, &quot;&gt;&quot;, etc., in their yml files.<\/p>\n<p>What do they mean, and where can I find explanations of possible syntax for the Azure CLI v2 engine?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649142429250,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71747545",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":3.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Commands in the Azure ML yml files",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":100.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620049475608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denmark",
        "Poster_reputation_count":3.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>| - This pipe symbol in YAML document is used for <em><strong>&quot;Multiple line statements&quot;<\/strong><\/em><\/p>\n<pre><code>description: |\n  # Azure Machine Learning &quot;hello world&quot; job\n\n  This is a &quot;hello world&quot; job running in the cloud via Azure Machine Learning!\n\n  ## Description\n\n  Markdown is supported in the studio for job descriptions! You can edit the description there or via CLI.\n<\/code><\/pre>\n<p>in the above example, we need to write some multiple line description. So, we need to use &quot;|&quot; symbol<\/p>\n<p>&quot;&gt;&quot; - This symbol is used to save some content directly to a specific location document.<\/p>\n<pre><code>command: echo &quot;hello world&quot; &gt; .\/outputs\/helloworld.txt\n<\/code><\/pre>\n<p>In this above command, we need to post <strong>&quot;hello world&quot;<\/strong> to <em><strong>&quot;helloworld.txt&quot;<\/strong><\/em><\/p>\n<p>Check the below link for complete documentation regarding YAML files.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command<\/a><\/p>\n<p>All these symbols are the YAML job commands which are used to accomplish a specific task through CLI.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":10.6,
        "Solution_reading_time":16.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11,
        "Solution_word_count":139,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613428648627,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.1,
        "Challenge_reading_time":6.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":276.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585870038407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Christchurch, \u041d\u043e\u0432\u0430\u044f \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u044f",
        "Poster_reputation_count":45.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.5,
        "Solution_reading_time":13.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8,
        "Solution_word_count":93,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1474967309676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":59.0,
        "Answerer_view_count":66.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606886205577,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1606887753310,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":43.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"struggling to install python package via amazon sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1148.0,
        "Challenge_word_count":348,
        "Platform":"Stack Overflow",
        "Poster_created_time":1474967309676,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":13.1,
        "Solution_reading_time":1.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1,
        "Solution_word_count":19,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1598441874396,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Currently, I'm using kedro and kedro-viz.<\/p>\n<p>I can specify a layer of dataset from catalog.yml.<\/p>\n<pre><code>hoge:\n  type: MemoryDataSet\n  layer: raw\n<\/code><\/pre>\n<p>but I don't know how to do it with parameters.yml<\/p>\n<pre><code>step_size: 1\nlearning_rate: 0.01\n<\/code><\/pre>\n<p>if it can be done not in parameters.yml but in run.py, I want to see example code.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598383098480,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63585717",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":5.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"In Kedro, How to specify layer to parameters.yml?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":225.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545209959320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>At the moment layers can only be specified for datasets, not for nodes or parameters.<\/p>\n<p>If you have a specific use case for adding layers to nodes\/parameters, please let us know by opening a feature request in the Kedro repo: <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/issues<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":13.7,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":44,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"as the title suggests\r\n",
        "Challenge_closed_time":1641229,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641220236000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/38",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.0,
        "Challenge_reading_time":0.98,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"I just wonder if i can initialize my sagemaker studio lab? ",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":15,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello - if you have any specific technical issues please use our issue template here to describe it in detail. \r\n\r\nhttps:\/\/github.com\/aws\/studio-lab-examples\/issues\/new?assignees=&labels=bug&template=bug-report-for-sagemaker-studio-lab.md&title=\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":26.4,
        "Solution_reading_time":3.25,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":20,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1361339272692,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"NYC",
        "Answerer_reputation_count":6281.0,
        "Answerer_view_count":958.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591810831397,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":21.3,
        "Challenge_reading_time":11.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":400.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370231111260,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":5295.0,
        "Poster_view_count":455.0,
        "Solution_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":17.5,
        "Solution_reading_time":4.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3,
        "Solution_word_count":28,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1366551199192,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1789.0,
        "Answerer_view_count":102.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am interested if one can import sagemaker packages on your own local Python environment or whether they are restricted to AWS Sagemaker?<\/p>\n<pre><code>from sagemaker_automl import AutoMLInteractiveRunner, AutoMLLocalCandidate\n<\/code><\/pre>\n<p>For instance can I somehow download the sagemaker_automl?<\/p>\n<p>I know the there are no sagemaker packages available in the conda repository. Perhaps there is some other way to get them.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606595637157,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65054187",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":6.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Can you use sagemaker Python libraries on my localhost?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":162.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310821356880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":14913.0,
        "Poster_view_count":1093.0,
        "Solution_body":"<p>The Sagemaker Python SDK is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">open source and on GitHub<\/a>, as well as published on <a href=\"https:\/\/pypi.org\/project\/sagemaker\/\" rel=\"nofollow noreferrer\">Pypi<\/a>.<\/p>\n<p>You can install it by running:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install sagemaker\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":16.0,
        "Solution_reading_time":4.99,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4,
        "Solution_word_count":34,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1498123491552,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"\u4e2d\u56fdJiangsu Sheng",
        "Answerer_reputation_count":22369.0,
        "Answerer_view_count":3121.0,
        "Challenge_adjusted_solved_time":112.0978552778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would really like to get access to some of the updated functions in pandas 0.19, but Azure ML studio uses pandas 0.18 as part of the Anaconda 4.0 bundle. Is there a way to update the version that is used within the \"Execute Python Script\" components?<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1505401641617,
        "Challenge_favorite_count":2.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46222606",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.7,
        "Challenge_reading_time":3.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Updating pandas to version 0.19 in Azure ML Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1903.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421081882987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":588.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>I offer the below steps for you to show how to update the version of pandas  library in <code>Execute Python Script<\/code>.<\/p>\n\n<p><strong><em>Step 1<\/em><\/strong> : Use the <code>virtualenv<\/code> component to create an independent python runtime environment in your system.Please install it first with command <code>pip install virtualenv<\/code> if you don't have it.<\/p>\n\n<p>If you installed it successfully ,you could see it in your python\/Scripts file.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step2<\/em><\/strong> : Run the commad to create independent python runtime environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 3<\/em><\/strong> : Then go into the created directory's Scripts folder and activate it (this step is important , don't miss it)<\/p>\n\n<p>Please don't close this command window and use <code>pip install pandas==0.19<\/code> to download external libraries in this command window.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Wj857.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Wj857.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 4<\/em><\/strong> : Compress all of the files in the Lib\/site-packages folder into a zip package (I'm calling it pandas - package here)<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 5<\/em><\/strong> \uff1aUpload the zip package into the Azure Machine Learning WorkSpace DataSet.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/efRkK.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/efRkK.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>specific steps please refer to the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/\" rel=\"noreferrer\">Technical Notes<\/a>.<\/p>\n\n<p>After success, you will see the uploaded package in the DataSet List<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 6<\/em><\/strong> \uff1a Before the defination of method <code>azureml_main<\/code> in the Execute Python Script module, you need to remove the old <code>pandas<\/code> modules &amp; its dependencies, then to import <code>pandas<\/code> again, as the code below.<\/p>\n\n<pre><code>import sys\nimport pandas as pd\nprint(pd.__version__)\ndel sys.modules['pandas']\ndel sys.modules['numpy']\ndel sys.modules['pytz']\ndel sys.modules['six']\ndel sys.modules['dateutil']\nsys.path.insert(0, '.\\\\Script Bundle')\nfor td in [m for m in sys.modules if m.startswith('pandas.') or m.startswith('numpy.') or m.startswith('pytz.') or m.startswith('dateutil.') or m.startswith('six.')]:\n    del sys.modules[td]\nimport pandas as pd\nprint(pd.__version__)\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n<\/code><\/pre>\n\n<p>Then you can see the result from logs as below, first print the old version <code>0.14.0<\/code>, then print the new version <code>0.19.0<\/code> from the uploaded zip file.<\/p>\n\n<pre><code>[Information]         0.14.0\n[Information]         0.19.0\n<\/code><\/pre>\n\n<p>You could also refer to these threads: <a href=\"https:\/\/stackoverflow.com\/questions\/45749479\/access-blob-file-using-time-stamp-in-azure\/45814318#45814318\">Access blob file using time stamp in Azure<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/12669546\/reload-with-reset\">reload with reset<\/a>.<\/p>\n\n<p>Hope it helps you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1505805193896,
        "Solution_link_count":15,
        "Solution_readability":10.2,
        "Solution_reading_time":51.18,
        "Solution_score_count":6.0,
        "Solution_sentence_count":43,
        "Solution_word_count":375,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"One can either define a DVC option with default values in the init, which could be considered a constant, or change a DVC option that has no default values in the call method.\r\n\r\nIf a pre-intialized DVC option is being changed within the call that can lead to issues and should either raise an exception or at least log that it can lead to not supported problems",
        "Challenge_closed_time":1634716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633013182000,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/76",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"raise Error if pre-initialized DVC option is being changed",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":75,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1,
        "Solution_word_count":0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579147150087,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":20.5,
        "Challenge_reading_time":212.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":223,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker scikit_bring_your_own example",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":341.0,
        "Challenge_word_count":1082,
        "Platform":"Stack Overflow",
        "Poster_created_time":1241005356852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Malaysia",
        "Poster_reputation_count":15794.0,
        "Poster_view_count":1032.0,
        "Solution_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":6.2,
        "Solution_reading_time":2.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2,
        "Solution_word_count":27,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1447151270223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I created a docker image for training. In the dockerfile I have an entrypoint defined such that when <code>docker run<\/code> is executed, it will start running my python code.\nTo use this on aws sagemaker in my understanding I need to create a pytorch estimator in a jupyter notebook in sagemaker. I tried something like this:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nsagemaker_session = sagemaker.Session()\n\nrole = sagemaker.get_execution_role()\n\nestimator = PyTorch(entry_point='train.py',\n                    role=role,\n                    framework_version='1.3.1',\n                    image_name='xxx.ecr.eu-west-1.amazonaws.com\/xxx:latest',\n                    train_instance_count=1,\n                    train_instance_type='ml.p3.xlarge',\n                    hyperparameters={})\n\nestimator.fit({})\n\n<\/code><\/pre>\n\n<p>In the documentation I found that as image name I can specify the link the my docker image on aws ecr. When I try to execute this it keeps complaining<\/p>\n\n<pre><code>[Errno 2] No such file or directory: 'train.py'\n<\/code><\/pre>\n\n<p>It complains immidiatly, so surely I am doing something completely wrong. I would expect that first my docker image should run, and than it could find out that the entry point does not exist.<\/p>\n\n<p>But besides this, why do I need to specify an entry point, as in, should it not be clear that the entry to my training is simply <code>docker run<\/code>?<\/p>\n\n<p>For maybe better understanding. The entrypoint python file in my docker image looks like this:<\/p>\n\n<pre><code>if __name__=='__main__':\n    parser = argparse.ArgumentParser()\n\n    # Hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--batch_size', type=int, default=16)\n    parser.add_argument('--learning_rate', type=float, default=0.0001)\n\n    # Data and output directories\n    parser.add_argument('--output_data_dir', type=str, default=os.environ['OUTPUT_DATA_DIR'])\n    parser.add_argument('--train_data_path', type=str, default=os.environ['CHANNEL_TRAIN'])\n    parser.add_argument('--valid_data_path', type=str, default=os.environ['CHANNEL_VALID'])\n\n    # Start training\n    ...\n<\/code><\/pre>\n\n<p>Later I would like to specify the hyperparameters and data channels. But for now I simply do not understand what to put as entry point. In the documentation it says that the entrypoint is required and it should be a local\/global path to the entrypoint...<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":3,
        "Challenge_created_time":1578494679740,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1578495990750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59648275",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.7,
        "Challenge_reading_time":31.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"What to define as entrypoint when initializing a pytorch estimator with a custom docker image for training on AWS Sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":300,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447151270223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":141.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>If you really would like to use a complete separate by yourself build docker image, you should create an Amazon Sagemaker algorithm (which is one of the options in the Sagemaker menu). Here you have to specify a link to your docker image on amazon ECR as well as the input parameters and data channels etc. When choosing this options, you should <strong>not<\/strong> use the PyTorch estimater but the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/algorithm.html\" rel=\"nofollow noreferrer\">Algoritm estimater<\/a>. This way you indeed don't have to specify an entrypoint because it simple runs the docker when training and the default entrypoint can be defined in your docker file.<\/p>\n\n<p>The Pytorch estimator can be used when having you own model code, but you would like to run this code in an off-the-shelf Sagemaker PyTorch docker image. This is why you have to for example specify the PyTorch framework version. In this case the entrypoint file by default should be placed next to where your jupyter notebook is stored (just upload the file by clicking on the upload button). The PyTorch estimator inherits all options from the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"nofollow noreferrer\">framework estimator<\/a> where options can be found where to place the entrypoint and model, for example <em>source_dir<\/em>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2,
        "Solution_readability":12.3,
        "Solution_reading_time":17.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10,
        "Solution_word_count":200,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I need some help trying to understand why I can't see any GIT options (left panel and top selection drop down menu) in my Azure machine learning JupyterLab.<\/p>\n<p>I did the following steps:<\/p>\n<pre><code> jupyter labextension install @jupyterlab\/git\n pip install --upgrade jupyterlab-git\n jupyter serverextension enable --py jupyterlab_git\n jupyter lab build\n<\/code><\/pre>\n<p>I've restarted my jupyterLab a couple of times, if I check the command:<\/p>\n<pre><code> jupyter labextension list\n<\/code><\/pre>\n<p>I get that @jupyterlab\/git v0,20,0 is enabled and ok.  <br \/>\nWhat am I doing wrong?<\/p>\n<p>Thank you in advance,  <br \/>\nCarla<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/12015-issue1.png?platform=QnA\" alt=\"12015-issue1.png\" \/><\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":5,
        "Challenge_created_time":1594716551237,
        "Challenge_favorite_count":null,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/46614\/azure-machine-learning-and-jupyterlab-git-extensio",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":10.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning and jupyterlab git extension not working",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a>@ramr-msft<\/a> ,<\/p>\n<p>I did the steps mention in the link you gave me (<a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab-git\">https:\/\/github.com\/jupyterlab\/jupyterlab-git<\/a>) but still I can't open the Git extension from the Git tab on the left panel because it still doesn't exists.<\/p>\n<p>You mentioned we can still manage git repositories using the command line. Do you have any useful documentation on this approach?<\/p>\n<p>Once again, thank you in advance.  <br \/>\nCarla<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1,
        "Solution_readability":8.4,
        "Solution_reading_time":6.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5,
        "Solution_word_count":66,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1408574571227,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Toronto, Canada",
        "Answerer_reputation_count":2754.0,
        "Answerer_view_count":124.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a model that was trained on a Machine Learning Compute on Azure Machine Learning Service. The registered model already lives my workspace and I would like to deploy it to a pre-existing AKS instance that I previously provisioned in my workspace. I am able to successfully configure and register the container image: <\/p>\n\n<pre><code># retrieve cloud representations of the models\nrf = Model(workspace=ws, name='pumps_rf')\nle = Model(workspace=ws, name='pumps_le')\nohc = Model(workspace=ws, name='pumps_ohc')\nprint(rf); print(le); print(ohc)\n\n&lt;azureml.core.model.Model object at 0x7f66ab3b1f98&gt;\n&lt;azureml.core.model.Model object at 0x7f66ab7e49b0&gt;\n&lt;azureml.core.model.Model object at 0x7f66ab85e710&gt;\n\npackage_list = [\n  'category-encoders==1.3.0',\n  'numpy==1.15.0',\n  'pandas==0.24.1',\n  'scikit-learn==0.20.2']\n\n# Conda environment configuration\nmyenv = CondaDependencies.create(pip_packages=package_list)\nconda_yml = 'file:'+os.getcwd()+'\/myenv.yml'\n\nwith open(conda_yml,\"w\") as f:\n    f.write(myenv.serialize_to_string())\n<\/code><\/pre>\n\n<p>Configuring and registering the image works:<\/p>\n\n<pre><code># Image configuration\nimage_config = ContainerImage.image_configuration(execution_script='score.py', \n                                                  runtime='python', \n                                                  conda_file='myenv.yml',\n                                                  description='Pumps Random Forest model')\n\n\n# Register the image from the image configuration\n# to Azure Container Registry\nimage = ContainerImage.create(name = Config.IMAGE_NAME, \n                              models = [rf, le, ohc],\n                              image_config = image_config,\n                              workspace = ws)\n\nCreating image\nRunning....................\nSucceededImage creation operation finished for image pumpsrfimage:2, operation \"Succeeded\"\n<\/code><\/pre>\n\n<p>Attaching to an existing cluster also works:<\/p>\n\n<pre><code># Attach the cluster to your workgroup\nattach_config = AksCompute.attach_configuration(resource_group = Config.RESOURCE_GROUP,\n                                                cluster_name = Config.DEPLOY_COMPUTE)\naks_target = ComputeTarget.attach(workspace=ws, \n                                  name=Config.DEPLOY_COMPUTE, \n                                  attach_configuration=attach_config)\n\n# Wait for the operation to complete\naks_target.wait_for_completion(True)\nSucceededProvisioning operation finished, operation \"Succeeded\"\n<\/code><\/pre>\n\n<p>However, when I try to deploy the image to the existing cluster, it fails with a <code>WebserviceException<\/code>. <\/p>\n\n<pre><code># Set configuration and service name\naks_config = AksWebservice.deploy_configuration()\n\n# Deploy from image\nservice = Webservice.deploy_from_image(workspace = ws,\n                                       name = 'pumps-aks-service-1' ,\n                                       image = image,\n                                       deployment_config = aks_config,\n                                       deployment_target = aks_target)\n# Wait for the deployment to complete\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n\nWebserviceException: Unable to create service with image pumpsrfimage:1 in non \"Succeeded\" creation state.\n---------------------------------------------------------------------------\nWebserviceException                       Traceback (most recent call last)\n&lt;command-201219424688503&gt; in &lt;module&gt;()\n      7                                        image = image,\n      8                                        deployment_config = aks_config,\n----&gt; 9                                        deployment_target = aks_target)\n     10 # Wait for the deployment to complete\n     11 service.wait_for_deployment(show_output = True)\n\n\/databricks\/python\/lib\/python3.5\/site-packages\/azureml\/core\/webservice\/webservice.py in deploy_from_image(workspace, name, image, deployment_config, deployment_target)\n    284                         return child._deploy(workspace, name, image, deployment_config, deployment_target)\n    285 \n--&gt; 286         return deployment_config._webservice_type._deploy(workspace, name, image, deployment_config, deployment_target)\n    287 \n    288     @staticmethod\n\n\/databricks\/python\/lib\/python3.5\/site-packages\/azureml\/core\/webservice\/aks.py in _deploy(workspace, name, image, deployment_config, deployment_target)\n<\/code><\/pre>\n\n<p>Any ideas on how to solve this issue? I am writing the code in a Databricks notebook. Also, I am able to create and deploy the cluster using Azure Portal no problem so this appears to be an issue with my code\/Python SDK or the way Databricks works with AMLS.<\/p>\n\n<p>UPDATE:\nI was able to deploy my image to AKS using Azure Portal and the webservice works as expected. This means the issue lies somewhere between Databricks, the Azureml Python SDK and Machine Learning Service.<\/p>\n\n<p>UPDATE 2:\nI'm working with Microsoft to fix this issue. Will report back once we have a solution.<\/p>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":2,
        "Challenge_created_time":1550704526117,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":1554647342087,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54796762",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":16.2,
        "Challenge_reading_time":57.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":null,
        "Challenge_title":"Cannot deploy a trained model to an existing AKS compute target",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1051.0,
        "Challenge_word_count":430,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408574571227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":2754.0,
        "Poster_view_count":124.0,
        "Solution_body":"<p>In my initial code, when creating the image, I was not using:<\/p>\n\n<pre><code>image.wait_for_creation(show_output=True)\n<\/code><\/pre>\n\n<p>As a consequence, I was calling <code>CreateImage<\/code> and <code>DeployImage<\/code> before the image was created which errored out. Can't believe it was that simple.. <\/p>\n\n<p>UPDATED IMAGE CREATION SNIPPET:<\/p>\n\n<pre><code># Register the image from the image configuration\n# to Azure Container Registry\nimage = ContainerImage.create(name = Config.IMAGE_NAME, \n                              models = [rf, le, ohc],\n                              image_config = image_config,\n                              workspace = ws)\n\nimage.wait_for_creation(show_output=True)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":15.6,
        "Solution_reading_time":8.09,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5,
        "Solution_word_count":67,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1473257955532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":145.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have read similar questions regarding azure app service, but I still can't find an answer. I was trying to deploy a model in azure kubernetes service, but I came across an error when importing cv2 (which is essential to me).<\/p>\n<p>Opencv-python is included in my environment .yaml file:<\/p>\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  # You must list azureml-defaults as a pip dependency\n  - azureml-defaults&gt;=1.0.45\n  - Cython\n  - matplotlib&gt;=3.2.2\n  - numpy&gt;=1.18.5\n  - opencv-python&gt;=4.1.2\n  - pillow\n  - PyYAML&gt;=5.3\n  - scipy&gt;=1.4.1\n  - torch&gt;=1.6.0\n  - torchvision&gt;=0.7.0\n  - tqdm&gt;=4.41.0\nchannels:\n- conda-forge\n<\/code><\/pre>\n<p>I am deploying as follows:<\/p>\n<pre><code>aks_service = Model.deploy(ws,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_config=gpu_aks_config,\n                       deployment_target=aks_target,\n                       name=aks_service_name)\n<\/code><\/pre>\n<p>And I get this error:<\/p>\n<pre><code>    Traceback (most recent call last):\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n    self.load_wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n    return self.load_wsgiapp()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n    __import__(module)\n  File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n    from app import main\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 23, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 8, in &lt;module&gt;\n    import cv2\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/cv2\/__init__.py&quot;, line 5, in &lt;module&gt;\n    from .cv2 import *\nImportError: libGL.so.1: cannot open shared object file: No such file or directory\nWorker exiting (pid: 41)\n<\/code><\/pre>",
        "Challenge_closed_time":1,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603802298680,
        "Challenge_favorite_count":0.0,
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64554615",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":41.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":null,
        "Challenge_title":"Import cv2 error when deploying in Azure Kubernetes Service - python",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":484.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473257955532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>This might go wrong at some point, but my workaround was installing opencv-python-headless instead of opencv.<\/p>\n<p>In the environment .yaml file, just replace:<\/p>\n<pre><code>- opencv-python&gt;=4.1.2\n<\/code><\/pre>\n<p>with:<\/p>\n<pre><code>- opencv-python-headless\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0,
        "Solution_readability":5.0,
        "Solution_reading_time":3.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4,
        "Solution_word_count":30,
        "Tool":"Azure Machine Learning"
    }
]