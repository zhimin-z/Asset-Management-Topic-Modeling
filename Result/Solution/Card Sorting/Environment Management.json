[
    {
        "Answerer_created_time":1538275960603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":50.0,
        "Challenge_adjusted_solved_time":5.2167880555,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I'm following the guidelines (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a>) to use a custom docker file on Azure. My script to create the environment looks like this:<\/p>\n<pre><code>from azureml.core.environment import Environment\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n<\/code><\/pre>\n<p>Upon execution, this is totally ignored and libgl1 is not installed. Any ideas why?<\/p>\n<p>EDIT: Here's the rest of my code:<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n    script_params = script_params,\n    use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n    pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    )\n\nrun = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a><\/p>",
        "Challenge_closed_time":1594750426867,
        "Challenge_comment_count":2,
        "Challenge_created_time":1594687513407,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use a custom docker file on Azure ML by following the guidelines provided by Microsoft. However, upon execution, the installation of libgl1 is ignored and not installed. The user is seeking help to understand why this is happening.",
        "Challenge_last_edit_time":1594731646430,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62886435",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":16.1,
        "Challenge_reading_time":19.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":17.4759611111,
        "Challenge_title":"Using a custom docker with Azure ML",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1819.0,
        "Challenge_word_count":108,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366237908703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>This should work :<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.environment import Environment\nfrom azureml.train.estimator import Estimator\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core import Experiment\n\nws = Workspace (...)\nexp = Experiment(ws, 'test-so-exp')\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n\n## You need to instead put your packages in the Environment definition instead... \n## see below for some changes too\n\nmyenv.python.conda_dependencies = CondaDependencies.create(pip_packages = ['scipy==1.1.0', 'torch==1.5.1'])\n<\/code><\/pre>\n<p>Finally you can build your estimator a bit differently :<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n#     script_params = script_params,\n#     use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n#     pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    environment_definition=myenv\n    )\n<\/code><\/pre>\n<p>And submit it :<\/p>\n<pre><code>run = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>Let us know if that works.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":18.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":121.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ignored libgl1 installation"
    },
    {
        "Answerer_created_time":1420765480790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":340.0,
        "Answerer_view_count":61.0,
        "Challenge_adjusted_solved_time":2142.9330063889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Challenge_closed_time":1549414531500,
        "Challenge_comment_count":3,
        "Challenge_created_time":1541699972677,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to execute python from a conda environment using dvc run. Despite having a conda environment configured with python 3.6 and dvc installed, dvc is calling the python version of the main installation of conda and not finding the installed libraries.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2142.9330063889,
        "Challenge_title":"How to execute python from conda environment by dvc run",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":351.0,
        "Challenge_word_count":76,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420765480790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":340.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":0.69,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"DVC",
        "Challenge_type":"anomaly",
        "Challenge_summary":"wrong python version called"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":46.9458155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a notebook using Vertex AI without enabling terminal first, but I want to enable terminal now so that I can run a Python file from a terminal. Is there any way I can change the setting retrospectively?<\/p>",
        "Challenge_closed_time":1657251232892,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656960242433,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to enable terminal for a Vertex AI managed notebook after creating it without enabling terminal initially. They are seeking a way to change the security setting retrospectively to run a Python file from a terminal.",
        "Challenge_last_edit_time":1657082227956,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72860901",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":80.8306830556,
        "Challenge_title":"How can I change the security setting and enable terminal for a Vertex AI managed notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1656565016230,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>As of now, when you create a Notebook instance with unchecked <em>&quot;Enable terminal&quot;<\/em> like the below screenshot, you <strong>cannot re-enable this option once the Notebook instance is already created<\/strong>.\n<a href=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QM6xp.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The only workaround is to <strong>recreate the Notebook instance<\/strong> and then enable it.<\/p>\n<p>Right now, there is already a <a href=\"https:\/\/issuetracker.google.com\/222694899\" rel=\"nofollow noreferrer\">Feature Request<\/a> for this. You can <strong>star<\/strong> the public issue tracker feature request and add <strong>\u2018Me too\u2019<\/strong> in the thread. This will bring more attention to the request as more users request support for it.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":9.8,
        "Solution_reading_time":10.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":96.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to enable terminal"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":4.6192766667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to work with the capacities of the new Azure ML Workspace and I can't find any option to track my notebooks on git. <\/p>\n\n<p>It's this possible as well as you can do with Azure notebooks? If not is possible... how it's suposed to work with this notebooks? Only inside this workspace? <\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1582827641316,
        "Challenge_comment_count":3,
        "Challenge_created_time":1582811011920,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble finding an option to track their Azure ML Workspace notebooks on git and is wondering if it is possible to do so. They are also questioning if the notebooks can only be used within the workspace.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60434642",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":3.8,
        "Challenge_reading_time":4.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.6192766667,
        "Challenge_title":"Version control of azure machine learning workspace notebooks",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":344.0,
        "Challenge_word_count":63,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1348126905516,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Barcelona",
        "Poster_reputation_count":327.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>AFAIK, Git isn't currently supported by Azure Machine Learning Notebooks. If you're looking for a more fully-featured development environment, I suggest setting one up locally. There's more work up front, but it will give you the ability to version control. Check out this development environment set-up guide. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment<\/a><\/p>\n\n<pre><code>| Environment                                                   | Pros                                                                                                                                                                                                                                    | Cons                                                                                                                                                                                 |\n|---------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud-based Azure Machine Learning compute instance (preview) | Easiest way to get started. The entire SDK is already installed in your workspace VM, and notebook tutorials are pre-cloned and ready to run.                                                                                           | Lack of control over your development environment and dependencies. Additional cost incurred for Linux VM (VM can be stopped when not in use to avoid charges). See pricing details. |\n| Local environment                                             | Full control of your development environment and dependencies. Run with any build tool, environment, or IDE of your choice.                                                                                                             | Takes longer to get started. Necessary SDK packages must be installed, and an environment must also be installed if you don't already have one.                                      |\n| Azure Databricks                                              | Ideal for running large-scale intensive machine learning workflows on the scalable Apache Spark platform.                                                                                                                               | Overkill for experimental machine learning, or smaller-scale experiments and workflows. Additional cost incurred for Azure Databricks. See pricing details.                          |\n| The Data Science Virtual Machine (DSVM)                       | Similar to the cloud-based compute instance (Python and the SDK are pre-installed), but with additional popular data science and machine learning tools pre-installed. Easy to scale and combine with other custom tools and workflows. | A slower getting started experience compared to the cloud-based compute instance.                                                                                                    |\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.3,
        "Solution_reading_time":31.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":247.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"track notebooks on git"
    },
    {
        "Answerer_created_time":1264671735676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA",
        "Answerer_reputation_count":8619.0,
        "Answerer_view_count":1286.0,
        "Challenge_adjusted_solved_time":17.7899702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working in JupyterLab within a Managed Notebook instance, accessed through the Vertex AI workbench, as part of a Google Cloud Project. When the instance is created, there are a number of JupyterLab extensions that are installed by default. In the web GUI, one can click the puzzle piece icon and enable\/disable all extensions with a single button click. I currently run a post-startup bash script to manage environments and module installations, and I would like to add to this script whatever commands would turn on the existing extensions. My understanding is that I can do this with<\/p>\n<pre><code># Status of extensions\njupyter labextension list\n# Enable\/disable some extension\njupyter labextension enable extensionIdentifierHere\n<\/code><\/pre>\n<p>However, when I test the enable\/disable command in an instance Terminal window, I receive, for example<\/p>\n<pre><code>[Errno 13] Permission denied: '\/opt\/conda\/etc\/jupyter\/labconfig\/page_config.json'\n<\/code><\/pre>\n<p>If I try to run this with <code>sudo<\/code>, I am asked for a password, but have no idea what that would be, given that I just built the environment and didn't set any password.<\/p>\n<p>Any insights on how to set this up, what the command(s) may be, or how else to approach this, would be appreciated.<\/p>\n<p>Potentially relevant:<\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/65950610\/not-able-to-install-jupyterlab-extensions-on-gcp-ai-platform-notebooks\">Not able to install Jupyterlab extensions on GCP AI Platform Notebooks<\/a><\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/52753205\/unable-to-sudo-to-deep-learning-image\">Unable to sudo to Deep Learning Image<\/a><\/p>\n<p><a href=\"https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/extensions.html#enabling-and-disabling-extensions\" rel=\"nofollow noreferrer\">https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/extensions.html#enabling-and-disabling-extensions<\/a><\/p>\n<p>Edit 1:\nAdding more detail in response to answers and comments (@gogasca, @kiranmathew). My goal is to use ipyleaft-based mapping, through the geemap and earthengine-api python modules, within the notebook. If I create a Managed Notebook instance (service account, Networks shared with me, Enable terminal, all other defaults), launch JupyterLab, open the Terminal from the Launcher, and then run a bash script that creates a venv virtual environment, exposes a custom kernel, and performs the installations, I can use geemap and ipywidgets to visualize and modify (e.g., widget sliders that change map properties) Google Earth Engine assets in a Notebook. If I try to replicate this using a Docker image, it seems to break the connection with ipyleaflet, such that when I start the instance and use a Notebook, I have access to the modules (they can be imported) but can't use ipyleaflet to do the visualization. I thought the issue was that I was not properly enabling the extensions, per the &quot;Error displaying widget: model not found&quot; error, addressed in <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/504\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/889\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/github.com\/jupyter-widgets\/ipyleaflet\/issues\/547\" rel=\"nofollow noreferrer\">this<\/a>, <a href=\"https:\/\/leafmap.org\/faq\/\" rel=\"nofollow noreferrer\">this<\/a>, etc. -- hence the title of my post. I tried using and modifying @TylerErickson 's Dockerfile that modifies a Google deep learning container and should handle all of this (<a href=\"https:\/\/github.com\/gee-community\/ee-jupyter-contrib\/blob\/master\/docker\/gcp_ai_deep_learning_platform\/Dockerfile\" rel=\"nofollow noreferrer\">here<\/a>), but both the original and modifications break the ipyleaflet connection when booting the Managed Notebook instance from the Docker image.<\/p>",
        "Challenge_closed_time":1660628912063,
        "Challenge_comment_count":2,
        "Challenge_created_time":1660564868170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to programmatically enable JupyterLab extensions in a Managed Notebook instance accessed through the Vertex AI workbench. They have tried using the command \"jupyter labextension enable\" but receive a permission denied error and are unsure how to proceed. They are seeking insights on how to set this up or how else to approach the issue. The user has also provided potentially relevant links to similar issues encountered by others.",
        "Challenge_last_edit_time":1660731534620,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73360734",
        "Challenge_link_count":9,
        "Challenge_participation_count":3,
        "Challenge_readability":12.9,
        "Challenge_reading_time":50.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":17.7899702778,
        "Challenge_title":"Programmatically enable installed extensions in Vertex AI Managed Notebook instance",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":202.0,
        "Challenge_word_count":466,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459350905808,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Google Managed Notebooks do not support third-party JL extensions.  Most of these extensions require a rebuild of the JupyterLab static assets bundle. This requires root access which our Managed Notebooks do not support.<\/p>\n<p>Untangling this limitation would require a significant change to the permission and security model that Managed Notebooks provides. It would also have implications for the supportability of the product itself since a user could effectively break their Managed Notebook by installing something rogue.<\/p>\n<p>I would suggest to use User Managed Notebooks.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1660634341887,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":84.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"permission denied error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":123.0280555556,
        "Challenge_answer_count":0,
        "Challenge_body":"The team uses Azure ML CLI to deploy a container to AKS (az ml model deploy). Now and then (not always), they get an internal server error, see stack trace. They could not detect a clear pattern when this error occurs. Although it would be possible to create a retry loop in their Azure DevOps pipeline when this error occurs (as the error message also tells), this would not resolve the underlying issue.\r\n\r\n```\r\n2020-02-14T11:11:07.1739375Z ERROR: {'Azure-cli-ml Version': '1.0.85', 'Error': WebserviceException:\r\n\r\n2020-02-14T11:11:07.1739694Z \tMessage: Received bad response from Model Management Service:\r\n\r\n2020-02-14T11:11:07.1739785Z Response Code: 500\r\n\r\n2020-02-14T11:11:07.1740533Z Headers: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\r\n\r\n2020-02-14T11:11:07.1741400Z Content: b'{\"code\":\"InternalServerError\",\"statusCode\":500,\"message\":\"An internal server error occurred. Please try again. If the problem persists, contact support\"}'\r\n\r\n2020-02-14T11:11:07.1741516Z \tInnerException None\r\n\r\n2020-02-14T11:11:07.1741641Z \tErrorResponse \r\n\r\n2020-02-14T11:11:07.1741708Z {\r\n\r\n2020-02-14T11:11:07.1741813Z     \"error\": {\r\n\r\n2020-02-14T11:11:07.1742819Z         \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 500\\nHeaders: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"InternalServerError\\\",\\\"statusCode\\\":500,\\\"message\\\":\\\"An internal server error occurred. Please try again. If the problem persists, contact support\\\"}'\"\r\n\r\n2020-02-14T11:11:07.1743119Z     }\r\n\r\n2020-02-14T11:11:07.1743227Z }}\r\n```",
        "Challenge_closed_time":1583847372000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583404471000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an import error while running a sample notebook for image classification. The error occurs when trying to import 'Dataset' from 'azureml.core'. The user has provided a reference YAML file that includes the necessary dependencies, and is requesting assistance from Microsoft to investigate the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/841",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":123.0280555556,
        "Challenge_title":"Internal server error when deploying from Azure ML to AKS",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":201,
        "Discussion_body":"@robinvdheijden \r\n\r\nThanks for reaching out to us. This is forum for Machine Learning Notebook only. Please open a new forum thread in [MSDN forum](https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/home?forum=AzureMachineLearningService)as it could be better place to get help on your scenario. These forum community members could provide their expert guidance on your scenario based on their experience. Thanks.\r\n\r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"import error for Dataset"
    },
    {
        "Answerer_created_time":1418276189383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":272.0,
        "Answerer_view_count":142.0,
        "Challenge_adjusted_solved_time":1.9387202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have started using SageMaker Studio Lab.<\/p>\n<p>When I run &quot;apt install xvfb&quot; in SageMaker Studio Lab Notebook, I get the following error.<\/p>\n<pre><code>!apt install xvfb\n\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\n<\/code><\/pre>\n<p>Then I tried with sudo, but the sudo command was not installed.<\/p>\n<pre><code>!sudo apt install xvfb\n\n\/usr\/bin\/sh: 1: sudo: not found\n<\/code><\/pre>\n<p>Can you please tell me how to solve this problem?<\/p>",
        "Challenge_closed_time":1638468298390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638461318997,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to install xvfb in SageMaker Studio Lab Notebook using \"apt install\" command but is facing an error related to permission denied. The user also tried using sudo command but it was not installed. The user is seeking help to solve this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70202848",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.9387202778,
        "Challenge_title":"Is it possible to \"apt install\" in SageMaker Studio Lab?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1174.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1481861289276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>To date Studio Lab doesn't support package installs that require root access. It does support packages installable via pip and conda. You can do that either in your notebook with the %, rather than the !, or you can do that via opening a terminal.<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html<\/a><\/li>\n<\/ul>\n<p>If you'd like to open an issue you're welcome to do that on our repository right here:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md<\/a><\/li>\n<\/ul>\n<p>Thanks for trying out Studio Lab!<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.5,
        "Solution_reading_time":11.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"permission denied installing xvfb"
    },
    {
        "Answerer_created_time":1341161196310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo",
        "Answerer_reputation_count":1057.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When running <code>kedro install<\/code> I get the following error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>Attempting uninstall: terminado\n    Found existing installation: terminado 0.8.3\nERROR: Cannot uninstall 'terminado'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n<\/code><\/pre>\n<p>This github <a href=\"https:\/\/github.com\/jupyter\/notebook\/issues\/4543\" rel=\"nofollow noreferrer\">issue<\/a> suggests the following fix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install terminado --user --ignore-installed\n<\/code><\/pre>\n<p>But it does not work for me as I keep having the same error.<\/p>\n<p><strong>Note:<\/strong>\nThis question is similar to <a href=\"https:\/\/stackoverflow.com\/questions\/61770369\/docker-ubuntu-20-04-cannot-uninstall-terminado-and-problems-with-pip\">this<\/a> but different enough that I think it is worth asking separately.<\/p>",
        "Challenge_closed_time":1605936149736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605936149737,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running 'kedro install' due to the inability to uninstall 'terminado', which is a distutils installed project. The suggested fix of 'pip install terminado --user --ignore-installed' did not work for the user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64940102",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.3,
        "Challenge_reading_time":13.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro install - Cannot uninstall `terminado`",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2331.0,
        "Challenge_word_count":104,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1341161196310,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tokyo",
        "Poster_reputation_count":1057.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>The problem is the version that the kedro template project requires see <code>src\/requiremetns.txt<\/code><\/p>\n<p>In my project it is <code>terminado==0.9.1<\/code>, hence the following solves the problem:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install terminado==0.9.1  --user --ignore-installed\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":4.33,
        "Solution_score_count":10.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Kedro",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to uninstall terminado"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.7880555556,
        "Challenge_answer_count":1,
        "Challenge_body":"hi there,  \n  \nI am setting up a jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. But it shows the following error messages.  \n\": com.microsoft.sqlserver.jdbc.SQLServerException: The TCP\/IP connection to the host xxx.xxx.xxx.xxx, port 21000 has failed. Error: \"connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP\/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\"  \n  \nI used the exactly the same VPC, subnet, security group as what i used in a glue job to extract the data from the local db. While the glue job works but the SageMaker notebook failed. I am sure the firewalls are opened.  \nCould anyone tell me how to solve it?  \nI also came across the following articles, but i am not sure if it is the root cause.  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/",
        "Challenge_closed_time":1557180251000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557130614000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while setting up a Jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. The error message shows that the TCP\/IP connection to the host has failed, and the user is unsure about the root cause. The user has tried using the same VPC, subnet, and security group as used in a glue job, which works, but the SageMaker notebook failed. The user is seeking help to solve the issue.",
        "Challenge_last_edit_time":1668629494374,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUUUm8LxKZTzixOCI_IovK1A\/sagemaker-notebook-instance-in-vpc-failed-to-connect-to-local-database",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":13.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":13.7880555556,
        "Challenge_title":"SageMaker notebook instance in VPC failed to connect to local database",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":547.0,
        "Challenge_word_count":166,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,  \nThe principle here is that there much be network connectivity between the Notebook Instance and the DB Instance, and the security groups on the DB Instance should allow in-bound traffic from the Notebook Instance  \n  \nOne example of such as setup is  \n1. RDS DB Instance is VPC vpc-a and Subnet subnet-b.  \n2. SageMaker Notebook is launched in VPC vpc-a, Subnet subnet-b, with Security Group sg-c with DirectIntenetAccess \"Disabled\"  \n3. In the RDS DB Instance's Security Group rules, you can add an Inbound Rule to allow inbound traffic from the SageMaker Notebook security group \"sg-c\"  \n_-- Type - Protocol - Port Range - Source_  \n_-- MYSQL\/Aurora - TCP - 3306 - sg-c_  \n  \n  \n-----  \nSample Code:\n\n```\n! pip install mysql-connector\r\n\r\nimport mysql.connector\r\nmydb = mysql.connector.connect(\r\nhost=\"$RDS_ENDPOINT\",\r\nuser=\"$RDS_USERNAME\",\r\npasswd=\"$RDS_PASSWORD\"\r\n)\r\ncursor = mydb.cursor()\r\ncursor.execute(\"SHOW DATABASES\") \n```\n\nThanks for using Amazon SageMaker and let us know if there's anything else we can help with!  \n  \nEdited by: JaipreetS-AWS on May 6, 2019 3:04 PM",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1557180305000,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":12.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":148.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"TCP\/IP connection failed"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.7286111111,
        "Challenge_answer_count":0,
        "Challenge_body":"Context\n\nLet's say I have following structure\n\nroot\n\u251c\u2500\u2500model_files\n\u251c\u2500\u2500polyaxonfiles\n\u2502      \u251c\u2500\u2500build.yml\n\u2502      \u2514\u2500\u2500run.yml\n\u251c\u2500\u2500utils\n\u2514\u2500\u2500dockerfiles\n         \u251c\u2500\u2500Dockerfile.0\n         \u2514\u2500\u2500Dockerfile.1\n\n\nmy build looks as simple as:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  context:\n    value: \"{{ globals.run_artifacts_path }}\/uploads\"\n  destination:\n    connection: docker-registry\n    value: machine-learning\/polyaxon-tutorial:1\n\nhubRef: kaniko\n\nRunning this with command\n\npolyaxon run -f polyaxonfiles\/build.yml -u\n\nGives me very much expected error:\n\nError: error resolving dockerfile path: please provide a valid path to a Dockerfile within the build context with --dockerfile\n\nSo the Kaniko expects Dockerfile to exist in root of the context by default. Does the polyaxonfile specification exposes a way to overwrite the default as suggested with error message?",
        "Challenge_closed_time":1620665885000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620659662000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while using Kaniko with Polyaxon, as Kaniko expects the Dockerfile to exist in the root of the context by default. The user is looking for a way to overwrite this default behavior using the Polyaxonfile specification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1315",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":11.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.7286111111,
        "Challenge_title":"(How) Can I pass path to Dockerfile using Kaniko?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":112,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @captainCapitalism, next release we will be pushing a new set of guides, including guides for building containers and how to pass a custom dockerfile context.\n\nWe will update this thread as soon as we start updating the documentation's guides section.",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":3.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":41.0,
        "Tool":"Polyaxon",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Kaniko Dockerfile location"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2038.3938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Challenge_closed_time":1600718139000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593379921000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered an error in the pipeline in github actions which caused their tests to not pass when starting a new experiment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2038.3938888889,
        "Challenge_title":"Warning message appears when calling ``kedro mlflow init``",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"pipeline error, failed tests"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.7979425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When reading the examples from Microsoft on azure ML CLI v2, they use the symbols:\n&quot;|&quot;, &quot;&gt;&quot;, etc., in their yml files.<\/p>\n<p>What do they mean, and where can I find explanations of possible syntax for the Azure CLI v2 engine?<\/p>",
        "Challenge_closed_time":1649231701843,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649142429250,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the symbols used in yml files for Azure ML CLI v2 and is looking for resources to understand the syntax for the engine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71747545",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":3.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":24.7979425,
        "Challenge_title":"Commands in the Azure ML yml files",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":100.0,
        "Challenge_word_count":47,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1620049475608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Denmark",
        "Poster_reputation_count":3.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>| - This pipe symbol in YAML document is used for <em><strong>&quot;Multiple line statements&quot;<\/strong><\/em><\/p>\n<pre><code>description: |\n  # Azure Machine Learning &quot;hello world&quot; job\n\n  This is a &quot;hello world&quot; job running in the cloud via Azure Machine Learning!\n\n  ## Description\n\n  Markdown is supported in the studio for job descriptions! You can edit the description there or via CLI.\n<\/code><\/pre>\n<p>in the above example, we need to write some multiple line description. So, we need to use &quot;|&quot; symbol<\/p>\n<p>&quot;&gt;&quot; - This symbol is used to save some content directly to a specific location document.<\/p>\n<pre><code>command: echo &quot;hello world&quot; &gt; .\/outputs\/helloworld.txt\n<\/code><\/pre>\n<p>In this above command, we need to post <strong>&quot;hello world&quot;<\/strong> to <em><strong>&quot;helloworld.txt&quot;<\/strong><\/em><\/p>\n<p>Check the below link for complete documentation regarding YAML files.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command<\/a><\/p>\n<p>All these symbols are the YAML job commands which are used to accomplish a specific task through CLI.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.6,
        "Solution_reading_time":16.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":139.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"understand yml syntax"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5089333333,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>When I finished the training with the offline mode, I use  the following command to upload the trained results to the cloud service.<\/p>\n<pre><code class=\"lang-auto\">wandb  sync   MY_RUN_DIRECTORY\n<\/code><\/pre>\n<p>But I got the KeyError: \u2018run_url\u2019<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044.jpeg\" data-download-href=\"\/uploads\/short-url\/oQmDb7gGEFmjb7DrMnTKtSiKweM.jpeg?dl=1\" title=\"Screenshot 2023-04-24 202643\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_690x240.jpeg\" alt=\"Screenshot 2023-04-24 202643\" data-base62-sha1=\"oQmDb7gGEFmjb7DrMnTKtSiKweM\" width=\"690\" height=\"240\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_690x240.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_1035x360.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_1380x480.jpeg 2x\" data-dominant-color=\"181818\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2023-04-24 202643<\/span><span class=\"informations\">1396\u00d7487 192 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>How to solve this question?<\/p>",
        "Challenge_closed_time":1682341145608,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682339313448,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a KeyError: 'run_url' while trying to upload trained results to a cloud service using the 'wandb sync' command after finishing training in offline mode. The user is seeking a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sync-error\/4267",
        "Challenge_link_count":5,
        "Challenge_participation_count":7,
        "Challenge_readability":27.6,
        "Challenge_reading_time":23.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.5089333333,
        "Challenge_title":"Sync error",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":85,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lee086824\">@lee086824<\/a> thanks for reporting this issue. There was a regression in wandb <code>v0.14.1<\/code> that would throw this <code>KeyError: 'run_url'<\/code>. Is this your current version, and if so could you please upgrade to our most recent client\/SDK version and try to sync your runs again? Would it work for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":4.6,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"KeyError in wandb sync"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":63.7779516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Challenge_closed_time":1547708377156,
        "Challenge_comment_count":1,
        "Challenge_created_time":1547478776530,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS Sagemaker notebook where the package Scikit-Learn version is not updating to the desired version. The user tried using lifecycle configurations to update the package but it did not work. The correct version of the package is found in the terminal but not in the notebook. The user is seeking help to fix the notebook lifecycle configuration and wondering if there is any virtual environment on the SageMaker instances where the package should be installed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":63.7779516667,
        "Challenge_title":"AWS Sagemaker does not update the package",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1546.0,
        "Challenge_word_count":118,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527781503483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Metz, France",
        "Poster_reputation_count":352.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"package version not updating"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.3988888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Challenge_closed_time":1606993756000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606945520000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in opening a terminal on an m5.8xlarge instance in SageMaker Studio, as the terminal only shows 2 CPUs instead of the expected 32. They are seeking guidance on how to open a terminal on the correct instance.",
        "Challenge_last_edit_time":1668512453971,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sagemaker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":13.3988888889,
        "Challenge_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1054.0,
        "Challenge_word_count":57,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Where do you launch the terminal from? If you use the launcher window, it would start on the t2.medium as you are experiencing.\n\nHowever, if you use the **launch terminal button** in the toolbar that is displayed at the top of your notebook, it will launch the image terminal on the new instance the notebook's kernel is running on (your m5.8xlarge instance).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1610011918846,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":4.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"open terminal on correct instance"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.0516666667,
        "Challenge_answer_count":1,
        "Challenge_body":"How does one choose or tune the hardware backend of a Sagemaker Studio Notebook?",
        "Challenge_closed_time":1576708404000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576675818000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to select or adjust the hardware configuration of a SageMaker Studio Notebook.",
        "Challenge_last_edit_time":1667955285675,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUp5wKTB0URcCPyBgUcAWMww\/how-to-tune-sagemaker-studio-notebooks-hardware-config",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":1.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":9.0516666667,
        "Challenge_title":"How to tune SageMaker Studio Notebooks hardware config?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":21,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"At the top right of a notebook (near the kernel ) there will be a resource configurations button, you'll be able to choose the instance you want to run the notebook on.\n\nA nice feature of that is that all the instances shares the same EFS mount (SageMaker studio uses EFS for notebook storage) if you save a dataframe to the local disk (EFS) you can change instance type during your work and continue from the place you've been in (Move from a GPU instance to a CPU instance for cost effectiveness \/ back to GPU for performance)",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925589624,
        "Solution_link_count":0.0,
        "Solution_readability":18.5,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":96.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"select hardware for SageMaker"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":8.0431913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Azure ML, I'm trying to execute a Python module that needs to import the module pyxdameraulevenshtein (<a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>).<\/p>\n\n<p>I followed the usual way, which is to create a zip file and then import it; however for this specific module, it seems to never be able to find it. The error message is as usual:<\/p>\n\n<p><em>ImportError: No module named 'pyxdameraulevenshtein'<\/em><\/p>\n\n<p>Has anyone included this pyxdameraulevenshtein module in Azure ML with success ?<\/p>\n\n<p>(I took the package from <a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>.)<\/p>\n\n<p>Thanks for any help you can provide,<\/p>\n\n<p>PH<\/p>",
        "Challenge_closed_time":1496307870776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496235911927,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to execute a Python module that needs to import the module pyxdameraulevenshtein in Azure ML. Despite following the usual way of creating a zip file and importing it, the module seems to never be found, resulting in an ImportError. The user is seeking help to include this module in Azure ML successfully.",
        "Challenge_last_edit_time":1496278915287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44285641",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":11.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":19.9885691667,
        "Challenge_title":"Azure ML Python with Script Bundle cannot import module",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2395.0,
        "Challenge_word_count":99,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1496233557192,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I viewed the <code>pyxdameraulevenshtein<\/code> module page, there are two packages you can download which include a wheel file for MacOS and a source code tar file. I don't think you can directly use the both on Azure ML, because the MacOS one is just a share library <code>.so<\/code> file for darwin which is not compatible with Azure ML, and the other you need to first compile it.<\/p>\n\n<p>So my suggestion is as below for using <code>pyxdameraulevenshtein<\/code>.<\/p>\n\n<ol>\n<li>First, compile the source code of <code>pyxdameraulevenshtein<\/code> to a DLL file on Windows, please refer to the document for Python <a href=\"https:\/\/docs.python.org\/2\/extending\/windows.html\" rel=\"nofollow noreferrer\">2<\/a>\/<a href=\"https:\/\/docs.python.org\/3\/extending\/windows.html\" rel=\"nofollow noreferrer\">3<\/a> or search for doing this.<\/li>\n<li>Write a Python script using the DLL you compiled to implement your needs, please refer to the SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/252417\/how-can-i-use-a-dll-file-from-python\">How can I use a DLL file from Python?<\/a> for how to use DLL from Python and refer to the Azure offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts\" rel=\"nofollow noreferrer\">tutorial<\/a> to write your Python script<\/li>\n<li>Package your Python script and DLL file as a zip file, then to upload the zip file to use it in <code>Execute Python script<\/code> model of Azure ML.<\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.5,
        "Solution_reading_time":19.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":192.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"import error with pyxdameraulevenshtein"
    },
    {
        "Answerer_created_time":1469978721363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":0.2587683333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to build a Docker image from SageMaker studio using the CLI described here: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli<\/a><\/p>\n<p>This should be straight forward but I'm missing something because when running the command <code>sm-docker build .<\/code> I have a &quot;invalid syntax&quot; error. Any idea what I'm doing wrong?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/i89fH.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/i89fH.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1620075508956,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620074577390,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an \"invalid syntax\" error while trying to build a Docker image from SageMaker studio using the CLI. The user is seeking help to identify the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67375607",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":9.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":0.2587683333,
        "Challenge_title":"Using the Amazon SageMaker Studio Image Build CLI to build container images from Studio notebooks",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":901.0,
        "Challenge_word_count":74,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469978721363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":415.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>No actually I was missing the &quot;<code>!<\/code>&quot; in front of <code>sm-docker build .<\/code>\nSo <code>!sm-docker build .<\/code> is working.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":1.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid syntax error"
    },
    {
        "Answerer_created_time":1529439461716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":392.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":2.1989977778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I use Microsoft Azure Machine Learning (Azure-ml) to run my (python) experiments.<\/p>\n<p>For specifying the VM and python environment I use:<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core import ScriptRunConfig\n\n# Other imports and code...\n\n# Specify VM and Python environment:\nvm_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)\nvm_env.docker.enabled = True\nvm_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'\n\n# Finally, use the environment in the ScriptRunConfig:\nsrc = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,\n                      script=SCRIPT_FILE_TO_EXECUTE,\n                      arguments=EXECUTE_ARGUMENTS,\n                      compute_target=compute_target,\n                      environment=vm_env)\n<\/code><\/pre>\n<p>I get the following warning for the line <code>vm_env.docker.enabled = True<\/code>:<\/p>\n<pre><code>'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n<\/code><\/pre>\n<p>The documentation about the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.dockersection?view=azure-ml-py\" rel=\"noreferrer\"><code>DockerSection Class<\/code><\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.dockerconfiguration?view=azure-ml-py\" rel=\"noreferrer\"><code>DockerConfiguration Class<\/code><\/a> is not very clear about applying the <code>DockerConfiguration Class<\/code>.<\/p>\n<p><strong>I can't figure out how to use the <code>azureml.core.runconfig.DockerConfiguration<\/code> object. Can someone provide me with an example? Thank you!<\/strong><\/p>",
        "Challenge_closed_time":1620311022350,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620140343337,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in using the azureml.core.runconfig.DockerConfiguration class in azureml.core.Environment or azureml.core.ScriptRunConfig class. The user is getting a warning message for the line vm_env.docker.enabled = True and is unable to figure out how to use the azureml.core.runconfig.DockerConfiguration object. The user is seeking an example to resolve the issue.",
        "Challenge_last_edit_time":1620630619488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67387249",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":21.4,
        "Challenge_reading_time":24.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":47.4108369445,
        "Challenge_title":"How to use azureml.core.runconfig.DockerConfiguration class in azureml.core.Environment or azureml.core.ScriptRunConfig class",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2662.0,
        "Challenge_word_count":133,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1487162649612,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":755.0,
        "Poster_view_count":245.0,
        "Solution_body":"<p>The <code>ScriptRunConfig<\/code> class now accepts a <code>docker_runtime_config<\/code> argument, which is where you pass the <code>DockerConfiguration<\/code> object.<\/p>\n<p>So, the code would look something like this:<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core import ScriptRunConfig\nfrom azureml.core.runconfig import DockerConfiguration\n\n# Other imports and code...\n\n# Specify VM and Python environment:\nvm_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)\nvm_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'\n\ndocker_config = DockerConfiguration(use_docker=True)\n\n# Finally, use the environment in the ScriptRunConfig:\nsrc = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,\n                      script=SCRIPT_FILE_TO_EXECUTE,\n                      arguments=EXECUTE_ARGUMENTS,\n                      compute_target=compute_target,\n                      environment=vm_env,\n                      docker_runtime_config=docker_config)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1620638535880,
        "Solution_link_count":0.0,
        "Solution_readability":25.4,
        "Solution_reading_time":13.09,
        "Solution_score_count":12.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"example for DockerConfiguration object"
    },
    {
        "Answerer_created_time":1502815666600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Memphis, TN, USA",
        "Answerer_reputation_count":5028.0,
        "Answerer_view_count":957.0,
        "Challenge_adjusted_solved_time":0.08215,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.<\/p>\n\n<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\nmlflow from https:\/\/files.pythonhosted.org\/packages\/01\/ec\/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270\/mlflow-1.0.0-py3-none-any.whl#sha256=0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4:\n    Expected sha256 0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4\n         Got        eb34ea16ecfe02d474ce50fd1f88aba82d56dcce9e8fdd30193ab39edf32ac9e\n<\/code><\/pre>",
        "Challenge_closed_time":1560545869140,
        "Challenge_comment_count":1,
        "Challenge_created_time":1560545573400,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install mlflow using pip install on a Windows 10 machine. The error message indicates that the packages do not match the hashes from the requirements file, and suggests that someone may have tampered with them.",
        "Challenge_last_edit_time":1560799785056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56604989",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":10.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.08215,
        "Challenge_title":"How to install mlflow using pip install",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355343131932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5655.0,
        "Poster_view_count":629.0,
        "Solution_body":"<p>It is trying to check cache for packages. They were likely compiled in linux or some other OS and you are trying to install them in Windows.<\/p>\n\n<p>This should fix your issue:<\/p>\n\n<pre><code>pip install --no-cache-dir mlflow\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":3.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"hash mismatch during installation"
    },
    {
        "Answerer_created_time":1474967309676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":59.0,
        "Answerer_view_count":66.0,
        "Challenge_adjusted_solved_time":10362.7869172222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Challenge_closed_time":1644193786212,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606886205577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble installing the gaapi4py package via Anaconda Python3 notebook on Amazon Sagemaker. They have tried using both conda and pip commands, but both have failed with HTTP errors and a failure to establish a new connection. The user has also tried alternative methods recommended by Amazon, but they have not worked either.",
        "Challenge_last_edit_time":1606887753310,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":43.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":10363.2168430556,
        "Challenge_title":"struggling to install python package via amazon sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1148.0,
        "Challenge_word_count":348,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1474967309676,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.1,
        "Solution_reading_time":1.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"installation failure with HTTP errors"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.6138888889,
        "Challenge_answer_count":0,
        "Challenge_body":"as the title suggests\r\n",
        "Challenge_closed_time":1641229646000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1641220236000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to configure their profile with AWS CLI for using AWS Built-in sagemaker algorithms. They are encountering a ValueError that requires them to set up local AWS configuration with a region supported by SageMaker. The user is unsure if it is possible to link access AWS resources in Studiolab.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/38",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.0,
        "Challenge_reading_time":0.98,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":113.0,
        "Challenge_repo_issue_count":218.0,
        "Challenge_repo_star_count":408.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.6138888889,
        "Challenge_title":"I just wonder if i can initialize my sagemaker studio lab? ",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":15,
        "Discussion_body":"Hello - if you have any specific technical issues please use our issue template here to describe it in detail. \r\n\r\nhttps:\/\/github.com\/aws\/studio-lab-examples\/issues\/new?assignees=&labels=bug&template=bug-report-for-sagemaker-studio-lab.md&title=\r\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ValueError in AWS CLI configuration"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.0890694444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using SageMaker JupyterLab, but I found pandas is out of date, what's the process of updating it?<\/p>\n<p>I tried this:\nIn terminal:<\/p>\n<pre><code>cd SageMaker\nconda update pandas\n<\/code><\/pre>\n<p>The package has been updated to 1.0.5\nbut when I use this command in SageMaker instance:<\/p>\n<pre><code>import pandas\nprint(pandas,__version__)\n\nreturn:\n0.24.2\n<\/code><\/pre>\n<p>It didn't work at all, can someone help me? Thanks.<\/p>",
        "Challenge_closed_time":1593895412640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593894159730,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with updating the pandas version in SageMaker JupyterLab. They tried updating the package using the terminal command 'conda update pandas', which was successful, but when they checked the version using the 'import pandas' command, it still showed the old version. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1593895091990,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62734059",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":6.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3480305556,
        "Challenge_title":"How to update pandas version in SageMaker notebook terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1158.0,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540920956270,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":2385.0,
        "Poster_view_count":585.0,
        "Solution_body":"<p>If you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.4,
        "Solution_reading_time":3.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"pandas version not updating"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":5.2137747222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>My current project have the following structure:<\/p>\n<p>Starts with a script in jupyter notebook which dowloads data from a CRM API to put in a local PostgressSql database I run with PgAdmin. After that it runs cluster analysis, return some scoring values, creates a table in database with the results and updates this values in the CRM with another API call. This process will take between 10 to 20 hours (the API only allows 400 requests per minute).<\/p>\n<p>The second notebook reads the database, detects last update, runs api call to update database since the last call, runs kmeans analysis to cluster the data, compare results with the previous call, updates the new ones and the CRM via API. This second process takes less than 2 hours in my estimation and I want this script to run every 24 hours.<\/p>\n<p>After testing, this works fine. Now I'm evaluating how to put this in production in AWS. I understand for the notebooks I need Sagemaker and from I have seen is not that complicated, my only doubt here is if I can call the API without implementing aditional code or need some configuration. My second problem is database. I don't understand the difference between RDS which is the one I think I have to use for this and Aurora or S3. My goal is to write the less code as possible, but a have try some tutorial of RDS  like this one: [1]: <a href=\"https:\/\/www.youtube.com\/watch?v=6fDTre5gikg&amp;t=10s\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=6fDTre5gikg&amp;t=10s<\/a>, and I understand this connect my local postgress to AWS but I can't find the data in the amazon page, only creates an instance?? and how to connect to it to analysis this data from SageMaker. My final goal is to run the notebooks in the cloud and connect to my postgres in the cloud. Just some orientation about how to use this tools would be appreciated.<\/p>",
        "Challenge_closed_time":1609812237472,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609793467883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a Jupyter notebook project that involves downloading data from a CRM API, running cluster analysis, and updating the CRM with the results. The first notebook takes 10-20 hours to run, while the second takes less than 2 hours and needs to run every 24 hours. The user wants to put this project in production in AWS using Sagemaker, but is unsure about how to call the API and which database to use (RDS, Aurora, or S3). The user has tried a tutorial on RDS but is unable to find the data in the Amazon page and needs guidance on how to connect to the database from SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65569634",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":23.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":5.2137747222,
        "Challenge_title":"Best way to set up jupyter notebook project in AWS",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":322,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586658390352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lima, Per\u00fa",
        "Poster_reputation_count":89.0,
        "Poster_view_count":27.0,
        "Solution_body":"<blockquote>\n<p>I don't understand the difference between RDS which is the one I think I have to use for this and Aurora or S3<\/p>\n<\/blockquote>\n<p>RDS and Aurora are <a href=\"https:\/\/en.wikipedia.org\/wiki\/Relational_database\" rel=\"nofollow noreferrer\">relational databases<\/a> fully managed by AWS. &quot;Regular&quot; RDS allows you to launch the existing popular databases such as MySQL, PostgreSQSL and other which you can launch at home\/work as well.<\/p>\n<p>Aurora is <strong>in-house, cloud-native<\/strong> implementation databases compatible with MySQL and PosrgreSQL. It can store the same data as RDS MySQL or PosrgreSQL, but provides a number of features not available for RDS, such as more read replicas, distributed storage, global databases and more.<\/p>\n<p>S3 is <strong>not a database<\/strong>, but an <a href=\"https:\/\/en.wikipedia.org\/wiki\/Object_storage\" rel=\"nofollow noreferrer\">object storage<\/a>, where you can store your files, such as images, csv, excels, similarly like you would store them on your computer.<\/p>\n<blockquote>\n<p>I understand this connect my local postgress to AWS but I can't find the data in the amazon page, only creates an instance??<\/p>\n<\/blockquote>\n<p>You can migrate your data from your local postgress to RDS or Aurora if you wish. But RDS nor Aurora will not connect to your existing local database, as they are databases themselfs.<\/p>\n<blockquote>\n<p>My final goal is to run the notebooks in the cloud and connect to my postgres in the cloud.<\/p>\n<\/blockquote>\n<p>I don't see a reason why you wouldn't be able to connect to the database. You can try to make it work, and if you encounter difficulties you can make new question on SO with RDS\/Aurora setup details.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.9,
        "Solution_reading_time":21.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":254.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"connect to RDS from Sagemaker"
    },
    {
        "Answerer_created_time":1327302732867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"USA",
        "Answerer_reputation_count":19711.0,
        "Answerer_view_count":1030.0,
        "Challenge_adjusted_solved_time":0.1674261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Challenge_closed_time":1597366468132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597366468133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering errors when attempting to train an XGBoost model using the AWS hosted container in a local Jupyter notebook. The errors include a KeyError related to S3DistributionType and a runtime error when attempting to run the docker container. The issue does not occur when using the Amazon cloud SageMaker environment. It is suspected that there is an issue with how the LocalSession() works with the hosted docker container.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.3,
        "Challenge_reading_time":56.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1174.0,
        "Challenge_word_count":293,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327302732867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"USA",
        "Poster_reputation_count":19711.0,
        "Poster_view_count":1030.0,
        "Solution_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1597367070867,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":32.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":235.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"errors in local container"
    },
    {
        "Answerer_created_time":1572449042430,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":2082.0,
        "Answerer_view_count":238.0,
        "Challenge_adjusted_solved_time":2.8597922222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The entire error message after executing <code>terraform apply<\/code> within the terraform-folder of <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">this source code in my GitHub-repo<\/a> (inspired by <a href=\"https:\/\/www.linkedin.com\/pulse\/terraform-sagemaker-part-2a-creating-custom-notebook-instance-david\" rel=\"nofollow noreferrer\">this tutorial<\/a> and <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\" rel=\"nofollow noreferrer\">its related GitHub-repo<\/a>):<\/p>\n<pre><code>aws_sagemaker_notebook_instance.notebook_instance: Creating...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [10s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [20s elapsed]\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m21s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m31s elapsed]\n\u2577\n\u2502 Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)\n\u2502\n\u2502   with aws_sagemaker_notebook_instance.notebook_instance,\n\u2502   on notebook_instance.tf line 2, in resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot;:\n\u2502    2: resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n\u2502\n<\/code><\/pre>\n<p>Internet research seemed to provide the solution in <a href=\"https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\" rel=\"nofollow noreferrer\">this article<\/a>, which inspired be to increase the allowed <code>IDLE_TIME<\/code> in the <code>on-start.sh<\/code> - script to <code>IDLE_TIME=1800<\/code> (in seconds, which equals 30 minutes). This should've been sufficient for the deployment time of around 15 minutes; yet, it threw the same error again.<\/p>\n<p>Next, I found <a href=\"https:\/\/stackoverflow.com\/questions\/65884743\/resolving-broken-deleted-state-in-terraform\">this post on StackOverFlow<\/a> suggesting to<\/p>\n<blockquote>\n<p>run <code>terraform refresh<\/code>, which will cause Terraform to refresh its state\nfile against what actually exists with the cloud provider.<\/p>\n<\/blockquote>\n<p>Unfortunately, running <code>terraform apply<\/code> right after refreshing didn't resolve the issue either.\nI'm wondering why the aforementioned <code>IDLE_TIME=1800<\/code> - setting does not have any effect. This should be more than sufficient for a 15-minute apply-time.<\/p>\n<hr \/>\n<p><strong>EDIT: adding code specifics for enhanced understanding<\/strong><\/p>\n<p><strong>1. Creating the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name                    = &quot;aws-sm-notebook-instance&quot;\n  role_arn                = aws_iam_role.notebook_iam_role.arn\n  instance_type           = &quot;ml.t2.medium&quot;\n  lifecycle_config_name   = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p><strong>2. Defining the SageMaker notebook lifecycle configuration<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance_lifecycle_configuration&quot; &quot;notebook_config&quot; {\n  name      = &quot;dev-platform-al-sm-lifecycle-config&quot;\n  on_create = filebase64(&quot;..\/scripts\/on-create.sh&quot;)\n  on_start  = filebase64(&quot;..\/scripts\/on-start.sh&quot;)\n}\n<\/code><\/pre>\n<p><strong>3. Defining the Git repo to instantiate on the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_code_repository&quot; &quot;git_repo&quot; {\n  code_repository_name = &quot;aws-sm-notebook-instance-repo&quot;\n\n  git_config {\n    repository_url = &quot;https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance.git&quot;\n  }\n}\n<\/code><\/pre>\n<p><strong>Contents of <code>on-start.sh<\/code> (including IDLE_TIME - parameter)<\/strong>\nNote that this script will be invoked by the <code>scripts\/autostop.py<\/code> - script, which you can find <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\/blob\/main\/scripts\/autostop.py\" rel=\"nofollow noreferrer\">here<\/a> in the associated <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">public repo containing the source code<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\n\n## IDLE AUTOSTOP STEPS\n## ----------------------------------------------------------------\n\n## Setting the timeout (in seconds) for how long the SageMaker notebook can run idly before being auto-stopped\n# -&gt; e.g. 1800 s = 30 min since first deployment can take between 15 and 20 minutes which could then fail like so:\n# &quot;Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)&quot;\n# Hint for solution under following link: https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\nIDLE_TIME=1800\n\n# Getting the autostop.py script from GitHub\necho &quot;Fetching the autostop script...&quot;\nwget https:\/\/raw.githubusercontent.com\/andreasluckert\/aws-sm-notebook-instance\/main\/scripts\/autostop.py\n\n# Using crontab to autostop the notebook when idle time is breached\necho &quot;Starting the SageMaker autostop script in cron.&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/5 * * * * \/usr\/bin\/python $PWD\/autostop.py --time $IDLE_TIME --ignore-connections&quot;) | crontab -\n\n\n\n## CUSTOM CONDA KERNEL USAGE STEPS\n## ----------------------------------------------------------------\n\n# Setting the proper user credentials\nsudo -u ec2-user -i &lt;&lt;'EOF'\nunset SUDO_UID\n\n# Setting the source for the custom conda kernel\nWORKING_DIR=\/home\/ec2-user\/SageMaker\/custom-miniconda\nsource &quot;$WORKING_DIR\/miniconda\/bin\/activate&quot;\n\n# Loading all the custom kernels\nfor env in $WORKING_DIR\/miniconda\/envs\/*; do\n    BASENAME=$(basename &quot;$env&quot;)\n    source activate &quot;$BASENAME&quot;\n    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;\ndone\n<\/code><\/pre>",
        "Challenge_closed_time":1631187921612,
        "Challenge_comment_count":6,
        "Challenge_created_time":1631108848430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while creating a SageMaker notebook instance using Terraform. The error message showed that the instance creation failed with an unexpected state 'Failed' instead of the target state 'InService'. The user tried increasing the allowed IDLE_TIME in the on-start.sh script but it did not resolve the issue. The user also tried refreshing the state file using 'terraform refresh' but it did not help either. The user is wondering why the IDLE_TIME setting did not have any effect. The post includes code specifics for creating the SageMaker notebook instance, defining the lifecycle configuration, and defining the Git repo to instantiate on the instance.",
        "Challenge_last_edit_time":1631177626360,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69104302",
        "Challenge_link_count":10,
        "Challenge_participation_count":7,
        "Challenge_readability":16.9,
        "Challenge_reading_time":85.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":21.9647727778,
        "Challenge_title":"Terraform Error: error waiting for sagemaker notebook instance to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(<nil>)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":500,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572449042430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2082.0,
        "Poster_view_count":238.0,
        "Solution_body":"<p>The solution to the problem was to check the CloudWatch Log events under <code>CloudWatch -&gt; Log groups -&gt; \/aws\/sagemaker\/NotebookInstances -&gt; aws-sm-notebook-instance\/LifecycleConfigOnCreate<\/code> to find the following error-message:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnCreate_2021-09-08-12-24rw5al34g: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>A bit of internet research brought me to <a href=\"https:\/\/askubuntu.com\/questions\/304999\/not-able-to-execute-a-sh-file-bin-bashm-bad-interpreter\/305001#305001\">this solution related to newline characters in shell-scripts<\/a>, which depend on whether you are on <code>Windows<\/code> or a <code>UNIX<\/code>-system.\nAs I'm working on Windows, the shell-scripts created in VS-Code comprised dos-specific <code>CRLF<\/code> newline-handling, which could be resolved via the button on the bottom-right in <code>VS-Code<\/code> to switch the <em>carriage return<\/em> (CRLF) character to the <em>line feed<\/em> (LF) character used by UNIX.<\/p>\n<p>As the compute instance employed by AWS Sagemaker is a Linux-system, it cannot handle the dos-style CRLF newline-characters in the shell-scripts and this &quot;adds&quot; a <code>^M<\/code> after <code>\/bin\/bash<\/code> which obviously leads to an error as such an interpreter does not exist.<\/p>\n<p>So, finally <code>terraform apply<\/code> worked out well:<\/p>\n<pre><code>$ terraform apply\n...\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m30s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m40s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Creation complete after 7m43s [id=aws-sm-notebook-instance]\n\nApply complete! Resources: 1 added, 1 changed, 1 destroyed.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":23.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":184.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"instance creation failed unexpectedly"
    },
    {
        "Answerer_created_time":1433746746023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":22140.0,
        "Answerer_view_count":1710.0,
        "Challenge_adjusted_solved_time":6.9395733333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been trying to install a machine learning package that I can use in my R script.<\/p>\n\n<p>I have done placed the tarball of the installer inside a zip file and am doing <\/p>\n\n<pre><code>install.packages(\"src\/packagename_2.0-3.tar.gz\", repos = NULL, type=\"source\") \n<\/code><\/pre>\n\n<p>from within the R script. However, the progress indicator just circles indefinitely, and it's not installed in environment.<\/p>\n\n<p>How can I install this package?<\/p>\n\n<p><code>ada<\/code> is the package I'm trying to install and <code>ada_2.0-3.tar.gz<\/code> is the file I'm using.<\/p>",
        "Challenge_closed_time":1443095996316,
        "Challenge_comment_count":0,
        "Challenge_created_time":1443064053390,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in installing an R library in Azure ML. They have tried to install the package using a tarball of the installer inside a zip file, but the progress indicator just circles indefinitely, and the package is not installed in the environment. The package they are trying to install is \"ada\" using the file \"ada_2.0-3.tar.gz\".",
        "Challenge_last_edit_time":1443071013852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32752659",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":7.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":8.873035,
        "Challenge_title":"unable to install R library in azure ml",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":982.0,
        "Challenge_word_count":86,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426899441168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2024.0,
        "Poster_view_count":298.0,
        "Solution_body":"<p>You cannot use the tarball packages. If you are on windows you need to do the following:<\/p>\n\n<p>Once you install a package (+ it's dependencies) it will download the packages in a directory <\/p>\n\n<blockquote>\n  <p>C:\\Users\\xxxxx\\AppData\\Local\\Temp\\some directory\n  name\\downloaded_packages<\/p>\n<\/blockquote>\n\n<p>These will be in a zip format. These are the packages you need. <\/p>\n\n<p>Or download the windows binaries from cran.<\/p>\n\n<p>Next you need to put all the needed packages in one total zip-file and upload this to AzureML as a new dataset.<\/p>\n\n<p>in AzureML load the data package connected to a r-script<\/p>\n\n<pre><code>install.packages(\"src\/ada.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(ada, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>Be sure to check that all dependent packages are available in Azure. Rpart is available.<\/p>\n\n<p>For a complete overview, look at this <a href=\"http:\/\/blogs.msdn.com\/b\/benjguin\/archive\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning.aspx\" rel=\"nofollow\">msdn blog<\/a> explaining it a bit better with some visuals.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":13.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":135.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"indefinite installation progress"
    },
    {
        "Answerer_created_time":1318047784840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":6665.0,
        "Answerer_view_count":926.0,
        "Challenge_adjusted_solved_time":0.5016727778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to install the following library in my Azure ML instance:<\/p>\n<p><a href=\"https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI<\/a><\/p>\n<p>My Dockerfile looks like this:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04:20210615.v1\n\nENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.7\n\n# Create conda environment\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n    python=3.7 \\\n    pip=20.2.4 \\\n    pytorch=1.7.1 \\\n    torchvision=0.8.2 \\\n    torchaudio=0.7.2 \\\n    cudatoolkit=11.0 \\\n    nvidia-apex=0.1.0 \\\n    -c anaconda -c pytorch -c conda-forge\n\n# Prepend path to AzureML conda environment\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                'psutil&gt;=5.8,&lt;5.9' \\\n                'tqdm&gt;=4.59,&lt;4.60' \\\n                'pandas&gt;=1.1,&lt;1.2' \\\n                'scipy&gt;=1.5,&lt;1.6' \\\n                'numpy&gt;=1.10,&lt;1.20' \\\n                'azureml-core==1.31.0' \\\n                'azureml-defaults==1.31.0' \\\n                'azureml-mlflow==1.31.0' \\\n                'azureml-telemetry==1.31.0' \\\n                'tensorboard==2.4.0' \\\n                'tensorflow-gpu==2.4.1' \\\n                'onnxruntime-gpu&gt;=1.7,&lt;1.8' \\\n                'horovod[pytorch]==0.21.3' \\\n                'future==0.17.1' \\\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n\n# This is needed for mpi to locate libpython\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n<\/code><\/pre>\n<p>An error is thrown when the library is being installed:<\/p>\n<pre><code>  Cloning https:\/\/github.com\/philferriere\/cocoapi.git to \/tmp\/pip-install-_i3sjryy\/pycocotools\n[91m    ERROR: Command errored out with exit status 1:\n     command: \/azureml-envs\/pytorch-1.7\/bin\/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base \/tmp\/pip-pip-egg-info-o68by1_q\n         cwd: \/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\n    Complete output (5 lines):\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py&quot;, line 2, in &lt;module&gt;\n        from Cython.Build import cythonize\n    ModuleNotFoundError: No module named 'Cython'\n<\/code><\/pre>\n<p>I've tried adding Cython as a dependecy in both the pip section and as part of the conda environment but the error is still thrown.<\/p>",
        "Challenge_closed_time":1625324240192,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625322434170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to install a library in their Azure ML instance using a Dockerfile, but encounters an error stating that the 'Cython' module is not found. The user has attempted to add Cython as a dependency in both the pip section and as part of the conda environment, but the error persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68237132",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":20.7,
        "Challenge_reading_time":39.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.5016727778,
        "Challenge_title":"No module named 'Cython' setting up Azure ML docker instance",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":209.0,
        "Challenge_word_count":200,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1318047784840,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":6665.0,
        "Poster_view_count":926.0,
        "Solution_body":"<p>Solution was to add the following to the Dockerfile:<\/p>\n<pre><code># Install Cython\nRUN pip3 install Cython\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                ...\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":22.9,
        "Solution_reading_time":4.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":26.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing Cython module"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":3.0575130556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How do I upgrade <code>azureml-sdk<\/code> such that the newest release of <code>azureml-core<\/code>, <code>1.1.5.5<\/code>, is installed? \nIf <code>azureml-sdk<\/code> is not installed, <code>pip install --upgrade azureml-sdk<\/code> will install <code>azureml-core==1.1.5.5<\/code>. If it is already installed, then it won't.<\/p>\n\n<pre><code>$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n$ pip install --upgrade azureml-sdk[interpret,notebooks]\n$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n<\/code><\/pre>",
        "Challenge_closed_time":1584735835707,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584723850063,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in upgrading the azureml-sdk to the latest release of azureml-core, i.e., 1.1.5.5. The user has tried upgrading using pip install but it did not work. The user has shared the command used and the output received.",
        "Challenge_last_edit_time":1584724828660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60778546",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.3293455556,
        "Challenge_title":"pip install azureml-sdk with latest patches to underlying libraries",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1142.0,
        "Challenge_word_count":63,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405457120427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Solution_body":"<p>You can use the eager strategy to force an upgrade of requirements:<\/p>\n\n<pre><code>pip install -U --upgrade-strategy eager azureml-sdk\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":1.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed upgrade with pip"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":503.9755555556,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\n$ dvc repro run_benchmarks\r\nERROR: 'dvc.lock' is git-ignored.\r\n```\r\n\r\n`.dvc.lock` in `.gitignore` causes Exceptions at running benchmark. Delete this line solves this problem. And because of #168 maybe we need some better ways to deal with `dvc.lock`.",
        "Challenge_closed_time":1621495987000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619681675000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered several issues while running experiments using `example-dvc-experiments`. These issues include dvc not being installed by `pip install -r requirements.txt`, an error while running `dvc pull`, all images being listed when running the `extract` stage using `dvc exp run`, and unclear instructions regarding the use of `dvc repro` and `dvc exp run`. The user suggests including `dvc` in `requirements.txt` and clarifying the instructions for using `dvc repro` and `dvc exp run`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/255",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":5.8,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":12.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":458.0,
        "Challenge_repo_star_count":21.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":503.9755555556,
        "Challenge_title":"ERROR: 'dvc.lock' is git-ignored.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_type":"anomaly",
        "Challenge_summary":"multiple issues with DVC"
    },
    {
        "Answerer_created_time":1466260908296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":149.6007655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the Azure Recommendation API sample there is a snippet like this:<\/p>\n\n<pre><code>     if (itemSets.RecommendedItemSetInfo != null)\n        {\n            ...\n        }\n        else\n        {\n            Console.WriteLine(\"No recommendations found.\");\n        }\n<\/code><\/pre>\n\n<p>So I assume that nullable recommended set means no recommendations. But what is the case with this set being not nullable but still empty ( as I am having it running the example)?<\/p>\n\n<p>I provided my own usages and catalog files. I have not too many entries there however for i2i recommendations I have results and for u2i there is an empty set.\nAllowColdItemPlacement doesn't change a think here.<\/p>",
        "Challenge_closed_time":1478186851716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1477648288960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between null and empty results in the Azure Recommendation API sample. They have provided their own usage and catalog files and are experiencing an empty set for u2i recommendations, despite the set being not nullable. They are unsure if an empty set means no recommendations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40302499",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":149.6007655556,
        "Challenge_title":"Recommendation API: what is the difference between null results and empty results",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1354118434116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Wroc\u0142aw, Poland",
        "Poster_reputation_count":393.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>We did not mean to convey a difference in meaning between null recommendations and empty recommendations. I will check why we may be sending two different types of results. Either way, don't treat those two cases as different cases. <\/p>\n\n<p>If you are not getting results for user-to-item recommendations, most likely there was no data for that user when the build was created or the items that the user interacted with do not have enough co-occurrences with other items in the usage.<\/p>\n\n<p>What to do when you get empty recommendations is up to you, you may decide to not show any recommendations, or back-fill with popular items you may want to promote.<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Luis Cabrera\nProgram Manager - Recommendations API.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":119.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"difference between null and empty"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.9785680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Since the latest Azure ML release, we have been unable to submit any job using a private docker registry. Same jobs were working before the new release.  <br \/>\nWe configure the job as follows (all of this is automated and the code has not changed):<\/p>\n<p>base_image_name = 'REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_1601582281'<\/p>\n<pre><code># Set the container registry information  \n\nmyenv = Environment(name=&amp;#34;lb&amp;#34;)  \n\nmyenv.docker.enabled = True  \nmyenv.docker.base_image = base_image_name  \nmyenv.docker.base_image_registry.address = &amp;#39;REDACTED.azurecr.io\/lb\/&amp;#39;  \nmyenv.docker.base_image_registry.username, myenv.docker.base_image_registry.password = get_docker_secrets()  \nmyenv.python.user_managed_dependencies = True  \n\nmyenv.python.interpreter_path = &amp;#34;\/opt\/miniconda\/bin\/python&amp;#34;  \n<\/code><\/pre>\n<p>Instead of successful job submission, we are instead getting:  <br \/>\n{  <br \/>\n&quot;error&quot;: {  <br \/>\n&quot;message&quot;: &quot;Activity Failed:\\n{\\n \\&quot;error\\&quot;: {\\n \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n \\&quot;message\\&quot;: \\&quot;Unable to get image details : Specified base docker image REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_16\\&quot;,\\n \\&quot;details\\&quot;: []\\n },\\n \\&quot;correlation\\&quot;: {\\n \\&quot;operation\\&quot;: null,\\n \\&quot;request\\&quot;: \\&quot;c41448d429f9c80b\\&quot;\\n },\\n \\&quot;environment\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;location\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;time\\&quot;: \\&quot;2020-11-09T21:40:39.699533Z\\&quot;,\\n \\&quot;componentName\\&quot;: \\&quot;execution-worker\\&quot;\\n}&quot;  <br \/>\n}  <br \/>\n}  <br \/>\nThe image has not changed (we tried a few different ones from prior successful jobs) and the use of the SDK has not changed.  <br \/>\nHas anybody else encountered a similar problem since the Nov 5 upgrade (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes<\/a>)?  <br \/>\nThis is a major block as we cannot proceed with any project that depend on Azure ML at this time.<\/p>",
        "Challenge_closed_time":1605016029612,
        "Challenge_comment_count":4,
        "Challenge_created_time":1604958506767,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to submit any job using a private docker registry since the latest Azure ML release. The error message received is \"Unable to get image details\". The image and the use of the SDK have not changed. The user is seeking help from others who may have encountered a similar problem since the Nov 5 upgrade.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/157021\/unable-to-use-private-docker-registry-with-latest",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":14.5,
        "Challenge_reading_time":29.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15.9785680556,
        "Challenge_title":"Unable to use private docker registry with latest Azure ML release",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":191,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d02d6aeb-d51f-460e-9be5-e8da649952cc\">@Fabien Campagne  <\/a>  Thanks for the details, with fully qualified base image name you do not need to specify container registry address. container registry address itself should be just a host name.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":3.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to get image details"
    },
    {
        "Answerer_created_time":1622632545867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":1.2254397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using a <strong>Jupyter Lab<\/strong> instance on <strong>AWS SageMaker<\/strong>.<\/p>\n<p>Kernel: <code>conda_mxnet_latest_p37<\/code>.<\/p>\n<p><code>url_lib<\/code> contains some false urls, that I exception handle.<\/p>\n<pre><code>['15', '259', '26', '58', 'https:\/\/imagepool.1und1-drillisch.de\/v2\/download\/nachhaltigkeitsbericht\/1&amp;1Drillisch_Sustainability_Report_EN_2018.pdf', 'https:\/\/imagepool.1und1-drillisch.de\/\/v2\/download\/nachhaltigkeitsbericht\/2018-04-06_1und1-Drillisch_Sustainability_Report_eng.pdf', '6', 'http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf', '80', 'https:\/\/multimedia.3m.com\/mws\/media\/1691941O\/2019-sustainability-report.PDF', 'https:\/\/s3-us-west-2.amazonaws.com\/ungc-production\/attachments\/cop_2020\/483648\/original\/GPIC_Sustainability_Report_2020__-_40_Years_of_Sustainable_Success.pdf?1583154650', 'https:\/\/drive.google.com\/open?id=1_dnBcfXWjexy9QoWRhOk_3gnOkWfYRCw', 'http:\/\/aepsustainability.com\/performance\/docs\/2020AEPGRIReport.pdf']  # sample\n<\/code><\/pre>\n<p>However, ones that are working URLs, throw this error:<\/p>\n<pre><code>[Errno 13] Permission denied: '\/data'\n<\/code><\/pre>\n<p>I don't have the directory opened, nor files since I they're not downloaded.<\/p>\n<p>I ran in <strong>Terminal<\/strong> without luck:<\/p>\n<pre><code>sh-4.2$ chmod 777 data\nsh-4.2$ chmod 777 data\/\nsh-4.2$ chmod 777 data\/gri\nsh-4.2$ chmod 777 data\/gri\/\n<\/code><\/pre>\n<p><strong>Code:<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport opendatasets as od\nimport urllib\nimport zipfile\nimport os\n\ncsr_df = pd.read_excel('data\/Company Sustainability Reports.xlsx', index_col=None)\nurl_list = csr_df['Report PDF Address'].tolist()\n\nfor url in url_list:\n    try:\n        download = od.download(url, '\/data\/gri\/')\n        filename = url.rsplit('\/', 1)[1]\n\n        path_extract = 'data\/gri\/' + filename\n        with zipfile.ZipFile('data\/gri\/' + filename + '.zip', 'r') as zip_ref:\n            zip_ref.extractall(path_extract)\n\n        os.remove(path_extract + 'readme.txt')\n\n        filenames = os.listdir(path_extract)\n        scans = []\n        for f in filenames:\n            with Image.open(path_extract + f) as img:\n                matrix = np.array(img)\n                scans.append(matrix)\n\n        # shutil.rmtree(path_extract)\n        os.remove(path_extract[:-1] + '.zip')\n\n    except (urllib.error.URLError, IOError, RuntimeError) as e:\n        print('Download PDFs', e)\n<\/code><\/pre>\n<p><strong>Output:<\/strong><\/p>\n<pre><code>Download PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs list index out of range\nDownload PDFs [Errno 13] Permission denied: '\/data'\n...\n<\/code><\/pre>\n<p>Please let me know if there is anything else I should clarify.<\/p>",
        "Challenge_closed_time":1639406624843,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639402213260,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a permission denied error while trying to download files using Jupyter Lab on AWS SageMaker. The error occurs when trying to download working URLs, and the user has attempted to change permissions using Terminal without success. The user has provided code and output for reference.",
        "Challenge_last_edit_time":1639406637992,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70335470",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":41.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":1.2254397222,
        "Challenge_title":"'[Errno 13] Permission denied' - Jupyter Labs on AWS SageMaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":684.0,
        "Challenge_word_count":264,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1622632545867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p><code>download<\/code> has a forward-slash <code>\/<\/code> as first character of save directory (second parameter). I removed this:<\/p>\n<pre><code>download = od.download(url, 'data\/gri\/')\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>...\nDownloading http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf to data\/gri\/1556248045.pdf\n450560it [00:02, 207848.59it\/s]\n...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"permission denied error"
    },
    {
        "Answerer_created_time":1639907005592,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":323.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":1.51305,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Looking at <a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/r_examples\/r_in_sagemaker_processing\/r_in_sagemaker_processing.html\" rel=\"nofollow noreferrer\">this page<\/a> and this piece of code in particular:<\/p>\n<pre><code>import boto3\n\naccount_id = boto3.client(&quot;sts&quot;).get_caller_identity().get(&quot;Account&quot;)\nregion = boto3.session.Session().region_name\n\necr_repository = &quot;r-in-sagemaker-processing&quot;\ntag = &quot;:latest&quot;\n\nuri_suffix = &quot;amazonaws.com&quot;\nprocessing_repository_uri = &quot;{}.dkr.ecr.{}.{}\/{}&quot;.format(\n    account_id, region, uri_suffix, ecr_repository + tag\n)\n\n# Create ECR repository and push Docker image\n!docker build -t $ecr_repository docker\n!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n!aws ecr create-repository --repository-name $ecr_repository\n!docker tag {ecr_repository + tag} $processing_repository_uri\n!docker push $processing_repository_uri\n<\/code><\/pre>\n<p>This is not pure Python obviously? Are these AWS CLI commands? I have used docker previously but I find this example very confusing. Is anyone aware of an end-2-end example of simply running some R job in AWS using sage maker\/docker? Thanks.<\/p>",
        "Challenge_closed_time":1639923580880,
        "Challenge_comment_count":0,
        "Challenge_created_time":1639919443300,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty understanding the code provided in an AWS SageMaker example for running an R script. They are unsure if the code contains AWS CLI commands and are looking for a simpler end-to-end example for running an R job in AWS using SageMaker and Docker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70411715",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.7,
        "Challenge_reading_time":16.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1.1493277778,
        "Challenge_title":"running r script in AWS",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":113,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>This is Python code mixed with shell script magic calls (the <code>!commands<\/code>).<\/p>\n<p>Magic commands aren't unique to this platform, you can use them in <a href=\"https:\/\/ipython.readthedocs.io\/en\/stable\/interactive\/magics.html\" rel=\"nofollow noreferrer\">Jupyter<\/a>, but this particular code is meant to be run on their platform. In what seems like a fairly convoluted way of running R scripts as processing jobs.<\/p>\n<p>However, the only thing you really need to focus on is the R script, and the final two cell blocks. The instruction at the top (don't change this line) creates a file (preprocessing.R) which gets executed later, and then you can see the results.<\/p>\n<p>Just run all the code cells in that order, with your own custom R code in the first cell. Note the line <code>plot_key = &quot;census_plot.png&quot;<\/code> in the last cell. This refers to the image being created in the R code. As for other output types (eg text) you'll have to look up the necessary Python package (PIL is an image manipulation package) and adapt accordingly.<\/p>\n<p>Try this to get the CSV file that the R script is also generating (this code is not validated, so you might need to fix any problems that arise):<\/p>\n<pre><code>import csv\n\ncsv_key = &quot;plot_data.csv&quot;\ncsv_in_s3 = &quot;{}\/{}&quot;.format(preprocessed_csv_data, csv_key)\n!aws s3 cp {csv_in_s3} .\n\nfile = open(csv_key)\ndat = csv.reader(file)\n\ndisplay(dat)\n<\/code><\/pre>\n<p>So now you should have an idea of how two different output types the R script example generates are being handled, and from there you can try and adapt your own R code based on what it outputs.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1639924890280,
        "Solution_link_count":1.0,
        "Solution_readability":7.8,
        "Solution_reading_time":20.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":247.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"simpler R job example"
    },
    {
        "Answerer_created_time":1480951171328,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":3.2597472222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can I perform batch execution against Azure ML web service without using BLOB?<\/p>\n\n<p>I'm reading the data from SQL database into a CSV file stream. Can I stream this directly to the service without actually saving in and reading back the result from the BLOB?<\/p>",
        "Challenge_closed_time":1481708421107,
        "Challenge_comment_count":0,
        "Challenge_created_time":1481696686017,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of performing batch execution against Azure ML web service without using BLOB. They want to know if they can stream data directly to the service without saving it in and reading back the result from the BLOB.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41136102",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":3.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3.2597472222,
        "Challenge_title":"Perform batch execution without using BLOB",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":180.0,
        "Challenge_word_count":50,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1376999872723,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Malaysia",
        "Poster_reputation_count":998.0,
        "Poster_view_count":136.0,
        "Solution_body":"<p>BES doesn't work with streams. The data needs to be stored somewhere for AML to access the data. Take a look at: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-consume-web-services#batch-execution-service-bes\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-consume-web-services#batch-execution-service-bes<\/a> for more information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":20.9,
        "Solution_reading_time":5.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":29.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"batch execution without BLOB"
    },
    {
        "Answerer_created_time":1569518464147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":432.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":204.5016991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Python 3.10, Pip install azureml-sdk 1.39.0.<br \/>\nEnvironments: Win10 PS, VS2022, and a docker image- all same results . Pip show shows the azureml-core package.<\/p>\n<p>Simple (I thought) script, but it can't find &quot;azureml.core&quot;   No module named azureml is the error.\nHow do I make it &quot;find&quot; it? I'm new at python so it could be syntax.<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Experiment, Environment, Model,Dataset,Datastore,ScriptRunConfig\n     \n    # check core SDK version number\n    print(&quot;Azure ML SDK Version: &quot;, azureml.core.VERSION)\n<\/code><\/pre>",
        "Challenge_closed_time":1648177993987,
        "Challenge_comment_count":2,
        "Challenge_created_time":1647441787870,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to run a Python script that uses the AzureML SDK. Despite installing the required package and checking its presence, the script is unable to find the \"azureml.core\" module, resulting in a \"No module named azureml\" error. The user is seeking guidance on how to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71499094",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.5,
        "Challenge_reading_time":8.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":204.5016991667,
        "Challenge_title":"Python AzureML Hello world - Can't find module azureml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":310.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1263312456836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>azureml python sdk does not support py3.10 yet, AutoML sdk supports py&lt;=3.8.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":1.1,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing azureml.core module"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":66.2517636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>Is there a way to specify the disk storage type for Compute instances?     <br \/>\nBoth the Azure portal and ARM templates do not have an option to define the disk storage type, which defaults to the P10 disks (Premium SSD).     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/148883-azureml-compute.png?platform=QnA\" alt=\"148883-azureml-compute.png\" \/>Thanks    <\/p>",
        "Challenge_closed_time":1636952401092,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636713894743,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in specifying the disk storage type for Compute instances in Azure Machine Learning. The default disk storage type is P10 disks (Premium SSD) and there is no option to define it in both the Azure portal and ARM templates.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/625035\/azure-machine-learning-specify-disk-storage-type",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":66.2517636111,
        "Challenge_title":"Azure Machine Learning - Specify disk storage type",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=41b4924d-c8f5-4ca4-9844-0c0af46eb5d5\">@Simon Magrin  <\/a>  Thanks, Currently There's no way to change the disk storage type for CIs or compute clusters. We have added this to our product backlog item to support in the near future.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":13.8,
        "Solution_reading_time":17.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to specify disk storage type"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":16.1130741667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Similar question as <a href=\"https:\/\/stackoverflow.com\/questions\/43176442\/install-r-packages-in-azure-ml\">here<\/a> but now on Python packages. Currently, the CVXPY is missing in Azure ML. I am also trying to get other solvers such as GLPK, CLP and COINMP working in Azure ML.<\/p>\n<p><strong>How can I install Python packages in Azure ML?<\/strong><\/p>\n<hr \/>\n<p><em>Update about trying to install the Python packages not found in Azure ML.<\/em><\/p>\n<blockquote>\n<p>I did as instructed by Peter Pan but I think the 32bits CVXPY files are wrong for the Anaconda 4 and Python 3.5 in Azure ML, logs and errors are <a href=\"https:\/\/pastebin.com\/zN5QrPtL\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>[Information]         Running with Python 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)]\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9glSm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9glSm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/blockquote>\n<p><em>Update 2 with win_amd64 files (paste <a href=\"https:\/\/pastebin.com\/tisWuP5C\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code>[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         cvxopt-1.1.9-cp35-cp35m-win_amd64.whl          2017-06-07 01:03:34      1972074\n[Information]         __MACOSX\/                                      2017-06-07 01:26:28            0\n[Information]         __MACOSX\/._cvxopt-1.1.9-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:34          452\n[Information]         cvxpy-0.4.10-py3-none-any.whl                  2017-06-07 00:25:36       300880\n[Information]         __MACOSX\/._cvxpy-0.4.10-py3-none-any.whl       2017-06-07 00:25:36          444\n[Information]         ecos-2.0.4-cp35-cp35m-win_amd64.whl            2017-06-07 01:03:40        56522\n[Information]         __MACOSX\/._ecos-2.0.4-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:40          450\n[Information]         numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl   2017-06-07 01:25:02    127909457\n[Information]         __MACOSX\/._numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl 2017-06-07 01:25:02          459\n[Information]         scipy-0.19.0-cp35-cp35m-win_amd64.whl          2017-06-07 01:05:12     12178932\n[Information]         __MACOSX\/._scipy-0.19.0-cp35-cp35m-win_amd64.whl 2017-06-07 01:05:12          452\n[Information]         scs-1.2.6-cp35-cp35m-win_amd64.whl             2017-06-07 01:03:34        78653\n[Information]         __MACOSX\/._scs-1.2.6-cp35-cp35m-win_amd64.whl  2017-06-07 01:03:34          449\n[Information]         [ READING ] 0:00:00\n[Information]         Input pandas.DataFrame #1:\n[Information]         Empty DataFrame\n[Information]         Columns: [1]\n[Information]         Index: []\n[Information]         [ EXECUTING ] 0:00:00\n[Information]         [ WRITING ] 0:00:00\n<\/code><\/pre>\n<p>where <code>import cvxpy<\/code>, <code>import cvxpy-0.4.10-py3-none-any.whl<\/code> or <code>cvxpy-0.4.10-py3-none-any<\/code> do not work so<\/p>\n<p><strong>How can I use the following wheel files downloaded from <a href=\"http:\/\/www.lfd.uci.edu\/%7Egohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\">here<\/a> to use the external Python packages not found in Azure ML?<\/strong><\/p>\n<\/blockquote>\n<p><em>Update about permission problem about importing cvxpy (paste <a href=\"https:\/\/pastebin.com\/3kTKgLfc\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code> [Error]         ImportError: No module named 'canonInterface'\n<\/code><\/pre>\n<p>where the ZIP Bundle is organised a bit differently, the content of each wheel downloaded to a folder and the content having all zipped as a ZIP Bundle.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1496732346280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496674339213,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is trying to install Python packages such as CVXPY, GLPK, CLP, and COINMP in Azure ML but is encountering errors. They have tried installing 32-bit CVXPY files but it did not work. They have also downloaded win_amd64 files but are unsure how to use them. The user is seeking guidance on how to use the downloaded wheel files to install external Python packages not found in Azure ML. Additionally, they are facing a permission problem while importing cvxpy.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44371692",
        "Challenge_link_count":11,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":49.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":16.1130741667,
        "Challenge_title":"Install Python Packages in Azure ML?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":12625.0,
        "Challenge_word_count":346,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts#limitations\" rel=\"nofollow noreferrer\"><code>Limitations<\/code><\/a> and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of <code>Execute Python Script<\/code> tutorial, the only way to add custom Python modules is via the zip file mechanism to package the modules and all dependencies.<\/p>\n\n<p>For example to install <code>CVXPY<\/code>, as below.<\/p>\n\n<ol>\n<li>Download the wheel file of <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\"><code>CVXPY<\/code><\/a> and its dependencies like <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxopt\" rel=\"nofollow noreferrer\"><code>CVXOPT<\/code><\/a>.<\/li>\n<li>Decompress these wheel files, and package these files in the path <code>cvxpy<\/code> and <code>cvxopt<\/code>, etc as a zipped file with your script.<\/li>\n<li>Upload the zip file as a dataset and use it as the script bundle.<\/li>\n<\/ol>\n\n<p>If you were using IPython, you also can try to install the Python Package via the code <code>!pip install cvxpy<\/code>.<\/p>\n\n<p>And there are some similar SO threads which may be helpful for you, as below.<\/p>\n\n<ol>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/44285641\/azure-ml-python-with-script-bundle-cannot-import-module\">Azure ML Python with Script Bundle cannot import module<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/8663046\/how-to-install-a-python-package-from-within-ipython\">How to install a Python package from within IPython?<\/a><\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>\n\n<hr>\n\n<p>Update:<\/p>\n\n<p>For IPython interface of Azure ML, you move to the <code>NOTEBOOKS<\/code> tab to create a notebook via <code>ADD TO PROJECT<\/code> button at the bottom of the page, as the figure below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Or you can directly login to the website <code>https:\/\/notebooks.azure.com<\/code> to use it.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1496760284030,
        "Solution_link_count":9.0,
        "Solution_readability":10.8,
        "Solution_reading_time":28.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":215.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"installation errors and permission problem"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":91.5109113889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>During the preparation for a training (in <code>prepare_data<\/code> in pytorch lightning) I either create or update local data (download, prepare different encodings). I then create a W&amp;B artifact and wait for the upload to be complete. Later in the code (in <code>setup()<\/code> in pytorch lightning) I use the data. Strictly speaking, this is not necessary, because I have the files locally, but I want to track the usage of the data (and the IDs of the data used for training, validation, \u2026). I added the <code>wait()<\/code> statement, because wandb would download the previous version (v=n-1) of the data \/without the enoding just added). In mode <code>ONLINE<\/code> this works nicely. However, in mode <code>DISABLED<\/code> I get this error: <code>ValueError: Cannot call wait on an artifact before it has been logged or in offline mode<\/code>. How am I supposed to handle <code>wait()<\/code>in order to have it work in all modes? (it would be nice if <code>wait()<\/code> would do it).<\/p>\n<p>This is the sample code:<\/p>\n<pre><code class=\"lang-python\"># Upload the data\nartifact = wandb.Artifact(name=..., type=...)\nartifact.description = ...\nartifact.metadata = ...\nartifact.add_file(local_path=...)\nwandb.run.log_artifact(artifact)\nartifact.save()  # I think I don't need this, playing around because of this issue\nartifact.wait()\n<\/code><\/pre>\n<pre><code class=\"lang-python\"># Use (Download) the data\nartifact = wandb.run.use_artifact(artifact_or_name=... + \":latest\")\nartifact_entry = artifact.get_path(...)\nartifact_entry.download(root=...)\n<\/code><\/pre>",
        "Challenge_closed_time":1655462923780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655133484499,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the use of artifact.wait() in mode \"DISABLED\" while preparing for a training in PyTorch Lightning. The user wants to track the usage of data and IDs used for training, validation, etc. and added the wait() statement to ensure that the latest version of the data is downloaded. However, in mode \"DISABLED\", the user is getting a ValueError and is seeking a solution to handle wait() to make it work in all modes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-deal-with-artifact-wait-when-running-in-mode-disabled\/2607",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.1,
        "Challenge_reading_time":20.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":91.5109113889,
        "Challenge_title":"How to deal with artifact.wait() when running in mode \"DISABLED\"",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":210,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>, sorry about the late response. Runs have a <code>disabled<\/code> attribute. Here is a code snippet you can use:<\/p>\n<pre><code class=\"lang-auto\">run = wandb.init(mode=\"disabled\")\nif run.disabled:\n    \/\/ your code\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":3.6,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ValueError with artifact.wait()"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.2737619444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I launched ipython session and trying to load a dataset.<br \/>\nI am running<br \/>\ndf = catalog.load(&quot;test_dataset&quot;)<br \/>\nFacing the below error<br \/>\n<code>NameError: name 'catalog' is not defined<\/code><\/p>\n<p>I also tried %reload_kedro but got the below error<\/p>\n<p><code>UsageError: Line magic function `%reload_kedro` not found.<\/code><\/p>\n<p>Even not able to load context either.\nI am running the kedro environment from a Docker container.\nI am not sure where I am going wrong.<\/p>",
        "Challenge_closed_time":1637671420496,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637670434953,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to load a dataset in an ipython session using Kedro. The error message indicates that the 'catalog' is not defined. The user also tried to reload Kedro using '%reload_kedro' but received an error message. Additionally, the user is unable to load the context and is running Kedro from a Docker container.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70080915",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":7.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.2737619444,
        "Challenge_title":"kedro context and catalog missing from ipython session",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":617.0,
        "Challenge_word_count":74,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1495105930728,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>new in 0.17.5 there is a fallback option, please run the following commands in your Jupyter\/IPython session:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>%load_ext kedro.extras.extensions.ipython\n%reload_kedro &lt;path_to_project_root&gt;\n<\/code><\/pre>\n<p>This should help you get up and running.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":4.1,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"Kedro",
        "Challenge_type":"anomaly",
        "Challenge_summary":"undefined catalog error"
    },
    {
        "Answerer_created_time":1583491811220,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Baku, Azerbaijan",
        "Answerer_reputation_count":23.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":95.6068944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Challenge_closed_time":1633946464143,
        "Challenge_comment_count":0,
        "Challenge_created_time":1633602279323,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error message \"Could not find main among entry points\" while trying to implement an MLFlow project on their own ML model. The user has provided the required files to run the MLFlow project, including the conda.yaml file, MLProject file, and Test.py file. The user has also tried the solutions suggested in a GitHub issue but the error persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69479488",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":44.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":95.6068944445,
        "Challenge_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":418.0,
        "Challenge_word_count":296,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1583491811220,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Baku, Azerbaijan",
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":6.6,
        "Solution_reading_time":11.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":112.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing main entry point"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.724685,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I need some help trying to understand why I can't see any GIT options (left panel and top selection drop down menu) in my Azure machine learning JupyterLab.<\/p>\n<p>I did the following steps:<\/p>\n<pre><code> jupyter labextension install @jupyterlab\/git\n pip install --upgrade jupyterlab-git\n jupyter serverextension enable --py jupyterlab_git\n jupyter lab build\n<\/code><\/pre>\n<p>I've restarted my jupyterLab a couple of times, if I check the command:<\/p>\n<pre><code> jupyter labextension list\n<\/code><\/pre>\n<p>I get that @jupyterlab\/git v0,20,0 is enabled and ok.  <br \/>\nWhat am I doing wrong?<\/p>\n<p>Thank you in advance,  <br \/>\nCarla<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/12015-issue1.png?platform=QnA\" alt=\"12015-issue1.png\" \/><\/p>",
        "Challenge_closed_time":1594762360103,
        "Challenge_comment_count":5,
        "Challenge_created_time":1594716551237,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning and jupyterlab git extension as they are unable to see any GIT options in their JupyterLab despite following the necessary steps to install the extension. The user has restarted JupyterLab multiple times and confirmed that the extension is enabled, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/46614\/azure-machine-learning-and-jupyterlab-git-extensio",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":10.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":12.724685,
        "Challenge_title":"Azure Machine Learning and jupyterlab git extension not working",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a>@ramr-msft<\/a> ,<\/p>\n<p>I did the steps mention in the link you gave me (<a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab-git\">https:\/\/github.com\/jupyterlab\/jupyterlab-git<\/a>) but still I can't open the Git extension from the Git tab on the left panel because it still doesn't exists.<\/p>\n<p>You mentioned we can still manage git repositories using the command line. Do you have any useful documentation on this approach?<\/p>\n<p>Once again, thank you in advance.  <br \/>\nCarla<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":6.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing GIT options in JupyterLab"
    },
    {
        "Answerer_created_time":1536318818623,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"\u0130zmit, Kocaeli, T\u00fcrkiye",
        "Answerer_reputation_count":1033.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":1138.2136686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute the Azure ml sdk from the local system using the Jupyter notebook. When I run the below code i am getting an error.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\n\nModuleNotFoundError: No module named 'ruamel' \n<\/code><\/pre>",
        "Challenge_closed_time":1626743937907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622646368700,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when trying to execute the Azure ml sdk from their local system using Jupyter notebook. The error message specifically states that there is no module named 'ruamel'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67807756",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":4.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1138.2136686111,
        "Challenge_title":"ModuleNotFoundError: No module named 'ruamel' when excuting from azureml.core",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":48,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1599816833352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New Delhi, Delhi, India",
        "Poster_reputation_count":329.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>You have to add pip 20.1.1<\/p>\n<p>Conda ruamel needs higher version of pip<\/p>\n<pre><code>conda install pip=20.1.1\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ModuleNotFoundError: 'ruamel'"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.6887980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What kind of collaboration do we need among the data scientists or developers who need to share these notebooks? What kind of compute does these notebooks require? Is it all single node? <\/p>",
        "Challenge_closed_time":1591962208856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591956129183,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on collaboration and compute requirements for sharing Azure ML notebooks among data scientists and developers, specifically whether single node compute is sufficient.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/35432\/azure-ml-notebooks-sharing-and-compute-selection",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.9,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.6887980556,
        "Challenge_title":"Azure ml notebooks sharing and compute selection.",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a>@azureml056-5112<\/a> Please follow the below for managing compute instances. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#managing-a-compute-instance\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#managing-a-compute-instance<\/a> All data scientists or developers need is access to the AzureML Workspace and they will have access to a shared file share where everyone\u2019s notebooks can be accessed.<\/p>\n<p>All notebook require a Compute Instance(CI). CI is a managed VM that exists in AzureML.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":7.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"collaboration and compute requirements"
    },
    {
        "Answerer_created_time":1426046529436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":180.0,
        "Answerer_view_count":20.0,
        "Challenge_adjusted_solved_time":207.9795563889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use XGBoost on Sagemaker notebook.<\/p>\n\n<p>I am using <code>conda_python3<\/code> kernel, and the following packages are installed:<\/p>\n\n<ul>\n<li>py-xgboost-mutex<\/li>\n<li>libxgboost<\/li>\n<li>py-xgboost<\/li>\n<li>py-xgboost-gpu<\/li>\n<\/ul>\n\n<p>But once I am trying to import xgboost it fails on import:<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-5-5943d1bfe3f1&gt; in &lt;module&gt;()\n----&gt; 1 import xgboost as xgb\n\nModuleNotFoundError: No module named 'xgboost'\n<\/code><\/pre>",
        "Challenge_closed_time":1592571589163,
        "Challenge_comment_count":2,
        "Challenge_created_time":1591825043587,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to import xgboost on Sagemaker notebook using conda_python3 kernel. The user has installed all the required packages, but the import fails with a ModuleNotFoundError.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62313532",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":15.1,
        "Challenge_reading_time":7.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":207.3737711111,
        "Challenge_title":"xgboost on Sagemaker notebook import fails",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1532.0,
        "Challenge_word_count":64,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1391632571720,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv-Yafo, Israel",
        "Poster_reputation_count":1248.0,
        "Poster_view_count":137.0,
        "Solution_body":"<p>In Sagemaker notebooks  use the below steps <\/p>\n\n<h3>a) If in Notebook<\/h3>\n\n<p>i)  <code>!type python3<\/code><\/p>\n\n<p>ii) Say the above is \/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 for you <\/p>\n\n<p>iii) <code>!\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 -m pip install  xgboost<\/code><\/p>\n\n<p>iv)  <code>import xgboost<\/code><\/p>\n\n<hr>\n\n<h3>b) If using Terminal<\/h3>\n\n<p>i) <code>conda activate conda_python3<\/code><br>\nii) <code>pip install xgboost<\/code><\/p>\n\n<p>Disclaimer :  sometimes the installation would fail with gcc version ,in that case  update pip version before running install<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1592573769990,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":7.84,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"import failure for xgboost"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.0833333333,
        "Challenge_answer_count":5,
        "Challenge_body":"Hi, I am following this tutorial on model deployment (https:\/\/codelabs.developers.google.com\/vertex-image-deploy#6), but I ran into a issue when importing the aiplatform library.\n\nWhen running \"from google.cloud import aiplatform\", I get the following error message:\n\nImportError                               Traceback (most recent call last)\n\/tmp\/ipykernel_22080\/3236611779.py in <module>\n      4 #!python #3.7.12\n      5 \n----> 6 from google.cloud import aiplatform\n      7 \n      8 import tensorflow as tf\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/__init__.py in <module>\n     22 \n     23 \n---> 24 from google.cloud.aiplatform import initializer\n     25 \n     26 from google.cloud.aiplatform.datasets import (\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/initializer.py in <module>\n     24 \n     25 from google.api_core import client_options\n---> 26 from google.api_core import gapic_v1\n     27 import google.auth\n     28 from google.auth import credentials as auth_credentials\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/__init__.py in <module>\n     17 from google.api_core.gapic_v1 import config_async\n     18 from google.api_core.gapic_v1 import method\n---> 19 from google.api_core.gapic_v1 import method_async\n     20 from google.api_core.gapic_v1 import routing_header\n     21 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method_async.py in <module>\n     20 import functools\n     21 \n---> 22 from google.api_core import grpc_helpers_async\n     23 from google.api_core.gapic_v1 import client_info\n     24 from google.api_core.gapic_v1.method import _GapicCallable\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers_async.py in <module>\n     23 \n     24 import grpc\n---> 25 from grpc import aio\n     26 \n     27 from google.api_core import exceptions, grpc_helpers\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/aio\/__init__.py in <module>\n     21 \n     22 import grpc\n---> 23 from grpc._cython.cygrpc import (init_grpc_aio, shutdown_grpc_aio, EOF,     24                                  AbortError, BaseError, InternalError,\n     25                                  UsageError)\n\nImportError: cannot import name 'shutdown_grpc_aio' from 'grpc._cython.cygrpc' (\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_cython\/cygrpc.cpython-37m-x86_64-linux-gnu.so)\n\nThe versions of the concerned libraries are shown below.\n\ngoogle-api-core                       2.10.1\ngoogle-api-python-client              2.55.0\ngoogle-cloud-aiplatform               1.17.0\n\ngrpcio                                1.33.1\ngrpcio-gcp                            0.2.2\ngrpcio-status                         1.47.0\n\nI have tried grpcio versions 1.26, 1.27.2, and even the latest 1.50, but all of them had import errors (concerning importing of aio module for 1.26 and 127.2 and AbortError module for 1.50). Are there any additional steps or libraries that I need to take to avoid these import errors?\n\nThank you!",
        "Challenge_closed_time":1668446100000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668388200000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while importing the aiplatform library while following a tutorial on model deployment. The error message shows an ImportError and the concerned libraries' versions are also mentioned. The user has tried different versions of grpcio but still faces import errors. The user is seeking additional steps or libraries to avoid these errors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-with-importing-aiplatform\/m-p\/489087#M771",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":10.5,
        "Challenge_reading_time":34.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":16.0833333333,
        "Challenge_title":"Issues with importing aiplatform",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":0.0,
        "Challenge_word_count":265,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, thank you for your reply. I am running the code on Vertex AI.\n\nI realised I had to restart the kernel to refresh the package after updating grpcio, and I could then import aiplatform without any issues as shown below:\n\nfrom google.cloud import aiplatform\nprint(\"aiplatform version: \", aiplatform.__version__)\n\naiplatform version:  1.17.0\n\nThanks again for your help!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":4.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":61.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"import error with aiplatform"
    },
    {
        "Answerer_created_time":1239374952692,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Sydney, Australia",
        "Answerer_reputation_count":30129.0,
        "Answerer_view_count":2937.0,
        "Challenge_adjusted_solved_time":23.0400269445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the R <code>sentimentr<\/code> package on Azure ML Studio. As this package is not supported, I'm trying to install it and its dependencies as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-r-script#bkmk_AddingANewPackage\" rel=\"nofollow noreferrer\">in the documentation<\/a>.<\/p>\n\n<p>The steps that I have performed are:<\/p>\n\n<ul>\n<li><p>downloaded Windows binaries from the R Open 3.4.4 snapshot at <a href=\"https:\/\/mran.microsoft.com\/timemachine\" rel=\"nofollow noreferrer\">CRAN time machine<\/a><\/p>\n\n<ul>\n<li><code>sentimentr_2.2.3.zip<\/code><\/li>\n<li><code>syuzhet_1.0.4.zip<\/code><\/li>\n<li><code>textclean_0.6.3.zip<\/code><\/li>\n<li><code>lexicon_0.7.4.zip<\/code><\/li>\n<li><code>textshape_1.5.0.zip<\/code> <\/li>\n<\/ul><\/li>\n<li><p>zipped those zip files into a zipped folder <code>packages.zip<\/code><\/p><\/li>\n<li>uploaded <code>packages.zip<\/code> as a dataset to Microsoft Azure ML Studio<\/li>\n<\/ul>\n\n<p>In my ML experiment I connect the <code>packages.zip<\/code> dataset to the \"Script Bundle (Zip)\" input port on \"Execute R Script\" and include this code:<\/p>\n\n<pre><code># install R package contained in src  \ninstall.packages(\"src\/lexicon_0.7.4.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/textclean_0.6.3.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/textshape_1.5.0.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/syuzhet_1.0.4.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/sentimentr_2.2.3.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\n# load libraries\nlibrary(sentimentr, lib.loc = \".\", verbose = TRUE)\n<\/code><\/pre>\n\n<p>The experiment runs successfully, until I include a function from <code>sentimentr<\/code>:<\/p>\n\n<pre><code>mydata &lt;- mydata %&gt;%\n  get_sentences() %&gt;%\n  sentiment()\n<\/code><\/pre>\n\n<p>This gives the error:<\/p>\n\n<blockquote>\n  <p>there is no package called 'textshape'<\/p>\n<\/blockquote>\n\n<p>Which is difficult to understand given that the output log does not indicate an issue with the packages:<\/p>\n\n<pre><code>[Information]         The following files have been unzipped for sourcing in path=[\"src\"]:\n[Information]                           Name  Length                Date\n[Information]         1 sentimentr_2.2.3.zip 3366245 2019-08-07 14:57:00\n[Information]         2    syuzhet_1.0.4.zip 2918474 2019-08-07 15:05:00\n[Information]         3  textclean_0.6.3.zip 1154814 2019-08-07 15:13:00\n[Information]         4    lexicon_0.7.4.zip 4551995 2019-08-07 15:17:00\n[Information]         5  textshape_1.5.0.zip  463095 2019-08-07 15:42:00\n[Information]         Loading objects:\n[Information]           port1\n[Information]         [1] \"Loading variable port1...\"\n[Information]         package 'lexicon' successfully unpacked and MD5 sums checked   \n[Information]         package 'textclean' successfully unpacked and MD5 sums checked\n[Information]         package 'textshape' successfully unpacked and MD5 sums checked\n[Information]         package 'syuzhet' successfully unpacked and MD5 sums checked\n[Information]         package 'sentimentr' successfully unpacked and MD5 sums checked\n<\/code><\/pre>\n\n<p>Has anyone seen this, or similar issues? Is it possible that \"successfully unpacked\" is not the same as successfully installed and usable?<\/p>",
        "Challenge_closed_time":1565302474167,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565219530070,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to install the R sentimentr package on Azure ML Studio, which is not supported. They have downloaded the required packages and zipped them into a folder, which they have uploaded to Azure ML Studio. The experiment runs successfully until they include a function from sentimentr, which gives an error stating that there is no package called 'textshape', even though the output log indicates that all packages have been successfully unpacked and checked. The user is seeking help to understand the issue and whether \"successfully unpacked\" is the same as successfully installed and usable.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57403281",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":42.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":23.0400269445,
        "Challenge_title":"Installing textshape package for Microsoft R Open 3.4.4 on Azure ML Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":157.0,
        "Challenge_word_count":337,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1239374952692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sydney, Australia",
        "Poster_reputation_count":30129.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>I can now answer my own question thanks to <a href=\"https:\/\/twitter.com\/bryan_hepworth\/status\/1159432174225055749\" rel=\"nofollow noreferrer\">a hint on Twitter<\/a> from @bryan_hepworth.<\/p>\n\n<p>The R packages were installed correctly, but not in the standard library location. So when a function from <code>sentimentr<\/code> runs, R tries to load the dependency package <code>textshape<\/code>:<\/p>\n\n<pre><code>library(textshape)\n<\/code><\/pre>\n\n<p>Which of course does not exist <em>in the standard location<\/em> as Azure ML does not support it.<\/p>\n\n<p>The solution is to load <code>textshape<\/code> explicitly from its installed location:<\/p>\n\n<pre><code>library(textshape, lib.loc = \".\")\n<\/code><\/pre>\n\n<p>So the solution is: explicitly load packages that you installed at the start of your R code, rather than letting R try to load them as dependencies, which will fail.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":11.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing package error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.2260158334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.    <\/p>\n<p>Features in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python<\/a> are neat, but that of having API docs\/swagger for the webservice seems missing.    <\/p>\n<p>Having some documentation is useful especially if the model takes in input several features of different type.    <\/p>\n<p>To overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.     <\/p>\n<p>Can I do something equivalent to this with AKS in AzureML instead? If so, how?<\/p>",
        "Challenge_closed_time":1600930445547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600897231890,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to deploy a machine learning service using AzureML on AKS and add OpenAPI specification for it. The user finds that the API docs\/swagger feature is missing in AzureML on AKS and currently uses Docker containers with FastAPI library to build the API and OpenAPI\/Swagger specs. The user is seeking guidance on how to add OpenAPI specification to a webservice deployed with AzureML in AKS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":12.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":9.2260158334,
        "Challenge_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=9ced4628-b03a-4169-99b4-e42b0955c045\">@Davide Fiocco  <\/a> The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.akswebservice?view=azure-ml-py\">here<\/a>. You can print your <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service\">swagger_uri<\/a> of the web service and check if it confirms with the specifications you are creating currently.     <\/p>\n<p>If the above response helps, please accept the response as answer. Thanks!!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":8.84,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"add OpenAPI to AzureML AKS"
    },
    {
        "Answerer_created_time":1370286859500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16981.0,
        "Answerer_view_count":1733.0,
        "Challenge_adjusted_solved_time":21792.3181611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need the sacred package for a new code base I downloaded. It requires sacred. \n<a href=\"https:\/\/pypi.python.org\/pypi\/sacred\" rel=\"noreferrer\">https:\/\/pypi.python.org\/pypi\/sacred<\/a><\/p>\n\n<p>conda install sacred fails with \nPackageNotFoundError: Package missing in current osx-64 channels: \n  - sacred<\/p>\n\n<p>The instruction on the package site only explains how to install with pip. What do you do in this case?<\/p>",
        "Challenge_closed_time":1501767830263,
        "Challenge_comment_count":2,
        "Challenge_created_time":1501710168027,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to install the 'sacred' package using conda and is receiving a 'PackageNotFoundError'. The user is seeking guidance on how to install the package in this scenario as the package's website only provides instructions for installation using pip.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45471477",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.2,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.0172877778,
        "Challenge_title":"python package can be installed by pip but not conda",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":7845.0,
        "Challenge_word_count":60,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1321905325720,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California, United States",
        "Poster_reputation_count":3278.0,
        "Poster_view_count":399.0,
        "Solution_body":"<p>That package is not available as a conda package at all. You can search for packages on anaconda.org: <a href=\"https:\/\/anaconda.org\/search?q=sacred\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=sacred<\/a> You can see the type of package in the 4th column. Other Python packages may be available as conda packages, for instance, NumPy: <a href=\"https:\/\/anaconda.org\/search?q=numpy\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=numpy<\/a><\/p>\n\n<p>As you can see, the conda package numpy is available from a number of different channels (the channel is the name before the slash). If you wanted to install a package from a different channel, you can add the option to the install\/create command with the <code>-c<\/code>\/<code>--channel<\/code> option, or you can add the channel to your configuration <code>conda config --add channels channel-name<\/code>.<\/p>\n\n<p>If no conda package exists for a Python package, you can either install via pip (if available) or <a href=\"https:\/\/docs.conda.io\/projects\/conda-build\/en\/latest\/user-guide\/tutorials\/building-conda-packages.html\" rel=\"nofollow noreferrer\">build your own conda package<\/a>. This isn't usually too difficult to do for pure Python packages, especially if one can use <code>skeleton<\/code> to build a recipe from a package on PyPI.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1580162513407,
        "Solution_link_count":5.0,
        "Solution_readability":11.0,
        "Solution_reading_time":16.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":163.0,
        "Tool":"Sacred",
        "Challenge_type":"anomaly",
        "Challenge_summary":"PackageNotFoundError during installation"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":41.2755555556,
        "Challenge_answer_count":0,
        "Challenge_body":"I was getting an error when azuremllogonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. So, I installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below.\r\n\r\ngrep executes but now the error says \"Dataset with name 'mnist_opendataset' is not found\".\r\n\r\nAny help troubleshooting this error will be appreciated, I am trying to demo this to a customer. next week.\r\n\r\n**TEXT of the OUTPUT when error is encountered:**\r\n\r\n\r\nInstalling amlarc-compute K8s extension was successful.\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nLibrary configuration succeeded\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\n\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nfound compute target: ARC-ml\r\n\"\r\n Training model:\r\n                               \r\n            .....                                             .....\r\n         .........                                           .........\r\n        .........                 (((((((((##                 .........\r\n       .....                      (((((((####                      .....\r\n      ......                      #((########                      ......\r\n     ....... .............        ###########        ............. .......\r\n     ......................       ###########       ......................\r\n    .................*.....       ###########       ....,*.................\r\n    .........*******......       (((((((((((         ......*******.........\r\n         ............          (((((((((((     (.         ............\r\n                            .(((((((((((     (((((\/\r\n                          ((((((((((((     #(((((((##\r\n                        \/\/\/\/(((((((*     ##############\r\n                      \/\/\/\/\/\/(((((.         ,#############.\r\n                   ,**\/\/\/\/\/\/\/((               #############\/\r\n                    *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%##########\r\n                    \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######(\r\n                     \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%####\r\n                     .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#.\r\n\r\n\"\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nWARNING: Command group 'ml job' is experimental and under development. Reference and support levels: https:\/\/aka.ms\/CLI_refstatus\r\nUploading src:   0%|                                                                                                                                | 0.00\/3.08k [00:00<?, ?B\/s]\r\n\r\n**ERROR: Code: UserError**\r\n**Message: Dataset with name 'mnist_opendataset' is not found.**\r\n**You cannot call a method on a null-valued expression.**\r\n**At C:\\Temp\\AzureMLLogonScript.ps1:267 char:4**\r\n**+    $RunId = ($Job | grep '\\\"name\\\":').Split('\\\"')[3]**\r\n**+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**\r\n    **+ CategoryInfo          : InvalidOperation: (:) [], RuntimeException**\r\n    **+ FullyQualifiedErrorId : InvokeMethodOnNull**\r\n\r\n**RunId:**\r\n**Training model, hold tight...**\r\n**ERROR: argument --name\/-n: expected one argument**_****\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n",
        "Challenge_closed_time":1631711922000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1631563330000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered broken links in the introduction section of the 11_exploring_hyperparameters_on_azureml notebook related to object detection. The links to two notebooks, 02_mask_rcnn.ipynb and 03_training_accuracy_vs_speed.ipynb, are not working. The user is working from the master branch of the repo and expects the notebooks to be present or the links to be removed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/758",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":10.4,
        "Challenge_reading_time":56.26,
        "Challenge_repo_contributor_count":69.0,
        "Challenge_repo_fork_count":439.0,
        "Challenge_repo_issue_count":1880.0,
        "Challenge_repo_star_count":588.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":41.2755555556,
        "Challenge_title":"error when installing AZURE ML training model piece",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":477,
        "Discussion_body":"Hi @arturoqu77 - thanks for reaching out. We tried to repro this issue but couldn't.\r\n\r\nThis [line of code](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/AzureMLLogonScript.ps1#L266) leverages grep to parse the file name. `grep` should have been installed as part of the [bootstrap](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73). If  `grep` wasn't installed, this implies something must have interrupted the install before it got there.\r\n\r\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working. \r\n\r\nAre you seeing Postman installed - this happens [after `grep`](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73)? If not, this is probably what happened.\r\n\r\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above. Hello,\n\nThank you for your reply. I may have logged on before the bootstrap completed, I re-started the deployment to a new RG and seems to be working now.\n\nThanks for the help.\n\nRegards\n\n***@***.***\nArturo Quiroga\nSr. Cloud Solutions Architect (CSA)\nAzure Applications & Infrastructure\n***@***.******@***.***>\n[MSFT_logo_Gray DE sized SIG1.png]\n\n\nFrom: Raki ***@***.***>\nDate: Tuesday, September 14, 2021 at 6:22 PM\nTo: microsoft\/azure_arc ***@***.***>\nCc: Arturo Quiroga ***@***.***>, Mention ***@***.***>\nSubject: Re: [microsoft\/azure_arc] error when installing AZURE ML training model piece (#758)\n\nHi @arturoqu77<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Farturoqu77&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666347896%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=r1kAuKxYlYhONjoSTk83SERggUvNcbP1Hr4vmNh29io%3D&reserved=0> - thanks for reaching out. We tried to repro this issue but couldn't.\n\nThis line of code<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FAzureMLLogonScript.ps1%23L266&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=oAjL%2BfBBF4QXfnwN9gcM9UqEB4OA0ZZrzMuKilatz5A%3D&reserved=0> leverages grep to parse the file name. grep should have been installed as part of the bootstrap<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=V8dzJxj3W5a6IL8T%2BvB0mijBm5Ng4G46bb%2Fcdo2uvz4%3D&reserved=0>. If grep wasn't installed, this implies something must have interrupted the install before it got there.\n\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working.\n\nAre you seeing Postman installed - this happens after grep<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=XvpeFo2T7Kjr4qrIZYKO7eM0khlOddES9O3DGaw1yZ4%3D&reserved=0>? If not, this is probably what happened.\n\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fissues%2F758%23issuecomment-919554382&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ZBGNkrDGFcqvrdWHXy5iEGluQiq2Ph%2BZnfosqC3qTTU%3D&reserved=0>, or unsubscribe<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAHV4QUFA72NR7CEJ3UPS5NLUB7DLDANCNFSM5D6SSBHA&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ctEevpiqzC%2FQnTc6ho2hfr2PVGA%2FqwGJzj1pPUCEylY%3D&reserved=0>.\nTriage notifications on the go with GitHub Mobile for iOS<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=cMCZqYPB6q8c9n%2BgPTk9f3MCQr%2BlV4GsOW9iPFSZtgE%3D&reserved=0> or Android<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666387876%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=6B5T09q%2Bx2Q2rWftui6b32lD1VLrCRMPiLSrTUS7xnI%3D&reserved=0>.\n Great!",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"broken links in notebook"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":12.2421333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie in this, and I am facing some problems with the Azure ML workspace. I ran a python code from the terminal, and then I opened another terminal to check the process. I got the following message in the terminal that checked the process:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<p>I appreciate any tips.<\/p>",
        "Challenge_closed_time":1651825622423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651781550743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Azure ML workspace while running a python code from the terminal. They opened another terminal to check the process and received a message stating \"Reconnecting terminal.\" The user is unsure of the meaning of the message and is concerned about losing processing time. They are seeking tips to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72133111",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":8.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.2421333334,
        "Challenge_title":"Azure ML: What means reconnecting terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":96,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575137776887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":21.0,
        "Solution_body":"<blockquote>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<\/blockquote>\n<ul>\n<li><code>Reconnecting terminal<\/code> message can appear for multiple reasons like intermittent connectivity issues, unused active terminal sessions, processing of different size\/format of data.<\/li>\n<li>Make sure you close any unused terminal sessions to preserve your compute instance's resources. Idle terminals may impact the performance of compute instances.<\/li>\n<\/ul>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#manage-terminal-sessions\" rel=\"nofollow noreferrer\">Access a compute instance terminal in your workspace<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\" rel=\"nofollow noreferrer\">Optimize data processing with Azure Machine Learning<\/a> and <a href=\"https:\/\/www.youtube.com\/watch?v=kiScfw9i4FM\" rel=\"nofollow noreferrer\">Azure ML: Speed up processing time<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":14.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":114.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"reconnecting terminal message"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":115.1236055556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Dear community,<\/p>\n<p>I want to use WandB locally in my VSCode project, but my Ipython kernel keeps dying. After restarting the kernel it always prints out the errore message: \u201cFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\u201d and \u201cwandb: Currently logged in as: XXX. Use <code>wandb login --relogin<\/code> to force relogin\u201d<\/p>\n<p>I already tried to import os, as well as setting the environment variabele to my local notebook, but this didnt change a thing. I am using python 3.9.12<\/p>\n<p>I hope you can help me in this matter<\/p>",
        "Challenge_closed_time":1675104696691,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674690251711,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues while trying to use WandB locally in their VSCode project. Their Ipython kernel keeps dying and they are receiving error messages related to failed detection of notebook name and WandB login. They have tried importing os and setting environment variables but the issue persists. They are using Python 3.9.12 and seeking help to resolve the matter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-wandb-in-visual-studio-code\/3752",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.1,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":115.1236055556,
        "Challenge_title":"Using WandB in Visual Studio Code",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":127.0,
        "Challenge_word_count":107,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/martin-woschitz\">@martin-woschitz<\/a> thank you for reporting this issue. This first message is just a warning so it shouldn\u2019t cause any issues running your code, also the second message  is an informatio output about the user account that you\u2019ve logged in. When does the Ipython kernel stops working, is it when you\u2019re running a python script? would it be possible to make a new virtual environment and install <code>wandb<\/code> only there to test if that\u2019s what\u2019s causing the issue for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":6.6,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Ipython kernel keeps dying"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":35.0730002778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Challenge_closed_time":1579273412888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579147150087,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user encountered challenges while following the scikit_bring_your_own example for product recommendations, including errors when attempting to install the scikit-surprise package in the dockerfile.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":20.5,
        "Challenge_reading_time":212.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":223,
        "Challenge_solved_time":35.0730002778,
        "Challenge_title":"AWS Sagemaker scikit_bring_your_own example",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":341.0,
        "Challenge_word_count":1082,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1241005356852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Malaysia",
        "Poster_reputation_count":15794.0,
        "Poster_view_count":1032.0,
        "Solution_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":2.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"installation error with scikit-surprise"
    },
    {
        "Answerer_created_time":1550902509267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2669.0,
        "Answerer_view_count":3292.0,
        "Challenge_adjusted_solved_time":9.0819222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Issue<\/strong> : Unable to get best model from AutoML run.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>best_run, fitted_model = automl_run.get_output()\nprint(best_run.properties[&quot;run_algorithm&quot;])\n<\/code><\/pre>\n<p><strong>Error Message :<\/strong><\/p>\n<pre><code>ErrorResponse \n[stderr]{\n[stderr]    &quot;error&quot;: {\n[stderr]        &quot;code&quot;: &quot;UserError&quot;,\n[stderr]        &quot;message&quot;: &quot;The model you attempted to retrieve requires 'xgboost' to be installed at '==1.3.3'. You have 'xgboost==1.3.3', please reinstall 'xgboost==1.3.3' (e.g. `pip install xgboost==1.3.3`) and rerun the previous command.&quot;,\n[stderr]        &quot;target&quot;: &quot;get_output&quot;,\n[stderr]        &quot;inner_error&quot;: {\n[stderr]            &quot;code&quot;: &quot;NotSupported&quot;,\n[stderr]            &quot;inner_error&quot;: {\n[stderr]                &quot;code&quot;: &quot;IncompatibleOrMissingDependency&quot;\n[stderr]            }\n[stderr]        },\n[stderr]        &quot;reference_code&quot;: &quot;910310e6-2433-40cd-b597-9ec2950bc1d8&quot;\n[stderr]    }\n<\/code><\/pre>\n<p><strong>Conda Dependency<\/strong><\/p>\n<pre><code># Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n\n# Details about the Conda environment file format:\n# https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html#create-env-file-manually\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.12\n\n- pip:\n  - azureml-train-automl-runtime==1.38.0\n  - azureml-train-automl-client==1.38.0\n  - inference-schema\n  - azureml-interpret==1.38.0\n  - azureml-defaults==1.38.0\n- numpy&gt;=1.16.0,&lt;1.19.0\n- pandas==0.25.1\n- scikit-learn==0.22.1\n- py-xgboost&lt;=1.3.3\n- fbprophet==0.5\n- holidays==0.9.11\n- psutil&gt;=5.2.2,&lt;6.0.0\n- matplotlib=3.3.2\n- seaborn=0.9.0\n- joblib=0.13.2\n- joblib\nchannels:\n- anaconda\n- conda-forge\n<\/code><\/pre>\n<p><strong>Question:<\/strong><\/p>\n<ul>\n<li>What should be in my conda dependency that can fix this error<\/li>\n<li>I've tried making <code>py-xgboost==1.3.3<\/code> , but it didn't work.<\/li>\n<li>Any luck - how to fix this ?<\/li>\n<\/ul>",
        "Challenge_closed_time":1648186385707,
        "Challenge_comment_count":2,
        "Challenge_created_time":1648153690787,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to get the best model from an Azure AutoML run due to a dependency failure. The error message indicates that the model requires 'xgboost' to be installed at '==1.3.3', but the user has 'xgboost==1.3.3' installed. The user is seeking advice on how to fix this issue and what should be included in the conda dependency to resolve it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71609028",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.9,
        "Challenge_reading_time":29.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":9.0819222222,
        "Challenge_title":"Azure AutoML dependency failure",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":142.0,
        "Challenge_word_count":173,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1642621384680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":11.0,
        "Solution_body":"<blockquote>\n<p>Error - &quot;The model you attempted to retrieve requires 'xgboost' to be\ninstalled at '==1.3.3'. You have 'xgboost==1.3.3', please reinstall\n'xgboost==1.3.3' (e.g. <code>pip install xgboost==1.3.3<\/code>) and rerun the\nprevious command.&quot;<\/p>\n<\/blockquote>\n<p>As given in above error message, it should be <code>pip install xgboost==1.3.3<\/code> not <code>py-xgboost&lt;=1.3.3<\/code><\/p>\n<p>If it does not work, try downgraded version of <code>xgboost<\/code><\/p>\n<pre><code>pip install xgboost==0.90\n<\/code><\/pre>\n<p>Refer this github <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1421\" rel=\"nofollow noreferrer\">link<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":8.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"dependency failure for AutoML model"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":947.3408333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### What steps did you take:\r\n[A clear and concise description of what the bug is.]\r\n\r\nI am use the re usable Sagemaker Components for building kubeflow pipelines.\r\n\r\nsagemaker_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/train\/component.yaml')\r\nsagemaker_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/model\/component.yaml')\r\nsagemaker_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/deploy\/component.yaml')\r\n\r\nWhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = sagemaker_deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='Endpoint-price-prediction-model',\r\n        endpoint_config_name='EndpointConfig-price-prediction-model',\r\n        update_endpoint=True,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### What happened:\r\nI am getting this error \r\nTypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\nI think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\nTraceback (most recent call last):\r\n--\r\n414 | File \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | File \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = sagemaker_deploy_op(\r\n424 | TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### What did you expect to happen:\r\nto update the endpoint without any issue\r\n### Environment:\r\n<!-- Please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\nsagemaker 2.1.0\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\n<!-- If you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nKFP version: <!-- If you are not sure, build commit shows on bottom of KFP UI left sidenav. -->\r\n\r\nKFP SDK version: <!-- Please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### Anything else you would like to add:\r\n[Miscellaneous information that will assist in solving the issue.]\r\n\r\nPlease help me out \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1611093472000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1607683045000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to create a custom model and perform batch transform in Amazon SageMaker without HPO and training jobs. They have removed HPO and training jobs but are facing issues while compiling kfp. They are getting the desired output in Kubeflow but want to see the custom model output in SageMaker without HPO and batch job. The user also requests for any open source loan data model using KF.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":14.3,
        "Challenge_reading_time":43.94,
        "Challenge_repo_contributor_count":346.0,
        "Challenge_repo_fork_count":1432.0,
        "Challenge_repo_issue_count":9548.0,
        "Challenge_repo_star_count":3200.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":947.3408333333,
        "Challenge_title":"TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":304,
        "Discussion_body":"\/assign @mameshini \r\n\/assign @PatrickXYS \r\n\r\nDo you mind taking a look? Thanks @numerology Thanks!\r\n\r\n@akartsky @RedbackThomson Can you take a look?  Hi @jchaudari, \r\nThanks for reporting the issue, we are taking a look at it. \r\n\r\nThanks,\r\nMeghna Hi @jchaudari, \r\nAre you certain you are using the latest version of the components ? The attached yaml files show that you are using version 0.3.0 of the image which is very old. This feature was added more recently in version 0.9.0 - \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/Changelog.md. \r\n\r\nCould you please try with the newer version and let us know if that fixes your issue ?\r\nThanks,\r\nMeghna Baijal If there aren't any further issues, we'll close this by the end of the week. Otherwise, let us know. \/close @akartsky: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888#issuecomment-763167821):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"compiling kfp issue"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5700419445,
        "Challenge_answer_count":1,
        "Challenge_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Challenge_closed_time":1652688680111,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652686627960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring Sagemaker Built-in algorithms and is having difficulty finding research background and implementation details for specific algorithms. They are seeking help in finding this information.",
        "Challenge_last_edit_time":1668094438488,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":5.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.5700419445,
        "Challenge_title":"Sagemaker Built-in Algorithms",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":66,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\n - **BlazingText**: *[BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs](https:\/\/dl.acm.org\/doi\/10.1145\/3146347.3146354)*, Gupta et Khare\n - **[DeepAR](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar_how-it-works.html)** *[DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks](https:\/\/arxiv.org\/abs\/1704.04110)*, Salinas et al.\n - **[Factorization Machines](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines-howitworks.html)**\n - **[IP Insights](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ip-insights-howitworks.html)**\n - **[KMeans](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algo-kmeans-tech-notes.html)**\n - **[KNN](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/kNN_how-it-works.html)**\n - **[LDA](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lda-how-it-works.html)**\n - **[Linear Learner](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html)**\n - **[NTM](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html)**\n - **[Object2Vec](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object2vec-howitworks.html)**\n - **[Object Detection](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algo-object-detection-tech-notes.html)** (it's an SSD model)\n - **[PCA](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-pca-works.html)**\n - **[Random Cut Forest](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rcf_how-it-works.html)**: *[Robust Random Cut Forest Based Anomaly Detection On Streams](https:\/\/proceedings.mlr.press\/v48\/guha16.pdf)*, Guha et al\n - **[Semantic Segmentation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html)**\n - **[Seq2seq](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/seq-2-seq-howitworks.html)**\n - **[XGBoost](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-HowItWorks.html)**",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1652688680112,
        "Solution_link_count":18.0,
        "Solution_readability":37.8,
        "Solution_reading_time":28.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"find algorithm research details"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.7994444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nUsing Amazon SageMaker on an AWS GPU-instance \"ml.p2.xlarge\", I was not able to run the example `benchmark_m4.py` script (copy\/pasted in SageMaker) on GPU. \r\n\r\n## To Reproduce\r\nAfter starting the instance: \r\n```\r\n!pip install gluonts\r\n```\r\n\r\nNext cell: paste the slightly modified script `benchmark_m4.py` with a little modification:\r\n \r\n```python\r\nestimators = [\r\n    partial(\r\n        DeepAREstimator,\r\n        trainer=Trainer(\r\n            epochs=epochs, \r\n            num_batches_per_epoch=num_batches_per_epoch,\r\n            ctx=\"gpu\"\r\n        ),\r\n    ),\r\n]\r\n```\r\n(without specifying the context this works fine, but is only running on CPU)\r\n\r\n## Error Message\r\n\r\n```\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_quarterly.\r\nINFO:root:Start model training\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_yearly.\r\nINFO:root:Start model training\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[24000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"3M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=8, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='3M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='24000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=8>, train=<gluonts.dataset.common.FileDataset object at 0x7f9377c9e748>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[23000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"12M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=6, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='12M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='23000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=6>, train=<gluonts.dataset.common.FileDataset object at 0x7f937812ce48>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-15-b3fbc3bdf424> in <module>()\r\n     88             \"MASE\",\r\n     89             \"sMAPE\",\r\n---> 90             \"MSIS\",\r\n     91         ]\r\n     92     ]\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/frame.py in __getitem__(self, key)\r\n   2999             if is_iterator(key):\r\n   3000                 key = list(key)\r\n-> 3001             indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\r\n   3002 \r\n   3003         # take() does not accept boolean indexers\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing)\r\n   1283                 # When setting, missing keys are not allowed, even with .loc:\r\n   1284                 kwargs = {\"raise_missing\": True if is_setter else raise_missing}\r\n-> 1285                 return self._get_listlike_indexer(obj, axis, **kwargs)[1]\r\n   1286         else:\r\n   1287             try:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)\r\n   1090 \r\n   1091         self._validate_read_indexer(\r\n-> 1092             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\r\n   1093         )\r\n   1094         return keyarr, indexer\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)\r\n   1175                 raise KeyError(\r\n   1176                     \"None of [{key}] are in the [{axis}]\".format(\r\n-> 1177                         key=key, axis=self.obj._get_axis_name(axis)\r\n   1178                     )\r\n   1179                 )\r\n\r\nKeyError: \"None of [Index(['dataset', 'estimator', 'RMSE', 'mean_wQuantileLoss', 'MASE', 'sMAPE',\\n       'MSIS'],\\n      dtype='object')] are in the [columns]\"\r\n```\r\n\r\n## Other\r\nIn addition, before installing gluonts (from https:\/\/beta.mxnet.io\/guide\/crash-course\/6-use_gpus.html): \r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n[[1. 1. 1. 1.]\r\n [1. 1. 1. 1.]\r\n [1. 1. 1. 1.]]\r\n<NDArray 3x4 @gpu(0)>\r\n```\r\n\r\nAfter installing gluonts: \r\n\r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nMXNetError                                Traceback (most recent call last)\r\n<ipython-input-16-749bd657d613> in <module>()\r\n      5 \r\n      6 \r\n----> 7 x = nd.ones((3,4), ctx=gpu())\r\n      8 x\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/ndarray.py in ones(shape, ctx, dtype, **kwargs)\r\n   2419     dtype = mx_real_t if dtype is None else dtype\r\n   2420     # pylint: disable= no-member, protected-access\r\n-> 2421     return _internal._ones(shape=shape, ctx=ctx, dtype=dtype, **kwargs)\r\n   2422     # pylint: enable= no-member, protected-access\r\n   2423 \r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/register.py in _ones(shape, ctx, dtype, out, name, **kwargs)\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/_ctypes\/ndarray.py in _imperative_invoke(handle, ndargs, keys, vals, out)\r\n     90         c_str_array(keys),\r\n     91         c_str_array([str(s) for s in vals]),\r\n---> 92         ctypes.byref(out_stypes)))\r\n     93 \r\n     94     if original_output is not None:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/base.py in check_call(ret)\r\n    250     \"\"\"\r\n    251     if ret != 0:\r\n--> 252         raise MXNetError(py_str(_LIB.MXGetLastError()))\r\n    253 \r\n    254 \r\n\r\nMXNetError: [22:29:51] src\/imperative\/imperative.cc:79: Operator _ones is not implemented for GPU.\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x9fb) [0x7f9397bb2abb]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f93d5b04e2e]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x12865) [0x7f93d5b05865]\r\n```\r\n\r\n## Environment\r\n\r\n- Amazon SageMaker, running on AWS instance \"ml.p2.xlarge\". \r\n- GluonTS version: 0.3.3 installed using pip.\r\n- Kernel: conda_mxnet_p36 \r\n\r\n",
        "Challenge_closed_time":1573208758000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1573166280000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered a challenge where the conda environment for python3.6 in notebooks could not find `pandas.csvdataset`, resulting in an error when running `context.catalog.list()`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/426",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":24.8,
        "Challenge_reading_time":190.21,
        "Challenge_repo_contributor_count":98.0,
        "Challenge_repo_fork_count":696.0,
        "Challenge_repo_issue_count":2506.0,
        "Challenge_repo_star_count":3591.0,
        "Challenge_repo_watch_count":70.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":81,
        "Challenge_solved_time":11.7994444444,
        "Challenge_title":"Problems using GPU with Amazon SageMaker on AWS-instance \"ml.p2.xlarge\".",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":785,
        "Discussion_body":"After installing GluonTS, could you try running in a cell \r\n\r\n```\r\n!pip show mxnet\r\n```\r\n\r\nand report the result? `!pip show mxnet` results in:\r\n\r\n```python\r\nName: mxnet\r\nVersion: 1.4.1\r\nSummary: MXNet is an ultra-scalable deep learning framework. This version uses openblas.\r\nHome-page: https:\/\/github.com\/apache\/incubator-mxnet\r\nAuthor: UNKNOWN\r\nAuthor-email: UNKNOWN\r\nLicense: Apache 2.0\r\nLocation: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\r\nRequires: numpy, graphviz, requests\r\nRequired-by: gluonts\r\nYou are using pip version 10.0.1, however version 19.3.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n```\r\n\r\n`!pip install mxnet --upgrade` led to conflicts with gluonts and numpy and the same error message with `GPU is not enabled`. \r\n @tm1611 hopefully this is solved with the upcoming `0.4` release (to be relased very soon), since #428 was merged. I think the problem is that with `gluonts<0.4` the vanilla mxnet package gets installed and replaces the one with built-in cuda for GPU processing. @tm1611 can you check if the problem is gone now when you install GluonTS from scratch on your instance? Yes, the problem with mxnet and dependencies was removed. Thanks a lot for the quick fix. \r\n\r\nUnrelated to this issue, but related to `m4_benchmark.py` and the sake of completeness: Using gluonts-version 0.4., one has to change `num_eval_samples` to `num_sampels` (see #421 ). ",
        "Discussion_score_count":2.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing pandas.csvdataset"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":10.5594166667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a pytorch model in sagemaker <\/p>\n\n<pre><code>from sagemaker.pytorch import PyTorch\n\nestimator = PyTorch(entry_point='train.py',\n                    role=role,\n                    framework_version='1.0.0',\n                    train_instance_count=1,\n                    train_instance_type='ml.m4.xlarge',\n                    source_dir='source', #the directory where the supporting files are\n\n                    #what is passed in\n                    hyperparameters={\n                        'max_epochs' : 6,\n                        'layer_dim'  : \"2500,500,100,1\",\n                        'batch_size' : 64,\n                        'seed'       : 4524,\n                        'cuda'       : False\n                        }\n                   )\n<\/code><\/pre>\n\n<p>where the entry script train.py includes several imports<\/p>\n\n<pre><code>import argparse\nimport math\nimport os\nfrom shutil import copy\nimport time\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler\n<\/code><\/pre>\n\n<p>But the sklearn call fails:<\/p>\n\n<pre><code>  File \"\/opt\/ml\/code\/train.py\", line 9, in &lt;module&gt;\n    from sklearn.preprocessing import StandardScaler\nModuleNotFoundError: No module named 'sklearn'\n<\/code><\/pre>\n\n<p>Questions:<\/p>\n\n<ol>\n<li>How do I use sklearn functions in this case?<\/li>\n<li>Can you add additional pip installs without going the custom docker route?<\/li>\n<\/ol>",
        "Challenge_closed_time":1559476232443,
        "Challenge_comment_count":0,
        "Challenge_created_time":1559438218543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while running a PyTorch model in Sagemaker. The entry script includes an import for sklearn.preprocessing, but the call fails with a \"ModuleNotFoundError\". The user is seeking guidance on how to use sklearn functions in this case and whether additional pip installs can be added without going the custom docker route.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56411563",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":14.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":10.5594166667,
        "Challenge_title":"Sagemaker Pytorch : Cant import sklearn from training script",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":571.0,
        "Challenge_word_count":127,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1294628108596,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1748.0,
        "Poster_view_count":393.0,
        "Solution_body":"<p>Yes, you can install dependencies without going the custom dockerfile route (also BYO Container)<\/p>\n\n<p>Use this in your code (where <code>mypackage<\/code> represents a pip package of your choice)<\/p>\n\n<pre><code>import subprocess as sb \nimport sys \n\nsb.call([sys.executable, \"-m\", \"pip\", \"install\", mypackage]) \n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.6,
        "Solution_reading_time":4.19,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"import error with sklearn"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":20.4100311111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have setup a mlflow server locally at http:\/\/localhost:5000<\/p>\n<p>I followed the instructions at <a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/docker<\/a> and tried to run the example docker with<\/p>\n<pre><code>\/mlflow\/examples\/docker$ mlflow run . -P alpha=0.5\n<\/code><\/pre>\n<p>but I encountered the following error.<\/p>\n<pre><code>2021\/05\/09 17:11:20 INFO mlflow.projects.docker: === Building docker image docker-example:7530274 ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.utils: === Created directory \/tmp\/tmp9wpxyzd_ for downloading remote URIs passed to arguments of type 'path' ===\n2021\/05\/09 17:11:20 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v \/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts:\/home\/mlf\/mlf\/0\/ae69145133bf49efac22b1d390c354f1\/artifacts -e MLFLOW_RUN_ID=ae69145133bf49efac22b1d390c354f1 -e MLFLOW_TRACKING_URI=http:\/\/localhost:5000 -e MLFLOW_EXPERIMENT_ID=0 docker-example:7530274 python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'ae69145133bf49efac22b1d390c354f1' === \n\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/__init__.py:55: DeprecationWarning: MLflow support for Python 2 is deprecated and will be dropped in a future release. At that point, existing Python 2 workflows that use MLflow will continue to work without modification, but Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3 - see https:\/\/docs.python.org\/3\/howto\/pyporting.html for a migration guide.\n  &quot;for a migration guide.&quot;, DeprecationWarning)\nTraceback (most recent call last):\n  File &quot;train.py&quot;, line 56, in &lt;module&gt;\n    with mlflow.start_run():\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 122, in start_run\n    active_run_obj = MlflowClient().get_run(existing_run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/client.py&quot;, line 96, in get_run\n    return self._tracking_client.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 49, in get_run\n    return self.store.get_run(run_id)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 92, in get_run\n    response_proto = self._call_endpoint(GetRun, req_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py&quot;, line 32, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 133, in call_endpoint\n    host_creds=host_creds, endpoint=endpoint, method=method, params=json_body)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 70, in http_request\n    url=url, headers=headers, verify=verify, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/mlflow\/utils\/rest_utils.py&quot;, line 51, in request_with_ratelimit_retries\n    response = requests.request(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/api.py&quot;, line 58, in request\n    return session.request(method=method, url=url, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 508, in request\n    resp = self.send(prep, **send_kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/sessions.py&quot;, line 618, in send\n    r = adapter.send(request, **kwargs)\n  File &quot;\/opt\/conda\/lib\/python2.7\/site-packages\/requests\/adapters.py&quot;, line 508, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?run_uuid=ae69145133bf49efac22b1d390c354f1&amp;run_id=ae69145133bf49efac22b1d390c354f1 (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f5cbd80d690&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))\n2021\/05\/09 17:11:22 ERROR mlflow.cli: === Run (ID 'ae69145133bf49efac22b1d390c354f1') failed ===\n<\/code><\/pre>\n<p>Any ideas how to fix this? I tried adding the following in MLproject file but it doesn't help<\/p>\n<pre><code>environment: [[&quot;network&quot;, &quot;host&quot;], [&quot;add-host&quot;, &quot;host.docker.internal:host-gateway&quot;]]\n<\/code><\/pre>\n<p>Thanks for your help! =)<\/p>",
        "Challenge_closed_time":1620627546968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620552530280,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to connect to MLFLOW_TRACKING_URI when running MLflow run in a Docker container. The error message suggests that the connection to the server is being refused, and the user has tried adding network and host configurations to the MLproject file but it did not help.",
        "Challenge_last_edit_time":1620554070856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67456172",
        "Challenge_link_count":5,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":61.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":20.8379688889,
        "Challenge_title":"Unable to connect to MLFLOW_TRACKING_URI when running MLflow run in Docker container",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1151.0,
        "Challenge_word_count":357,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1316620102630,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1308.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>Run MLflow server such was that it will use your machine IP instead of <code>localhost<\/code>.  Then point the <code>mlflow run<\/code> to that IP instead of <code>http:\/\/localhost:5000<\/code>.   The main reason is that <code>localhost<\/code> of Docker process is its own, not your machine.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.5,
        "Solution_reading_time":3.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"connection refused error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1329.0686111111,
        "Challenge_answer_count":0,
        "Challenge_body":"I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\r\nThe name of the inputs are:\r\n1. `input_image` \r\n2. `input_image_meta` \r\n3. `input_anchors` \r\n\r\n\r\nand the name of outputs are:\r\n1.  `output_detections`\r\n2.  `output_mrcnn_class`\r\n3.  `output_mrcnn_bbox`\r\n4.  `output_mrcnn_mask`\r\n5.  `output_rois`\r\n\r\nI have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting `{'error': \"Missing 'inputs' or 'instances' key\"}` in return.\r\n \r\nI have made a model.tar.gz file which has the following structure:\r\n\r\n    mymodel\r\n        |__1\r\n            |__variables\r\n            |__saved_model.pb\r\n\r\n    code\r\n        |__inference.py\r\n        |__requirements.txt\r\n\r\nAs specified in the documentation, inference.py has input_handler and output handler functions. From the client-side, I pass the S3 link of the image which then transforms to the three inputs for the model. \r\n\r\nThe structure of input_handler is as follows:\r\n\r\n```\r\ndef input_handler(data, context):\r\n     input_data = json.loads(data.read().decode('utf-8'))\r\n\r\n    obj = bucket.Object(input_data['img_link'])\r\n    tmp = tempfile.NamedTemporaryFile()\r\n    \r\n    # download image from AWS S3\r\n    with open(tmp.name, 'wb') as f:\r\n        obj.download_fileobj(f)\r\n        image=mpimg.imread(tmp.name)\r\n    \r\n    # make preprocessing\r\n    image = Image.fromarray(image)\r\n     \r\n     ...... # some more transformations \r\n     return = {\"input_image\": Python list for image,\r\n                    \"input_image_meta: Python list for input image meta,\r\n                    \"input_anchors\": Python list for input anchors}\r\n\r\n```\r\nThe deifinition of output_handler is as follows:\r\n\r\n```\r\ndef output_handler(data, context):\r\n      output_string = data.content.decode('unicode-escape')\r\n      return output_string, context.accept_header\r\n```\r\n\r\nThe sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\r\nOn the client side, I call the predictor using follwoing code:\r\n\r\n```\r\nrequest = {}\r\nrequest[\"img_link\"] = \"image.jpg\"\r\nresult = predictor.predict(request)\r\n```\r\n\r\nBut when I print the result the following gets printed out, `{'error': \"Missing 'inputs' or 'instances' key\"}`\r\nAll the bucket connections for loading the image are in inference.py\r\n      ",
        "Challenge_closed_time":1571957138000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1567172491000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error message stating that no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/73",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":28.7,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":229.0,
        "Challenge_repo_star_count":169.0,
        "Challenge_repo_watch_count":41.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1329.0686111111,
        "Challenge_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":283,
        "Discussion_body":"Hello @janismdhanbad,\r\n\r\nI believe your inference requests will have to follow the TensorFlow serving REST API specifications defined here: https:\/\/www.tensorflow.org\/tfx\/serving\/api_rest#request_format_2\r\n\r\n```\r\nThe request body for predict API must be JSON object formatted as follows:\r\n\r\n{\r\n  \/\/ (Optional) Serving signature to use.\r\n  \/\/ If unspecifed default serving signature is used.\r\n  \"signature_name\": <string>,\r\n\r\n  \/\/ Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\r\n  \/\/ A request can have either of them but NOT both.\r\n  \"instances\": <value>|<(nested)list>|<list-of-objects>\r\n  \"inputs\": <value>|<(nested)list>|<object>\r\n}\r\n``` closing due to inactivity. feel free to reopen if necessary.",
        "Discussion_score_count":2.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unregistered TrainingJob version"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":168.8386111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nUsers get the error \"null is not an object\" when pop-ups are enabled in SWB (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620))\r\nThis error is illegible to the user and causes confusion. Can we make the error message more clear such as:\r\n\"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Diable pop-ups \r\n2. Connect to a workspace\r\n\r\n**Expected behavior**\r\nIf the workspace is unable to open, a more legible error message should be shown, such as \"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v4.3.1 and v5.0.0]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671209314000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1670601495000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the idle Sagemaker Notebook instances are not stopping automatically after the specified time. The autostop.py script used by the `on-start` lifecycle rule of the instance CFN template is not working due to missing packages. The expected behavior is that the idle instances should stop automatically after the specified time. The user is using Release Version v5.0.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1081",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.6,
        "Challenge_repo_contributor_count":43.0,
        "Challenge_repo_fork_count":116.0,
        "Challenge_repo_issue_count":1196.0,
        "Challenge_repo_star_count":165.0,
        "Challenge_repo_watch_count":25.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":168.8386111111,
        "Challenge_title":"[Bug] More descriptive error message for \"null is not an object\" while trying to connect to Sagemaker notebook. ",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Discussion_body":"Hi @simranmakwana, thank you for creating this issue. There are currently no plans to enrich the error messages in the UI; the recommendation is for you to customize the error messages within your installation of SWB as you see fit. Please reply back if there are any concerns with this approach. Thank you! ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"idle instances not stopping"
    },
    {
        "Answerer_created_time":1312912826288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":5843.0,
        "Answerer_view_count":153.0,
        "Challenge_adjusted_solved_time":0.9586563889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have deployed a build of mlflow to a pod in my kubernetes cluster. I'm able to port forward to the mlflow ui, and now I'm attempting to test it. To do this, I am running the following test on a jupyter notebook that is running on another pod in the same cluster.<\/p>\n<pre><code>import mlflow\n\nprint(&quot;Setting Tracking Server&quot;)\ntracking_uri = &quot;http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000&quot;\n\nmlflow.set_tracking_uri(tracking_uri)\n\nprint(&quot;Logging Artifact&quot;)\nmlflow.log_artifact('\/home\/test\/mlflow-example-artifact.png')\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p>When I run this though, I get<\/p>\n<pre><code>ConnectionError: HTTPConnectionPool(host='mlflow-tracking-server.default.svc.cluster.local', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get? (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>\n<p>The way I have deployed the mlflow pod is shown below in the yaml and docker:<\/p>\n<p>Yaml:<\/p>\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: &lt;ECR_IMAGE&gt;\n        ports:\n        - containerPort: 5000\n        env:\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;S3_BUCKET&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\n  annotations:\n    service.beta.kubernetes.io\/aws-load-balancer-type: nlb\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  selector:\n    app: mlflow-tracking-server\n  ports:\n    - name: http\n      port: 5000\n      targetPort: http\n<\/code><\/pre>\n<p>While the dockerfile calls a script that executes the mlflow server command: <code>mlflow server --default-artifact-root ${AWS_MLFLOW_BUCKET} --host 0.0.0.0 --port 5000<\/code>, I cannot connect to the service I have created using that mlflow pod.<\/p>\n<p>I have tried using the tracking uri <code>http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000<\/code>, I've tried using the service EXTERNAL-IP:5000, but everything I tried cannot connect and log using the service. Is there anything that I have missed in deploying my mlflow server pod to my kubernetes cluster?<\/p>",
        "Challenge_closed_time":1587498737656,
        "Challenge_comment_count":6,
        "Challenge_created_time":1587495286493,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has deployed an MLflow build to a pod in their Kubernetes cluster and is attempting to test it by running a Jupyter notebook on another pod in the same cluster. However, they are encountering a connection error when trying to set the tracking server using the mlflow library. The user has provided the YAML and Docker files used to deploy the MLflow pod and has tried various tracking URIs but cannot connect to the service. The user is seeking assistance in identifying any missed steps in deploying the MLflow server pod to their Kubernetes cluster.",
        "Challenge_last_edit_time":1599477403816,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61351024",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":14.0,
        "Challenge_reading_time":34.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.9586563889,
        "Challenge_title":"Kubernetes MLflow Service Pod Connection",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":276,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417012835812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":945.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>Your <strong>mlflow-tracking-server<\/strong> service should have <em>ClusterIP<\/em> type, not <em>LoadBalancer<\/em>. <\/p>\n\n<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer<\/em> Service type.<\/p>\n\n<blockquote>\n  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that\u2019s outside of your cluster.\n  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.<\/p>\n  \n  <p>Type values and their behaviors are:<\/p>\n  \n  <ul>\n  <li><p><strong>ClusterIP<\/strong>: Exposes the Service on a cluster-internal IP. Choosing this\n  value makes the Service only reachable from within the cluster. This\n  is the default ServiceType. <\/p><\/li>\n  <li><p><strong>NodePort<\/strong>: Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll > be able to contact the NodePort Service, from outside the cluster, by\n  requesting :. <\/p><\/li>\n  <li><strong>LoadBalancer<\/strong>: Exposes the Service\n  externally using a cloud provider\u2019s load balancer. NodePort and\n  ClusterIP Services, to which the external load balancer routes, are\n  automatically created. <\/li>\n  <li><strong>ExternalName<\/strong>: Maps the Service to the contents\n  of the externalName field (e.g. foo.bar.example.com), by returning a\n  CNAME record with its value. No proxying of any kind is set up.<\/li>\n  <\/ul>\n  \n  <p><a href=\"https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#publishing-services-service-types\" rel=\"nofollow noreferrer\">kubernetes.io<\/a><\/p>\n<\/blockquote>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1587499353943,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":21.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":206.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"connection error with tracking server"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":129.76,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nAttempting to deploy the docker image build yaml file available under https:\/\/github.com\/polyaxon\/polyaxon-examples\/blob\/master\/in_cluster\/build_image\/build-ml.yaml using the polyaxon cli returns the following error:\n\n>polyaxon run -f build_docker_image.yaml -l\nPolyaxonfile is not valid.\nError message: The Polyaxonfile's version specified is not supported by your current CLI.Your CLI support Polyaxonfile versions between: 1.1 <= v <= 1.1.You can run `polyaxon upgrade` and check documentation for the specification..\n\nTo reproduce\n\nThe contents of build_docker_image.yaml\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: python:3.8.8-buster\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nExpected behavior\n\nA build operation should begin that results in an image being saved to the docker registry connected under destination: connection:\n\nEnvironment\n\nPolyaxon 1.8.0\nPolyaxon CLI 1.8.0",
        "Challenge_closed_time":1618580258000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618113122000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while attempting to deploy a docker image build yaml file using the Polyaxon CLI. The error message indicates that the Polyaxonfile's version specified is not supported by the current CLI version. The user is using Polyaxon 1.8.0 and Polyaxon CLI 1.8.0. The expected behavior is to begin a build operation resulting in an image being saved to the docker registry.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1303",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.9,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":129.76,
        "Challenge_title":"Issue building docker image using example build config",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":140,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Very strange, and I could not reproduce. Did you try other files in the same examples folder https:\/\/github.com\/polyaxon\/polyaxon-examples\/tree\/master\/in_cluster\/build_image ?\nNot sure if quoting all values would work on your system:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: \"polyaxon-examples:ml\"\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"python:3.8.8-buster\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - 'pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nFinally you may try with polyaxon -v ... to trigger some additional debug information.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":8.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":78.0,
        "Tool":"Polyaxon",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported Polyaxonfile version"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.415,
        "Challenge_answer_count":0,
        "Challenge_body":"mlflow raises error if length of key\/value exceeds 250. If the length of gbdt parameters or cat_columns is long, experiment_gbdt will raise an exception.\r\n\r\nPossible option:\r\n- catch and ignore all errors from mlflow\r\n- truncate logging parameters automatically ",
        "Challenge_closed_time":1580353817000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1580251523000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user installed the mlflow helm chart with changed settings on a local minikube cluster, using bitnami\/postgresql for the db backend and minio for s3 storage. They created an initial bucket named \"mlflow\" in minio and ran a simple training example from mlflow docs in a k8s pod with env variables set. While they can see the metadata about the model in UI, the artifact section in UI is empty and the bucket is also empty. The user expected the artifacts to be in the minio bucket. The user is using helm version v3.9.0 and kubectl version v1.23.3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/nyanp\/nyaggle\/issues\/19",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":4.0,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":31.0,
        "Challenge_repo_issue_count":95.0,
        "Challenge_repo_star_count":260.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.415,
        "Challenge_title":"experiment_gbdt raise errors with long parameters and mlflow",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":44,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"empty artifact section"
    },
    {
        "Answerer_created_time":1473257955532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":145.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":26.3318861111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have read similar questions regarding azure app service, but I still can't find an answer. I was trying to deploy a model in azure kubernetes service, but I came across an error when importing cv2 (which is essential to me).<\/p>\n<p>Opencv-python is included in my environment .yaml file:<\/p>\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  # You must list azureml-defaults as a pip dependency\n  - azureml-defaults&gt;=1.0.45\n  - Cython\n  - matplotlib&gt;=3.2.2\n  - numpy&gt;=1.18.5\n  - opencv-python&gt;=4.1.2\n  - pillow\n  - PyYAML&gt;=5.3\n  - scipy&gt;=1.4.1\n  - torch&gt;=1.6.0\n  - torchvision&gt;=0.7.0\n  - tqdm&gt;=4.41.0\nchannels:\n- conda-forge\n<\/code><\/pre>\n<p>I am deploying as follows:<\/p>\n<pre><code>aks_service = Model.deploy(ws,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_config=gpu_aks_config,\n                       deployment_target=aks_target,\n                       name=aks_service_name)\n<\/code><\/pre>\n<p>And I get this error:<\/p>\n<pre><code>    Traceback (most recent call last):\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n    self.load_wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n    return self.load_wsgiapp()\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n    __import__(module)\n  File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n    from app import main\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 23, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 8, in &lt;module&gt;\n    import cv2\n  File &quot;\/azureml-envs\/azureml_659b55e5b05510a45f41f0ca31d3ac02\/lib\/python3.6\/site-packages\/cv2\/__init__.py&quot;, line 5, in &lt;module&gt;\n    from .cv2 import *\nImportError: libGL.so.1: cannot open shared object file: No such file or directory\nWorker exiting (pid: 41)\n<\/code><\/pre>",
        "Challenge_closed_time":1603897093470,
        "Challenge_comment_count":0,
        "Challenge_created_time":1603802298680,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when trying to import cv2 while deploying a model in Azure Kubernetes Service. The error message indicates that the libGL.so.1 file cannot be found, which is preventing the import of cv2. The user has already included opencv-python in their environment .yaml file and is deploying the model using Model.deploy() function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64554615",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":41.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":26.3318861111,
        "Challenge_title":"Import cv2 error when deploying in Azure Kubernetes Service - python",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":484.0,
        "Challenge_word_count":243,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473257955532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":145.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>This might go wrong at some point, but my workaround was installing opencv-python-headless instead of opencv.<\/p>\n<p>In the environment .yaml file, just replace:<\/p>\n<pre><code>- opencv-python&gt;=4.1.2\n<\/code><\/pre>\n<p>with:<\/p>\n<pre><code>- opencv-python-headless\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":3.72,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing libGL.so.1 file"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.5969425,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>****Do I need to add the .ipynb extension manually to my notebook file ?**  <br \/>\n**I can't find the run button.****<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/85574-capture2.png?platform=QnA\" alt=\"85574-capture2.png\" \/><\/p>",
        "Challenge_closed_time":1617872811416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617852662423,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to find the run button in their notebook file and is unsure if they need to manually add the .ipynb extension to the file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/348777\/cant-find-the-run-button",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":3.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.5969425,
        "Challenge_title":"Can't find the run button",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=441c29e9-5f3d-4595-9f6c-dec6fcde5e85\">@paul gureghian  <\/a> You can re-name your file with .ipynb extension which should help to display the cells and the available options like run button for cell. Usually while creating new files in your workspace the UI prompts to select the extension of the file. If you can create a new file with .ipynb extension and copy these cells individually that should also work. Thanks.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":5.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing run button"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":17.6446433334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I made a classifier in Python that uses a lot of libraries. I have uploaded the model to Amazon S3 as a pickle (my_model.pkl). Ideally, every time someone uploads a file to a specific S3 bucket, it should trigger an AWS Lambda that would load the classifier, return predictions and save a few files on an Amazon S3 bucket.<\/p>\n<p>I want to know if it is possible to use a Lambda to execute a Jupyter Notebook in AWS SageMaker. This way I would not have to worry about the dependencies and would generally make the classification more straight forward.<\/p>\n<p>So, is there a way to use an AWS Lambda to execute a Jupyter Notebook?<\/p>",
        "Challenge_closed_time":1603033131243,
        "Challenge_comment_count":0,
        "Challenge_created_time":1602969610527,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to use AWS Lambda to trigger a Jupyter Notebook on AWS Sagemaker for executing a classifier that uses multiple libraries. The user has uploaded the model to Amazon S3 as a pickle and wants to trigger the Lambda every time someone uploads a file to a specific S3 bucket. The user is seeking a solution to avoid worrying about dependencies and make the classification more straightforward.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64407452",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.0,
        "Challenge_reading_time":8.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":17.6446433334,
        "Challenge_title":"Use AWS Lambda to execute a jupyter notebook on AWS Sagemaker",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2656.0,
        "Challenge_word_count":124,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526405779360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mexico City, CDMX, Mexico",
        "Poster_reputation_count":4713.0,
        "Poster_view_count":507.0,
        "Solution_body":"<p>Scheduling notebook execution is a bit of a SageMaker anti-pattern, because (1) you would need to manage data I\/O (training set, trained model) yourself, (2) you would need to manage metadata tracking yourself, (3) you cannot run on distributed hardware and (4) you cannot use Spot. Instead, it is recommended for scheduled task to leverage the various SageMaker long-running, background job APIs: SageMaker Training, SageMaker Processing or SageMaker Batch Transform (in the case of a batch inference).<\/p>\n<p>That being said, if you still want to schedule a notebook to run, you can do it in a variety of ways:<\/p>\n<ul>\n<li>in the <a href=\"https:\/\/www.youtube.com\/watch?v=6EYEoAqihPg\" rel=\"nofollow noreferrer\">SageMaker CICD Reinvent 2018 Video<\/a>, Notebooks are launched as Cloudformation templates, and their execution is automated via a SageMaker lifecycle configuration.<\/li>\n<li>AWS released <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/scheduling-jupyter-notebooks-on-sagemaker-ephemeral-instances\/\" rel=\"nofollow noreferrer\">this blog post<\/a> to document how to launch Notebooks from within Processing jobs<\/li>\n<\/ul>\n<p>But again, my recommendation for scheduled tasks would be to remove them from Jupyter, turn them into scripts and run them in SageMaker Training<\/p>\n<p>No matter your choices, all those tasks can be launched as API calls from within a Lambda function, as long as the function role has appropriate permissions<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.2,
        "Solution_reading_time":18.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"trigger Jupyter Notebook on Sagemaker with Lambda"
    },
    {
        "Answerer_created_time":1641102333407,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":67.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":1.1414036111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I tried importing cv2 from opencv but i got error saying<\/p>\n<blockquote>\n<p>import error: libgthread-2.0.so.0: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n<p>Since Sagemaker Studio Labs doesn't support installation of Ubuntu packages i couldn't use <code>apt-get<\/code> or <code>yum<\/code> to install libglib2.0-0.<\/p>",
        "Challenge_closed_time":1641103613576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641099504523,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to import cv2 in SageMaker Studio Lab due to the missing shared object file libgthread-2.0.so.0. The user was unable to install the required package libglib2.0-0 as Sagemaker Studio Labs does not support the installation of Ubuntu packages.",
        "Challenge_last_edit_time":1641107240987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70553701",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.1414036111,
        "Challenge_title":"Couldn't import cv2 in SageMaker Studio Lab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":534.0,
        "Challenge_word_count":51,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589906719620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":63.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>With this line, you can install the glib dependency for Amazon Sagemaker Studio Lab. Just run it on your notebook cell.<\/p>\n<pre><code>! conda install glib=2.51.0 -y\n<\/code><\/pre>\n<p>You also can create another virtual environment for your session that contains glib:<\/p>\n<pre><code>! conda create -n glib-test -c defaults -c conda-forge python=3 glib=2.51.0` -y\n<\/code><\/pre>\n<p>After that maybe you need albumentations to import cv2:<\/p>\n<pre><code>! pip install albumentations\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing shared object file"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":1833.7168480556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I trained a model with the built-in RESnet18 docker image, and now I want to deploy the model to an endpoint and classify ~ 1 million images. I have all my training, validation, and test images stored on S3 in RecordIO format (converted with <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/basic\/data.html?highlight=im2rec\" rel=\"nofollow noreferrer\">im2rec.py<\/a>). According to the <a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>The Amazon SageMaker Image Classification algorithm supports both RecordIO (application\/x-recordio) and image (application\/x-image) content types for training. The algorithm supports only\u00a0application\/x-image\u00a0for inference.<\/p>\n<\/blockquote>\n\n<p>So I cannot perform inference on my training data in RecordIO format. To overcome this I copied all the raw .jpg images (~ 2GB) onto my Sagemaker Jupyter Notebook instance and performed inference one at a time in the following way:<\/p>\n\n<pre><code>img_list = os.listdir('temp_data') # list of all ~1,000,000 images\n\nfor im in img_list:\n    with open('temp_data\/'+im, 'rb') as f:\n        payload = f.read()\n        payload = bytearray(payload)\n    response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n                                       ContentType='application\/x-image', \n                                       Body=payload)\n\n    etc...\n<\/code><\/pre>\n\n<p>Needless to say, transferring all the data onto my Notebook instance took a long time and I would prefer not having to do that before running inference. Why does the SageMaker Image Classification not support RecordIO for inference? And more importantly, what is the best way to run inference on many images without having to move them from S3?<\/p>",
        "Challenge_closed_time":1531369355823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524767975170,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model using the RESnet18 docker image in Sagemaker and wants to classify around 1 million images stored in S3 in RecordIO format. However, the Sagemaker Image Classification algorithm only supports application\/x-image for inference, not RecordIO. Therefore, the user had to copy all the raw .jpg images onto their Sagemaker Jupyter Notebook instance and perform inference one at a time, which took a long time. The user is seeking a better way to run inference on many images without having to move them from S3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50049928",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":22.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":1833.7168480556,
        "Challenge_title":"Sagemaker image classification: Best way to perform inference on many images in S3?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2600.0,
        "Challenge_word_count":217,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1474520506390,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":832.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>The RecordIO format is designed to pack a large number of images into a single file, so I don't think it would work well for predicting single images.<\/p>\n\n<p>When it comes to prediction, you definitely don't have to copy images to a notebook instance or to S3. You just have to load them from anywhere and inline them in your prediction requests.<\/p>\n\n<p><strong>If you want HTTP-based prediction, here are your options:<\/strong><\/p>\n\n<p>1) Use the SageMaker SDK Predictor.predict() API on any machine (as long as it has proper AWS credentials) <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk<\/a><\/p>\n\n<p>2) Use the AWS Python SDK (aka boto3) API invoke_endpoint() on any machine (as long as it has proper AWS credentials)<\/p>\n\n<p>You can even build a simple service to perform pre-processing or post-processing with Lambda. Here's an example: <a href=\"https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/@julsimon\/using-chalice-to-serve-sagemaker-predictions-a2015c02b033<\/a><\/p>\n\n<p><strong>If you want batch prediction:<\/strong>\n the simplest way is to retrieve the trained model from SageMaker, write a few lines of ad-hoc MXNet code to load it and run all your predictions. Here's an example: <a href=\"https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">https:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":12.1,
        "Solution_reading_time":20.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":177.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported RecordIO format"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.8151808334,
        "Challenge_answer_count":2,
        "Challenge_body":"I am running a Sagemaker Notebook instance. How can I tell if my Notebook is frozen or just taking a long time? I am using a 24xlarge and querying from Athena in parallel and it seems to be stuck on the same query for a long time. How can I tell if I need more Memory or more VCPUs?",
        "Challenge_closed_time":1683362949783,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683306015132,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running a Sagemaker Notebook instance and is unsure if it is frozen or just taking a long time. They are using a 24xlarge and querying from Athena in parallel, but it seems to be stuck on the same query for a long time. The user is unsure if they need more Memory or more VCPUs.",
        "Challenge_last_edit_time":1683644479959,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUK_hTODBBTeWX_c-lcO34mg\/how-can-i-tell-if-my-notebook-instance-is-frozen",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.9,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":15.8151808334,
        "Challenge_title":"How can I tell if my Notebook instance is frozen?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there,\n\nGreetings for the day!\n\nI understand that you wanted to know how can you determine if you need more VCPU or memory when your SageMaker Notebook Instance is frozen or just taking a long time?\n\nI\u2019d like to inform you that If your Sagemaker Notebook instance is taking a long time, you can check if it is frozen or still running by monitoring the CPU and memory usage. If the CPU usage is low or zero, it may be frozen. \n\nIf the CPU usage is high but the memory usage is low, you may need more VCPUs and If the memory usage is high, you may need more memory. \n\nYou can check it via SageMaker Notebook Instance terminal:\n\nTo see the memory and CPU information in detail , kindly follow the below instructions:-\n\n[1] Start Your Notebook Instance\n[2] Go to the  Jupyter Home Page \n[3] Right hand side ,Click on DropDown Option \u201cNew\u201d \n[4] Select \u201cTerminal\u201d.\n\nIn the Jupyter terminal, Run the below commands to see the information of Memory and CPUs.\n\n[+] To see the memory information:\n\n$ free -h\n\n=> output of \u201cfree -h\u201d will provide the information of total memory, used memory, free memory, shared memory etc in human readable form.\n\n[+] To see the CPU information, you can run any of the commands:\n\n$ mpstat -u\n\n=> Output of \u201cmpstat -u\u201d consists of different fields like %guest, %gnice, %steal etc.\n\n%steal\nShow the percentage of time spent in involuntary wait by the virtual CPU or CPUs while the hyper\u2010\nvisor was servicing another virtual processor.\n\n%guest\nShow the percentage of time spent by the CPU or CPUs to run a virtual processor.\n\n%idle\nShow the percentage of time that the CPU or CPUs were idle and the system did not have an out\u2010\nstanding disk I\/O request.\n\nmany more.. You can find more detail about the each field of mpstat command by visiting the manual page of it. To see the manual page of mpstat command, use \u201c$ man mpstst\u201d.\n\nAlong with \u201cmpstat -u\u201d, you can also try the below listed commands to get information about the cpu:\n\n$ lscpu\n$ cat \/proc\/cpuinfo\n$ top\n\nAdditionally, You can also check the cloudwatch logs for any errors or warnings that may indicate the cause of the issue. Most of the time, Cloud watch logs helps to find out the root cause of the issue.\n\nYou can find the CloudWatch logs under CloudWatch \u2192 Log Groups \u2192 \/aws\/sagemaker\/NotebookInstances -> Notebook Instance Name\n\nBased on the analysis , you can select different Notebook Instance type, You can find more detail of SageMaker Instance Here [1].\n\nI request you to kindly follow the above suggested workarounds. \n\nIf you have any difficulty or run into any issue, Please reach out to AWS Support[+] (Sagemaker), along with your issue\/use case in details and we would be happy to assist you further.\n\n[+] Creating support cases and case management - https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-casehttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-case \n\nI hope this information would be useful to you.\n\nThank you!\n\nREFERENCES:\n\n[1] https:\/\/aws.amazon.com\/sagemaker\/pricing\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1683362949783,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":37.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":488.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"determine frozen status"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":622.6719444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Seems like recent upgrade to V1.33 for Azure ML SDK has changed how identity based access worked? Previously if you had a datastore (ex. SQL) with no credentials and then tried to register a dataset, it would prompt you to login to get your AAD auth token to see if you had permission to get access to the underlying data source. Seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab AD auth token:\r\n**_Getting data access token with Assigned Identity (client_id=clientid)._**\r\n\r\n\r\nI have verified the underlying datastore does not have Managed Identity on and V1.32 SDK Prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. Has any changes been made to the identity based access feature from on V1.33 SDK? According to the SDK docs, running the TabularDataset.to_pandas_dataframe() command should prompt an AD login if using no credentialed datastore into dataset creation. FYI currently using Azure SQL DB as datastore, any clarifications would be appreciated!\r\nazureml.core.Datastore class - Azure Machine Learning Python | Microsoft Docs\r\n",
        "Challenge_closed_time":1632248052000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1630006433000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install `azureml-core` package using pip on Windows 10 with Python 3.9.5 64-bit. The error is specifically related to the `ruamel.yaml` package during installation. The user suggests using a later version of `ruamel.yaml` as a possible solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1584",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":8.6,
        "Challenge_reading_time":15.54,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":622.6719444444,
        "Challenge_title":"Identity Based Access No longer works (with Azure SQL DB datastore) in V1.33 of Azure ML SDK",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":204,
        "Discussion_body":"[70_driver_log.txt](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/7062339\/70_driver_log.txt)\r\n\r\nError generated in new compute that uses the V1.33 SDK Any updates to this? I had created another AML Workspace and issue disappeared but for some other subscriptions it still doesnt work and errors with the same thing as in the logs. Everything works perfectly fine in V1.32 of the SDK so not sure if new update changed some sort of Identity SDK used in Azure? The driver log had error message \"Compute has no identity provisioned.\" Try updating the compute to enable managed identity, and grant managed identity access to the data storage. Ah ok I was under the impression only the compute clusters had MI and not the compute instance. I'll take a look at the docs and will also re-configure the datastore which might be issue. @rudizhou428 we had some new feature for Compute Instance, which can use your identity in the CI, but, you need to re-create the CI as it won't automatically update the existing one. @chunyli0328 Ah ok cool, I ended up creating a new Azure ML Workspace and moved all my files over and since you need to recreate the CI and clusters, I'm guessing thats why it started to work again. Closing this issue, thanks for the help everyone!",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"installation error with ruamel.yaml"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":124.1602897223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pkl package saved in my azure devops repository<\/p>\n<p>using below code it searches for package in workspace.\nHow to provide package saved in repository<\/p>\n<pre><code> ws = Workspace.get(\n         name=workspace_name,\n         subscription_id=subscription_id,\n        resource_group=resource_group,\n        auth=cli_auth)\n\nimage_config = ContainerImage.image_configuration(\n    execution_script=&quot;score.py&quot;,\n    runtime=&quot;python-slim&quot;,\n    conda_file=&quot;conda.yml&quot;,\n    description=&quot;Image with ridge regression model&quot;,\n    tags={&quot;area&quot;: &quot;ml&quot;, &quot;type&quot;: &quot;dev&quot;},\n)\n\nimage = Image.create(\n    name=image_name,  models=[model], image_config=image_config, workspace=ws\n)\n\nimage.wait_for_creation(show_output=True)\n\nif image.creation_state != &quot;Succeeded&quot;:\n    raise Exception(&quot;Image creation status: {image.creation_state}&quot;)\n\nprint(\n    &quot;{}(v.{} [{}]) stored at {} with build log {}&quot;.format(\n        image.name,\n        image.version,\n        image.creation_state,\n        image.image_location,\n        image.image_build_log_uri,\n    )\n)\n\n# Writing the image details to \/aml_config\/image.json\nimage_json = {}\nimage_json[&quot;image_name&quot;] = image.name\nimage_json[&quot;image_version&quot;] = image.version\nimage_json[&quot;image_location&quot;] = image.image_location\nwith open(&quot;aml_config\/image.json&quot;, &quot;w&quot;) as outfile:\n    json.dump(image_json, outfile)\n<\/code><\/pre>\n<p>I tried to provide path to models but its fails saying package not found<\/p>\n<p>models = $(System.DefaultWorkingDirectory)\/package_model.pkl<\/p>",
        "Challenge_closed_time":1627276170736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626831896507,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an Azure Machine Learning scoring image using a local package saved in their Azure DevOps repository. They are encountering an issue where the code is searching for the package in the workspace and they are unsure how to provide the package saved in the repository. They have attempted to provide the path to the models but it fails, stating that the package is not found.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68463080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.9,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":123.4095080556,
        "Challenge_title":"how create azure machine learning scoring image using local package",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":121,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567209656790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":417.0,
        "Poster_view_count":233.0,
        "Solution_body":"<p>Register model:\nRegister a file or folder as a model by calling <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-\" rel=\"nofollow noreferrer\">Model.register()<\/a>.<\/p>\n<p>In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace.<\/p>\n<pre><code>model = Model.register(workspace=ws,\n                       model_name='',                # Name of the registered model in your workspace.\n                       model_path='',  # Local file to upload and register as a model.\n                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n                       sample_input_dataset=input_dataset,\n                       sample_output_dataset=output_dataset,\n                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n                       description='Ridge regression model to predict diabetes progression.',\n                       tags={'area': 'diabetes', 'type': 'regression'})\n\nprint('Name:', model.name)\nprint('Version:', model.version)\n<\/code><\/pre>\n<p>Deploy machine learning models to Azure: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a><\/p>\n<p>To Troubleshooting remote model deployment Please follow the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?tabs=azcli#function-fails-get_model_path\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1627278873550,
        "Solution_link_count":6.0,
        "Solution_readability":19.4,
        "Solution_reading_time":29.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":159.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"package not found"
    },
    {
        "Answerer_created_time":1548188011640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bellevue, WA, USA",
        "Answerer_reputation_count":131.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":102.9447575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using Azure Machine Learning Service with the azureml-sdk python library.<\/p>\n\n<p>I'm using azureml.core version 1.0.8<\/p>\n\n<p>I'm following this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline<\/a> tutorial.<\/p>\n\n<p>I've got it working when I use Azure Compute resources. But I would like to run it locally.<\/p>\n\n<p>I get the following error<\/p>\n\n<pre><code>raise ErrorResponseException(self._deserialize, response)\nazureml.pipeline.core._restclients.aeva.models.error_response.ErrorResponseException: (BadRequest) Response status code does not indicate success: 400 (Bad Request).\nTrace id: [uuid], message: Can't build command text for [train.py], moduleId [uuid] executionId [id]: Assignment for parameter Target is not specified\n<\/code><\/pre>\n\n<p>My code looks like:<\/p>\n\n<pre><code>run_config = RunConfiguration()\ncompute_target = LocalTarget()\nrun_config.target = LocalTarget()    \nrun_config.environment.python.conda_dependencies = CondaDependencies(conda_dependencies_file_path='environment.yml')\nrun_config.environment.python.interpreter_path = 'C:\/Projects\/aml_test\/.conda\/envs\/aml_test_env\/python.exe'\nrun_config.environment.python.user_managed_dependencies = True\nrun_config.environment.docker.enabled = False\n\ntrainStep = PythonScriptStep(\n    script_name=\"train.py\",\n    compute_target=compute_target,\n    source_directory='.',\n    allow_reuse=False,\n    runconfig=run_config\n)\n\nsteps = [trainStep]\n\n# Build the pipeline\npipeline = Pipeline(workspace=ws, steps=[steps])\npipeline.validate()\n\nexperiment = Experiment(ws, 'Test')\n\n# Fails, locally, works on Azure Compute\nrun = experiment.submit(pipeline)\n\n\n# Works both locally and on Azure Compute\nsrc = ScriptRunConfig(source_directory='.', script='train.py', run_config=run_config)\nrun = experiment.submit(src)\n<\/code><\/pre>\n\n<p>The <code>train.py<\/code> is a very simple self contained script only dependent on numpy that approximates pi.<\/p>",
        "Challenge_closed_time":1548188011640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547817410513,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run an Azure Machine Learning Service pipeline locally using the azureml-sdk python library. The error message indicates that the target parameter is not specified in the train.py script. The user's code includes a run configuration and a PythonScriptStep, and the train.py script is a simple self-contained script that approximates pi. The code works when using Azure Compute resources but fails when running locally.",
        "Challenge_last_edit_time":1554703788007,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54254830",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.2,
        "Challenge_reading_time":28.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":102.9447575,
        "Challenge_title":"Running Azure Machine Learning Service pipeline locally",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2476.0,
        "Challenge_word_count":164,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1313998995867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3148.0,
        "Poster_view_count":347.0,
        "Solution_body":"<p>Local compute cannot be used with ML Pipelines. Please see this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-set-up-training-targets#supported-compute-targets\" rel=\"noreferrer\">article<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1548188395572,
        "Solution_link_count":1.0,
        "Solution_readability":23.7,
        "Solution_reading_time":3.2,
        "Solution_score_count":7.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing target parameter"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":73.3434230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use Facebook Prophet on an AWS Sagemaker Jupyter notebook. I've tried installing fbprophet in two ways:<\/p>\n\n<p><code>!{sys.executable} -m pip install fbprophet<\/code><\/p>\n\n<p>and<\/p>\n\n<pre><code>!conda install -c conda-forge fbprophet --yes\n<\/code><\/pre>\n\n<p>(that last one comes from several answers I saw on other forums)<\/p>\n\n<p>However, none of them seem to work. In particular, the latest one seems to work for the installation, but the subsequent import of fbprophet results in an error, that seems related to matplotlib:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-8-d9f3d4c04a60&gt; in &lt;module&gt;()\n      1 # Imports\n----&gt; 2 from fbprophet import Prophet\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/fbprophet\/__init__.py in &lt;module&gt;()\n      6 # of patent rights can be found in the PATENTS file in the same directory.\n      7 \n----&gt; 8 from fbprophet.forecaster import Prophet\n      9 \n     10 __version__ = '0.6'\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/fbprophet\/forecaster.py in &lt;module&gt;()\n     17 from fbprophet.make_holidays import get_holiday_names, make_holidays_df\n     18 from fbprophet.models import StanBackendEnum\n---&gt; 19 from fbprophet.plot import (plot, plot_components)\n     20 \n     21 logger = logging.getLogger('fbprophet')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/fbprophet\/plot.py in &lt;module&gt;()\n     19 \n     20 try:\n---&gt; 21     from matplotlib import pyplot as plt\n     22     from matplotlib.dates import (\n     23         MonthLocator,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/pyplot.py in &lt;module&gt;()\n     30 from cycler import cycler\n     31 import matplotlib\n---&gt; 32 import matplotlib.colorbar\n     33 import matplotlib.image\n     34 from matplotlib import rcsetup, style\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/colorbar.py in &lt;module&gt;()\n     25 \n     26 import matplotlib as mpl\n---&gt; 27 import matplotlib.artist as martist\n     28 import matplotlib.cbook as cbook\n     29 import matplotlib.collections as collections\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/artist.py in &lt;module&gt;()\n     55 \n     56 \n---&gt; 57 class Artist(object):\n     58     \"\"\"\n     59     Abstract base class for objects that render into a FigureCanvas.\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/artist.py in Artist()\n     62     \"\"\"\n     63     @cbook.deprecated(\"3.1\")\n---&gt; 64     @property\n     65     def aname(self):\n     66         return 'Artist'\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/cbook\/deprecation.py in deprecate(obj, message, name, alternative, pending, addendum)\n    180                 pass\n    181     \"\"\"\n--&gt; 182 \n    183     def deprecate(obj, message=message, name=name, alternative=alternative,\n    184                   pending=pending, obj_type=obj_type, addendum=addendum):\n\nAttributeError: 'property' object has no attribute '__name__'\n<\/code><\/pre>\n\n<p>Has anyone else run into this issue \/ managed to get fbprophet working on sagemaker? I'm thinking this might be an issue of versions of fbprophet and matplotlib, but couldn't find information on which ones to use.<\/p>",
        "Challenge_closed_time":1584358813960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584094777637,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to install and import Facebook Prophet on an AWS Sagemaker Jupyter notebook. The user has tried two methods of installation, but none of them seem to work. The latest method seems to work for installation, but the subsequent import of fbprophet results in an error related to matplotlib. The user is seeking help to resolve the issue and is thinking that it might be an issue of versions of fbprophet and matplotlib.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60668635",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":41.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":73.3434230556,
        "Challenge_title":"Import problem for Facebook Prophet on AWS Sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":3052.0,
        "Challenge_word_count":329,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369829834347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The following works for me  on a fresh notebook instance. I'm using the python3 environment.<\/p>\n\n<pre><code>sh-4.2$ source activate python3\n(python3) sh-4.2$ conda install -c conda-forge fbprophet --yes\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.5.12\n  latest version: 4.8.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\n\n\n## Package Plan ##\n\n  environment location: \/home\/ec2-user\/anaconda3\/envs\/python3\n\n  added \/ updated specs:\n    - fbprophet\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    pykerberos-1.2.1           |   py36h505690d_0          26 KB  conda-forge\n    python-3.6.7               |    h381d211_1004        34.5 MB  conda-forge\n    cryptography-2.8           |   py36h45558ae_2         628 KB  conda-forge\n    python_abi-3.6             |          1_cp36m           4 KB  conda-forge\n    binutils_impl_linux-64-2.33.1|       h53a641e_8         9.1 MB  conda-forge\n    certifi-2019.11.28         |   py36h9f0ad1d_1         149 KB  conda-forge\n    gcc_linux-64-7.3.0         |      h553295d_17          21 KB  conda-forge\n    gxx_linux-64-7.3.0         |      h553295d_17          21 KB  conda-forge\n    fbprophet-0.6              |   py36he1b5a44_0         642 KB  conda-forge\n    holidays-0.10.1            |             py_0          56 KB  conda-forge\n    tk-8.6.10                  |       hed695b0_0         3.2 MB  conda-forge\n    lunarcalendar-0.0.9        |             py_0          20 KB  conda-forge\n    convertdate-2.1.3          |          py_1000          30 KB  conda-forge\n    curl-7.68.0                |       hf8cf82a_0         137 KB  conda-forge\n    krb5-1.16.4                |       h2fd8d38_0         1.4 MB  conda-forge\n    libcurl-7.68.0             |       hda55be3_0         564 KB  conda-forge\n    expat-2.2.9                |       he1b5a44_2         191 KB  conda-forge\n    binutils_linux-64-2.33.1   |      h9595d00_17          21 KB  conda-forge\n    python-dateutil-2.8.0      |             py_0         219 KB  conda-forge\n    matplotlib-base-3.1.0      |   py36h5f35d83_0         6.7 MB  conda-forge\n    pycurl-7.43.0.5            |   py36h16ce93b_0          69 KB  conda-forge\n    ld_impl_linux-64-2.33.1    |       h53a641e_8         589 KB  conda-forge\n    pystan-2.17.1.0            |py36hf2d7682_1004        14.0 MB  conda-forge\n    ephem-3.7.7.1              |   py36h516909a_0         722 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        72.9 MB\n\nThe following NEW packages will be INSTALLED:\n\n    binutils_impl_linux-64: 2.33.1-h53a641e_8          conda-forge\n    binutils_linux-64:      2.33.1-h9595d00_17         conda-forge\n    convertdate:            2.1.3-py_1000              conda-forge\n    ephem:                  3.7.7.1-py36h516909a_0     conda-forge\n    fbprophet:              0.6-py36he1b5a44_0         conda-forge\n    gcc_impl_linux-64:      7.3.0-habb00fd_1\n    gcc_linux-64:           7.3.0-h553295d_17          conda-forge\n    gettext:                0.19.8.1-hc5be6a0_1002     conda-forge\n    gxx_impl_linux-64:      7.3.0-hdf63c60_1\n    gxx_linux-64:           7.3.0-h553295d_17          conda-forge\n    holidays:               0.10.1-py_0                conda-forge\n    ld_impl_linux-64:       2.33.1-h53a641e_8          conda-forge\n    lunarcalendar:          0.0.9-py_0                 conda-forge\n    matplotlib-base:        3.1.0-py36h5f35d83_0       conda-forge\n    pystan:                 2.17.1.0-py36hf2d7682_1004 conda-forge\n    python_abi:             3.6-1_cp36m                conda-forge\n\nThe following packages will be UPDATED:\n\n    ca-certificates:        2019.10.16-0                           --&gt; 2019.11.28-hecc5488_0     conda-forge\n    certifi:                2019.9.11-py36_0                       --&gt; 2019.11.28-py36h9f0ad1d_1 conda-forge\n    cryptography:           2.2.2-py36h14c3975_0                   --&gt; 2.8-py36h45558ae_2        conda-forge\n    curl:                   7.60.0-h84994c4_0                      --&gt; 7.68.0-hf8cf82a_0         conda-forge\n    expat:                  2.2.5-he0dffb1_0                       --&gt; 2.2.9-he1b5a44_2          conda-forge\n    glib:                   2.56.1-h000015b_0                      --&gt; 2.58.3-h6f030ca_1002      conda-forge\n    krb5:                   1.14.2-hcdc1b81_6                      --&gt; 1.16.4-h2fd8d38_0         conda-forge\n    libcurl:                7.60.0-h1ad7b7a_0                      --&gt; 7.68.0-hda55be3_0         conda-forge\n    libpng:                 1.6.34-hb9fc6fc_0                      --&gt; 1.6.37-hed695b0_0         conda-forge\n    libssh2:                1.8.0-h9cfc8f7_4                       --&gt; 1.8.2-h22169c7_2          conda-forge\n    openssl:                1.0.2t-h7b6447c_1                      --&gt; 1.1.1d-h516909a_0         conda-forge\n    pycurl:                 7.43.0.1-py36hb7f436b_0                --&gt; 7.43.0.5-py36h16ce93b_0   conda-forge\n    pykerberos:             1.2.1-py36h14c3975_0                   --&gt; 1.2.1-py36h505690d_0      conda-forge\n    python:                 3.6.5-hc3d631a_2                       --&gt; 3.6.7-h381d211_1004       conda-forge\n    python-dateutil:        2.7.3-py36_0                           --&gt; 2.8.0-py_0                conda-forge\n    qt:                     5.9.6-h52aff34_0                       --&gt; 5.9.7-h5867ecd_1\n    sqlite:                 3.23.1-he433501_0                      --&gt; 3.28.0-h8b20d00_0         conda-forge\n    tk:                     8.6.7-hc745277_3                       --&gt; 8.6.10-hed695b0_0         conda-forge\n\n\nDownloading and Extracting Packages\npykerberos-1.2.1     | 26 KB     | ################################################################################ | 100%\npython-3.6.7         | 34.5 MB   | ################################################################################ | 100%\ncryptography-2.8     | 628 KB    | ################################################################################ | 100%\npython_abi-3.6       | 4 KB      | ################################################################################ | 100%\nbinutils_impl_linux- | 9.1 MB    | ################################################################################ | 100%\ncertifi-2019.11.28   | 149 KB    | ################################################################################ | 100%\ngcc_linux-64-7.3.0   | 21 KB     | ################################################################################ | 100%\ngxx_linux-64-7.3.0   | 21 KB     | ################################################################################ | 100%\nfbprophet-0.6        | 642 KB    | ################################################################################ | 100%\nholidays-0.10.1      | 56 KB     | ################################################################################ | 100%\ntk-8.6.10            | 3.2 MB    | ################################################################################ | 100%\nlunarcalendar-0.0.9  | 20 KB     | ################################################################################ | 100%\nconvertdate-2.1.3    | 30 KB     | ################################################################################ | 100%\ncurl-7.68.0          | 137 KB    | ################################################################################ | 100%\nkrb5-1.16.4          | 1.4 MB    | ################################################################################ | 100%\nlibcurl-7.68.0       | 564 KB    | ################################################################################ | 100%\nexpat-2.2.9          | 191 KB    | ################################################################################ | 100%\nbinutils_linux-64-2. | 21 KB     | ################################################################################ | 100%\npython-dateutil-2.8. | 219 KB    | ################################################################################ | 100%\nmatplotlib-base-3.1. | 6.7 MB    | ################################################################################ | 100%\npycurl-7.43.0.5      | 69 KB     | ################################################################################ | 100%\nld_impl_linux-64-2.3 | 589 KB    | ################################################################################ | 100%\npystan-2.17.1.0      | 14.0 MB   | ################################################################################ | 100%\nephem-3.7.7.1        | 722 KB    | ################################################################################ | 100%\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n(python3) sh-4.2$\n(python3) sh-4.2$\n(python3) sh-4.2$\n(python3) sh-4.2$ python\nPython 3.6.7 | packaged by conda-forge | (default, Feb 28 2019, 09:07:38)\n[GCC 7.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import fbprophet\n&gt;&gt;&gt; \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":93.78,
        "Solution_score_count":6.0,
        "Solution_sentence_count":121.0,
        "Solution_word_count":491.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"import error with fbprophet"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":72.1441666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Challenge_closed_time":1652640252000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1652380533000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the `ui` command in `kedro mlflow` is not using the options specified in the `mlflow.yml` file. The expected result is for the MLflow UI to open on port 5001, but it opens on the default port 5000. The user's environment includes `kedro` version 0.17.0, `kedro-mlflow` version 0.6.0, Python version 3.6.8, and Windows operating system. The solution is to pass the arguments in the command.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":21.0,
        "Challenge_repo_issue_count":414.0,
        "Challenge_repo_star_count":145.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":72.1441666667,
        "Challenge_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":185,
        "Discussion_body":"Closed by #313 ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect MLflow UI port"
    },
    {
        "Answerer_created_time":1511812251067,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":98.2814944445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I just started with using the Azure Machine Learning Services and ran into this problem. Creating a local environment and deploying my model to localhost works perfectly fine. \nCan anyone identify what could have caused this error, because i do not know where to start..<\/p>\n\n<p>I tried to create a cluster for Location \"eastus2\" aswell, which caused the same error.\nThank you very much in advance!<\/p>\n\n<p>Btw, the ressource group and ressources are being created into my azure account.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wYwJD.png\" rel=\"nofollow noreferrer\">Image of error<\/a><\/p>",
        "Challenge_closed_time":1511812251067,
        "Challenge_comment_count":1,
        "Challenge_created_time":1511458774783,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while creating a cluster environment for Azure Machine Learning Services. The error message reads \"Failed to get scoring front-end info\". The user has tried creating a cluster for Location \"eastus2\" as well, but the error persists. The user is seeking help to identify the cause of the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47460981",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":98.1878566667,
        "Challenge_title":"Error when creating cluster environment for azureML: \"Failed to get scoring front-end info\"",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":381.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1511458091352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Ashvin [MSFT]<\/p>\n\n<p>Sorry to hear that you were facing issues. We checked logs on our side using the info you provided in the screenshot. The cluster setup failed because there weren't enough cores to fit AzureML and system components in the cluster. You specified agent-vm-size of D1v2 which has 1 CPU core. By default we create 2 agents so total cores were 2. To resolve, can you please try creating a new cluster without specifying agent size? Then AzureML will create 2 agents of D3v2 which is 8 cores total. This should fit the AzureML and system components and leave some room for you to deploy your services. <\/p>\n\n<p>If you wish a bigger cluster you could specify agent-count along with agent-vm-size to appropriately size your cluster but please have minimum total of 8 cores with each individual VM >= 2 cores to ensure cluster works smoothly. Hope this helps.<\/p>\n\n<p>We are working on our side to add error handling to ensure request fails with clear error message. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1511812588163,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":11.99,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":168.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed to get front-end info"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6838888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nI am trying to launch an endpoint locally, to do couple inferences from my dev notebook (without having to wait for instanciation time of actual endpoint or batch training). I am running the following code:\n\n    # get trained model from s3\n    trained_S2S = SM.model.Model(\n        image=seq2seq,\n        model_data=('s3:\/\/XXXXXXXXXXXXX\/'\n            + 'output\/seq2seq-2018-07-30-16-55-12-521\/output\/model.tar.gz'),\n        role=role) \n    \n    S2S = trained_S2S.deploy(1, instance_type='local')    \n\nI get the following error (several hundreds of lines repeated):\n\n    WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa5c61eaac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': \/ping \n`RuntimeError: Giving up, endpoint: seq2seq-2018-08-01-14-18-06-555 didn't launch correctly`",
        "Challenge_closed_time":1533135863000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533133401000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"RuntimeError: Giving up, endpoint: didn't launch correctly\" error while trying to launch an endpoint locally to perform inferences from their dev notebook. The error message shows that the connection was broken and the endpoint failed to launch correctly.",
        "Challenge_last_edit_time":1668556657875,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU54PWM3V9QoybCiO5GvB03g\/sagemaker-local-deployment-runtimeerror-giving-up-endpoint-didn-t-launch-correctly",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":13.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.6838888889,
        "Challenge_title":"Sagemaker local deployment: \"RuntimeError: Giving up, endpoint: didn't launch correctly\"",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":431.0,
        "Challenge_word_count":102,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Currently, SageMaker local is supported only for SageMaker framework containers (MXNet, TensorFlow, PyTorch, Chainer and Spark) and not for the Builtin algorithms",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925544815,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"endpoint launch failure"
    },
    {
        "Answerer_created_time":1405882600928,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":115.0,
        "Challenge_adjusted_solved_time":0.3938825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an existing project, cloned with <code>git clone<\/code>.<\/p>\n\n<p>After I <code>pip install kedro<\/code> I can run <code>kedro info<\/code> fine but I dont seem to have access to the projects CLI for example if I try to run<code>kedro install<\/code> I get the following error:<\/p>\n\n<pre><code>Usage: kedro [OPTIONS] COMMAND [ARGS]...\nTry 'kedro -h' for help.\n\nError: No such command 'install'.\n<\/code><\/pre>\n\n<p>Any clues on what to do for existing projects are much appreciated.<\/p>\n\n<p>Not sure if this matters but I am working inside a conda environment which is inside a docker container.<\/p>",
        "Challenge_closed_time":1589565622150,
        "Challenge_comment_count":2,
        "Challenge_created_time":1589564204173,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has an existing project cloned with git and has installed Kedro using pip. While the user can run \"kedro info\" without any issues, they are unable to access the project's CLI and receive an error message when attempting to run \"kedro install\". The user is seeking guidance on how to access the CLI for existing projects. The user is working within a conda environment inside a docker container.",
        "Challenge_last_edit_time":1589721006283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61825202",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3938825,
        "Challenge_title":"Accessing Kedro CLI from an existing project",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1175.0,
        "Challenge_word_count":97,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1387377887347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Edinburgh, United Kingdom",
        "Poster_reputation_count":1240.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>Project CLIs are available if you run <code>kedro<\/code> at your Kedro project directory. <\/p>\n\n<ol>\n<li><p>Run <code>kedro new<\/code> to create a Kedro project<\/p><\/li>\n<li><p><code>cd &lt;your-kedro-project&gt;<\/code><\/p><\/li>\n<li><p><code>kedro<\/code> at the project directory<\/p><\/li>\n<\/ol>\n\n<p>And you should see the project level CLIs<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Also for your existing project, check if you have <code>kedro_cli.py<\/code> at your project directory.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":8.23,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Kedro",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to access CLI"
    },
    {
        "Answerer_created_time":1565022238448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":370.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":3.4553277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to be able to debug a running <code>entry_script.py<\/code> script in VSCode. This code runs in the container created through <code>az ml deploy<\/code> with its own docker run command. This is a local deployment so I'm using a deployment config that looks like this:<\/p>\n\n<pre><code>{\n    \"computeType\": \"LOCAL\",\n    \"port\": 32267\n}\n<\/code><\/pre>\n\n<p>I was thinking about using <code>ptvsd<\/code> to set up a VSCode server but I need to also expose\/map the 5678 port in addition to that 32267 port for the endpoint itself. So it's not clear to me how to map an additional exposed port (typically using the <code>-p<\/code> or <code>-P<\/code> flags in the <code>docker run<\/code> command). <\/p>\n\n<p>Sure, I can <code>EXPOSE<\/code> it in the <code>extra_dockerfile_steps<\/code> configuration but that won't actually map it to a host port that I can connect to\/attach to in VSCode.<\/p>\n\n<p>I tried to determine the run command and maybe modify it but I couldn't find out what that run command is. If I knew how to run the image that's created through AzureML local deployment then I could modify these flags. <\/p>\n\n<p>Ultimately it felt too hacky - if there was a more supported way through <code>az ml deploy<\/code> or through the deployment configuration that would be preferred.<\/p>\n\n<p>This is the code I'm using at the start of the entry_script to enable attachment via <code>ptvsd<\/code>: <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># 5678 is the default attach port in the VS Code debug configurations\nprint(\"Waiting for debugger attach\")\nptvsd.enable_attach(address=('localhost', 5678), redirect_output=True)\n<\/code><\/pre>",
        "Challenge_closed_time":1566419274223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566406835043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to debug a running script in VSCode that runs in a container created through \"az ml deploy\" with its own docker run command. The user wants to expose\/map an additional port (5678) in addition to the endpoint port (32267) for the VSCode server using \"ptvsd\", but is unsure how to map an additional exposed port using the \"-p\" or \"-P\" flags in the \"docker run\" command. The user tried to modify the run command but couldn't find it and is looking for a more supported way through \"az ml deploy\" or the deployment configuration.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57596224",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":3.4553277778,
        "Challenge_title":"How to expose port from locally-deployed AzureML container?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":206.0,
        "Challenge_word_count":250,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254279877887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA",
        "Poster_reputation_count":153.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>Unfortunately az ml deploy local doesn't support binding any ports other then the port hosting the scoring server. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":1.53,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"map additional exposed port"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":41.0340352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use a Jupyter Notebook instance on Sagemaker to run a code that took around 3 hours to complete. Since I pay for hour use, I would like to automatically \"Close and Halt\" the Notebook as well as stop the \"Notebook instance\" after running that code. Is this possible?<\/p>",
        "Challenge_closed_time":1568364157240,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568216434713,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a Jupyter code that can automatically close and halt the Notebook instance on Sagemaker and stop it after running a code that takes around 3 hours to complete, in order to save on hourly usage costs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57892580",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":4.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":41.0340352778,
        "Challenge_title":"Is there a Jupyter code that I can use \"to stop\" a Notebook Instances on Sagemaker, after running any code?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1393.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Jupyter Notebook are predominantly designed for exploration and\n   development. If you want to launch long-running or scheduled jobs on\n   ephemeral hardware, it will be a much better experience to use the\n   training API, such as the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\"><code>create_training_job<\/code><\/a> in boto3 or the\n   <code>estimator.fit()<\/code> of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>. The code passed to training jobs\n   can be completely arbitrary - not necessarily ML code - so whatever\n   you write in jupyter could likely be scheduled and ran in those\n   training jobs. See the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">random forest sklearn demo here<\/a> for an\n   example. That being said, if you still want to programmatically shut\n   down a SageMaker notebook instance, you can use that boto3 call:<\/p>\n\n<pre><code>import boto3\n\nsm = boto3.client('sagemaker')\nsm.stop_notebook_instance(NotebookInstanceName='string')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.9,
        "Solution_reading_time":16.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"automatically close Sagemaker Notebook"
    },
    {
        "Answerer_created_time":1431525955023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cherry Hill, NJ, United States",
        "Answerer_reputation_count":2069.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":8487.4160213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use the mlflow databricks integration, specifically the tracking API. Normally, I can view past runs info in the handy sidebar of a notebook, as you can see <a href=\"https:\/\/i.stack.imgur.com\/JWY1c.png\" rel=\"nofollow noreferrer\">here<\/a> and which I got from the <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/quick-start.html#mlflow-mlflow-quick-start-python\" rel=\"nofollow noreferrer\">tutorial<\/a>. However, what I want now is to use multiple notebooks to send runs to the same experiment. Additionally, I would like to view the results of all these common runs in each of the notebooks. To do this, I need to change the (default) experiment tracked by the \"runs\" tab. <\/p>\n\n<p>Ultimately, my question boils down to the following: how can I set the experiment being tracked by the \"runs\" tab? I have tried using <code>mlflow.set_tracking_uri<\/code> and <code>mlflow.set_experiment(mlflow_experiment_name)<\/code><\/p>",
        "Challenge_closed_time":1566518807736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562089677383,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use the mlflow databricks integration and wants to send runs to the same experiment using multiple notebooks. However, the default experiment tracked by the \"runs\" tab needs to be changed to view the results of all these common runs in each of the notebooks. The user is seeking help to set the experiment being tracked by the \"runs\" tab.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56857709",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":12.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1230.3139869444,
        "Challenge_title":"How to change experiment being tracked by Databricks \"runs\" tab?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":257.0,
        "Challenge_word_count":131,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464482523396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Canada",
        "Poster_reputation_count":2168.0,
        "Poster_view_count":1631.0,
        "Solution_body":"<p>I don't believe this is possible today, as the design choice is to associate the runs tab to the notebook experiment.  From the <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/tracking.html#notebook-experiments\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>Every Python and R notebook in a Databricks workspace has its own experiment. When you use MLflow in a notebook, it records runs in the notebook experiment.<\/p>\n<p>A notebook experiment shares the same name and ID as its corresponding notebook. The notebook ID is the numerical identifier at the end of a Notebook URL.<\/p>\n<\/blockquote>\n<p>You can create experiments independent of the notebook experiment and log runs to it from different sources.  You'll still have to open up the tracking UI to explore the results though.<\/p>\n<p>In other words, you can send multiple runs from different notebooks to the same experiment, but today you cannot log multiple runs to the 'Runs' tab in a specific notebook.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1592644375060,
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":12.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":144.0,
        "Tool":"MLflow",
        "Challenge_type":"inquiry",
        "Challenge_summary":"change tracked experiment in mlflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":117.2208333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n```shell\r\ntransformer: 4.17.0\r\ntorch: 1.10.2\r\n\r\nPlatform: Sagemaker Deep Learning Container\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n@NielsRogge\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nThe error only comes when training on Sagemaker using Huggingface.\r\n\r\nScripts to start training on Sagemaker:\r\n\r\nFolder organization:\r\n```\r\n.\/\r\n----sg_training.py\r\n----scripts\r\n-------requirements.txt\r\n-------train.py \r\n```\r\n\r\nsg_training.py:\r\n```\r\nimport boto3\r\nimport sagemaker\r\nfrom sagemaker.huggingface import HuggingFace\r\n\r\nif __name__ == \"__main__\":\r\n    iam_client = boto3.client(...)\r\n\r\n    role = iam_client.get_role(...)['Role']['Arn']\r\n    sess = sagemaker.Session()\r\n\r\n    sagemaker_session_bucket = 's3-sagemaker-session'\r\n\r\n    hyperparameters = {'epochs': 20,\r\n                       'train_batch_size': 1,\r\n                       'model_name': \"microsoft\/layoutxlm-base\",\r\n                       'output_dir': '\/opt\/ml\/model\/',\r\n                       'checkpoints': '\/opt\/ml\/checkpoints\/',\r\n                       'combine_train_val': True,\r\n                       'exp_tracker': \"all\",\r\n                       'exp_name': 'Sagemaker Training'\r\n                       }\r\n\r\n    huggingface_estimator = HuggingFace(entry_point='train.py',\r\n                                        source_dir='scripts',\r\n                                        instance_type='ml.p3.2xlarge',\r\n                                        instance_count=1,\r\n                                        role=role,\r\n                                        transformers_version='4.17.0',\r\n                                        pytorch_version='1.10.2',\r\n                                        py_version='py38',\r\n                                        hyperparameters=hyperparameters,\r\n                                        environment={'HF_TASK': 'text-classification'},\r\n                                        code_location='s3:\/\/dummy_code_location')\r\n\r\n    huggingface_estimator.fit()\r\n```\r\n\r\nEntrypoint scripts folder:\r\n\r\n\r\nrequirements.txt:\r\n```\r\ngit+https:\/\/github.com\/facebookresearch\/detectron2.git\r\n```\r\n\r\ntrain.py:\r\n```\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport sys\r\n\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n\r\n\r\ndef run():\r\n    model = LayoutLMv2ForSequenceClassification.from_pretrained('microsoft\/layoutxlm-base',\r\n                                                                num_labels=5)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--epochs\", type=int, default=3)\r\n    parser.add_argument(\"--exp_name\", type=str, default=\"Sagemaker Training\")\r\n    parser.add_argument(\"--train-batch-size\", type=int, default=2)\r\n    parser.add_argument(\"--eval-batch-size\", type=int, default=1)\r\n    parser.add_argument(\"--warmup_steps\", type=int, default=500)\r\n    parser.add_argument(\"--model_name\", type=str)\r\n    parser.add_argument(\"--learning_rate\", type=str, default=1e-5)\r\n    parser.add_argument(\"--combine_train_val\", type=bool, default=False)\r\n    # Data, model, and output directories\r\n    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\r\n    parser.add_argument(\"--checkpoints\", type=str, default=\"\/opt\/ml\/checkpoints\")\r\n    parser.add_argument(\"--model-dir\", type=str, default='\/opt\/ml\/code\/model')\r\n    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\r\n    args, _ = parser.parse_known_args()\r\n\r\n    logger = logging.getLogger(__name__)\r\n    logging.basicConfig(\r\n        level=logging.getLevelName(\"INFO\"),\r\n        handlers=[logging.StreamHandler(sys.stdout)],\r\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r\n    )\r\n\r\n    run()\r\n\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n```shell\r\nHere the log on the error from AWS Cloud Watch:\r\n\r\nInvoking script with the following command:\r\n\/opt\/conda\/bin\/python3.8 train.py --checkpoints \/opt\/ml\/checkpoints\/ --combine_train_val True --epochs 20 --exp_name Sagemaker_Training_doc_cls --exp_tracker all --model_name microsoft\/layoutxlm-base --output_dir \/opt\/ml\/model\/ --train_batch_size 1\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2777, in _get_module\r\nreturn importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"\/opt\/conda\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\nFile \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nFile \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/models\/layoutlmv2\/modeling_layoutlmv2.py\", line 48, in <module>\r\nfrom detectron2.modeling import META_ARCH_REGISTRY\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/modeling\/__init__.py\", line 2, in <module>\r\nfrom detectron2.layers import ShapeSpec\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/__init__.py\", line 2, in <module>\r\nfrom .batch_norm import FrozenBatchNorm2d, get_norm, NaiveSyncBatchNorm, CycleBatchNormList\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/batch_norm.py\", line 4, in <module>\r\n    from fvcore.nn.distributed import differentiable_all_reduce\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/__init__.py\", line 4, in <module>\r\n    from .focal_loss import (\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 52, in <module>\r\n    sigmoid_focal_loss_jit: \"torch.jit.ScriptModule\" = torch.jit.script(sigmoid_focal_loss)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_recursive.py\", line 838, in try_compile_fn\r\nreturn torch.jit.script(fn, _rcb=rcb)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\nRuntimeError: \r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 6, in <module>\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2768, in __getattr__\r\nvalue = getattr(module, name)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2767, in __getattr__\r\nmodule = self._get_module(self._class_to_module[name])\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2779, in _get_module\r\nraise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.layoutlmv2.modeling_layoutlmv2 because of the following error (look up to see its traceback):\r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\n\r\n```\r\n```\r\n",
        "Challenge_closed_time":1656429784000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1656007789000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue where \"pip install sacremoses>=0.0.50\" breaks on SageMaker Studio. This is likely to break \"pip install transformers\" on SageMaker Studio at some point. The user has provided a workaround by suggesting to install \"sacremoses==0.0.49\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17855",
        "Challenge_link_count":1,
        "Challenge_participation_count":8,
        "Challenge_readability":16.7,
        "Challenge_reading_time":107.62,
        "Challenge_repo_contributor_count":437.0,
        "Challenge_repo_fork_count":20818.0,
        "Challenge_repo_issue_count":23745.0,
        "Challenge_repo_star_count":102900.0,
        "Challenge_repo_watch_count":1019.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":81,
        "Challenge_solved_time":117.2208333333,
        "Challenge_title":"LayoutLMv2 training on sagemaker error: undefined value has_torch_function_variadic",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":580,
        "Discussion_body":"cc @philschmid (hope I am tagging correctly) @Natlem could you try adding `debugger_hook_config=False` to the `HuggingFace` estimator? \r\n\r\n```python\r\n    huggingface_estimator = HuggingFace(entry_point='train.py',\r\n                                        source_dir='scripts',\r\n                                        instance_type='ml.p3.2xlarge',\r\n                                        instance_count=1,\r\n                                        role=role,\r\n                                        transformers_version='4.17.0',\r\n                                        pytorch_version='1.10.2',\r\n                                        py_version='py38',\r\n                                        hyperparameters=hyperparameters,\r\n                                        environment={'HF_TASK': 'text-classification'},\r\n                                        code_location='s3:\/\/dummy_code_location',\r\n                                        debugger_hook_config=False,\r\n)\r\n``` Hi @philschmid ,\r\n\r\nAdded the `debugger_hook_config=False`, the error is gone now. Thanks ! Awesome, closing the issue.  Feel free to reopen if you have more issues. @Natlem i forwarded the error to the AWS team to be able use the debugger soon.  @philschmid  Thanks ! @philschmid do you have any idea why this solves the problem? Is it documented by AWS anywhere?\r\n\r\nSagemaker Debugger has cost me multiple days of time in the mysterious problems it produces. Far more than anything else on Sagemaker. I posted an issue on awslabs about this awhile back and never got a reply. I would really like to know what is going on here\r\n\r\n**For anyone encountering this while using a HyperparameterTuner**\r\nPassing `debugger_hook_config=False` in the `Estimator` will not the solve the problem. Further, passing `environment={'USE_SMDEBUG':0}` also will not solve the problem. Somehow these settings never make it to a tuner's constituent training jobs.\r\n\r\nThe only way to solve it is to set `ENV USE_SMDEBUG=\"0\"` in the docker container that will be running the constituent training jobs. > Somehow these settings never make it to a tuner's constituent training jobs.\r\n\r\nAre you using the `HuggingFace` estimator or the `HyperparameterTuner`",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"pip install breaks on SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":122.8697222222,
        "Challenge_answer_count":0,
        "Challenge_body":"On chart release v0.13.2 the default value for projectOperator.mlflow.image.tag is set to latest when it should be set to v0.13.2.\r\n\r\nCheck values.yml:\r\n\r\n```yaml\r\nprojectOperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullPolicy: IfNotPresent\r\n  mlflow:\r\n    image:\r\n      repository: konstellation\/mlflow\r\n      tag: latest\r\n      pullPolicy: IfNotPresent\r\n    volume:\r\n      storageClassName: standard\r\n      size: 1Gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullPolicy: IfNotPresent\r\n```",
        "Challenge_closed_time":1635871931000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635429600000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue where all MLflow experiments are visible to any user. To address this, they suggest creating a separate instance of MLflow for each project. They propose several steps to implement this solution, including creating a project operator, updating the KDL APP API, and adding the operator to the KDL server helm chart. They also plan to publish the project-operator in Docker Hub using GitHub workflows.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/623",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":15.5,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":933.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":122.8697222222,
        "Challenge_title":"Project operator mlflow image tag is set to \"latest\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"experiments visible to all users"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":420.1906525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Please see the screenshots below. Once it said terminated but without reason:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/140829-screenshot-2021-10-13-221133.png?platform=QnA\" alt=\"140829-screenshot-2021-10-13-221133.png\" \/>    <br \/>\nThe other time there was nothing just stopped:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/140846-screenshot-2021-10-13-215523.png?platform=QnA\" alt=\"140846-screenshot-2021-10-13-215523.png\" \/>    <\/p>",
        "Challenge_closed_time":1635818701492,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634306015143,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's script is stopping without any explanation or error message, as shown in the attached screenshots. The script has terminated once without any reason and another time it just stopped abruptly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/592153\/my-script-stops-running-without-any-message-explai",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.6,
        "Challenge_reading_time":7.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":420.1906525,
        "Challenge_title":"my script stops running without any message explaining the reason",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>Hope you have solved this issue and we are sorry not seeing your response. Since this issue happened without any error details, support ticket would be the best way to debug that. Please let me know if you still need that. Thanks.  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":3.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"script stops abruptly"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.6766972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Started up a STANDARD_D11_V2 cluster to run some notebooks on.<\/p>\n<p>Wanted to use json_normalize from pandas: <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.json_normalize.html\">https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.json_normalize.html<\/a> and I get the below error:<\/p>\n<pre><code>AttributeError: module 'pandas' has no attribute 'json_normalize'\n<\/code><\/pre>\n<p>Pandas seems to be out of date. Checked the installed version of pandas:<\/p>\n<pre><code>$ python\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import pandas\n\n&gt;&gt;&gt; pandas.__version__\n\n'0.23.4\n<\/code><\/pre>\n<p>Pandas is indeed out of date, the latest version is v1.1.1. Fired up a terminal to run:<\/p>\n<pre><code>conda update --all\n<\/code><\/pre>\n<p>On the azureml_py36 environment which I had selected to run the notebook on. It hangs on:<\/p>\n<pre><code>Solving environment: \/\n<\/code><\/pre>\n<p>Went to update conda to see if that would help:<\/p>\n<pre><code>conda update conda\n<\/code><\/pre>\n<p>But I get this error:<\/p>\n<pre><code>PackageNotInstalledError: Package is not installed in prefix.\n  prefix: \/anaconda\/envs\/azureml_py36\n  package name: conda\n<\/code><\/pre>\n<p>Which leads me to think this is not a typical installation of conda.<\/p>\n<p>Would like to run the most up to date packages on Azure Machine Learning to replicate the local environment I have setup. Does anyone know how to do this?<\/p>",
        "Challenge_closed_time":1598622188187,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598608952077,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with updating out of date packages on an Azure Machine Learning Compute instance. They have identified that Pandas is out of date and attempted to update it using \"conda update --all\" command, but it hangs on \"Solving environment\". The user also tried to update conda but received an error message. They are seeking help to run the most up-to-date packages on Azure Machine Learning to replicate their local environment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/80212\/update-out-of-date-packages-on-azure-machine-learn",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":21.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":3.6766972222,
        "Challenge_title":"Update out of date packages on Azure Machine Learning Compute instance",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":200,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@philmariusnew-9791 I have tried the above steps and the installation completed successfully for conda. But when we upgrade pandas azureml package has a dependency  so it cannot use version v1.1.1     <\/p>\n<p>I have went ahead and upgrade the pandas version but there is a warning as seen below:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/21216-image.png?platform=QnA\" alt=\"21216-image.png\" \/>    <\/p>\n<p>We would recommend to use the package that is compatible with azureml so your environment setup works as expected.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.8,
        "Solution_reading_time":6.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to update packages"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.1720713889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy an R inference script to Azure ML Service Endpoint as an Azure Container Instance. I have made the following steps:<\/p>\n<ul>\n<li>   created a custom Docker image from scratch and pushed it to the Azure Container Registry (associated with AML Workspace)<\/li>\n<li>   registered a custom environment in AML Workspace, based on the image in ACR<\/li>\n<li>   deployed R entry script (just a simple hello world script with init() and run() functions defined)\n<ul>\n<li>   the inference configuration uses the custom AML environment<\/li>\n<li>   deployment is made with Azure ML R SDK<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>The container instance is created, but the endpoint startup runs into error. Here is the output from the container instance:<\/p>\n<pre><code>2020-10-16T12:56:21,639812796+00:00 - gunicorn\/run \n2020-10-16T12:56:21,639290594+00:00 - iot-server\/run \n2020-10-16T12:56:21,640405198+00:00 - rsyslog\/run \n2020-10-16T12:56:21,735291424+00:00 - nginx\/run \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2020-10-16T12:56:23,736657191+00:00 - iot-server\/finish 1 0\n2020-10-16T12:56:23,834747728+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 20.0.4\nListening at: http:\/\/127.0.0.1:31311 (11)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 38\n\/bin\/bash: \/root\/miniconda3\/lib\/libtinfo.so.6: no version information available (required by \/bin\/bash)\nSPARK_HOME not set. Skipping PySpark Initialization.\nException in worker process\nTraceback (most recent call last):\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 43, in &lt;module&gt;\n    from azureml.api.exceptions.ClientSideException import ClientSideException\nModuleNotFoundError: No module named 'azureml.api'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/util.py&quot;, line 383, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.8\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1014, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 671, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 783, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n    from app import main\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 45, in &lt;module&gt;\n    from azure.ml.api.exceptions.ClientSideException import ClientSideException\nModuleNotFoundError: No module named 'azure.ml'\nWorker exiting (pid: 38)\nShutting down: Master\nReason: Worker failed to boot.\n2020-10-16T12:56:39,434787859+00:00 - gunicorn\/finish 3 0\n2020-10-16T12:56:39,435715063+00:00 - Exit code 3 is not normal. Killing image.\n<\/code><\/pre>\n<p>How do I install the azureml.api dependency, which can not be found? It doesn't seem to be part of the Azure ML SDK. I have installed the following dependencies in my Dockerfile:<\/p>\n<pre><code>RUN apt-get -y install python3-flask python3-rpy2 python3-azure python3-applicationinsights\nRUN pip install azureml-core\n<\/code><\/pre>\n<p>I also have Miniconda installed. Pip refers to Miniconda's pip.<\/p>\n<p>Or, is this dependency available to install at all? Should I use some pre-defined AML environment as the base Docker image? (Note: I am currently using bare FROM: ubuntu). Suggestions how to find and use the base images are also welcome, since this is not documented very well.<\/p>",
        "Challenge_closed_time":1603104240927,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602855221470,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while deploying an R inference script to Azure ML Service Endpoint as an Azure Container Instance. The container instance is created, but the endpoint startup runs into an error due to the missing 'azureml.api' module. The user has installed the required dependencies in the Dockerfile, but the issue persists. The user is seeking suggestions on how to find and use the base images and whether to use a pre-defined AML environment as the base Docker image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/129038\/r-model-deployment-with-custom-docker-image-module",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":62.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":69.1720713889,
        "Challenge_title":"R model deployment with custom Docker image: \"ModuleNotFoundError: No module named 'azureml.api'\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":500,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c02d5983-5261-45cb-9bb0-3e5f19b42ae9\">@Lauri Lehman  <\/a> Thanks for the question. Here is the <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fmedium.com%2Fmicrosoftazure%2Fhow-to-create-custom-docker-base-images-for-azure-machine-learning-environments-86aa4c7bc7b9&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7C49414af813754671ec7908d83863d1b5%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637321350687268940&amp;sdata=f5ebitkOUNxY8v7cOS2vFy12mBfuP%2BJVVzLbhAKVyXE%3D&amp;reserved=0\">document<\/a>  to Create Custom Docker Base Images for Azure Machine Learning Environments for R people.    <br \/>\nWe have used the AzureML <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.r_script_step.rscriptstep?view=azure-ml-py\">RScriptStep<\/a>  pipeline feature which allows you to point to CRAN or Github or custom URLS, but this requires authoring the pipeline in python or YAML.. In R You can also  these arguments in the Azuremlsdk R estimator function:  <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html\">https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html<\/a>    <br \/>\nAnother option that are not available through conda install as part of the R script with install.packages(\u201cpath\/*.tar.gz\u201d, repos=NULL))    <\/p>\n<p>One of the challenges is that the build at runtime can take a while to prepare the environment.  R likes to compile packages on Linux environments and a large package could have lots of dependencies which would take a while.  This is an R on Linux\/PaaS thing, rather than specific to AzureML    <\/p>\n<p>To make start up fast we created a custom docker image where you can tightly control the image ahead of runtime.  If you want to go in this direction you can find an example Dockerfile to get you started here..    <br \/>\n<a href=\"https:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile\">https:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile<\/a>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.4,
        "Solution_reading_time":27.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":191.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing 'azureml.api' module"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":98.9333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"I was able to create a deep learning VM from the marketplace and when I open up the VM instance in the Console I see a metadata tag called `proxy-url` which has a format like `https:\/\/[alphanumeric string]-dot-us-central1.notebooks.googleusercontent.com\/lab`\n\nClicking on that link takes me to a JupyterLab UI that is running on my VM. Amazing! Unfortunately, when I try opening that link on an incognito window, I'm asked to sign in. If I sign in, I get a 403 forbidden.\n\nMy question now is, how can I make that link available to someone else?",
        "Challenge_closed_time":1643637480000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643281320000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a deep learning VM from the marketplace and found a metadata tag called `proxy-url` that takes them to a JupyterLab UI running on their VM. However, when they try to open the link on an incognito window, they are asked to sign in and get a 403 forbidden error. The user wants to know how to make the link available to someone else.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Make-deep-learning-VM-JupyterLab-publicly-available\/m-p\/386576#M191",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":98.9333333333,
        "Challenge_title":"Make deep learning VM JupyterLab publicly available?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":99,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi gopalv\n\nAs far as I understand, it sounds like your Jupyter Notebook isn't configured for remote access. since it doesn't work when trying to access it from the incognito window with a 403 error.\n\nYou can try looking here and here for details on how to set up a publicly accessible\/remote access notebook. There are additional troubleshooting steps in our documentation here as well.\n\nHope this helps!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":73.0,
        "Tool":"Vertex AI",
        "Challenge_type":"inquiry",
        "Challenge_summary":"share JupyterLab link"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":375.8113888889,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThank you for creating such a helpful tool!\r\nThe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the MLFlow experiment artifacts dir. I think the issue is with inconsistent naming for the saved png for certain plot types.\r\nThank you for your help!\r\n<!--\r\n-->\r\n\r\n**To Reproduce**\r\n<!--\r\nAdd a Minimal, Complete, and Verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nIf the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=False, \r\n      silent=True,\r\n      verbose=False,\r\n      # for MLFlow logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = True, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**Expected behavior**\r\n<!--\r\n-->\r\nI expect ALL of the plot types to be logged under the MLFlow artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nHowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\r\nI think the issue is with inconsistent naming of the file. Here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:INFO:Saving 'calibration.png'\r\n2021-10-11 19:03:20,064:INFO:Visual Rendered Successfully\r\n2021-10-11 19:03:20,213:INFO:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:WARNING:[Errno 2] No such file or directory: 'Calibration Curve.png'\r\n```\r\nSo you can see that it is looking for 'Calibration Curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**Versions**\r\nPython 3.8.11\r\n\r\n<!--\r\nPlease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\nPycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1635405096000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1634052175000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a bug while trying to convert an experiment using the aim convert mlflow command with the experiment ID. The process failed with an error message. However, using the experiment name instead of the ID worked successfully. The expected behavior was to convert the experiment by ID. The user's environment included Aim Version 3.6, Python 3.8.1, pip3, and Ubuntu 20.04.3 LTS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1674",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":27.37,
        "Challenge_repo_contributor_count":105.0,
        "Challenge_repo_fork_count":1603.0,
        "Challenge_repo_issue_count":2975.0,
        "Challenge_repo_star_count":7363.0,
        "Challenge_repo_watch_count":128.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":375.8113888889,
        "Challenge_title":"[BUG] some types plot types are not getting saved to the MLFlow experiment artifacts dir",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":268,
        "Discussion_body":"@ejohnson-amerilife Thank you so much for bringing this up. Would you like to submit a PR for this? ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed experiment conversion by ID"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.4035702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Working on deployment of 170 ml models using ML studio and azure Kubernetes service which is referred on the  below doc  link &quot;https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-deploy-azure-kubernetes-service.md&quot;.     <\/p>\n<p>We are training the model using python script with the custom environment and we are registering the ml model on the  Azure ML services. Once we register the mode we are deploying it on the AKS by using the container images.     <\/p>\n<p>While deploying the ML model we are able to deploy up 10 to 11 models per pods for each Node in AKS. When we try to deploy the model on the same node we are getting deployment timeout error and we are getting the below error message.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/129300-image-2021-09-04t13-25-12-512z.png?platform=QnA\" alt=\"129300-image-2021-09-04t13-25-12-512z.png\" \/>    <\/p>",
        "Challenge_closed_time":1630758725400,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630746472547,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to deploy 170 ml models using ML studio and Azure Kubernetes service. They are able to deploy up to 10-11 models per pod for each node in AKS, but when they try to deploy more models on the same node, they encounter a deployment timeout error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/540001\/how-many-models-can-be-deployed-in-single-node-in",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.4035702778,
        "Challenge_title":"how many models can be deployed in single node in azure kubernetes service?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=f023f08d-7d4a-4ac5-ba62-e9d37f7e7c70\">@suvedharan  <\/a>     <\/p>\n<p>The number of models to be deployed is limited to 1,000 models per deployment (per container).    <\/p>\n<p>Autoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">more details<\/a>    <\/p>\n<p>If the Answer is helpful, please click <code>Accept Answer<\/code> and <strong>up-vote<\/strong>, so that it can help others in the community looking for help on similar topics.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":9.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":86.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deployment timeout error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":431.4666666667,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThe product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nDSVM, DB\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\neverything runs\r\n\r\n### Other Comments\r\nquestion to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?\r\n\r\n",
        "Challenge_closed_time":1555413510000,
        "Challenge_comment_count":12,
        "Challenge_created_time":1553860230000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user needs to remove the reference to Azure ML SDK preview private index from an operationalize notebook as it is now available through regular PyPi as a GA product and preview versions are unsupported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/695",
        "Challenge_link_count":0,
        "Challenge_participation_count":12,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2749.0,
        "Challenge_repo_issue_count":1927.0,
        "Challenge_repo_star_count":15795.0,
        "Challenge_repo_watch_count":264.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":431.4666666667,
        "Challenge_title":"[BUG] Remove contrib from azureml",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Discussion_body":"azureml_hyperdrive_wide_and_deep notebook does**n't** use any azureml contrib modules. papermill PR in progress is using azureml.contrib.notebook. If it's not desired in the yaml file, I can install this package only inside the notebook. Will this work? mmm, yeah that would be a workaround.  Not in the Hyperdrive notebooks. since @jingyanwangms is the only one using it, can you take care of this issue in your PR? Do we want to require people to install things at the beginning of notebooks, though?  azureml.contrib.notebook is required for submitting notebook through aml. But if we don't want azureml.contrib to install as default in base yaml files, @heatherbshapiro what would you recommend doing here? @miguelgfierro Sure. I can do it in my PR. Hey guys\r\nI am getting an error while trying to run this line \"from azureml.contrib.notebook import NotebookRunConfig, AzureMLNotebookHandler\".\r\n**The error is ModuleNotFoundError: No module named 'azureml.contrib'** although I have installed azureml-contrib-notebook from pip. What should I do?\r\n HI @Raman1121 , which file in the code is this line in? I am trying to experiment with jupyter notebook on Azure through API calls by following the code snippet given here - \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-notebook\/azureml.contrib.notebook.azuremlnotebookhandler?view=azure-ml-py Well, that code is not related to the Recommenders GitHub repo. Most likely you have not configured your conda or python settings appropriately.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unsupported preview version"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.6105555556,
        "Challenge_answer_count":0,
        "Challenge_body":"### Contact Details [Optional]\n\n_No response_\n\n### System Information\n\nZenml == 0.10.0\n\n### What happened?\n\nZenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### Reproduction steps\n\n1. Create a SageMaker pipeline.\r\n2. Create a s3 artifact store.\r\n3. Run the pipeline\r\n\n\n### Relevant log output\n\n```shell\nCreating run for pipeline: mnist_pipeline\r\nCache enabled for pipeline mnist_pipeline\r\nUsing stack sagemaker_stack to run pipeline mnist_pipeline...\r\nStep importer has started.\r\nUsing cached version of importer.\r\nStep importer has finished in 0.045s.\r\nStep trainer has started.\r\nINFO:botocore.credentials:Found credentials in shared credentials file: ~\/.aws\/credentials\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:752 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    749 \u2502   \u2502   \u2502   \u2502   \u2502   params[\"CreateBucketConfiguration\"] = {          \u2502\r\n\u2502    750 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \"LocationConstraint\": region_name            \u2502\r\n\u2502    751 \u2502   \u2502   \u2502   \u2502   \u2502   }                                                \u2502\r\n\u2502 >  752 \u2502   \u2502   \u2502   \u2502   await self._call_s3(\"create_bucket\", **params)       \u2502\r\n\u2502    753 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(\"\")                            \u2502\r\n\u2502    754 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(bucket)                        \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:302 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    299 \u2502   \u2502   \u2502   except Exception as e:                                   \u2502\r\n\u2502    300 \u2502   \u2502   \u2502   \u2502   err = e                                              \u2502\r\n\u2502    301 \u2502   \u2502   err = translate_boto_error(err)                              \u2502\r\n\u2502 >  302 \u2502   \u2502   raise err                                                    \u2502\r\n\u2502    303 \u2502                                                                    \u2502\r\n\u2502    304 \u2502   call_s3 = sync_wrapper(_call_s3)                                 \u2502\r\n\u2502    305                                                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:282 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    279 \u2502   \u2502   additional_kwargs = self._get_s3_method_kwargs(method, *akwa \u2502\r\n\u2502    280 \u2502   \u2502   for i in range(self.retries):                                \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   try:                                                     \u2502\r\n\u2502 >  282 \u2502   \u2502   \u2502   \u2502   out = await method(**additional_kwargs)              \u2502\r\n\u2502    283 \u2502   \u2502   \u2502   \u2502   return out                                           \u2502\r\n\u2502    284 \u2502   \u2502   \u2502   except S3_RETRYABLE_ERRORS as e:                         \u2502\r\n\u2502    285 \u2502   \u2502   \u2502   \u2502   logger.debug(\"Retryable error: %s\", e)               \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:198 in _make_api_call                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   195 \u2502   \u2502   \u2502   'has_streaming_input': operation_model.has_streaming_inpu \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   'auth_type': operation_model.auth_type,                   \u2502\r\n\u2502   197 \u2502   \u2502   }                                                             \u2502\r\n\u2502 > 198 \u2502   \u2502   request_dict = await self._convert_to_request_dict(           \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   api_params, operation_model, context=request_context)     \u2502\r\n\u2502   200 \u2502   \u2502   resolve_checksum_context(request_dict, operation_model, api_p \u2502\r\n\u2502   201                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:246 in _convert_to_request_dict                            \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   243 \u2502                                                                     \u2502\r\n\u2502   244 \u2502   async def _convert_to_request_dict(self, api_params, operation_mo \u2502\r\n\u2502   245 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      context=None):                 \u2502\r\n\u2502 > 246 \u2502   \u2502   api_params = await self._emit_api_params(                     \u2502\r\n\u2502   247 \u2502   \u2502   \u2502   api_params, operation_model, context)                     \u2502\r\n\u2502   248 \u2502   \u2502   request_dict = self._serializer.serialize_to_request(         \u2502\r\n\u2502   249 \u2502   \u2502   \u2502   api_params, operation_model)                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:275 in _emit_api_params                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502                                                                 \u2502\r\n\u2502   273 \u2502   \u2502   event_name = (                                                \u2502\r\n\u2502   274 \u2502   \u2502   \u2502   'before-parameter-build.{service_id}.{operation_name}')   \u2502\r\n\u2502 > 275 \u2502   \u2502   await self.meta.events.emit(                                  \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   event_name.format(                                        \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   service_id=service_id,                                \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   \u2502   operation_name=operation_name),                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\hooks.py:29 in _emit                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   26 \u2502   \u2502   \u2502   if asyncio.iscoroutinefunction(handler):                   \u2502\r\n\u2502   27 \u2502   \u2502   \u2502   \u2502   response = await handler(**kwargs)                     \u2502\r\n\u2502   28 \u2502   \u2502   \u2502   else:                                                      \u2502\r\n\u2502 > 29 \u2502   \u2502   \u2502   \u2502   response = handler(**kwargs)                           \u2502\r\n\u2502   30 \u2502   \u2502   \u2502                                                              \u2502\r\n\u2502   31 \u2502   \u2502   \u2502   responses.append((handler, response))                      \u2502\r\n\u2502   32 \u2502   \u2502   \u2502   if stop_on_response and response is not None:              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\botoc \u2502\r\n\u2502 ore\\handlers.py:243 in validate_bucket_name                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    240 \u2502   \u2502   \u2502   'Invalid bucket name \"%s\": Bucket name must match '      \u2502\r\n\u2502    241 \u2502   \u2502   \u2502   'the regex \"%s\" or be an ARN matching the regex \"%s\"' %  \u2502\r\n\u2502    242 \u2502   \u2502   \u2502   \u2502   bucket, VALID_BUCKET.pattern, VALID_S3_ARN.pattern)) \u2502\r\n\u2502 >  243 \u2502   \u2502   raise ParamValidationError(report=error_msg)                 \u2502\r\n\u2502    244                                                                      \u2502\r\n\u2502    245                                                                      \u2502\r\n\u2502    246 def sse_md5(params, **kwargs):                                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nParamValidationError: Parameter validation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\run-sagemaker.py:87 in       \u2502\r\n\u2502 <module>                                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   84 \u2502   \u2502   trainer=trainer(),                                             \u2502\r\n\u2502   85 \u2502   \u2502   evaluator=evaluator(),                                         \u2502\r\n\u2502   86 \u2502   )                                                                  \u2502\r\n\u2502 > 87 \u2502   pipeline.run()                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\pipelines\\base_pipeline.py:489 in run                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   486 \u2502   \u2502   self._reset_step_flags()                                      \u2502\r\n\u2502   487 \u2502   \u2502   self.validate_stack(stack)                                    \u2502\r\n\u2502   488 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 489 \u2502   \u2502   return stack.deploy_pipeline(                                 \u2502\r\n\u2502   490 \u2502   \u2502   \u2502   self, runtime_configuration=runtime_configuration         \u2502\r\n\u2502   491 \u2502   \u2502   )                                                             \u2502\r\n\u2502   492                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\stack\\stack.py:595 in deploy_pipeline                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   pipeline=pipeline, runtime_configuration=runtime_configur \u2502\r\n\u2502   593 \u2502   \u2502   )                                                             \u2502\r\n\u2502   594 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 595 \u2502   \u2502   return_value = self.orchestrator.run(                         \u2502\r\n\u2502   596 \u2502   \u2502   \u2502   pipeline, stack=self, runtime_configuration=runtime_confi \u2502\r\n\u2502   597 \u2502   \u2502   )                                                             \u2502\r\n\u2502   598                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:212 in run                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   pipeline=pipeline, pb2_pipeline=pb2_pipeline              \u2502\r\n\u2502   210 \u2502   \u2502   )                                                             \u2502\r\n\u2502   211 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 212 \u2502   \u2502   result = self.prepare_or_run_pipeline(                        \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   sorted_steps=sorted_steps,                                \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   pipeline=pipeline,                                        \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                                \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\local\\local_orchestrator.py:68 in prepare_or_run_pipeline    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   65 \u2502   \u2502                                                                  \u2502\r\n\u2502   66 \u2502   \u2502   # Run each step                                                \u2502\r\n\u2502   67 \u2502   \u2502   for step in sorted_steps:                                      \u2502\r\n\u2502 > 68 \u2502   \u2502   \u2502   self.run_step(                                             \u2502\r\n\u2502   69 \u2502   \u2502   \u2502   \u2502   step=step,                                             \u2502\r\n\u2502   70 \u2502   \u2502   \u2502   \u2502   run_name=runtime_configuration.run_name,               \u2502\r\n\u2502   71 \u2502   \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:316 in run_step                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   313 \u2502   \u2502   # This is where the step actually gets executed using the     \u2502\r\n\u2502   314 \u2502   \u2502   # component_launcher                                          \u2502\r\n\u2502   315 \u2502   \u2502   repo.active_stack.prepare_step_run()                          \u2502\r\n\u2502 > 316 \u2502   \u2502   execution_info = self._execute_step(component_launcher)       \u2502\r\n\u2502   317 \u2502   \u2502   repo.active_stack.cleanup_step_run()                          \u2502\r\n\u2502   318 \u2502   \u2502                                                                 \u2502\r\n\u2502   319 \u2502   \u2502   return execution_info                                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:340 in _execute_step                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   337 \u2502   \u2502   start_time = time.time()                                      \u2502\r\n\u2502   338 \u2502   \u2502   logger.info(f\"Step `{pipeline_step_name}` has started.\")      \u2502\r\n\u2502   339 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 > 340 \u2502   \u2502   \u2502   execution_info = tfx_launcher.launch()                    \u2502\r\n\u2502   341 \u2502   \u2502   \u2502   if execution_info and get_cache_status(execution_info):   \u2502\r\n\u2502   342 \u2502   \u2502   \u2502   \u2502   logger.info(f\"Using cached version of `{pipeline_step \u2502\r\n\u2502   343 \u2502   \u2502   except RuntimeError as e:                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:528 in launch                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   525 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self._pipeline_runtime_spe \u2502\r\n\u2502   526 \u2502                                                                     \u2502\r\n\u2502   527 \u2502   # Runs as a normal node.                                          \u2502\r\n\u2502 > 528 \u2502   execution_preparation_result = self._prepare_execution()          \u2502\r\n\u2502   529 \u2502   (execution_info, contexts,                                        \u2502\r\n\u2502   530 \u2502    is_execution_needed) = (execution_preparation_result.execution_i \u2502\r\n\u2502   531 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    execution_preparation_result.contexts,   \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:388 in _prepare_execution                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   385 \u2502   \u2502   \u2502     output_dict=output_artifacts,                           \u2502\r\n\u2502   386 \u2502   \u2502   \u2502     exec_properties=exec_properties,                        \u2502\r\n\u2502   387 \u2502   \u2502   \u2502     execution_output_uri=(                                  \u2502\r\n\u2502 > 388 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_executor_output_uri(execu \u2502\r\n\u2502   389 \u2502   \u2502   \u2502     stateful_working_dir=(                                  \u2502\r\n\u2502   390 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_stateful_working_director \u2502\r\n\u2502   391 \u2502   \u2502   \u2502     tmp_dir=self._output_resolver.make_tmp_dir(execution.id \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\outputs_utils.py:172 in get_executor_output_uri       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   169 \u2502   \"\"\"Generates executor output uri given execution_id.\"\"\"           \u2502\r\n\u2502   170 \u2502   execution_dir = os.path.join(self._node_dir, _SYSTEM, _EXECUTOR_E \u2502\r\n\u2502   171 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    str(execution_id))                   \u2502\r\n\u2502 > 172 \u2502   fileio.makedirs(execution_dir)                                    \u2502\r\n\u2502   173 \u2502   return os.path.join(execution_dir, _EXECUTOR_OUTPUT_FILE)         \u2502\r\n\u2502   174                                                                       \u2502\r\n\u2502   175   def get_driver_output_uri(self) -> str:                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\d \u2502\r\n\u2502 sl\\io\\fileio.py:80 in makedirs                                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    77                                                                       \u2502\r\n\u2502    78 def makedirs(path: PathType) -> None:                                 \u2502\r\n\u2502    79   \"\"\"Make a directory at the given path, recursively creating parents \u2502\r\n\u2502 >  80   _get_filesystem(path).makedirs(path)                                \u2502\r\n\u2502    81                                                                       \u2502\r\n\u2502    82                                                                       \u2502\r\n\u2502    83 def mkdir(path: PathType) -> None:                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\integrations\\s3\\artifact_stores\\s3_artifact_store.py:275 in makedirs       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502   Args:                                                         \u2502\r\n\u2502   273 \u2502   \u2502   \u2502   path: The path to create.                                 \u2502\r\n\u2502   274 \u2502   \u2502   \"\"\"                                                           \u2502\r\n\u2502 > 275 \u2502   \u2502   self.filesystem.makedirs(path=path, exist_ok=True)            \u2502\r\n\u2502   276 \u2502                                                                     \u2502\r\n\u2502   277 \u2502   def mkdir(self, path: PathType) -> None:                          \u2502\r\n\u2502   278 \u2502   \u2502   \"\"\"Create a directory at the given path.                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:85 in wrapper                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    82 \u2502   @functools.wraps(func)                                            \u2502\r\n\u2502    83 \u2502   def wrapper(*args, **kwargs):                                     \u2502\r\n\u2502    84 \u2502   \u2502   self = obj or args[0]                                         \u2502\r\n\u2502 >  85 \u2502   \u2502   return sync(self.loop, func, *args, **kwargs)                 \u2502\r\n\u2502    86 \u2502                                                                     \u2502\r\n\u2502    87 \u2502   return wrapper                                                    \u2502\r\n\u2502    88                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:65 in sync                                                        \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    62 \u2502   \u2502   # suppress asyncio.TimeoutError, raise FSTimeoutError         \u2502\r\n\u2502    63 \u2502   \u2502   raise FSTimeoutError from return_result                       \u2502\r\n\u2502    64 \u2502   elif isinstance(return_result, BaseException):                    \u2502\r\n\u2502 >  65 \u2502   \u2502   raise return_result                                           \u2502\r\n\u2502    66 \u2502   else:                                                             \u2502\r\n\u2502    67 \u2502   \u2502   return return_result                                          \u2502\r\n\u2502    68                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:25 in _runner                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    22 \u2502   if timeout is not None:                                           \u2502\r\n\u2502    23 \u2502   \u2502   coro = asyncio.wait_for(coro, timeout=timeout)                \u2502\r\n\u2502    24 \u2502   try:                                                              \u2502\r\n\u2502 >  25 \u2502   \u2502   result[0] = await coro                                        \u2502\r\n\u2502    26 \u2502   except Exception as ex:                                           \u2502\r\n\u2502    27 \u2502   \u2502   result[0] = ex                                                \u2502\r\n\u2502    28 \u2502   finally:                                                          \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:767 in _makedirs                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    764 \u2502                                                                    \u2502\r\n\u2502    765 \u2502   async def _makedirs(self, path, exist_ok=False):                 \u2502\r\n\u2502    766 \u2502   \u2502   try:                                                         \u2502\r\n\u2502 >  767 \u2502   \u2502   \u2502   await self._mkdir(path, create_parents=True)             \u2502\r\n\u2502    768 \u2502   \u2502   except FileExistsError:                                      \u2502\r\n\u2502    769 \u2502   \u2502   \u2502   if exist_ok:                                             \u2502\r\n\u2502    770 \u2502   \u2502   \u2502   \u2502   pass                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:758 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502    756 \u2502   \u2502   \u2502   \u2502   raise translate_boto_error(e)                        \u2502\r\n\u2502    757 \u2502   \u2502   \u2502   except ParamValidationError as e:                        \u2502\r\n\u2502 >  758 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"Bucket create failed %r: %s\" % (bu \u2502\r\n\u2502    759 \u2502   \u2502   else:                                                        \u2502\r\n\u2502    760 \u2502   \u2502   \u2502   # raises if bucket doesn't exist and doesn't get create  \u2502\r\n\u2502    761 \u2502   \u2502   \u2502   await self._ls(bucket)                                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nValueError: Bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': Parameter \r\nvalidation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1657782683000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657726485000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug in SWB 5.2.6 version of SageMaker Jupyter Notebook workspace where a study fails to mount. The error message indicates that the FUSE package failed to install during on-start. The user can resolve the issue by running \"sudo yum install fuse\" and then running \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json to mount the study.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":154.35,
        "Challenge_repo_contributor_count":66.0,
        "Challenge_repo_fork_count":300.0,
        "Challenge_repo_issue_count":1578.0,
        "Challenge_repo_star_count":2887.0,
        "Challenge_repo_watch_count":40.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":15.6105555556,
        "Challenge_title":"[BUG]: SageMaker + S3 artifact store fails trying to create a new bucket",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":829,
        "Discussion_body":"Hi @danguitavinas,\r\n\r\nI'm guessing from the stack trace that you're running on windows with the local orchestrator? If that's the case, my guess is that this issue should be fixed by #735.\r\n\r\nIf you're interested in trying this, you could install ZenML from that branch using the command `pip install git+https:\/\/github.com\/zenml-io\/zenml.git@bugfix\/windows-source-utils` @schustmi Thank you so much, that worked! Im closing the issue!",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"study fails to mount"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":1229.6615519445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to Azure ML. I am having some doubts .Could anyone please clarify my doubts listed below.<\/p>\n\n<ol>\n<li>What is the difference between Azure ML service Azure ML experimentation service.<\/li>\n<li>What is the difference between Azure ML workbench and Azure ML Studio.<\/li>\n<li>I want to use azure ML Experimentation service for building few models and creating web API's. Is it possible to do the same with ML studio. <\/li>\n<li>And also ML Experimentation service requires me to have a docker for windows installed for creating web services.\nCan i create web services without using docker?<\/li>\n<\/ol>",
        "Challenge_closed_time":1525629753687,
        "Challenge_comment_count":1,
        "Challenge_created_time":1521202972100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to Azure ML and has some doubts regarding the differences between Azure ML service and Azure ML experimentation service, as well as between Azure ML workbench and Azure ML Studio. They also want to know if it is possible to use Azure ML Studio for building models and creating web APIs, and if it is necessary to have Docker for Windows installed for creating web services using ML Experimentation service.",
        "Challenge_last_edit_time":1554674738927,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49320679",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1229.6615519445,
        "Challenge_title":"Difference between Azure ML and Azure ML experimentation",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":763.0,
        "Challenge_word_count":105,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1422010576070,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"United Kingdom House, London, UK",
        "Poster_reputation_count":388.0,
        "Poster_view_count":124.0,
        "Solution_body":"<ol>\n<li><p>The AML Experimentation is one of our many new ML offerings, including data preparation, experimentation, model management, and operationalization. Workbench is a PREVIEW product that provides a GUI for some of these services. But it is just a installer\/wrapper for the CLI that is needed to run. The services are Spark and Python based. Other Python frameworks will work, and you can get a little hacky to call Java\/Scala from Python. Not really sure what you mean by an \"Azure ML Service\", perhaps you are referring to the operationalization service I mentioned above. This will quickly let you create new Python based APIs using Docker containers, and will connect with the model management account to keep track of the linage between your models and your services. All services here are still in preview and may breaking change before GA release. <\/p><\/li>\n<li><p>Azure ML Studio is an older product that is perhaps simpler for some(myself an engineer not a data scientist). It offers a drag and drop experience, but is limited in it's data size to about 10G. This product is GA. <\/p><\/li>\n<li><p>It is, but you need smaller data sizes, and the job flow is not spark based. I use this to do rapid PoC's. Also you will less control over the scalability of your scoring (batch or real time), because it is PaaS, compared to the newer service which is more IaaS. I would recommend looking at the new service instead of studio for most use cases. <\/p><\/li>\n<li><p>The web services are completely based on Docker. Needing docker for experimentation is more about running things locally, which I myself rarely do. But, for the real time service, everything you package is placed into a docker container so it can be deployed to an ACS cluster. <\/p><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":21.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":299.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"Azure ML differences and capabilities"
    },
    {
        "Answerer_created_time":1456411465888,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":105.0,
        "Answerer_view_count":25.0,
        "Challenge_adjusted_solved_time":22032.4912397222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have built my own Docker container that provides inference code to be deployed as endpoint on Amazon Sagemaker. However, this container needs to have access to some files from s3. The used IAM role has access to all s3 buckets that I am trying to reach.<\/p>\n\n<p>Code to download files using a boto3 client:<\/p>\n\n<pre><code>import boto3\n\nmodel_bucket = 'my-bucket'\n\ndef download_file_from_s3(s3_path, local_path):\n    client = boto3.client('s3')\n    client.download_file(model_bucket, s3_path, local_path)\n<\/code><\/pre>\n\n<p>The IAM role's policies:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::my-bucket\/*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Starting the docker container locally allows me to download files from s3 just like expected. <\/p>\n\n<p>Deploying as an endpoint on Sagemaker, however, the request times out:<\/p>\n\n<pre><code>botocore.vendored.requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='my-bucket.s3.eu-central-1.amazonaws.com', port=443): Max retries exceeded with url: \/path\/to\/my-file (Caused by ConnectTimeoutError(&lt;botocore.awsrequest.AWSHTTPSConnection object at 0x7f66244e69b0&gt;, 'Connection to my-bucket.s3.eu-central-1.amazonaws.com timed out. (connect timeout=60)'))\n<\/code><\/pre>\n\n<p>Any help is appreciated!<\/p>",
        "Challenge_closed_time":1562142120396,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561982365937,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has built a Docker container for inference code to be deployed as an endpoint on Amazon Sagemaker. The container needs to access files from S3, and the IAM role used has access to all S3 buckets. The user has provided code to download files using a boto3 client, and the IAM role's policies have been shared. While the container can download files from S3 locally, it times out when deployed as an endpoint on Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56835306",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":44.3762386111,
        "Challenge_title":"Download file using boto3 within Docker container deployed on Sagemaker Endpoint",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1171.0,
        "Challenge_word_count":149,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456411465888,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>For anyone coming across this question, when creating a model, the 'Enable Network Isolation' property defaults to True.\nFrom AWS docs:<\/p>\n<blockquote>\n<p>If you enable network isolation, the containers are not able to make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment.<\/p>\n<\/blockquote>\n<p>So this property needs to be set to False in order to connect to any other AWS service.<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/5qERm.jpg\" alt=\"AWS Sagemaker UI Network Isolation set to False\" \/><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1641299334400,
        "Solution_link_count":1.0,
        "Solution_readability":9.1,
        "Solution_reading_time":7.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"timeout on Sagemaker endpoint"
    },
    {
        "Answerer_created_time":1456986606312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":757.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":7933.4568986111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>There's probably something very obvious I'm missing or Sagemaker just doesn't support these kinds of extensions, but I've been trying to enable toc2 (Table of Contents) jupyter extension for my Sagemaker notebook via lifecycle configurations, but for whatever reason it still isn't showing up.<\/p>\n\n<p>I built my script out combining a sample AWS script and a quick article on the usual ways of enabling extensions:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh<\/a><\/p>\n\n<p><a href=\"https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231<\/a><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;EOF\n\n--Activate notebook environment\nsource activate JupyterSystemEnv\n\n--Install extensions\npip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib\nnbextension install\njupyter nbextension enable toc2 --py --sys-prefix\n\nsource deactivate\n\n\nEOF\n<\/code><\/pre>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1610748471692,
        "Challenge_comment_count":2,
        "Challenge_created_time":1586792396013,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to install the toc2 (Table of Contents) Jupyter extension for their AWS Sagemaker notebook via lifecycle configurations, despite following a sample AWS script and an article on enabling extensions. The user is unsure if Sagemaker supports these types of extensions.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61191412",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":20.5,
        "Challenge_reading_time":18.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6654.4654663889,
        "Challenge_title":"Unable to install toc2 notebook extension for AWS Sagemaker Instance (Lifecycle Configurations)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1027.0,
        "Challenge_word_count":119,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456986606312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":757.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user<\/code> to copy the JS\/CSS files into Jupyter's search directory and some config updates (<a href=\"https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\" rel=\"nofollow noreferrer\">https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions<\/a>).<\/p>\n<p>Corrected statement<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable toc2\/main\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n\nEOF\n\n##Below may be unnecessary, but other user needed to run to see success\ninitctl restart jupyter-server --no-wait\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1615352840848,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":11.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to install toc2 extension"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3914.1044444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello, receiving the following error in an Azure Notebook VM while trying to import the ML library - \r\n\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\n# error here!!!\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b8d543bb7111> in <module>\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n----> 5 from azureml.train.automl import AutoMLConfig\r\n      6 from sklearn.externals import joblib\r\n      7 from azureml.core.model import Model\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/__init__.py in <module>\r\n     23     # Suppress the warnings at the import phase.\r\n     24     warnings.simplefilter(\"ignore\")\r\n---> 25     from ._automl import fit_pipeline\r\n     26     from .automlconfig import AutoMLConfig\r\n     27     from .automl_step import AutoMLStep, AutoMLStepRun\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_automl.py in <module>\r\n     17 from automl.client.core.runtime.cache_store import CacheStore\r\n     18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities\r\n---> 19 from azureml.automl.core import data_transformation, fit_pipeline as fit_pipeline_helper\r\n     20 from azureml.automl.core.automl_pipeline import AutoMLPipeline\r\n     21 from azureml.automl.core.data_context import RawDataContext, TransformedDataContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/fit_pipeline.py in <module>\r\n     18 from automl.client.core.common.limit_function_call_exceptions import TimeoutException\r\n     19 from automl.client.core.runtime.datasets import DatasetBase\r\n---> 20 from . import package_utilities, pipeline_run_helper, training_utilities\r\n     21 from .automl_base_settings import AutoMLBaseSettings\r\n     22 from .automl_pipeline import AutoMLPipeline\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/pipeline_run_helper.py in <module>\r\n     18 from automl.client.core.common.exceptions import ClientException\r\n     19 from automl.client.core.runtime import metrics\r\n---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module\r\n     21 from automl.client.core.runtime.datasets import DatasetBase\r\n     22 from automl.client.core.runtime.execution_context import ExecutionContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in <module>\r\n     21 \r\n     22 from automl.client.core.common import constants\r\n---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers\r\n     24 from automl.client.core.runtime.nimbus_wrappers import AveragedPerceptronBinaryClassifier, \\\r\n     25     AveragedPerceptronMulticlassClassifier, NimbusMlClassifierMixin, NimbusMlRegressorMixin\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in <module>\r\n     34 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n     35 if tf_found:\r\n---> 36     tf.logging.set_verbosity(tf.logging.ERROR)\r\n     37 \r\n     38     OPTIMIZERS = {\r\n \r\nAttributeError: module 'tensorflow' has no attribute 'logging'\r\n",
        "Challenge_closed_time":1587086020000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1572995244000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when attempting to import the \"ExplainationDashboard\" module from \"azureml.contrib.interpret\" in build 1.0.72, despite having installed and updated the SDK without any errors. The failing line is in a sample notebook for \"how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/644",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":45.44,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":3914.1044444444,
        "Challenge_title":"Error trying to load azureml.train.automl",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":272,
        "Discussion_body":"Do you know which version of tensorflow you are using? \r\n\r\nThis SO question may be applicable: https:\/\/stackoverflow.com\/questions\/55318626\/module-tensorflow-has-no-attribute-logging Hello, Not sure about tensorflow.  This is a \"stock\" Notebook VM that was created last week, so no changes were made to the libraries. Hello,\r\n\r\nSorry for the inconvenience. This issue has been fixed since v1.0.72 but, it's related to the fact that tf==2.0. is installed by default on the notebook instance. It broke other things too as TF2.0 has many changes in its API. Your two options are to upgrade to v1.0.72+ or use the following code to downgrade tensorflow.\r\n\r\npip install -U tensorflow-gpu==1.14.0 \r\ntensorflow==estimator==1.14.0 \r\n\r\nThat should fix it for you.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ModuleNotFoundError in sample notebook"
    },
    {
        "Answerer_created_time":1290013527427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin",
        "Answerer_reputation_count":15929.0,
        "Answerer_view_count":1202.0,
        "Challenge_adjusted_solved_time":1080.3870858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Challenge_closed_time":1655459795692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651570402183,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a segfault error while trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. The error message indicates that there is no version information available for libncursesw.so.6, which is required by htop. The user has successfully used htop in other images before, but is facing issues with this particular image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72097417",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1080.3870858333,
        "Challenge_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":147.0,
        "Challenge_word_count":124,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1551431163127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paris, France",
        "Poster_reputation_count":494.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.0,
        "Solution_reading_time":2.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"segfault error in htop"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.5986111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI want to use `awswrangler` package in my Jupyter Notebook instance of SageMaker.\n\nI understand that we have to use **Lifecycle configuration**. I tried to do it using the following script:\n\n    #!\/bin\/bash\n    \n    pip install awswrangler==0.2.2\n\nBut when I import that package into my Notebook:\n\n    import boto3                                      # For executing native S3 APIs\n    import pandas as pd                               # For munging tabulara data\n    import numpy as np                                # For doing some calculation\n    import awswrangler as wr\n    import io\n    from io import StringIO\n\nI still get the following error:\n\n    ---------------------------------------------------------------------------\n    ModuleNotFoundError                       Traceback (most recent call last)\n    <ipython-input-1-f3d85c7dd0f6> in <module>()\n          2 import pandas as pd                               # For munging tabulara data\n          3 import numpy as np                                # For doing some calculation\n    ----> 4 import awswrangler as wr\n          5 import io\n          6 from io import StringIO\n    \n    ModuleNotFoundError: No module named 'awswrangler'\n\nAny documentation or reference on how to install certain package for Jupyter Notebook in SageMaker?",
        "Challenge_closed_time":1592832724000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592823369000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to install the `awswrangler` package in their Jupyter Notebook instance of SageMaker using a lifecycle configuration script, but is still getting a \"ModuleNotFoundError\" when trying to import the package. They are seeking documentation or reference on how to properly install a package for Jupyter Notebook in SageMaker.",
        "Challenge_last_edit_time":1668557190488,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU4pvReJNZS6eDLxhd4pK-tQ\/how-to-install-phyton-package-in-jupyter-notebook-instance-in-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.5986111111,
        "Challenge_title":"How to install Phyton package in Jupyter Notebook instance in SageMaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1292.0,
        "Challenge_word_count":154,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\n example how to use lifecycle config to install python package in one environment : https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-single-environment\/on-start.sh\n\nand to all conda env - https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-all-environments\/on-start.sh",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925572268,
        "Solution_link_count":2.0,
        "Solution_readability":40.4,
        "Solution_reading_time":6.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"install package in SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.0739286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Challenge_closed_time":1661603882123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661517215980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"BAD_REQUEST\" error while trying to fetch runs from search_runs in mlflow while training a machine learning model in an Azure environment. The error message indicates an issue with the input string. The mlflow version being used is 1.28.0 and the user has successfully created and run jobs in Azure studio.",
        "Challenge_last_edit_time":1661625379892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24.0739286111,
        "Challenge_title":"Getting Bad request while searching run in mlflow",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":65,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582101477803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":171.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":17.0,
        "Solution_reading_time":16.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"BAD_REQUEST error in mlflow"
    },
    {
        "Answerer_created_time":1631092954063,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":88.9021997222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy my custom container in vertex ai endpoint for predictions. The contents of the application are as follows.<\/p>\n<ol>\n<li>Flask - app.py<\/li>\n<\/ol>\n<pre><code>import pandas as pd\nfrom flask import Flask, jsonify,request\nimport tensorflow\nimport pre_process\nimport post_process\n\n\napp = Flask(__name__)\n\n\n@app.route('\/predict',methods=['POST'])\ndef predict():\n    req = request.json.get('instances')\n    \n    input_data = req[0]['email']\n\n    #preprocessing\n    text = pre_process.preprocess(input_data)\n    vector = pre_process.preprocess_tokenizing(text)\n\n    model = tensorflow.keras.models.load_model('model')\n\n    #predict\n    prediction = model.predict(vector)\n\n    #postprocessing\n    value = post_process.postprocess(list(prediction[0])) \n    \n    return jsonify({'output':{'doc_class':value}})\n\n\nif __name__=='__main__':\n    app.run(host='0.0.0.0')\n<\/code><\/pre>\n<ol start=\"2\">\n<li>Dockerfile<\/li>\n<\/ol>\n<pre><code>FROM python:3.7\n\nWORKDIR \/app\n\nCOPY . \/app\n\nRUN pip install --trusted-host pypi.python.org -r requirements.txt \n\n\nCMD [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;0.0.0.0:5000&quot;, &quot;app:app&quot;]\n\nEXPOSE 5050\n<\/code><\/pre>\n<ol start=\"3\">\n<li>pre_process.py<\/li>\n<\/ol>\n<pre><code>#import \nimport pandas as pd\nimport pickle\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ndef preprocess(text):\n    &quot;&quot;&quot;Do all the Preprocessing as shown above and\n    return a tuple contain preprocess_email,preprocess_subject,preprocess_text for that Text_data&quot;&quot;&quot;\n         \n    \n    #After you store it in the list, Replace those sentances in original text by space.\n    text = re.sub(&quot;(Subject:).+&quot;,&quot; &quot;,text,re.I)\n    \n    #Delete all the sentances where sentence starts with &quot;Write to:&quot; or &quot;From:&quot;.\n    text = re.sub(&quot;((Write to:)|(From:)).+&quot;,&quot;&quot;,text,re.I)\n    \n    #Delete all the tags like &quot;&lt; anyword &gt;&quot;\n    text = re.sub(&quot;&lt;[^&gt;&lt;]+&gt;&quot;,&quot;&quot;,text)\n    \n    #Delete all the data which are present in the brackets.\n    text = re.sub(&quot;\\([^()]+\\)&quot;,&quot;&quot;,text)\n    \n    #Remove all the newlines('\\n'), tabs('\\t'), &quot;-&quot;, &quot;&quot;.\n    text = re.sub(&quot;[\\n\\t\\\\-]+&quot;,&quot;&quot;,text)\n    \n    #Remove all the words which ends with &quot;:&quot;.\n    text = re.sub(&quot;(\\w+:)&quot;,&quot;&quot;,text)\n    \n    #Decontractions, replace words like below to full words.\n\n    lines = re.sub(r&quot;n\\'t&quot;, &quot; not&quot;, text)\n    lines = re.sub(r&quot;\\'re&quot;, &quot; are&quot;, lines)\n    lines = re.sub(r&quot;\\'s&quot;, &quot; is&quot;, lines)\n    lines = re.sub(r&quot;\\'d&quot;, &quot; would&quot;, lines)\n    lines = re.sub(r&quot;\\'ll&quot;, &quot; will&quot;, lines)\n    lines = re.sub(r&quot;\\'t&quot;, &quot; not&quot;, lines)\n    lines = re.sub(r&quot;\\'ve&quot;, &quot; have&quot;, lines)\n    lines = re.sub(r&quot;\\'m&quot;, &quot; am&quot;, lines)\n    text = lines\n    \n        #replace numbers with spaces\n    text = re.sub(&quot;\\d+&quot;,&quot; &quot;,text)\n    \n        # remove _ from the words starting and\/or ending with _\n    text = re.sub(&quot;(\\s_)|(_\\s)&quot;,&quot; &quot;,text)\n    \n        #remove 1 or 2 letter word before _\n    text = re.sub(&quot;\\w{1,2}_&quot;,&quot;&quot;,text)\n    \n        #convert all letters to lowercase and remove the words which are greater \n        #than or equal to 15 or less than or equal to 2.\n    text = text.lower()\n    \n    text =&quot; &quot;.join([i for i in text.split() if len(i)&lt;15 and len(i)&gt;2])\n    \n    #replace all letters except A-Z,a-z,_ with space\n    preprocessed_text = re.sub(&quot;\\W+&quot;,&quot; &quot;,text)\n\n    return preprocessed_text\n\ndef preprocess_tokenizing(text):\n        \n    #from tf.keras.preprocessing.text import Tokenizer\n    #from tf.keras.preprocessing.sequence import pad_sequences\n    \n    tokenizer = pickle.load(open('tokenizer.pkl','rb'))\n\n    max_length = 1019\n    tokenizer.fit_on_texts([text])\n    encoded_docs = tokenizer.texts_to_sequences([text])\n    text_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n    \n    return text_padded\n<\/code><\/pre>\n<ol start=\"4\">\n<li>post_process.py<\/li>\n<\/ol>\n<pre><code>def postprocess(vector):\n    index = vector.index(max(vector))\n    classes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n    return classes[index]\n<\/code><\/pre>\n<ol start=\"4\">\n<li>requirements.txt<\/li>\n<\/ol>\n<pre><code>gunicorn\npandas==1.3.3\nnumpy==1.19.5\nflask\nflask-cors\nh5py==3.1.0\nscikit-learn==0.24.2\ntensorflow==2.6.0\n\n<\/code><\/pre>\n<ol start=\"5\">\n<li><p>model<\/p>\n<\/li>\n<li><p>tokenizer.pkl<\/p>\n<\/li>\n<\/ol>\n<p>I am following this blog <a href=\"https:\/\/medium.com\/mlearning-ai\/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290\" rel=\"nofollow noreferrer\">vertex ai deployment<\/a> for gcloud console commands to containerise and deploy the model to endpoint.But the model is taking forever to get deployed and ultimately fails to get deployed.<\/p>\n<p>After running the container in local host, it runs as expected but it is not getting deployed into vertex ai endpoint. I don't understand whether the problem is in flask app.py or Dockerfile or whether the problem lies somewhere else.<\/p>",
        "Challenge_closed_time":1632806171387,
        "Challenge_comment_count":2,
        "Challenge_created_time":1632490921653,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in deploying a custom container in Vertex AI endpoint for predictions. The user has followed a blog for GCloud console commands to containerize and deploy the model to the endpoint, but the deployment is taking too long and ultimately failing. The user is unsure whether the issue is with the Flask app.py, Dockerfile, or elsewhere.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69316032",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":66.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":57,
        "Challenge_solved_time":87.5693705556,
        "Challenge_title":"Custom Container deployment in vertex ai",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":629.0,
        "Challenge_word_count":475,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1631092954063,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":111.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>I was able to resolve this issue by adding health route to http server. I added the following piece of code in my flask app.<\/p>\n<pre><code>@app.route('\/healthz')\ndef healthz():\n    return &quot;OK&quot;\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1632810969572,
        "Solution_link_count":0.0,
        "Solution_readability":4.9,
        "Solution_reading_time":2.73,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"deployment failure"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":3103.0390416667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Challenge_closed_time":1650761211590,
        "Challenge_comment_count":1,
        "Challenge_created_time":1648821712310,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully created three different experiments using MLFlow and Jupyter Notebook on their local machine. However, when trying to run the MLFlow UI, they are unable to see any experiments. The user is seeking assistance in identifying the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71708147",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":15.0,
        "Challenge_reading_time":52.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":43,
        "Challenge_solved_time":538.7498,
        "Challenge_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":936.0,
        "Challenge_word_count":377,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1648820658172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1659992652860,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.82,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":28.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to see experiments"
    },
    {
        "Answerer_created_time":1276712500692,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tandil, Buenos Aires Province, Argentina",
        "Answerer_reputation_count":7226.0,
        "Answerer_view_count":637.0,
        "Challenge_adjusted_solved_time":7952.5209027778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to train my model using Bring your own container technique in sagemaker. My model training runs correctly without any issues locally. But my docker image takes env-file as an input that could change at different runs. But in sagemaker when passing the ECR image, I don't know how to pass this env-file. So instead, inside the <code>train<\/code> script, which is called by the sagemaker, I added <code>export KEY=value<\/code> statements to create my variables. Even that did not expose my variables. Another way I tried it was by executing <code>RUN source file.env<\/code> while building my image. Even this approach did not work out as I got an error <code>\/bin\/sh: 1: source: not found<\/code>.<\/p>\n<p>I could try <code>ENV<\/code> while building my image and that would probably work but this approach won't be flexible as my variables could change at different runs. Is there any way to pass docker run arguments from a sagemaker estimator or notebook? I checked out the documentation but I couldn't find anything.<\/p>",
        "Challenge_closed_time":1662661497360,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634032422110,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in passing arguments to their own docker container in Sagemaker while using the Bring Your Own Container technique. The docker image takes an env-file as input that could change at different runs, but the user is unsure how to pass this env-file while passing the ECR image. The user tried adding export KEY=value statements to create variables, but it did not expose the variables. The user also tried executing RUN source file.env while building the image, but it resulted in an error. The user is looking for a way to pass docker run arguments from a Sagemaker estimator or notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69538469",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":13.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":7952.5209027778,
        "Challenge_title":"Is there a way to pass arguments to our own docker container in sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":292.0,
        "Challenge_word_count":180,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1633433905427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train<\/code> method states that:<\/p>\n<pre><code>environment (dict[str, str]) : Environment variables to be set for\n            use during training job (default: ``None``): \n<\/code><\/pre>\n<p>For reference, the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/5bc3ccf\/src\/sagemaker\/session.py#L569\" rel=\"nofollow noreferrer\">SDK source<\/a>.<\/p>\n<p>Because the SDK is a wrapper on top of <a href=\"https:\/\/pypi.org\/project\/boto3\/\" rel=\"nofollow noreferrer\">Boto3<\/a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=\"https:\/\/aws.amazon.com\/developer\/tools\/#SDKs\" rel=\"nofollow noreferrer\">Amazon Services SDK<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":15.2,
        "Solution_reading_time":11.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":92.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"difficulty passing docker arguments"
    },
    {
        "Answerer_created_time":1428454496052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":61.6148286111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a docker container that is using Sagemaker via the java sdk. This container is deployed on a k8s cluster with several replicas. <\/p>\n\n<p>The container is doing simple requests to Sagemaker to list some models that we have trained and deployed. However we are now having issues with some java certificate. I am quite novice with k8s and certificates so I will appreciate if you could provide some help to fix the issue.<\/p>\n\n<p>Here are some traces from the log when it tries to list the endpoints:<\/p>\n\n<pre><code>org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:394)\n    at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:353)\n    at com.amazonaws.http.conn.ssl.SdkTLSSocketFactory.connectSocket(SdkTLSSocketFactory.java:132)\n    at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:141)\n    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n    at com.amazonaws.http.conn.$Proxy67.connect(Unknown Source)\n    at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n    at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1236)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056)\n    ... 70 common frames omitted\nCaused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:397)\n    at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:302)\n    at sun.security.validator.Validator.validate(Validator.java:262)\n    at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324)\n    at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229)\n    at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:124)\n    at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1621)\n    ... 97 common frames omitted\nCaused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)\n    at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)\n    at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280)\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:392)\n    ... 103 common frames omitted \n<\/code><\/pre>",
        "Challenge_closed_time":1544024416203,
        "Challenge_comment_count":2,
        "Challenge_created_time":1543802602820,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a docker container that uses Sagemaker via the Java SDK and deployed it on a k8s cluster with several replicas. However, the container is facing issues with a Java certificate when making simple requests to Sagemaker to list some models that have been trained and deployed. The user is seeking help to fix the issue. The log traces show that the issue is related to the inability to find a valid certification path to the requested target.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53586515",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":37.1,
        "Challenge_reading_time":51.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":61.6148286111,
        "Challenge_title":"Sagemaker certificate issue with Kubernetes",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":488.0,
        "Challenge_word_count":199,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>I think I have found the answer to my problem. I have set up another k8s cluster and deployed the container there as well. They are working fine and the certificate issues does not happen. When investigating more I have noticed that they were some issues with DNS resolution on the first k8s cluster. In fact the containers with certificate issues could not ping google.com for example.\nI fixed the DNS issue by not relying on core-dns and setting the DNS configuration in the deployment.yaml file. I am not sure to understand why exactly but this seems to have fixed the certificate issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":7.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":103.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"Java certificate issue"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":14.6338211111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a private Azure Container Registry, and pushed a docker image to that registry. I was trying to understand the correct way to access that registry in my pipeline, and my understanding was that I needed to set the following info in the run configuration:<\/p>\n\n<pre><code>        run_config.environment.docker.base_image = \"myprivateacr.azurecr.io\/mydockerimage:0.0.1\"\n        run_config.environment.docker.base_image_registry.username = \"MyPrivateACR\"\n        run_config.environment.docker.base_image_registry.password = \"&lt;the password for the registry&gt;\"\n<\/code><\/pre>\n\n<p>Let's assume that I correctly provided the username and password. Any idea why this didn't work? Or: is there an example of a pipeline notebook that uses a docker image that's in a private docker registry, and thus deals with this type of authentication issue?<\/p>",
        "Challenge_closed_time":1566481255416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566428573660,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in correctly specifying a private Azure Container Registry (ACR) Docker image in an Azure ML Pipeline. They have set the required information in the run configuration, including the base image, username, and password for the registry, but are still unable to access it. They are seeking help to understand why this is not working and if there are any examples of pipeline notebooks that deal with this authentication issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57600154",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":11.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":14.6338211111,
        "Challenge_title":"How to correctly specify a private ACR Docker image in an Azure ML Pipeline?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":278.0,
        "Challenge_word_count":116,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1491928725063,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA, United States",
        "Poster_reputation_count":45.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>There's a separate address property for a custom image registry. Try specifying it this way:<\/p>\n\n<pre><code>run_config.environment.docker.base_image = \"mydockerimage:0.0.1\"\nrun_config.environment.docker.base_image_registry.address = \"myprivateacr.azurecr.io\"\nrun_config.environment.docker.base_image_registry.username = \"MyPrivateACR\"\nrun_config.environment.docker.base_image_registry.password = \"&lt;the password for the registry&gt;\"\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":27.3,
        "Solution_reading_time":6.18,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":28.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to access ACR image"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3654761111,
        "Challenge_answer_count":1,
        "Challenge_body":"I work in SM Studio, and I do not understand why CPU and memory usage do not appear in the notebook toolbar. These metrics should be there, at least given this description:\n\nhttps:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/notebooks-menu.html\n\nWhen I open a notebook in SM Studio, I see the same toolbar but without CPU and memory usage listed. Moreover, I see 'cluster' before the kernel's name in my toolbar.\n\nHas anyone experienced sth similar? I assume an alternative for me would be to use CloudWatch.",
        "Challenge_closed_time":1652798353412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652797037698,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue in SM Studio where CPU and memory usage are missing from the notebook toolbar, despite being listed in the documentation. The user also notes that 'cluster' appears before the kernel's name in the toolbar. They are seeking advice from others who may have experienced a similar issue and suggest using CloudWatch as an alternative.",
        "Challenge_last_edit_time":1668563929559,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGQfGnTgqQcyNbWVb3U9V8Q\/cpu-memory-usage-missing-from-sm-studio-notebook-toolbar",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3654761111,
        "Challenge_title":"CPU + memory usage missing from SM Studio notebook toolbar",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":614.0,
        "Challenge_word_count":88,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, you should be able to see your CPU and Memory on the bottom toolbar, looks like `Kernel: Idle | Instance MEM`. You can click on that text to show the kernel and instance usage metrics.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1652798353412,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":2.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":35.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing CPU\/memory usage"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":0.3643536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to AWS and I'm trying out AWS Sagemaker. I'm currently doing my project which involves quite a long time to finish and I don't think I can finish it in a day. I'm worried if I close my JupyterLab of my notebook instance in SageMaker, my code will be gone. How do I save my code and cell run progress when using Sagemaker?<\/p>",
        "Challenge_closed_time":1619085494700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619084183027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to AWS Sagemaker and is concerned about losing their code if they close their JupyterLab from their notebook instance. They are seeking advice on how to save their code and cell run progress when using Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67210677",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.8,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.3643536111,
        "Challenge_title":"If I close my JupyterLab from notebook instance, would my code be gone?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":77,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605938672327,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Poster_reputation_count":97.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>If you are training directly in the notebook the answer is yes.\nHowever the best practice is not to train directly with the notebook.\nUse instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!\nAs a quick example.. in my notebook I have this cell:<\/p>\n<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :\n0.01}\n\nmetrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n\ninputs = {'train': train_data, 'test': validation_data}\n\nmyJobName = 'myname'\n\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n<\/code><\/pre>\n<p>My training script training.py is something like this:<\/p>\n<pre><code>if __name__ =='__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # input data and model directories\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--learning-rate', type=float, default=0.0001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--epochs', type=int, default=1)\n....\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.4,
        "Solution_reading_time":32.13,
        "Solution_score_count":2.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":227.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"save code in Sagemaker"
    },
    {
        "Answerer_created_time":1362580980910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Akron, OH, USA",
        "Answerer_reputation_count":4013.0,
        "Answerer_view_count":72.0,
        "Challenge_adjusted_solved_time":61.5306563889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Challenge_closed_time":1621009841940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620788331577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to mount an S3 container inside a docker image to be used as a ClearML agent. They have tried three solutions, including using prefabbed configuration in ClearML, mounting using s3fuse, and passing arguments to docker using docker_args and docker_bash_setup_script arguments to Task.create(). However, they have been unable to get any of these solutions to work.",
        "Challenge_last_edit_time":1621022250560,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67496760",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":21.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":61.5306563889,
        "Challenge_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":770.0,
        "Challenge_word_count":202,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1362580980910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Akron, OH, USA",
        "Poster_reputation_count":4013.0,
        "Poster_view_count":72.0,
        "Solution_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1621012052112,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":29.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":277.0,
        "Tool":"ClearML",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to mount S3 container"
    },
    {
        "Answerer_created_time":1261400320736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Antwerp, Belgium",
        "Answerer_reputation_count":7876.0,
        "Answerer_view_count":924.0,
        "Challenge_adjusted_solved_time":80.7204538889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I run <code>mlflow ui<\/code> the following error occurred:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\gunicorn.exe\\__main__.py\", line 5, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\wsgiapp.py\", line 9, in &lt;module&gt;\n    from gunicorn.app.base import Application\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\app\\base.py\", line 12, in &lt;module&gt;\n    from gunicorn import util\n  File \"c:\\anaconda3\\lib\\site-packages\\gunicorn\\util.py\", line 9, in &lt;module&gt;\n    import fcntl\nModuleNotFoundError: No module named 'fcntl'\nTraceback (most recent call last):\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Anaconda3\\Scripts\\mlflow.exe\\__main__.py\", line 9, in &lt;module&gt;\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\anaconda3\\lib\\site-packages\\click\\core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\cli.py\", line 131, in ui\n    mlflow.server._run_server(file_store, file_store, host, port, 1)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 48, in _run_server\n    env=env_map, stream_output=True)\n  File \"c:\\anaconda3\\lib\\site-packages\\mlflow\\utils\\process.py\", line 38, in exec_cmd\n    raise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\nmlflow.utils.process.ShellCommandException: Non-zero exitcode: 1\n<\/code><\/pre>\n\n<p>I used anaconda + python 3.6.5 and I installed git and set path with <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> and <code>C:\\Program Files\\Git\\cmd<\/code>.<\/p>\n\n<p>I installed <code>mlflow<\/code> whit <code>pip install mlflow<\/code> and its version is 0.2.1.<\/p>\n\n<p>I set a variable with name <code>GIT_PYTHON_GIT_EXECUTABLE<\/code> and value <code>C:\\Program Files\\Git\\bin\\git.exe<\/code> in Environment Variables. <\/p>\n\n<p>How can I solve this?<\/p>",
        "Challenge_closed_time":1531837039907,
        "Challenge_comment_count":2,
        "Challenge_created_time":1531546446273,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when trying to run \"mlflow ui\" on MS Windows. The error message indicates that there is no module named 'fcntl'. The user has installed mlflow version 0.2.1 using pip and has set the path for Git in the Environment Variables. The user is seeking a solution to this issue.",
        "Challenge_last_edit_time":1531837117032,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51335594",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.0,
        "Challenge_reading_time":34.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":80.7204538889,
        "Challenge_title":"Error with \"mlflow ui\" when trying to run it on MS Windows",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":4688.0,
        "Challenge_word_count":235,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1308552848512,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1177.0,
        "Poster_view_count":144.0,
        "Solution_body":"<p><a href=\"https:\/\/github.com\/databricks\/mlflow\" rel=\"nofollow noreferrer\">mlflow documentation<\/a> already says that <\/p>\n\n<blockquote>\n  <p>Note 2: We <strong>do not currently support running MLflow on Windows<\/strong>.\n  Despite this, we would appreciate any contributions to make MLflow\n  work better on Windows.<\/p>\n<\/blockquote>\n\n<p>You're hitting <code>fcntl<\/code> problem: it's not available on MS Windows platform because it's a \"wrapper\" around the <a href=\"http:\/\/man7.org\/linux\/man-pages\/man2\/fcntl.2.html\" rel=\"nofollow noreferrer\">fcntl function<\/a> that's available on POSIX-compatible systems. (See <a href=\"https:\/\/stackoverflow.com\/a\/1422436\/236007\">https:\/\/stackoverflow.com\/a\/1422436\/236007<\/a> for more details.)<\/p>\n\n<p>Solving this requires modifying the source code of mlflow accordingly. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":10.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing 'fcntl' module"
    },
    {
        "Answerer_created_time":1361339272692,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"NYC",
        "Answerer_reputation_count":6281.0,
        "Answerer_view_count":958.0,
        "Challenge_adjusted_solved_time":3630.1468475,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Challenge_closed_time":1604879360048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591810831397,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between ScriptProcessor and Processor in AWS Sagemaker SDK. Both APIs can achieve the same goals, but ScriptProcessor supports docker command parameter while Processor supports docker entrypoint parameter. The user is asking if there are any other differences between the two.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":21.3,
        "Challenge_reading_time":11.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3630.1468475,
        "Challenge_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":400.0,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370231111260,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":5295.0,
        "Poster_view_count":455.0,
        "Solution_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.5,
        "Solution_reading_time":4.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"difference between ScriptProcessor and Processor"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.6111111111,
        "Challenge_answer_count":0,
        "Challenge_body":"its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in sagemaker\r\nhow long is this going to even take man",
        "Challenge_closed_time":1667627477000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1667438077000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to install libraries in SageMaker Studio Lab that require root privileges, but they are not able to access root user. They have tried running `whoami` and `sudo` commands, but the latter is not found. They have also tried to install `sudo` by following a link, but they are prompted for a password which they do not have. The user is seeking help to gain root access or install libraries that require root access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/155",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":113.0,
        "Challenge_repo_issue_count":218.0,
        "Challenge_repo_star_count":408.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":52.6111111111,
        "Challenge_title":"we are experiencing elevated fault rate in start runtime API. The SageMaker Studio Lab team is working to restore the service.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":51,
        "Discussion_body":"up Ah finally btw, I can run the CPU but not GPU today\r\n @saleemmalik10835 Sorry for the long inconvenience of Studio Lab. As you know, the service was back and we confirmed that we can say it to you. I will close this issue because the mentioned problem is solved.\r\n\r\nBut as @aozorahime said, the GPU instance is sometime unavailable because of another instance allocation issue. Of course, we deal with this problem now. ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no root access"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3662.6730555556,
        "Challenge_answer_count":0,
        "Challenge_body":"### Summary\r\n\r\n[<!-- Summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb)\r\n### Steps to Reproduce it\r\n\r\nUsed Binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         mlflow.whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (Optional). If not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is None:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is None:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise RuntimeError(\"Session is already closed. Cannot create more loggers\")\r\n    175 \r\n    176         # Explicitly set the default timezone to utc if none was provided. Helps with equality testing\r\n\r\nRuntimeError: Session is already closed. Cannot create more loggers\r\n```\r\n### Example\r\n\r\n",
        "Challenge_closed_time":1655127397000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1641941774000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with a project template in which the `.drone.yaml` file is referencing a Kubernetes secret named `mlflow-server-secret` that does not exist by default. The solution to this issue is to change the name of the secret to `{{ .ProjectID }}-mlflow-secret`.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/411",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":14.3,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":95.0,
        "Challenge_repo_issue_count":1268.0,
        "Challenge_repo_star_count":2244.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3662.6730555556,
        "Challenge_title":"MLflow example: close session error",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":192,
        "Discussion_body":"The example closes the default session, and then later the mlflow.whylogs wrapper is using that closed session to create loggers. Need to update that example's initial session creation to something like:\r\n```\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session()\r\nsummary = session.profile_dataframe(train, \"training-data\").flat_summary()['summary']\r\n\r\nsummary\r\n``` Still need to update the example to work in Binder better:\r\n* install dependencies\r\n* coinfigure mlflow writer, currently the default session will just write to local disk Part of the reason is that it picks up the default YAML file with default list of writers - and they don't contain mlflow (for obvious reason): https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs.yaml\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/26821974\/149250188-154d5b19-348e-44ea-b64a-0c4724b3c0cd.png)\r\n\r\nHere's my fix in the notebook\r\n\r\nNow this poses interesting quesiton:\r\n* Should mlflow writer be allowed if you don't run mlflow? My instinct is to say yes, but you get a big warning. Or exception?\r\n* If you specify a config without mlflow writer, should we implicitly add mlflow writer? Maybe yes.\r\n\r\nHowever so far I'm not a fan of implicit behaviors because it's freaking hard for us to reason about (see this issue - took a bit of debugging to find out that it's config related). My vote is to throw exception with an option to disable that exception if user chooses the path of ignorance. Drop in the code of the two cells:\r\n\r\n```\r\nconfig = \"\"\"\r\nproject: example-project\r\npipeline: example-pipeline\r\nverbose: false\r\nwriters:\r\n# Save to mlflow\r\n- formats:\r\n    - protobuf\r\n  output_path: mlflow\r\n  type: mlflow\r\n\"\"\"\r\ncfg_file = \"mlflow_config.yaml\"\r\n\r\n!echo \"{config}\" > {cfg_file}\r\n\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session(cfg_file)\r\n\r\nassert whylogs.__version__ >= \"0.1.13\" # we need 0.1.13 or later for MLflow integration\r\nwhylogs.enable_mlflow(session)\r\n``` This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing Kubernetes secret"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":11055.1413697222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to install mlflow in R and im getting this error message saying <\/p>\n\n<blockquote>\n  <p>mlflow::install_mlflow()\n  Error in mlflow_conda_bin() :\n    Unable to find conda binary. Is Anaconda installed?\n    If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of yourpython executable.<\/p>\n<\/blockquote>\n\n<p>I have tried the following<\/p>\n\n<pre><code>export MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\" \nsource ~\/.bashrc\necho $MLFLOW_PYTHON_BIN  -&gt; this prints the \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>or in R,<\/p>\n\n<pre><code>sys.setenv(MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\")\nsys.getenv() -&gt; prints MLFLOW_PYTHON_BIN is set to \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>however, it still does not work<\/p>\n\n<p>I do not want to use conda environment.<\/p>\n\n<p>how to I get past this error?<\/p>",
        "Challenge_closed_time":1584554585176,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583947052940,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install mlflow in R, which says that the conda binary cannot be found. The user has tried setting the environment variable MLFLOW_PYTHON_BIN to the path of their python executable, but it still does not work. The user does not want to use a conda environment and is seeking a solution to this error.",
        "Challenge_last_edit_time":1584403666972,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60641337",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":168.7589544444,
        "Challenge_title":"mlflow R installation MLFLOW_PYTHON_BIN",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1141.0,
        "Challenge_word_count":103,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539211301843,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":117.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:<\/p>\n<ul>\n<li>install conda - this is the recommended way of installing and using mlflow<\/li>\n<\/ul>\n<p>or<\/p>\n<ul>\n<li>install mlflow python package yourself via pip<\/li>\n<\/ul>\n<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.<\/p>\n<pre><code>library(mlflow)\nsystem(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))\nSys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))\nSys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1624202175903,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":9.75,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"conda binary not found"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":15.5657958333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have done quite a few google searches but have not found a clear answer to the following use case. Basically, I would rather use cloud 9 (most of the time) as my IDE rather than Jupyter. What I am confused\/not sure about is, how I could executed long running jobs like (Bayesian) hyper parameter optimisation from there. Can I use Sagemaker capabilities? Should I use docker and deploy to ECR (looking for the cheapest-ish option)? Any pointers w.r.t. to this particular issue would be very much appreciated. Thanks.<\/p>",
        "Challenge_closed_time":1641735920212,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641644416603,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for guidance on how to execute long running jobs like hyper parameter optimization using Cloud 9 as their IDE instead of Jupyter. They are unsure if they can use Sagemaker capabilities or if they should use docker and deploy to ECR. They are seeking advice on the most cost-effective option.",
        "Challenge_last_edit_time":1641679883347,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70632239",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":25.4176691667,
        "Challenge_title":"cloud 9 and sagemaker - hyper parameter optimisation",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":95,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>You could use whatever IDE you choose (including your laptop).<br \/>\nSaegMaker tuning job (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex.html\" rel=\"nofollow noreferrer\">example<\/a>) is <strong>asynchronous<\/strong>, so you can safely close your IDE after launching it. You can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-monitor.html\" rel=\"nofollow noreferrer\">monitor the job the AWS web console,<\/a> or with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeHyperParameterTuningJob.html\" rel=\"nofollow noreferrer\">DescribeHyperParameterTuningJob API call<\/a>.<\/p>\n<p>You can launch TensorFlow, PyTorch, XGBoost, Scikit-learn, and other <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/frameworks.html\" rel=\"nofollow noreferrer\">popular ML frameworks<\/a>, using one of the built-in framework containers, avoiding the extra work of bringing your own container.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":20.2,
        "Solution_reading_time":13.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"execute long running jobs"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5563888889,
        "Challenge_answer_count":1,
        "Challenge_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Challenge_closed_time":1606709124000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606707121000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information about the advantages of using SageMaker Jupyter instance instead of running it locally and whether there is any special integration with SageMaker that is lost if not using SageMaker Jupyter instance.",
        "Challenge_last_edit_time":1668525106056,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":2.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.5563888889,
        "Challenge_title":"Sagemaker Jupyter notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":35,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Some useful points:\n\n- The **typical arguments of cloud vs local** will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\n- SageMaker notebooks already run in an explicit **IAM context** (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run `sagemaker.get_execution_role()`\n- **Pre-built environments** for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and **easy wiping\/reset** of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\n- Linux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\n- If you started using **SageMaker Studio**, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565128,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":179.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"advantages of SageMaker Jupyter"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":195.7456172222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie to aws sagemaker.\nI am trying to setup a model in aws sagemaker using keras with GPU support.\nThe docker base image used to infer the model is given below<\/p>\n\n<pre><code>FROM tensorflow\/tensorflow:1.10.0-gpu-py3\n\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx curl\n...\n<\/code><\/pre>\n\n<p>This is the keras code I'm using to check if a GPU is identified by keras in flask.<\/p>\n\n<pre><code>import keras\n@app.route('\/ping', methods=['GET'])\ndef ping():\n\n    keras.backend.tensorflow_backend._get_available_gpus()\n\n    return flask.Response(response='\\n', status=200,mimetype='application\/json')\n<\/code><\/pre>\n\n<p>When I spin up a notebook instance in a sagemaker using the GPU the keras code shows available GPUs.\nSo, in order to access GPU in the inference phase(model) do I need to install any additional libraries in the docker file apart from the tensorflow GPU base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1545210167392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544505483170,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is a newbie to AWS Sagemaker and is trying to set up a model using Keras with GPU support. They are using a Docker base image with TensorFlow 1.10.0 GPU and are trying to check if a GPU is identified by Keras in Flask. The user is asking if they need to install any additional libraries in the Docker file to access the GPU in the inference phase.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53717800",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":12.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":195.7456172222,
        "Challenge_title":"Configuring GPU in aws sagemaker with keras and tensorflow as backend",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1958.0,
        "Challenge_word_count":137,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544503799112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":57.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>You shouldn't need to install anything else. Keras relies on TensorFlow for GPU detection and configuration.<\/p>\n\n<p>The only thing worth noting is how to use multiple GPUs during training. I'd recommend passing 'gpu_count' as an hyper parameter, and setting things up like so:<\/p>\n\n<pre><code>from keras.utils import multi_gpu_model\nmodel = Sequential()\nmodel.add(...)\n...\nif gpu_count &gt; 1:\n    model = multi_gpu_model(model, gpus=gpu_count)\nmodel.compile(...)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":6.08,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"GPU access in inference"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.5513888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nThe conda environment for python3.6 in notebooks cannot find `pandas.CSVDataSet`\r\n\r\n## Context\r\nI'm wanting to use sagemaker as my development environment. However, I cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## Steps to Reproduce\r\n\r\n0. Startup a Sagemaker instance with defaults\r\n\r\nTerminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => Success!\r\n\r\nNotebook fail:\r\n1. Create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> The environments for the terminal and notebooks are separate by design in Sagemaker\r\n2. Load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> Note that I've started to use the code below; Without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `Path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # Check it exists first. For some reason this is not an idempotent operation?\r\n    current_dir = Path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. Run `context.catalog.list()`\r\n\r\n## Expected Result\r\nThe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## Actual Result\r\n```\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\nFull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not None)\r\n    418         except StopIteration:\r\n\r\nStopIteration: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except StopIteration:\r\n--> 419             raise DataSetError(\"Class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\nDataSetError: Class `pandas.CSVDataSet` not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = AbstractDataSet.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise DataSetError(\r\n    153                 \"An exception occurred when parsing config \"\r\n--> 154                 \"for DataSet `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\nDataSetError: An exception occurred when parsing config for DataSet `example_iris_data`:\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\n## Investigations so far\r\n\r\n### `CSVLocalDataSet`\r\nUpon changing the yaml type for iris.csv from `pandas.CSVDataSet` to `CSVLocalDataSet`, we get success on both the terminal and the notebook. However, this is not my desired outcome; The transition to using `pandas.CSVDataSet` makes it easier, for me at least, to use both S3 and local datasets.\r\n\r\n### `pip install kedro` output from notebook\r\n```\r\nCollecting kedro\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/6f\/4faaa0e58728a318aeabc490271a636f87f6b9165245ce1d3adc764240cf\/kedro-0.15.8-py3-none-any.whl (12.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.5MB 4.1MB\/s eta 0:00:01\r\nRequirement already satisfied: xlsxwriter<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.0.4)\r\nCollecting azure-storage-file<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c9\/33\/6c611563412ffc409b2413ac50e3a063133ea235b86c137759774c77f3ad\/azure_storage_file-1.4.0-py2.py3-none-any.whl\r\nCollecting fsspec<1.0,>=0.5.1 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6e\/2b\/63420d49d5e5f885451429e9e0f40ad1787eed0d32b1aedd6b10f9c2719a\/fsspec-0.7.1-py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 33.5MB\/s ta 0:00:01\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (0.24.2)\r\nCollecting s3fs<1.0,>=0.3.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b8\/e4\/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675\/s3fs-0.4.2-py3-none-any.whl\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nCollecting tables<3.6,>=3.4.4 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/f7\/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a\/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 10.2MB\/s ta 0:00:01\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (2.20.0)\r\nCollecting toposort<2.0,>=1.5 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e9\/8a\/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4\/toposort-1.5-py2.py3-none-any.whl\r\nRequirement already satisfied: click<8.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (6.7)\r\nCollecting azure-storage-queue<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/72\/94\/4db044f1c155b40c5ebc037bfd9d1c24562845692c06798fbe869fe160e6\/azure_storage_queue-1.4.0-py2.py3-none-any.whl\r\nCollecting cookiecutter<2.0,>=1.6.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/86\/c9\/7184edfb0e89abedc37211743d1420810f6b49ae4fa695dfc443c273470d\/cookiecutter-1.7.0-py2.py3-none-any.whl (40kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 24.6MB\/s ta 0:00:01\r\nCollecting pandas-gbq<1.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c3\/74\/126408f6bdb7b2cb1dcb8c6e4bd69a511a7f85792d686d1237d9825e6194\/pandas_gbq-0.13.1-py3-none-any.whl\r\nCollecting pip-tools<5.0.0,>=4.0.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/94\/8f\/59495d651f3ced9b06b69545756a27296861a6edd6c5709fbe1265ed9032\/pip_tools-4.5.1-py2.py3-none-any.whl (41kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 27.5MB\/s ta 0:00:01\r\nCollecting azure-storage-blob<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/25\/f4\/a307ed89014e9abb5c5cfc8ca7f8f797d12f619f17a6059a6fd4b153b5d0\/azure_storage_blob-1.5.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 35.2MB\/s ta 0:00:01\r\nCollecting pyarrow<1.0.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/10\/93fad5849418eade4a4cd581f8cd27be1bbe51e18968ba1492140c887f3f\/pyarrow-0.16.0-cp36-cp36m-manylinux1_x86_64.whl (62.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.9MB 779kB\/s eta 0:00:01    40% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 25.7MB 56.1MB\/s eta 0:00:01\r\nRequirement already satisfied: SQLAlchemy<2.0,>=1.2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.2.11)\r\nRequirement already satisfied: xlrd<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.1.0)\r\nCollecting python-json-logger<1.0,>=0.1.9 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/80\/9d\/1c3393a6067716e04e6fcef95104c8426d262b4adaf18d7aa2470eab028d\/python-json-logger-0.1.11.tar.gz\r\nCollecting anyconfig<1.0,>=0.9.7 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4c\/00\/cc525eb0240b6ef196b98300d505114339bbb7ddd68e3155483f1eb32050\/anyconfig-0.9.10.tar.gz (103kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 34.4MB\/s ta 0:00:01\r\nCollecting azure-storage-common~=1.4 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/6c\/b2285bf3687768dbf61b6bc085b0c1be2893b6e2757a9d023263764177f3\/azure_storage_common-1.4.2-py2.py3-none-any.whl (47kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 25.9MB\/s ta 0:00:01\r\nCollecting azure-common>=1.1.5 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e5\/4d\/d000fc3c5af601d00d55750b71da5c231fcb128f42ac95b208ed1091c2c1\/azure_common-1.1.25-py2.py3-none-any.whl\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.7.3)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.14.3)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2018.4)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (4.0.1)\r\nRequirement already satisfied: numexpr>=2.6.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (2.6.5)\r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.11.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.23)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.6)\r\nCollecting whichcraft>=0.4.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b5\/a2\/81887a0dae2e4d2adc70d9a3557fdda969f863ced51cd3c47b587d25bce5\/whichcraft-0.6.1-py2.py3-none-any.whl\r\nCollecting future>=0.15.2 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/45\/0b\/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9\/future-0.18.2.tar.gz (829kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 27.8MB\/s ta 0:00:01\r\nCollecting poyo>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/42\/50\/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc\/poyo-0.5.0-py2.py3-none-any.whl\r\nCollecting binaryornot>=0.2.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/7e\/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2\/binaryornot-0.4.4-py2.py3-none-any.whl\r\nCollecting jinja2-time>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6a\/a1\/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4\/jinja2_time-0.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.10)\r\nCollecting google-auth-oauthlib (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7b\/b8\/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b\/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\r\nCollecting google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/b0\/cc391ebf8ebf7855cdcfe0a9a4cdc8dcd90287c90e1ac22651d104ac6481\/google_auth-1.12.0-py2.py3-none-any.whl (83kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 35.5MB\/s ta 0:00:01\r\nCollecting pydata-google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/ed\/9c9f410c032645632de787b8c285a78496bd89590c777385b921eb89433d\/pydata_google_auth-0.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (39.1.0)\r\nCollecting google-cloud-bigquery>=1.11.1 (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/8f\/f7\/b6f55e144da37f38a79552a06103f2df4a9569e2dfc6d741a7e2a63d3592\/google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 39.2MB\/s ta 0:00:01\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.14)\r\nCollecting arrow (from jinja2-time>=0.1.0->cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/fa\/f84896dede5decf284e6922134bf03fd26c90870bbf8015f4e8ee2a07bcc\/arrow-0.15.5-py2.py3-none-any.whl (46kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 26.3MB\/s ta 0:00:01\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.0)\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a3\/12\/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379\/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/95\/de\/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d\/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 32.5MB\/s ta 0:00:01\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/08\/6a\/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425\/cachetools-4.0.0-py3-none-any.whl\r\nCollecting google-api-core<2.0dev,>=1.15.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/63\/7e\/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77\/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 29.9MB\/s ta 0:00:01\r\nCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/89\/3c\/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97\/google_cloud_core-1.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.6.1)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/35\/9e\/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098\/google_resumable_media-0.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.11.5)\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/57\/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704\/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 42.0MB\/s ta 0:00:01\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/46\/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf\/googleapis-common-protos-1.51.0.tar.gz\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.18)\r\nBuilding wheels for collected packages: python-json-logger, anyconfig, future, googleapis-common-protos\r\n  Running setup.py bdist_wheel for python-json-logger ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\r\n  Running setup.py bdist_wheel for anyconfig ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\r\n  Running setup.py bdist_wheel for future ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\r\n  Running setup.py bdist_wheel for googleapis-common-protos ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\r\nSuccessfully built python-json-logger anyconfig future googleapis-common-protos\r\ncookiecutter 1.7.0 has requirement click>=7.0, but you'll have click 6.7 which is incompatible.\r\ngoogle-auth 1.12.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\r\ngoogle-cloud-bigquery 1.24.0 has requirement six<2.0.0dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\r\npip-tools 4.5.1 has requirement click>=7, but you'll have click 6.7 which is incompatible.\r\nInstalling collected packages: azure-common, azure-storage-common, azure-storage-file, fsspec, s3fs, tables, toposort, azure-storage-queue, whichcraft, future, poyo, binaryornot, arrow, jinja2-time, cookiecutter, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, pandas-gbq, pip-tools, azure-storage-blob, pyarrow, python-json-logger, anyconfig, kedro\r\n  Found existing installation: s3fs 0.1.5\r\n    Uninstalling s3fs-0.1.5:\r\n      Successfully uninstalled s3fs-0.1.5\r\n  Found existing installation: tables 3.4.3\r\n    Uninstalling tables-3.4.3:\r\n      Successfully uninstalled tables-3.4.3\r\nSuccessfully installed anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 cookiecutter-1.7.0 fsspec-0.7.1 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 oauthlib-3.1.0 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 s3fs-0.4.2 tables-3.5.2 toposort-1.5 whichcraft-0.6.1\r\n```\r\n\r\n### `pip install kedro` output from terminal\r\n```\r\nCollecting kedro\r\n  Using cached kedro-0.15.8-py3-none-any.whl (12.5 MB)\r\nCollecting pandas<1.0,>=0.24.0\r\n  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4 MB 9.6 MB\/s \r\nCollecting azure-storage-file<2.0,>=1.1.0\r\n  Using cached azure_storage_file-1.4.0-py2.py3-none-any.whl (30 kB)\r\nCollecting click<8.0\r\n  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 1.7 MB\/s \r\nCollecting cookiecutter<2.0,>=1.6.0\r\n  Using cached cookiecutter-1.7.0-py2.py3-none-any.whl (40 kB)\r\nCollecting SQLAlchemy<2.0,>=1.2.0\r\n  Downloading SQLAlchemy-1.3.15.tar.gz (6.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.1 MB 49.2 MB\/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting tables<3.6,>=3.4.4\r\n  Using cached tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\/python_json_logger-0.1.11-py2.py3-none-any.whl\r\nCollecting azure-storage-blob<2.0,>=1.1.0\r\n  Using cached azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\r\nCollecting pandas-gbq<1.0,>=0.12.0\r\n  Using cached pandas_gbq-0.13.1-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: fsspec<1.0,>=0.5.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.6.3)\r\nCollecting xlsxwriter<2.0,>=1.0.0\r\n  Downloading XlsxWriter-1.2.8-py2.py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 65.9 MB\/s \r\nCollecting pip-tools<5.0.0,>=4.0.0\r\n  Using cached pip_tools-4.5.1-py2.py3-none-any.whl (41 kB)\r\nCollecting pyarrow<1.0.0,>=0.12.0\r\n  Downloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63.1 MB 25 kB\/s \r\nCollecting xlrd<2.0,>=1.0.0\r\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 66.5 MB\/s \r\nRequirement already satisfied: s3fs<1.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.4.0)\r\nCollecting azure-storage-queue<2.0,>=1.1.0\r\n  Using cached azure_storage_queue-1.4.0-py2.py3-none-any.whl (23 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\/anyconfig-0.9.10-py2.py3-none-any.whl\r\nCollecting toposort<2.0,>=1.5\r\n  Using cached toposort-1.5-py2.py3-none-any.whl (7.6 kB)\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (2.23.0)\r\nRequirement already satisfied: pytz>=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2019.3)\r\nRequirement already satisfied: numpy>=1.13.3 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.18.1)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.8.1)\r\nCollecting azure-common>=1.1.5\r\n  Using cached azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\r\nCollecting azure-storage-common~=1.4\r\n  Using cached azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\r\nCollecting poyo>=0.1.0\r\n  Using cached poyo-0.5.0-py2.py3-none-any.whl (10 kB)\r\nCollecting jinja2-time>=0.1.0\r\n  Using cached jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\r\nCollecting whichcraft>=0.4.0\r\n  Using cached whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\r\nCollecting binaryornot>=0.2.0\r\n  Using cached binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.11.1)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\/future-0.18.2-cp36-none-any.whl\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (3.0.5)\r\nCollecting numexpr>=2.6.2\r\n  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 66.7 MB\/s \r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.14.0)\r\nCollecting pydata-google-auth\r\n  Using cached pydata_google_auth-0.3.0-py2.py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting google-cloud-bigquery>=1.11.1\r\n  Using cached google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\r\nCollecting google-auth\r\n  Using cached google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (46.1.1.post20200323)\r\nRequirement already satisfied: boto3>=1.9.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.12.27)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: idna<3,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nCollecting arrow\r\n  Using cached arrow-0.15.5-py2.py3-none-any.whl (46 kB)\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.1.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0\r\n  Using cached google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\r\nCollecting google-cloud-core<2.0dev,>=1.1.0\r\n  Using cached google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.11.3)\r\nCollecting google-api-core<2.0dev,>=1.15.0\r\n  Using cached google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.3.3)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.15.2)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.14.0)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\/googleapis_common_protos-1.51.0-cp36-none-any.whl\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.20)\r\nBuilding wheels for collected packages: SQLAlchemy\r\n  Building wheel for SQLAlchemy (PEP 517) ... done\r\n  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.15-cp36-cp36m-linux_x86_64.whl size=1215829 sha256=112167e02a19acada7f367d8aca55bbd1e0c655de9edfabebae5e9d055d9a9a6\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/4a\/1b\/3a\/c73044d7be48baeb47cbee343334f7803726ca1e9ba7b29095\r\nSuccessfully built SQLAlchemy\r\nInstalling collected packages: pandas, azure-common, azure-storage-common, azure-storage-file, click, poyo, arrow, jinja2-time, whichcraft, binaryornot, future, cookiecutter, SQLAlchemy, numexpr, tables, python-json-logger, azure-storage-blob, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery, pandas-gbq, xlsxwriter, pip-tools, pyarrow, xlrd, azure-storage-queue, anyconfig, toposort, kedro\r\n  Attempting uninstall: pandas\r\n    Found existing installation: pandas 0.22.0\r\n    Uninstalling pandas-0.22.0:\r\n      Successfully uninstalled pandas-0.22.0\r\nSuccessfully installed SQLAlchemy-1.3.15 anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 click-7.1.1 cookiecutter-1.7.0 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 numexpr-2.7.1 oauthlib-3.1.0 pandas-0.25.3 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 tables-3.5.2 toposort-1.5 whichcraft-0.6.1 xlrd-1.2.0 xlsxwriter-1.2.8\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n|environment | terminal | notebook|\r\n|----|----|----|\r\n|`kedro -V` | kedro, version 0.15.8 | kedro, version 0.15.8|\r\n|`python -V` | Python 3.6.10 :: Anaconda, Inc. | Python 3.6.5 :: Anaconda, Inc.|\r\n|os |  `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"` | `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"`|\r\n|`pip freeze` | anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==1.3.0<br>attrs==19.3.0<br>autovizwidget==0.12.9<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>backcall==0.1.0<br>bcrypt==3.1.7<br>binaryornot==0.4.4<br>bleach==3.1.0<br>boto3==1.12.27<br>botocore==1.15.27<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.14.0<br>chardet==3.0.4<br>click==7.1.1<br>colorama==0.4.3<br>cookiecutter==1.7.0<br>cryptography==2.8<br>decorator==4.4.2<br>defusedxml==0.6.0<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.15.2<br>entrypoints==0.3<br>environment-kernels==1.1.1<br>fsspec==0.6.3<br>future==0.18.2<br>gitdb==4.0.2<br>GitPython==3.1.0<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>hdijupyterutils==0.12.9<br>idna==2.9<br>importlib-metadata==1.5.0<br>ipykernel==5.1.4<br>ipython==7.13.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.5.1<br>jedi==0.16.0<br>Jinja2==2.11.1<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>json5==0.9.3<br>jsonschema==3.2.0<br>jupyter==1.0.0<br>jupyter-client==6.0.0<br>jupyter-console==6.1.0<br>jupyter-core==4.6.1<br>jupyterlab==1.2.7<br>jupyterlab-git==0.9.0<br>jupyterlab-server==1.0.7<br>kedro==0.15.8<br>MarkupSafe==1.1.1<br>mistune==0.8.4<br>mock==3.0.5<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.3<br>nbconvert==5.6.1<br>nbdime==2.0.0<br>nbexamples==0.0.0<br>nbformat==5.0.4<br>nbserverproxy==0.3.2<br>nose==1.3.7<br>notebook==5.7.8<br>numexpr==2.7.1<br>numpy==1.18.1<br>oauthlib==3.1.0<br>packaging==20.3<br>pandas==0.25.3<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.6.2<br>pexpect==4.8.0<br>pickleshare==0.7.5<br>pid==3.0.0<br>pip-tools==4.5.1<br>plotly==4.5.4<br>poyo==0.5.0<br>prometheus-client==0.7.1<br>prompt-toolkit==3.0.3<br>protobuf==3.11.3<br>protobuf3-to-dict==0.1.5<br>psutil==5.7.0<br>psycopg2==2.8.4<br>ptyprocess==0.6.0<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycparser==2.20<br>pydata-google-auth==0.3.0<br>pygal==2.4.0<br>Pygments==2.6.1<br>pykerberos==1.1.14<br>PyNaCl==1.3.0<br>pyOpenSSL==19.1.0<br>pyparsing==2.4.6<br>pyrsistent==0.15.7<br>PySocks==1.7.1<br>pyspark==2.3.2<br>python-dateutil==2.8.1<br>python-json-logger==0.1.11<br>pytz==2019.3<br>PyYAML==5.3.1<br>pyzmq==18.1.1<br>qtconsole==4.7.1<br>QtPy==1.9.0<br>requests==2.23.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rsa==3.4.2<br>s3fs==0.4.0<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-experiments==0.1.10<br>sagemaker-nbi-agent==1.0<br>sagemaker-pyspark==1.2.8<br>scipy==1.4.1<br>Send2Trash==1.5.0<br>six==1.14.0<br>smdebug-rulesconfig==0.1.2<br>smmap==3.0.1<br>sparkmagic==0.15.0<br>SQLAlchemy==1.3.15<br>tables==3.5.2<br>terminado==0.8.3<br>testpath==0.4.4<br>texttable==1.6.2<br>toposort==1.5<br>tornado==6.0.4<br>traitlets==4.3.3<br>urllib3==1.22<br>wcwidth==0.1.8<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>whichcraft==0.6.1<br>widgetsnbextension==3.5.1<br>xlrd==1.2.0<br>XlsxWriter==1.2.8<br>zipp==2.2.0 | alabaster==0.7.10<br>anaconda-client==1.6.14<br>anaconda-project==0.8.2<br>anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==0.24.0<br>astroid==1.6.3<br>astropy==3.0.2<br>attrs==18.1.0<br>Automat==0.3.0<br>autovizwidget==0.15.0<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>Babel==2.5.3<br>backcall==0.1.0<br>backports.shutil-get-terminal-size==1.0.0<br>bcrypt==3.1.7<br>beautifulsoup4==4.6.0<br>binaryornot==0.4.4<br>bitarray==0.8.1<br>bkcharts==0.2<br>blaze==0.11.3<br>bleach==2.1.3<br>bokeh==1.0.4<br>boto==2.48.0<br>boto3==1.12.27<br>botocore==1.15.27<br>Bottleneck==1.2.1<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.11.5<br>characteristic==14.3.0<br>chardet==3.0.4<br>click==6.7<br>cloudpickle==0.5.3<br>clyent==1.2.2<br>colorama==0.3.9<br>contextlib2==0.5.5<br>cookiecutter==1.7.0<br>cryptography==2.8<br>cycler==0.10.0<br>Cython==0.28.4<br>cytoolz==0.9.0.1<br>dask==0.17.5<br>datashape==0.5.4<br>decorator==4.3.0<br>defusedxml==0.6.0<br>distributed==1.21.8<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.14<br>entrypoints==0.2.3<br>enum34==1.1.9<br>environment-kernels==1.1.1<br>et-xmlfile==1.0.1<br>fastcache==1.0.2<br>filelock==3.0.4<br>Flask==1.0.2<br>Flask-Cors==3.0.4<br>fsspec==0.7.1<br>future==0.18.2<br>gevent==1.3.0<br>glob2==0.6<br>gmpy2==2.0.8<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>greenlet==0.4.13<br>h5py==2.8.0<br>hdijupyterutils==0.15.0<br>heapdict==1.0.0<br>html5lib==1.0.1<br>idna==2.6<br>imageio==2.3.0<br>imagesize==1.0.0<br>importlib-metadata==1.5.0<br>ipykernel==4.8.2<br>ipyparallel==6.2.2<br>ipython==6.4.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.4.0<br>isort==4.3.4<br>itsdangerous==0.24<br>jdcal==1.4<br>jedi==0.12.0<br>Jinja2==2.10<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>jsonschema==2.6.0<br>jupyter==1.0.0<br>jupyter-client==5.2.3<br>jupyter-console==5.2.0<br>jupyter-core==4.4.0<br>jupyterlab==0.32.1<br>jupyterlab-launcher==0.10.5<br>kedro==0.15.8<br>kiwisolver==1.0.1<br>lazy-object-proxy==1.3.1<br>llvmlite==0.23.1<br>locket==0.2.0<br>lxml==4.2.1<br>MarkupSafe==1.0<br>matplotlib==3.0.3<br>mccabe==0.6.1<br>mistune==0.8.3<br>mkl-fft==1.0.0<br>mkl-random==1.0.1<br>mock==4.0.1<br>more-itertools==4.1.0<br>mpmath==1.0.0<br>msgpack==0.6.0<br>msgpack-python==0.5.6<br>multipledispatch==0.5.0<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.2<br>nbconvert==5.4.1<br>nbformat==4.4.0<br>networkx==2.1<br>nltk==3.3<br>nose==1.3.7<br>notebook==5.5.0<br>numba==0.38.0<br>numexpr==2.6.5<br>numpy==1.14.3<br>numpydoc==0.8.0<br>oauthlib==3.1.0<br>odo==0.5.1<br>olefile==0.45.1<br>opencv-python==3.4.2.17<br>openpyxl==2.5.3<br>packaging==20.1<br>pandas==0.24.2<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.2.0<br>partd==0.3.8<br>path.py==11.0.1<br>pathlib2==2.3.2<br>patsy==0.5.0<br>pep8==1.7.1<br>pexpect==4.5.0<br>pickleshare==0.7.4<br>Pillow==5.1.0<br>pip-tools==4.5.1<br>pkginfo==1.4.2<br>plotly==4.5.2<br>pluggy==0.6.0<br>ply==3.11<br>poyo==0.5.0<br>prompt-toolkit==1.0.15<br>protobuf==3.6.1<br>protobuf3-to-dict==0.1.5<br>psutil==5.4.5<br>psycopg2==2.7.5<br>ptyprocess==0.5.2<br>py==1.5.3<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycodestyle==2.4.0<br>pycosat==0.6.3<br>pycparser==2.18<br>pycrypto==2.6.1<br>pycurl==7.43.0.1<br>pydata-google-auth==0.3.0<br>pyflakes==1.6.0<br>pygal==2.4.0<br>Pygments==2.2.0<br>pykerberos==1.2.1<br>pylint==1.8.4<br>PyNaCl==1.3.0<br>pyodbc==4.0.23<br>pyOpenSSL==18.0.0<br>pyparsing==2.2.0<br>PySocks==1.6.8<br>pyspark==2.3.2<br>pytest==3.5.1<br>pytest-arraydiff==0.2<br>pytest-astropy==0.3.0<br>pytest-doctestplus==0.1.3<br>pytest-openfiles==0.3.0<br>pytest-remotedata==0.2.1<br>python-dateutil==2.7.3<br>python-json-logger==0.1.11<br>pytz==2018.4<br>PyWavelets==0.5.2<br>PyYAML==5.3.1<br>pyzmq==17.0.0<br>QtAwesome==0.4.4<br>qtconsole==4.3.1<br>QtPy==1.4.1<br>requests==2.20.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rope==0.10.7<br>rsa==3.4.2<br>ruamel-yaml==0.15.35<br>s3fs==0.4.2<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-pyspark==1.2.8<br>scikit-image==0.13.1<br>scikit-learn==0.20.3<br>scipy==1.1.0<br>seaborn==0.8.1<br>Send2Trash==1.5.0<br>simplegeneric==0.8.1<br>singledispatch==3.4.0.3<br>six==1.11.0<br>smdebug-rulesconfig==0.1.2<br>snowballstemmer==1.2.1<br>sortedcollections==0.6.1<br>sortedcontainers==1.5.10<br>sparkmagic==0.12.5<br>Sphinx==1.7.4<br>sphinxcontrib-websupport==1.0.1<br>spyder==3.2.8<br>SQLAlchemy==1.2.11<br>statsmodels==0.9.0<br>sympy==1.1.1<br>tables==3.5.2<br>TBB==0.1<br>tblib==1.3.2<br>terminado==0.8.1<br>testpath==0.3.1<br>texttable==1.6.2<br>toolz==0.9.0<br>toposort==1.5<br>tornado==5.0.2<br>traitlets==4.3.2<br>typing==3.6.4<br>unicodecsv==0.14.1<br>urllib3==1.23<br>wcwidth==0.1.7<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>Werkzeug==0.14.1<br>whichcraft==0.6.1<br>widgetsnbextension==3.4.2<br>wrapt==1.10.11<br>xlrd==1.1.0<br>XlsxWriter==1.0.4<br>xlwt==1.3.0<br>zict==0.1.3<br>zipp==3.0.0|\r\n",
        "Challenge_closed_time":1585791347000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1585713762000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running a query on the `aws_sagemaker_notebook_instance` table in Steampipe. The error message indicates that the `hydrate call listAwsSageMakerNotebookInstanceTags` failed with a panic interface conversion. The user is using Steampipe version v0.4.1 and aws plugin version v0.15.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro\/issues\/308",
        "Challenge_link_count":35,
        "Challenge_participation_count":5,
        "Challenge_readability":22.7,
        "Challenge_reading_time":580.3,
        "Challenge_repo_contributor_count":175.0,
        "Challenge_repo_fork_count":793.0,
        "Challenge_repo_issue_count":2409.0,
        "Challenge_repo_star_count":8408.0,
        "Challenge_repo_watch_count":105.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":434,
        "Challenge_solved_time":21.5513888889,
        "Challenge_title":"Sagemaker notebooks raise error for `pandas.CSVDataSet`",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1973,
        "Discussion_body":"Kedro aside there are a couple of things that you can do to ensure that your environments match from the terminal vs notebook.  I am not familiar with the new `pandas.CSVDataSet` as I am just now starting with my first `0.15.8` myself.  We have struggled to get package installs correct through our notebooks, I make sure my team is all using their own environment, created from the terminal.\r\n\r\n## activate python3 from the terminal before install\r\n\r\nNote that the file browser on the left hand side of a SageMaker notebook is really mounted at `~\/SageMaker`.\r\n\r\n``` bash\r\nsource activate python3\r\n# may also be - conda activate python3\r\n# unrelated on windows it was - activate python 3\r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n## install ipykernel in your terminal env\r\n\r\nFor conda environments to show up in the notebook dropdown selection you will need `ipykernel` installed. see [docs](https:\/\/ipython.readthedocs.io\/en\/stable\/install\/kernel_install.html)\r\n\r\n```\r\nconda create -n testing python=3.6\r\npip install ipykernel\r\n# I typically don't have to go this far, but installing ipykernel is recommended by the docs\r\nipykernel install --user \r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n\r\n\r\nDo note that if you shut down your SageMaker notebook you will loose your packages and environments by default.\r\n\r\nI also noticed that you have a difference between pandas.  I have no idea if that changes things, but might be a simple fix. Your second idea worked @WaylonWalker. I slightly adapted it as it didn't work straight up:\r\n```\r\nconda create --yes --name kedroenv python=3.6 ipykernel\r\nsource activate kedroenv\r\npython -m ipykernel install --user --name kedroenv --display-name \"Kedro py3.6\"\r\n\r\ncd ~\/Sagemaker\r\nkedro new # Name testing and example pipeline\r\ncd testing\/\r\nkedro run\r\n```\r\nWith a reasonable solution, I'll call this issue closed. Massive thank you @WaylonWalker for pointing me in the right direction.\r\n\r\nCheers,\r\nTom @tjcuddihy We're working with the AWS team to produce a knowledge document on using Kedro and Sagemaker. Would we be able to talk to you about how you used them together? I'd be keen on learning more about how to make Sagemaker play nicely with kedro so I can still access everything I need from my kedro context. @yetudada I have an alpha version of a kedro plugin that plays nicely with sagemaker and allows you to run processing jobs. @uwaisiqbal then you might be interested in this knowledge article that was just published on AWS: https:\/\/aws.amazon.com\/blogs\/opensource\/using-kedro-pipelines-to-train-amazon-sagemaker-models\/ \ud83d\ude80 ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"query error on Steampipe"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1746825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello community,     <br \/>\nI'm facing a problem, my ACR in my resource group was deleted and I couldn't create any instance. I created again and now I can create instances but i'm having problems to run the dataset profile. It's failing to pull the image docker.    <\/p>\n<p>This is the output    <\/p>\n<pre><code>AzureMLCompute job failed.  \nFailedPullingImage: Unable to pull docker image  \n\timageName: 19acd0cdf57549bcace363c924cf045b.azurecr.io\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f  \n\terror: Run docker command to pull public image failed with error: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.  \n.  \n\tReason: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.  \n  \n\tInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.  \n<\/code><\/pre>\n<p>The ML Studio has the following permissions on the ACR permissions    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/132698-unbenannt.png?platform=QnA\" alt=\"132698-unbenannt.png\" \/>    <\/p>\n<p>The docker image appears in the repositories of the ACR    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/132781-unbenannt2.png?platform=QnA\" alt=\"132781-unbenannt2.png\" \/>    <\/p>\n<p>Any hint how can i solve this problem?    <\/p>\n<p>Thanks in advance    <\/p>",
        "Challenge_closed_time":1631803071767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631798842910,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to pull a docker image from the Container Registry after recreating it due to deletion. The error message indicates an authentication issue and the ML Studio has the necessary permissions on the ACR. The user is seeking help to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/555024\/not-able-to-pull-docker-image-from-container-regis",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":23.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1.1746825,
        "Challenge_title":"Not able to pull docker image from Container Registry",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":175,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=5565f700-af3b-41a9-b47f-9b8a6276d8fd\">@Moresi, Marco  <\/a> Does this container registry have the admin account enabled? A requirement while creating a workspace with an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace-cli?tabs=bringexistingresources1%2Cvnetpleconfigurationsv1cli#create-a-workspace\">existing container registry<\/a> is to have the admin account enabled.     <\/p>\n<p>If you have already enabled it then a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace-cli?tabs=bringexistingresources1%2Cvnetpleconfigurationsv1cli#sync-keys-for-dependent-resources\">re-sync of keys<\/a> might be required for your workspace.    <\/p>\n<pre><code>az ml workspace sync-keys -w &lt;workspace-name&gt; -g &lt;resource-group-name&gt;  \n<\/code><\/pre>\n<p>Deleting the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace?tabs=python#deleting-the-azure-container-registry\">default container registry<\/a> used by the workspace can also cause the workspace to break.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.3,
        "Solution_reading_time":14.97,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"authentication issue with ACR"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":12.0333325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to construct an <code>Explanation<\/code> object for a unit test, but can't seem to get it to work. Here's what I'm trying:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.compat.types.explanation_v1.Explanation(\n    attributions=aiplatform.compat.types.explanation_v1.Attribution(\n        {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n    )\n)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>&quot;.venv\/lib\/python3.7\/site-packages\/proto\/message.py&quot;, line 521, in __init__\n    super().__setattr__(&quot;_pb&quot;, self._meta.pb(**params))\nTypeError: Value must be iterable\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/413#issuecomment-872094378\" rel=\"nofollow noreferrer\">this<\/a> on github, but I'm not sure how to apply that workaround here.<\/p>",
        "Challenge_closed_time":1644385932888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644344024883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble constructing an Explanation object for a unit test using the Google Cloud AI Platform library. The code they are trying to use is giving a TypeError, and they are unsure how to apply a workaround they found on GitHub.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71038823",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":16.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":11.6411125,
        "Challenge_title":"Cannot construct an Explanation object",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":83,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519933306780,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3576.0,
        "Poster_view_count":203.0,
        "Solution_body":"<p>As the error mentioned value to be passed at <code>attributions<\/code> should be <strong>iterable<\/strong>. See <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.Explanation\" rel=\"nofollow noreferrer\">Explanation attributes documentation<\/a>.<\/p>\n<p>I tried your code and placed the <code>Attribution<\/code> object in a list and the error is gone. I assigned your objects in variables just so the code is readable.<\/p>\n<p>See code and testing below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\ntest = {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n\nattributions=aiplatform.compat.types.explanation_v1.Attribution(test)\nx  = aiplatform.compat.types.explanation_v1.Explanation(\n    attributions=[attributions]\n)\nprint(x)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>attributions {\n  baseline_output_value: 0.9280818700790405\n  instance_output_value: 0.6717480421066284\n  feature_attributions {\n    struct_value {\n      fields {\n        key: &quot;feature_1&quot;\n        value {\n          number_value: -0.0410824716091156\n        }\n      }\n      fields {\n        key: &quot;feature_2&quot;\n        value {\n          number_value: 0.01155053575833639\n        }\n      }\n    }\n  }\n  output_index: 0\n  output_display_name: &quot;true&quot;\n  approximation_error: 0.010399332817679649\n  output_name: &quot;scores&quot;\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1644387344880,
        "Solution_link_count":1.0,
        "Solution_readability":17.4,
        "Solution_reading_time":22.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":111.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"TypeError constructing Explanation object"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.8511111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Customer who has created a AWS glue dev endpoint and want to run two Sagemaker notebooks in parallel on same single Dev endpoint but its not working .\n\nThe one which is invoked first is only able to run the job, while another one fails. what could be possible reasons and fix for it?",
        "Challenge_closed_time":1591030326000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591020062000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run two Sagemaker notebooks in parallel on a single AWS Glue Dev Endpoint, but only the first one is able to run successfully while the second one fails. The user is seeking possible reasons and solutions for this issue.",
        "Challenge_last_edit_time":1668561655230,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUIDitlJMgTlGai61w_Zvqdg\/running-concurrent-sessions-from-sagemaker-notebooks-on-glue-dev-endpoints",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":4.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.8511111111,
        "Challenge_title":"Running concurrent sessions from SageMaker notebooks on Glue Dev Endpoints.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":311.0,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"SageMaker notebooks are Jupyter notebooks that uses the SparkMagic module to connect to a local Livy setup. The local Livy does an SSH tunnel to Livy service on the Glue Spark server. Apache Livy binds to post 8998 and is a RESTful service that can relay multiple Spark session commands at the same time so multiple port binding conflicts cannot happen. So yes, you can have multiple sessions as long as the backend cluster has resources to serve that many sessions.\n\nYou can run the following command in a notebook to check the defaults for Spark sessions:\n\n```\nspark.sparkContext.getConf().getAll()\n```\n\nI see the following defaults in my Spark session. You can easily override them from the config file at ~\/.sparkmagic\/config.json or by using the %%configure magic from within the notebook. \n\n```\nspark.executor.cores 4\nspark.executor.memory 5g\nspark.driver.memory 5g\n```\n\nNote that spark.executor.instances is not set and spark.dynamicAllocation.enabled is not overridden which means that it is true, so if you have a demanding Spark job in one notebook, it can take over all resources in the cluster and prevent other Spark sessions from starting. The recommendation when sharing a single Glue Dev endpoint is to limit each session to a few executors so that multiple sessions can acquire resources from the cluster e.g.:\n\n```\n%%configure -f\n{\"executorMemory\": \"5G\", \"executorCores\":4,\"numExecutors\":2}\n```\n\n(Note: Tested on multiple SageMaker PySpark notebooks in single SageMaker notebook instances as well as multiple SageMaker notebook instances.)",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925564412,
        "Solution_link_count":0.0,
        "Solution_readability":9.3,
        "Solution_reading_time":19.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":231.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"second notebook fails"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4417.9619444444,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Challenge_closed_time":1599677796000,
        "Challenge_comment_count":15,
        "Challenge_created_time":1583773133000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to install AWS stepfunctions using pip install in SageMaker Studio Notebook. The error message shows that the metadata generation failed due to an AttributeError, and the user received a CryptographyDeprecationWarning. The user expected to be able to install AWS stepfunctions, but the installation failed. The environment used was AWS Step Functions Data Science Python SDK version 2.3.0 and Python version 3.7.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Challenge_link_count":0,
        "Challenge_participation_count":15,
        "Challenge_readability":18.6,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":52.0,
        "Challenge_repo_issue_count":216.0,
        "Challenge_repo_star_count":146.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4417.9619444444,
        "Challenge_title":"unable to kick off the sagemaker job",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":193,
        "Discussion_body":"@charlesa101  Thanks for trying out. I am assuming you have replaced input, output buckets and role Arn. \r\n\r\nWould you please run the following command provide the output ?\r\n\r\n```\r\nkubectl  get trainingjobs xgboost-mnist\r\nkubectl describe trainingjob xgboost-mnist\r\n``` @gautamkmr, here you go thank you! yeah i have my own bucket and sagemaker executor role\r\n\r\n```kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nxgboost-mnist                               2020-03-09T16:51:08Z ```\r\n\r\n```kubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400``` @charlesa101  Thanks for providing the output. It appears that operator is not running successfully on your k8s cluster.  you can verify that \r\n\r\n```\r\n kubectl get pods -A | grep -i sagemaker\r\n```\r\n\r\nYou can follow steps from [here](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#setup-and-operator-deployment) to install the operator, let us know if you face any issue. yeah that's what i noticed as well now\r\n\r\n```kubectl get pods -n sagemaker-k8s-operator-system\r\nNAME                                                         READY   STATUS    RESTARTS   AGE\r\nsagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8   0\/2     Pending   0          24h``` ```kubectl describe pod  -n sagemaker-k8s-operator-system                                             \r\nName:               sagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8\r\nNamespace:          sagemaker-k8s-operator-system\r\nPriority:           0\r\nPriorityClassName:  <none>\r\nNode:               <none>\r\nLabels:             control-plane=controller-manager\r\n                    pod-template-hash=5858fd7b8d\r\nAnnotations:        kubernetes.io\/psp: eks.privileged\r\nStatus:             Pending\r\nIP:                 \r\nControlled By:      ReplicaSet\/sagemaker-k8s-operator-controller-manager-5858fd7b8d\r\nContainers:\r\n  kube-rbac-proxy:\r\n    Image:      gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Port:       8443\/TCP\r\n    Host Port:  0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    Environment:\r\n      AWS_ROLE_ARN:                 arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\n  manager:\r\n    Image:      957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:  \r\n      AWS_ROLE_ARN:                    arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\nConditions:\r\n  Type           Status\r\n  PodScheduled   False \r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  sagemaker-k8s-operator-default-token-rwdkn:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  sagemaker-k8s-operator-default-token-rwdkn\r\n    Optional:    false\r\nQoS Class:       Burstable\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type     Reason            Age                   From               Message\r\n  ----     ------            ----                  ----               -------\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n my eks\/ecr is on us-east2, but it seems all the crd artifacts are coming from us-east1 could that be the issue?\r\n EKS can pull the image from other region too. I think in your case it seems that you don't have any worker node associated to cluster?  At least thats what below message says.\r\n```\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n```\r\n\r\nCan you run ?  \r\n```\r\nkubectl get node\r\n``` @charlesa101  did you get chance to review it again? ``` kubectl get nodes\r\nNAME                                           STATUS   ROLES    AGE     VERSION\r\nip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n yeah i did, recreated the cluster again but still the same issue\r\n @charlesa101   In previous describe output of `pod` it appears that cluster did not have any worker nodes available `(no nodes available to schedule pods)`.\r\n\r\nBut based on recent output it appears that you have three worker nodes available. \r\n\r\n> NAME                                           STATUS   ROLES    AGE     VERSION\r\n> ip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n\r\n\r\nCould you please describe each of these nodes and operator pod ?\r\n\r\n```\r\n# Describe nodes , assuming the names of nodes are same as you mentioned in previous comment.\r\nkubectl describe node ip-172-16-116-51.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-121-255.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-137-197.us-east-2.compute.internal \r\n```\r\n\r\n\r\n```\r\n#Get the operator pod name \r\nkubectl get pods -A | grep -i sagemaker\r\nkubectl describe pod <put the pod name here>  -n sagemaker-k8s-operator-system\r\n```\r\n\r\n\r\nIf operator has been deployed successfully and if trainingjob is still not yet running please attach the out put of describe trainingjob as well ? \r\n```\r\nkubectl describe trainingjob xgboost-mnist\r\n\r\n```\r\n\r\n i tried to look checked the operator pod, here is  the log @gautamkmr \r\n\r\n```\r\nkubectl logs -f sagemaker-k8s-operator-controller-manager-5858fd7b8d-2dk5c  -n sagemaker-k8s-operator-system manager\r\n2020-03-15T18:09:13.864Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    setup   starting manager\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"trainingjob\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"model\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"batchtransformjob\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hostingdeployment\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"endpointconfig\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hyperparametertuningjob\"}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"trainingjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"model\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Job status is empty, setting to intermediate status     {\"trainingjob\": \"default\/xgboost-mnist\", \"status\": \"SynchronizingK8sJobWithSageMaker\"}\r\n2020-03-15T19:09:19.963Z        INFO    controllers.TrainingJob Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"new-status\": {\"trainingJobStatus\":\"SynchronizingK8sJobWithSageMaker\",\"lastCheckTime\":\"2020-03-15T19:09:19Z\"}}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Adding generated name to spec   {\"trainingjob\": \"default\/xgboost-mnist\", \"new-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}\r\n2020-03-15T19:09:19.982Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:20.916Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:09:20.916Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\",\"lastCheckTime\":\"2020-03-15T19:09:20Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:09:20.924Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:42.150Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:11:42.150Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\",\"lastCheckTime\":\"2020-03-15T19:11:42Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:11:42.159Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n```\r\n @charlesa101  Thanks for sharing the log. You are on right track. I think the issue now is operator pod is unable to retrieve credentials from IAM service to talk to sagemaker. \r\n\r\n`\"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n`\r\n\r\nCould you please check your [trust.json](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#create-an-iam-role) basically **trust policy have three places to update cluster region and OIDC ID and one place to add your AWS account number.** Hi @charlesa101\r\n\r\nClosing this issue since there has been no activity in 90 days. Please re-open if you still need help\r\n\r\nThanks Hi, I'm having the exact same issue except that my pod is running fine. I setup my k8s cluster using terraform with 1 master node and 1 worker node. When I submit the trainingjob, there is no status or job name or anything else. I tried all the commands above and it looks like the scheduler was able to assign the pods to the worker node. Any help would be appreciated! Please see outputs for commands below:\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get pods -A                                                                                                                                                                                                                                                    \r\nNAMESPACE        NAME                                                         READY   STATUS    RESTARTS   AGE                                                                                                                                                                                                                \r\nkube-system      aws-node-67tgx                                               1\/1     Running   0          2d18h\r\nkube-system      aws-node-k2q7z                                               1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-cwfvj                                     1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-x5ld9                                     1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-54vm5                                             1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-r8j7j                                             1\/1     Running   0          2d18h\r\nkube-system      metrics-server-64cf6869bd-6nppx                              1\/1     Running   0          2d18h\r\nsagemaker-jobs   sagemaker-k8s-operator-controller-manager-855f498957-fhkvv   2\/2     Running   0          2d18h\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe pod sagemaker-k8s-operator-controller-manager-855f498957-fhkvv -n sagemaker-jobs\r\nName:         sagemaker-k8s-operator-controller-manager-855f498957-fhkvv\r\nNamespace:    sagemaker-jobs\r\nPriority:     0\r\nNode:         ip-10-0-1-245.us-west-2.compute.internal\/10.0.1.245\r\nStart Time:   Fri, 24 Jun 2022 22:26:03 +0000\r\nLabels:       control-plane=controller-manager\r\n              pod-template-hash=855f498957\r\nAnnotations:  kubernetes.io\/psp: eks.privileged\r\nStatus:       Running\r\nIP:           10.0.1.144\r\nIPs:\r\n  IP:           10.0.1.144\r\nControlled By:  ReplicaSet\/sagemaker-k8s-operator-controller-manager-855f498957\r\nContainers:\r\n  manager:\r\n    Container ID:  docker:\/\/d8fc52b3e20a050999d3f24ab914f1d865a84a168a8b038f3fa81ce59cccbced\r\n    Image:         957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Image ID:      docker-pullable:\/\/957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s@sha256:94ffbba68954249b1724fdb43f1e8ab13547114555b4a217849687d566191e23\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n      --namespace=sagemaker-jobs\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:09 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:\r\n      AWS_DEFAULT_REGION:              us-west-2\r\n      AWS_REGION:                      us-west-2\r\n      AWS_ROLE_ARN:                    arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nkube-rbac-proxy:\r\n    Container ID:  docker:\/\/4ecdaa395fdc70d5cead609465dbf21f6e11771a80ad5db0a6125053ab08b9d3\r\n    Image:         gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Image ID:      docker-pullable:\/\/gcr.io\/kubebuilder\/kube-rbac-proxy@sha256:297896d96b827bbcb1abd696da1b2d81cab88359ac34cce0e8281f266b4e08de\r\n    Port:          8443\/TCP\r\n    Host Port:     0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:11 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:\r\n      AWS_DEFAULT_REGION:           us-west-2\r\n      AWS_REGION:                   us-west-2\r\n      AWS_ROLE_ARN:                 arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             True\r\n  ContainersReady   True\r\n  PodScheduled      True\r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  kube-api-access-6j8rt:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io\/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io\/unreachable:NoExecute op=Exists for 300s\r\nEvents:                      <none>\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl logs sagemaker-k8s-operator-controller-manager-855f498957-fhkvv manager -n sagemaker-jobs\r\nI0624 22:26:11.339445       1 request.go:621] Throttling request took 1.046981399s, request: GET:https:\/\/172.20.0.1:443\/apis\/extensions\/v1beta1?timeout=32s\r\n2022-06-24T22:26:12.443Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2022-06-24T22:26:12.443Z        INFO    Starting manager in the namespace:      sagemaker-jobs\r\n2022-06-24T22:26:12.443Z        INFO    setup   starting manager\r\n2022-06-24T22:26:12.444Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.665Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\"}\r\n2022-06-24T22:26:12.746Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\"}\r\n2022-06-24T22:26:12.747Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nosic-test-run                               2022-06-24T22:38:13Z  \r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe trainingjob osic-test-run                                                                                                                                                                                                                             \r\nName:         osic-test-run                                                                                                                                                                                                                                                                                                   \r\nNamespace:    default                                                                                                                                                                                                                                                                                                         \r\nLabels:       <none>                                                                                                                                                                                                                                                                                                          \r\nAnnotations:  <none>                                                                                                                                                                                                                                                                                                          \r\nAPI Version:  sagemaker.aws.amazon.com\/v1                                                                                                                                                                                                                                                                                     \r\nKind:         TrainingJob                                                                                                                                                                                                                                                                                                     \r\nMetadata:                                                                                                                                                                                                                                                                                                                     \r\n  Creation Timestamp:  2022-06-24T22:38:13Z                                                                                                                                                                                                                                                                                   \r\n  Generation:          1                                                                                                                                                                                                                                                                                                      \r\n  Managed Fields:\r\n    API Version:  sagemaker.aws.amazon.com\/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:kubectl.kubernetes.io\/last-applied-configuration:\r\n      f:spec:\r\n        .:\r\n        f:algorithmSpecification:\r\n          .:\r\n          f:trainingImage:\r\n          f:trainingInputMode:\r\n        f:inputDataConfig:\r\n        f:outputDataConfig:\r\n          .:\r\n          f:s3OutputPath:\r\n        f:region:\r\n        f:resourceConfig:\r\n          .:\r\n          f:instanceCount:\r\n          f:instanceType:\r\n          f:volumeSizeInGB:\r\n        f:roleArn:\r\n        f:stoppingCondition:\r\n          .:\r\n          f:maxRuntimeInSeconds:\r\n        f:trainingJobName:\r\n    Manager:         kubectl-client-side-apply\r\n    Operation:       Update\r\n    Time:            2022-06-24T22:38:13Z\r\n  Resource Version:  3182\r\n  UID:               0a0880c0-baf9-4f1a-8aa3-37480520c3e2\r\nSpec:\r\n  Algorithm Specification:\r\nTraining Image:       438029713005.dkr.ecr.us-west-2.amazonaws.com\/model-training:latest\r\n    Training Input Mode:  File\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Data Source:\r\n      s3DataSource:\r\n        s3DataDistributionType:  FullyReplicated\r\n        s3DataType:              S3Prefix\r\n        s3Uri:                   s3:\/\/osic-full-including-override\r\n  Output Data Config:\r\n    s3OutputPath:  s3:\/\/osic-full-including-override\/experiments\r\n  Region:          us-west-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.p3.2xlarge\r\n    Volume Size In GB:  500\r\n  Role Arn:             arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  900\r\n  Training Job Name:         osic-test-run\r\nEvents:                      <none>\r\n```\r\n\r\nplease let me know if you need to see anything else!",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"installation failed with AttributeError"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":224.7902777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nSWB 5.2.6 version - SageMaker jupyter notebook workspace can not mount a study.  see error in \/var\/log\/message\r\n\/usr\/local\/bin\/goofys[7248]: main.FATAL Mounting file system: Mount: mount: running fusermount: exec: \"fusermount\": executable file not found in $PATH#012#012stderr:\r\n\r\nLooks like the FUSE package failed to install during on-start.  if run \"sudo yum install fuse\" then you can run \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json to mount the study. \r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n - Is the deployment from a forked version of the repository?\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1672645855000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1671836610000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error message \"null is not an object\" while trying to connect to Sagemaker notebook. This error occurs when pop-ups are disabled in SWB and is confusing to the user. The user suggests a more descriptive error message such as \"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1089",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.1,
        "Challenge_reading_time":13.85,
        "Challenge_repo_contributor_count":43.0,
        "Challenge_repo_fork_count":116.0,
        "Challenge_repo_issue_count":1196.0,
        "Challenge_repo_star_count":165.0,
        "Challenge_repo_watch_count":25.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":224.7902777778,
        "Challenge_title":"[Bug] study fail to mount in SWB 5.2.6 SageMaker Jupyter Notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":146,
        "Discussion_body":"Thanks for reporting this issue. We are currently investigating the issue on versions 5.2.5 and 5.2.6. With the linked PR, the issue is now resolved and part of the 5.2.7 release. ",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"null object error"
    },
    {
        "Answerer_created_time":1622117284230,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":316.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":21.7817686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running this notebook in my managed notebooks environment on Google Cloud and I'm getting the following error when trying to install the packages: &quot;WARNING: The script google-oauthlib-tool is installed in '\/home\/jupyter\/.local\/bin' which is not on PATH.\nConsider adding this directory to PATH.&quot;<\/p>\n<p>Here is the python code that I'm trying to run for reference. <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb<\/a><\/p>\n<p>Any suggestions on how I can update the package installation so it is on path and resolve the error? I'm currently working on GCP user-managed notebooks on a Mac.<\/p>\n<p>Thanks so much for any tips!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>",
        "Challenge_closed_time":1658596861407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658518447040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install packages in their managed notebooks environment on Google Cloud. The error message suggests that the library is not installed on PATH and recommends adding the directory to PATH. The user is seeking suggestions on how to update the package installation to resolve the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73085293",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":21.7817686111,
        "Challenge_title":"Library is not installed on PATH - How can I install on path?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":109,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1621620820567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Open up your shell config file (likely .zshrc because the default shell on Mac is now zsh and that's the name of the zsh config file) located at your home directory in a text editor (TextEdit, etc) and add the path to the  executable.\nLike this:\nOpen the file:\n<code>open -e ~\/.zshrc<\/code>\nEdit the file:\nAdd this line at the top (may vary, check the documentation):\n<code>export PATH=&quot;\/home\/jupyter\/.local\/bin&quot;<\/code>\nThat may not work, try this:\n<code>export PATH=&quot;$PATH:\/home\/jupyter\/.local\/bin&quot;<\/code>\nYour best bet is to read the package documentation.<\/p>\n<p>After saving the config file, run <code>source ~\/.zshrc<\/code> and replace .zshrc with the config file name if it's different OR open a new terminal tab.<\/p>\n<p>What this does is tells the shell that the command exists and where to find it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.8,
        "Solution_reading_time":10.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":126.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"library not on PATH"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":558.1344444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug Description\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nwhen I do the example:\r\nqrun qrun benchmarks\\GATs\\workflow_config_gats_Alpha158.yaml\r\n\r\nI got the error info:\r\n\r\n\r\n\r\n(py38) D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples>qrun benchmarks\\GATs\\workflow_config_gats_Alpha158_full02.yaml\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [config.py:413] - default_conf: client.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.workflow - [expm.py:31] - experiment manager uri is at file:D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples\\mlruns\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:\/Users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [expm.py:316] - <mlflow.tracking.client.MlflowClient object at 0x0000017B5D406F40>\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [exp.py:260] - Experiment 3 starts running ...\r\n[7724:MainThread](2022-10-14 07:53:34,124) INFO - qlib.workflow - [recorder.py:339] - Recorder 41d40d173e614811bad721127a3204b8 starts running under Experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,140) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,158) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,164) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff --cached`\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"Param value\", MAX_PARAM_VAL_LENGTH, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nThe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nIncorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    mlflow.log_param(\"depth\", 3)\r\n    mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will throw an MlflowException for overwriting a\r\nlogged parameter.\r\n\r\nCorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 3)\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:MainThread](2022-10-14 07:53:35,515) INFO - qlib.GATs - [pytorch_gats_ts.py:81] - GATs pytorch version...\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:100] - GATs parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : LSTM\r\nmodel_path : None\r\nvisible_GPU : 0\r\nuse_GPU : True\r\nseed : None\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:146] - model:\r\nGATModel(\r\n  (rnn): LSTM(158, 64, num_layers=2, batch_first=True, dropout=0.7)\r\n  (transformation): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc_out): Linear(in_features=64, out_features=1, bias=True)\r\n  (leaky_relu): LeakyReLU(negative_slope=0.01)\r\n  (softmax): Softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nThen the program re-run again.\r\nI am wondering how to fix it.\r\nThanks a lot.\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - Qlib version:\r\n - 0.8.6.99'\r\n - Python version:\r\n - 3.8.5\r\n - OS (`Windows`, `Linux`, `MacOS`):\r\n - windows 10\r\n - Commit number (optional, please provide it if you are using the dev version):\r\n\r\n## Additional Notes\r\n\r\n<!-- Add any other information about the problem here. -->\r\n",
        "Challenge_closed_time":1667718001000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1665708717000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a 500 internal server error when using the metadata docker image to interact with the Neptune database. When testing locally, the user found two problems: an error with the read_timeout argument and a MalformedQueryException error. The user suggests removing the read_timeout and write_timeout arguments and replacing Order.decr with Order.desc to fix the issues.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1317",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.5,
        "Challenge_reading_time":92.22,
        "Challenge_repo_contributor_count":106.0,
        "Challenge_repo_fork_count":1980.0,
        "Challenge_repo_issue_count":1539.0,
        "Challenge_repo_star_count":11074.0,
        "Challenge_repo_watch_count":263.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":558.1344444444,
        "Challenge_title":"on qrun:\"mlflow.exceptions.MlflowException: Param value .... had length 780, which exceeded length limit of 500 \"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":583,
        "Discussion_body":"I had the same problem TT Same for all the example in `benchmarks\/LightGBM`. This is because mlflow limits the length of params since 1.28.0.\r\nWhile waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution. > This is because mlflow limits the length of params since 1.28.0. While waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution.\r\n\r\nThank you for help. Wish you have a good day.",
        "Discussion_score_count":5.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"500 internal server error"
    },
    {
        "Answerer_created_time":1518706063680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":95.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":100.6858180556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When AzureML creates a python environment and runs <code>pip install<\/code>, I'd like it to use additional non-public indices. Is there a way to do that?<\/p>\n\n<p>I'm running my python script on an AzureML compute. The environment is created from pip requirements as per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-use-environments#conda-and-pip-specification-files\" rel=\"nofollow noreferrer\">docs<\/a>. The script now references a package in a private index. To run the script on a local or build machine I just specify <code>PIP_EXTRA_INDEX_URL<\/code> environment variable with credentials to the index before running <code>pip install -c ...<\/code>. How to enable same functionality on AzureML environment prep process?<\/p>\n\n<p>AzureML docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-use-environments#private-wheel-files\" rel=\"nofollow noreferrer\">suggest<\/a> that I directly supply wheel files instead of package names. That means I have to manually do all the work that pip is built for: identify private packages among other requirements, choose right versions and platform, download them.<\/p>\n\n<p>Ideally, I would have to just write something like this:<\/p>\n\n<pre><code>myenv = Environment.from_pip_requirements(\n    name = \"myenv\",\n    file_path = \"path-to-pip-requirements-file\",\n    extra-index-url = [\"url1\", \"url2\"])\n<\/code><\/pre>",
        "Challenge_closed_time":1572973318072,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572610849127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a python environment on AzureML and wants to use additional non-public indices during the pip install process. They are looking for a way to specify the PIP_EXTRA_INDEX_URL environment variable with credentials to the index before running pip install -c, similar to how they do it on a local or build machine. The AzureML docs suggest supplying wheel files instead of package names, which requires manual work. The user is looking for a way to specify extra-index-url when creating an AzureML environment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58659160",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":19.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":100.6858180556,
        "Challenge_title":"How to specify pip extra-index-url when creating an azureml environment?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":5362.0,
        "Challenge_word_count":167,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518706063680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":95.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>It appears, there is a <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py#set-pip-option-pip-option-\" rel=\"nofollow noreferrer\"><code>set_pip_option<\/code> method<\/a> in the SDK which sorts out the problem with one single extra-index-url, e.g.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.environment import CondaDependencies\ndep = CondaDependencies.create(pip_packages=[\"pyyaml\", \"param\"])\ndep.set_pip_option(\"--extra-index-url https:\/\/user:password@extra.index\/url\")\n<\/code><\/pre>\n\n<p>Unfortunately, second call to this function replaces the first value with the new one. For the <code>--extra-index-url<\/code> option this logic should be changed in order to support search in more than 2 indices (one public, one private).<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.6,
        "Solution_reading_time":11.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":72.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"specify PIP_EXTRA_INDEX_URL in AzureML"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":96.0740822222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Challenge_closed_time":1613774515323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1613428648627,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in changing the clearml.conf file in AWS Sagemaker Jupyter notebook to specify credentials to S3 for storing artifacts and models. The user is encountering permission denied issues and is seeking a better approach.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.1,
        "Challenge_reading_time":6.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":96.0740822222,
        "Challenge_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":276.0,
        "Challenge_word_count":82,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585870038407,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Christchurch, \u041d\u043e\u0432\u0430\u044f \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u044f",
        "Poster_reputation_count":45.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.5,
        "Solution_reading_time":13.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"ClearML",
        "Challenge_type":"anomaly",
        "Challenge_summary":"permission denied in S3 access"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.9641666667,
        "Challenge_answer_count":0,
        "Challenge_body":"![image](https:\/\/user-images.githubusercontent.com\/37505775\/120101493-4f481c80-c181-11eb-8a20-4a044c2bd51c.png)\r\n\r\n- lr\uc744 \uc81c\uc678\ud558\uace0 \ub098\uba38\uc9c0 value\uac00 \uc5c5\ub370\uc774\ud2b8\uac00 \ub418\uc9c0 \uc54a\ub294 \ubb38\uc81c \ubc1c\uc0dd\r\n- \uacc4\uc18d \uac12\uc774 \ucd94\uac00\ub418\ub294 \ub9ac\uc2a4\ud2b8\uc778 \uc904 \ubaa8\ub974\uace0 list[0]\uc73c\ub85c \uc778\ub371\uc2f1\ud574\uc11c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub77c\uace0 \uc0dd\uac01\ub428",
        "Challenge_closed_time":1622440667000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1622372396000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with importing wandb due to a module not being found on a server running centOS, specifically the 'six.moves.collections_abc' module. The issue cannot be reproduced on a local Intel i7 system.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pstage-ocr-team6\/ocr-teamcode\/issues\/5",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":3.11,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":43.0,
        "Challenge_repo_star_count":12.0,
        "Challenge_repo_watch_count":0.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":18.9641666667,
        "Challenge_title":"[BUG] wandb value doesn't update",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":25,
        "Discussion_body":"![image](https:\/\/user-images.githubusercontent.com\/26226101\/120102333-71439e00-c185-11eb-8a20-b113112b0b3f.png)\r\n\r\n\uc544\uc774\uac70 \uc65c \uadf8\ub7f0\uac8c \ud588\ub124\uc694.... \uc218\uc815\ud574 \uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4!",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"module not found on server"
    },
    {
        "Answerer_created_time":1476806455803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Holzkirchen, Deutschland",
        "Answerer_reputation_count":3068.0,
        "Answerer_view_count":386.0,
        "Challenge_adjusted_solved_time":1585.9678913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>An Azure Data Factory pipeline for updating a trained ML model returns this error:<\/p>\n\n<pre><code>HTTP 404. The resource you are looking for (or one of its dependencies) could have been removed, had its name changed, or is temporarily unavailable. Please review the following URL and make sure that it is spelled correctly.\nRequested URL: \/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update\n\nDiagnostic details: job ID xxxx. Endpoint https:\/\/services.azureml.net\/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update.\n<\/code><\/pre>\n\n<p>I don't even want to think about why it returned a HTML document...\nI am 100% sure that the endpoint exists and the key provided is correct.\nSo what is my mistake?<\/p>",
        "Challenge_closed_time":1508942544572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1503048195263,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a 404 error while trying to update a trained ML model using an Azure Data Factory pipeline. The error message suggests that the resource may have been removed, had its name changed, or is temporarily unavailable. The user is certain that the endpoint exists and the key provided is correct.",
        "Challenge_last_edit_time":1503233060163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45753090",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":9.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1637.3192525,
        "Challenge_title":"Azure ML endpoint 404 error",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":494.0,
        "Challenge_word_count":99,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476806455803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Holzkirchen, Deutschland",
        "Poster_reputation_count":3068.0,
        "Poster_view_count":386.0,
        "Solution_body":"<p>Deleting and creating the endpoint again fixed it.<\/p>\n\n<p>Microsoft...<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"404 error updating model"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":52.9681911111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I cannot execute sagemaker notebook anymore.<br>\nThe following error occurs.<\/p>\n\n<pre><code>Failed to start kernel\nAn error occurred (ThrottlingException) when calling the CreateApp operation (reached max retries: 4): \nRate exceeded\n<\/code><\/pre>\n\n<p>I checked my app list and there are only two.\nOne app is trying to delete but never stops, this could be one of the problem.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/M0iqo.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Challenge_closed_time":1590563891208,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590373205720,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to execute AWS Sagemaker Notebook and is receiving a \"Failed to start kernel\" error with a \"ThrottlingException\" message. The user suspects that the issue may be related to an app that is stuck in the process of being deleted.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61994821",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":52.9681911111,
        "Challenge_title":"Cannot execute AWS Sagemaker Notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":965.0,
        "Challenge_word_count":64,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1532422348876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":27.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>Happened to me too. Contact support and ask them to delete the kernel behind the scenes.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.1,
        "Solution_reading_time":1.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"kernel start failure"
    },
    {
        "Answerer_created_time":1463756509236,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Uppsala, Sverige",
        "Answerer_reputation_count":400.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":3335.9770963889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently working on machine learning project with Azure Machine Learning Services. But I found the problem that I can't update a new docker image to the existing web service (I want to same url as running we service). <\/p>\n\n<p>I have read the documentation but it doesn't really tell me how to update (documentation link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where<\/a>). \nThe documentation said that we have to use update() with image = new-image. <\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\n\nservice_name = 'aci-mnist-3\n\n# Retrieve existing service\nservice = Webservice(name = service_name, workspace = ws)\n\n# Update the image used by the service\nservice.update(image = new-image)\n\nprint(service.state)\n<\/code><\/pre>\n\n<p>But the <code>new-image<\/code> isn't described where it comes from. <\/p>\n\n<p>Does anyone know how to figure out this problem?<\/p>\n\n<p>Thank you<\/p>",
        "Challenge_closed_time":1544528992767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544435800590,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in updating an existing web service with a new docker image on Azure Machine Learning Services. The user has read the documentation but is unsure how to update the image using the \"update()\" function as the documentation does not provide clear instructions on where to obtain the new image. The user is seeking assistance in resolving this issue.",
        "Challenge_last_edit_time":1554674817316,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53703191",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":25.8867158334,
        "Challenge_title":"How to update the existing web service with a new docker image on Azure Machine Learning Services?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":281.0,
        "Challenge_word_count":136,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1541599822383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangkok, \u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22",
        "Poster_reputation_count":23.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The documentation could be a little more clear on this part, I agree. The <code>new-image<\/code> is an image object that you should pass into the <code>update()<\/code> function. If you just created the image you might already have the object in a variable, then just pass it. If not, then you can obtain it from your workspace using<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.image.image import Image\nnew_image = Image(ws, image_name)\n<\/code><\/pre>\n\n<p>where <code>ws<\/code> is your workspace object and <code>image_name<\/code> is a string with the name of the image you want to obtain. Then you go on calling <code>update()<\/code> as<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.webservice import Webservice\n\nservice_name = 'aci-mnist-3'\n\n# Retrieve existing service\nservice = Webservice(name = service_name, workspace = ws)\n\n# Update the image used by the service\nservice.update(image = new_image) # Note that dash isn't supported in variable names\n\nprint(service.state)\n<\/code><\/pre>\n\n<p>You can find more information in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/intro?view=azure-ml-py\" rel=\"nofollow noreferrer\">SDK documentation<\/a><\/p>\n\n<p>EDIT:\nBoth the <code>Image<\/code> and the <code>Webservice<\/code> classes above are abstract parent classes.<\/p>\n\n<p>For the <code>Image<\/code> object, you should really use one of these classes, depending on your case:<\/p>\n\n<ul>\n<li><code>ContainerImage<\/code><\/li>\n<li><code>UnknownImage<\/code><\/li>\n<\/ul>\n\n<p>(see <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.image?view=azure-ml-py\" rel=\"nofollow noreferrer\">Image package<\/a> in the documentation).<\/p>\n\n<p>For the <code>Webservice<\/code> object, you should use one of these classes, depending on your case:<\/p>\n\n<ul>\n<li><code>AciWebservice<\/code><\/li>\n<li><code>AksWebservice<\/code><\/li>\n<li><code>UnknownWebservice<\/code><\/li>\n<\/ul>\n\n<p>(see <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice?view=azure-ml-py\" rel=\"nofollow noreferrer\">Webservice package<\/a> in the documentation).<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1566684334863,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":28.15,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":213.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unclear update instructions"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2342.6461111111,
        "Challenge_answer_count":0,
        "Challenge_body":"I'm trying to use Azure Computer Vision's OCR API in an Azure Machine Learning Notebook. However there seems to be an error when trying to call the Computer Vision API from an Azure Machine Learning Notebook. The same code works when I'm running it on a local machine.\r\nI'm following Azure Computer Vision's OCR Quickstart: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/quickstarts-sdk\/client-library?tabs=visual-studio&pivots=programming-language-python\r\n\r\nWhen running the following code, `computervision_client.read(read_image_url, raw=True)` does not return but throws an exception.\r\nException:\r\n`ClientRequestError: Error occurred in request., ConnectionError: HTTPSConnectionPool(host='some-host.cognitiveservices.azure.com', port=443): Max retries exceeded with url: \/vision\/v3.2\/read\/analyze?model-version=latest&readingOrder=basic (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f59e0102820>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))`\r\n\r\nCode:\r\n```python\r\nfrom azure.cognitiveservices.vision.computervision import ComputerVisionClient\r\nfrom azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\r\nfrom azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\r\nfrom msrest.authentication import CognitiveServicesCredentials\r\n\r\nfrom array import array\r\nimport os\r\nfrom PIL import Image\r\nimport sys\r\nimport time\r\n\r\n'''\r\nAuthenticate\r\nAuthenticates your credentials and creates a client.\r\n'''\r\nsubscription_key = os.environ[\"COMPUTERVISION_KEY\"]\r\nendpoint = os.environ[\"COMPUTERVISION_URL\"]\r\n\r\ncomputervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\r\n\r\n'''\r\nOCR: Read File using the Read API, extract text - remote\r\nThis example will extract text in an image, then print results, line by line.\r\nThis API call can also extract handwriting style text (not shown).\r\n'''\r\nprint(\"===== Read File - remote =====\")\r\n# Get an image with text\r\nread_image_url = \"https:\/\/raw.githubusercontent.com\/MicrosoftDocs\/azure-docs\/master\/articles\/cognitive-services\/Computer-vision\/Images\/readsample.jpg\"\r\n\r\n# Call API with URL and raw response (allows you to get the operation location)\r\nread_response = computervision_client.read(read_image_url,  raw=True) # <- THROWS EXCEPTION\r\n```\r\n\r\nUsed azure packages:\r\n```\r\nazure-ai-textanalytics                        5.1.0b7\r\nazure-cognitiveservices-vision-computervision 0.9.0\r\nazure-common                                  1.1.27\r\nazure-core                                    1.14.0\r\nazure-cosmos                                  4.2.0\r\nazure-graphrbac                               0.61.1\r\nazure-identity                                1.4.1\r\nazure-mgmt-authorization                      0.61.0\r\nazure-mgmt-containerregistry                  8.0.0\r\nazure-mgmt-core                               1.2.2\r\nazure-mgmt-keyvault                           2.2.0\r\nazure-mgmt-resource                           13.0.0\r\nazure-mgmt-storage                            11.2.0\r\nazure-storage-blob                            12.8.0\r\nazureml-automl-core                           1.29.0\r\nazureml-contrib-dataset                       1.29.0\r\nazureml-core                                  1.29.0.post1\r\nazureml-dataprep                              2.15.1\r\nazureml-dataprep-native                       33.0.0\r\nazureml-dataprep-rslex                        1.13.0\r\nazureml-dataset-runtime                       1.29.0\r\nazureml-pipeline-core                         1.29.0\r\nazureml-pipeline-steps                        1.29.0\r\nazureml-telemetry                             1.29.0\r\nazureml-train-automl-client                   1.29.0\r\nazureml-train-core                            1.29.0\r\nazureml-train-restclients-hyperdrive          1.29.0\r\nazureml-widgets                               1.29.0.post1\r\n```\r\n\r\nMaybe related to #1107",
        "Challenge_closed_time":1631108652000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1622675126000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error message \"Environment name can not start with the prefix AzureML\" while running an experiment. They are following a GitHub tutorial and are unsure how to set the name of the environment. The user provided code snippets and references for context.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1501",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":43.59,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":2342.6461111111,
        "Challenge_title":"'ClientRequestError' when trying to use Azure Computer Vision API from Azure Machine Learning Notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":292,
        "Discussion_body":"After a long while I've tried the exact same script in the same compute instance again. I don't experience the connection error anymore so I will close this ticket. I don't know what changed.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid environment name prefix"
    },
    {
        "Answerer_created_time":1500385674112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":44.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":1.1039755556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I've created a Sagemaker notebook to dev AWS Glue jobs, but when running through the provided example (\"Joining, Filtering, and Loading Relational Data with AWS Glue\") I get the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3hF5s.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does anyone know what I've setup wrong\/haven't setup to cause the import to not work?<\/p>",
        "Challenge_closed_time":1570201013272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570197038960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running an AWS Glue job in a Sagemaker notebook. The error message indicates that the module \"awsglue.transforms\" is not found. The user is seeking assistance in identifying the cause of the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58237749",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":12.9,
        "Challenge_reading_time":6.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.1039755556,
        "Challenge_title":"AWS Glue Sagemaker Notebook \"No module named awsglue.transforms\"",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":3551.0,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384795490587,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1012.0,
        "Poster_view_count":102.0,
        "Solution_body":"<p>You'll need to download the library files from <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 0.9 or <a href=\"https:\/\/s3.amazonaws.com\/aws-glue-jes-prod-us-east-1-assets\/etl-1.0\/python\/PyGlue.zip\" rel=\"nofollow noreferrer\">here<\/a> for Glue 1.0 (Check your Glue jobs for the version). <\/p>\n\n<p>Put the zip in S3 and reference it in the \"Python library path\" on your Dev Endpoint.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":6.27,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":48.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"module not found"
    },
    {
        "Answerer_created_time":1294730361590,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY",
        "Answerer_reputation_count":57082.0,
        "Answerer_view_count":4597.0,
        "Challenge_adjusted_solved_time":4.4930697222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm quite new using Kedro and after installing kedro in my conda environment, I'm getting the following error when trying to list my catalog:<\/p>\n<p>Command performed: <code>kedro catalog list<\/code><\/p>\n<p>Error:<\/p>\n<blockquote>\n<p>kedro.io.core.DataSetError: An exception occurred when parsing config\nfor DataSet <code>df_medinfo_raw<\/code>: Object <code>ParquetDataSet<\/code> cannot be loaded\nfrom <code>kedro.extras.datasets.pandas<\/code>. Please see the documentation on\nhow to install relevant dependencies for\nkedro.extras.datasets.pandas.ParquetDataSet:<\/p>\n<\/blockquote>\n<p>I installed kedro trough conda-forge: <code>conda install -c conda-forge &quot;kedro[pandas]&quot;<\/code>. As far as I understand, this way to install kedro also installs the pandas dependencies.<\/p>\n<p>I tried to read the kedro documentation for dependencies, but it's not really clear how to solve this kind of issue.<\/p>\n<p>My kedro version is <strong>0.17.6<\/strong>.<\/p>",
        "Challenge_closed_time":1642280220676,
        "Challenge_comment_count":0,
        "Challenge_created_time":1642224358493,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an AttributeError while trying to list their catalog in Kedro. The error message indicates that the ParquetDataSet object cannot be loaded from kedro.extras.datasets.pandas. The user installed Kedro through conda-forge and installed pandas dependencies, but is unsure how to resolve the issue.",
        "Challenge_last_edit_time":1642264400916,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70719080",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":13.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":15.5172730556,
        "Challenge_title":"AttributeError: Object ParquetDataSet cannot be loaded from kedro.extras.datasets.pandas",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":863.0,
        "Challenge_word_count":119,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492098397316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":135.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>Kedro uses Pandas to load <code>ParquetDataSet<\/code> objects, and Pandas requires additional dependencies to accomplish this (see <a href=\"https:\/\/pandas.pydata.org\/docs\/getting_started\/install.html#other-data-sources\" rel=\"nofollow noreferrer\">&quot;Installation: Other data sources&quot;<\/a>). That is, in addition to Pandas, one must also install either <code>fastparquet<\/code> or <code>pyarrow<\/code>.<\/p>\n<p>For Conda you either want:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>## use pyarrow for parquet\nconda install -c conda-forge kedro pandas pyarrow\n<\/code><\/pre>\n<p>or<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>## or use fastparquet for parquet\nconda install -c conda-forge kedro pandas fastparquet\n<\/code><\/pre>\n<p>Note that the syntax used in the question <code>kedro[pandas]<\/code> is meaningless to Conda (i.e., it ultimately parses to just <code>kedro<\/code>). Conda package specification uses <a href=\"https:\/\/stackoverflow.com\/a\/57734390\/570918\">a custom grammar called <code>MatchSpec<\/code><\/a>, where anything inside a <code>[...]<\/code> is parsed for a <code>[key1=value1;key2=value2;...]<\/code> syntax. Essentially, the <code>[pandas]<\/code> is treated as an unknown key, which is ignored.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1642280575967,
        "Solution_link_count":2.0,
        "Solution_readability":13.5,
        "Solution_reading_time":16.42,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":127.0,
        "Tool":"Kedro",
        "Challenge_type":"anomaly",
        "Challenge_summary":"AttributeError in Kedro catalog"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":386.3980555556,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n<img width=\"1430\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/5203025\/123860354-63399680-d958-11eb-9dc8-dc0a52d67cc2.png\">\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 109d9284-e234-5086-5da6-4155291361c8\r\n* Version Independent ID: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d\r\n* Content: [azureml.core.ScriptRunConfig class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.scriptrunconfig?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1626388206000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1624997173000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an exception while importing azureml.core after installing azure ml using a conda environment yml. The error message indicates a failure while loading azureml_run_type_providers due to a cryptography version issue. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1534",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":25.6,
        "Challenge_reading_time":13.49,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":386.3980555556,
        "Challenge_title":"Broken link in AML doc to azureml.core.runconfig.MpiConfiguration",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Discussion_body":"Thanks for submitting the issue. I am fixing this broken link now.  The links should be fixed on next SDK release on Aug 3rd",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"cryptography version issue"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6365.2066666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello, I can't open my project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? A screenshot is attached here to understand better. Thanks!\r\n<img width=\"1363\" alt=\"Screen Shot 2022-02-22 at 9 45 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Challenge_closed_time":1668499115000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1645584371000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug when attempting to clone a single notebook using the \"Open In in Sagemaker Studio Lab\" button. The error message \"Unable to copy notebook to project\" appears when the user selects \"Copy Notebook Only\" in the modal. Cloning the whole repository works without any issues. The expected behavior is for the notebook to open and appear as it would with cloning a directory. The user is using Windows 11 and Chrome version 97.0.4692.71.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":113.0,
        "Challenge_repo_issue_count":218.0,
        "Challenge_repo_star_count":408.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6365.2066666667,
        "Challenge_title":"Can't open project on amazon sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Discussion_body":"Not sure if your issue has been resolved or not.\r\nA quick fix is to delete your account and recreate. You will by pass the approval process if you use the same email that has already been approved. @bsaha205 do you still have the problem to open the project? Please let us know about your situation. I'll close the issue. If you have the trouble. please try @MicheleMonclova solution.  Hi @icoxfog417, yes the issue is resolved. Thanks. I am glad to hear that. Please enjoy your ML journey!",
        "Discussion_score_count":1.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to copy notebook"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5544444444,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation.  I have some initialization to do at creation time (clone a github repo, etc.).  That all works fine.  The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a `Stopped` state.  A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\n```\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n```\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige.  I would have thought it would let me do the same in the lifecycle config.  Unfortunately, SageMaker still has the notebook in the `Pending` state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Challenge_closed_time":1539777191000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539775195000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to create a notebook instance using CloudFormation with SageMaker LifeCycleConfig, but there is no way to leave the newly-created instance in a `Stopped` state. The user tried using the AWS CLI to stop the instance from the lifecycle create script, but it failed. The user is looking for other hooks or creative options to solve this issue.",
        "Challenge_last_edit_time":1668613505334,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloudformation-with-sagemaker-lifecycleconfig-without-leaving-the-instance-running",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.5544444444,
        "Challenge_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":479.0,
        "Challenge_word_count":208,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"One solutions will be to create a [CFN custom resource](https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/template-custom-resources-lambda.html) backed by lambda.\nYou can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925548143,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":4.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to stop instance"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":0.0008886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I initially had a notebook in one directory in AWS SageMaker JupyterLab, say <code>\/A<\/code>, but then moved it into <code>\/A\/B<\/code>. However, when I run <code>!pwd<\/code> in a jupyter notebook cell, I still get <code>\/A<\/code>. This happens even when I press 'restart kernel'. How does the notebook remember this, and is there a way to prevent or reset this?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1597057792807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597055996810,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has moved a Jupyter notebook from one directory to another in AWS SageMaker JupyterLab, but when running the command \"!pwd\" in a notebook cell, the previous directory is still displayed instead of the current one. The user is seeking a solution to prevent or reset this issue.",
        "Challenge_last_edit_time":1597057789608,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63338577",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4988880556,
        "Challenge_title":"Jupyter notebook seems to remember previous path (!pwd) after being moved to a different directory?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":118.0,
        "Challenge_word_count":73,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>I was actually using AWS SageMaker, and restarting the kernel from the toolbar was not enough. I needed to restart the kernel session, by pressing 'shut down' in the &quot;Running terminals and kernels&quot; section on the left navigation.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/934Fe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/934Fe.png\" alt=\"Shut down button seen on left navigation of JupterLab\" \/><\/a><\/p>\n<p>They are currently <a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab\/issues\/5989\" rel=\"nofollow noreferrer\">discussing<\/a> warning users about the need to restart the kernel when a notebook is moved.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":10.4,
        "Solution_reading_time":8.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"incorrect directory displayed"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":1035.8951980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Challenge_closed_time":1594137982056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590408759343,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully created a training job on Sagemaker using the Tensorflow Object Detection API in a docker container. However, the user is facing challenges in monitoring the training job using Sagemaker and is unable to find any resources explaining how to do it. The user is considering saving the logs into an S3 bucket and pointing to a local tensorboard instance, but is unsure how to tell the Tensorflow Object Detection API where to save the logs. The user has tried using a script, but it failed because the training job did not have the required parameter. The user has found a solution that involves disabling network isolation in the training job configuration and providing credentials to the docker image to write to the S3 bucket. However, the user has",
        "Challenge_last_edit_time":1606323198012,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":27.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1035.8951980556,
        "Challenge_title":"Use tensorboard with object detection API in sagemaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":311.0,
        "Challenge_word_count":286,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.2,
        "Solution_reading_time":20.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to monitor training job"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1017638889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When users were creating a new compute in AML environment by default RStudio application was created.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238009-screenshot-2022-08-23-170502.png?platform=QnA\" alt=\"With RStudio\" \/>    <\/p>\n<p>However, from month of July, by default RStudio application is not getting created. Only JupyterLab, Jupyter, VS Code, Terminal, Notebook applications installed not a RStudio.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/237995-4.png?platform=QnA\" alt=\"without RStudio\" \/>    <\/p>\n<p>Is there any way to by default install RStudio application in azure ML compute instance?    <\/p>",
        "Challenge_closed_time":1662454114980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662442948630,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where RStudio application is not installed by default in Azure ML compute since July, and is seeking a solution to install it by default.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/995175\/rstudio-application-not-installed-in-azure-ml-comp",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":9.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.1017638889,
        "Challenge_title":"RStudio application not installed in Azure ML compute",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8876fdcf-bcee-403a-827a-7cff9a68f8ee\">@SHAIKH, Alif Abdul  <\/a> Yes, this is a recent change in the setup of compute instance that happens during the creation of compute instance. This change does not setup the rstudio community edition by default but you can set it up while creation by adding it as a custom application from advanced settings. This change is done as part of RStudio requirements to allow users to setup their own license key or use open studio version during creation. Please refer this documentation <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#add-custom-applications-such-as-rstudio-preview\">page<\/a> for details to setup licensed and open version of the studio.    <\/p>\n<p>If you add a license key then please use RStudio Workbench (bring your own license) option from the advanced options and enter your own license key.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238075-image.png?platform=QnA\" alt=\"238075-image.png\" \/>    <\/p>\n<p>If you need to add the open version you need to select custom application and enter the docker image and mount point details to setup the rstudio during creation.    <\/p>\n<p>Target\/Published port <code>8787<\/code>    <br \/>\nDocker image set to <code>ghcr.io\/azure\/rocker-rstudio-ml-verse:latest<\/code>    <br \/>\n<code>\/home\/azureuser\/cloudfiles<\/code> for Host path    <br \/>\n<code>\/home\/azureuser\/cloudfiles<\/code> for Container path    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238135-image.png?platform=QnA\" alt=\"238135-image.png\" \/>    <\/p>\n<p>Once the creation and setup is complete the options to use Rstudio for open and licensed version will be visible as links on the compute instances page.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238142-image.png?platform=QnA\" alt=\"238142-image.png\" \/>    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":14.3,
        "Solution_reading_time":30.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":240.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"RStudio not installed by default"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":107.9469136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to GCP's Vertex AI and suspect I am running into an error from my lack of experience, but Googling the answer has brought me no fruitful information.<\/p>\n<p>I created a Jupyter Notebook in AI Platform but wanted to schedule it to run at a set period of time. So I was hoping to use Vertex AI's Execute function. At first when I tried accessing Vertex I was unable to do so because the API had not been enabled in GCP. My IT team then enabled the Vertex AI API and I can now utilize Vertex. Here is a picture showing it is enabled. <a href=\"https:\/\/i.stack.imgur.com\/pUSRO.png\" rel=\"nofollow noreferrer\">Enabled API Picture<\/a><\/p>\n<p>I uploaded my notebook to a JupyterLab instance in Vertex, and when I click on the Execute button, I get an <a href=\"https:\/\/i.stack.imgur.com\/jnUDv.png\" rel=\"nofollow noreferrer\">error message<\/a> saying I need to &quot;Enable necessary APIs&quot;, specifically for Vertex AI API. I'm not sure why this is considering it's already been enabled. I try to click Enable, but it just spins and spins, and then I can only get out of it by closing or reloading the tab.<\/p>\n<p>One other thing I want to call out in case it's a settings issue is that currently my <a href=\"https:\/\/i.stack.imgur.com\/6UxKb.png\" rel=\"nofollow noreferrer\">Managed Notebooks tab says &quot;PREVIEW&quot;<\/a> in the Workbench. I started thinking maybe this was an indicator that there was a separate feature that needed to be enabled to use Managed Notebooks (which is where I can access the Execute button from). When I click on the User-Managed Notebooks and open JupyterLab from there, I don't have the Execute button.<\/p>\n<p>The GCP account I'm using does have billing enabled.<\/p>\n<p>Can anyone point me in the right direction to getting the Execute button to work?<\/p>",
        "Challenge_closed_time":1648544476136,
        "Challenge_comment_count":8,
        "Challenge_created_time":1648067877907,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with GCP Vertex AI where they are unable to use the Execute function even though the Vertex AI API has been enabled. When they try to enable the necessary APIs, the process keeps spinning and they are unable to proceed. Additionally, the user is unsure if the \"PREVIEW\" status in the Managed Notebooks tab is related to the issue. The user is seeking guidance to resolve the issue.",
        "Challenge_last_edit_time":1648155867247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71593747",
        "Challenge_link_count":3,
        "Challenge_participation_count":9,
        "Challenge_readability":9.0,
        "Challenge_reading_time":22.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":132.3883969444,
        "Challenge_title":"GCP Vertex AI \"Enable necessary APIs\" when already enabled",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":518.0,
        "Challenge_word_count":299,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1646671161350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Based on @JamesS comments, the issue was solved by adding necessary permissions on his individual account since it is the account configured on OP's <code>Managed Notebook Instance<\/code> in which has an access mode of <code>Single user only<\/code>.<\/p>\n<p>Based on my testing when I tried to replicate the scenario, &quot;Enable necessary APIs&quot; message box will continue to show when the user has no <em>&quot;Vertex AI User&quot;<\/em> role assigned to it. And in conclusion of my testing, below are the minimum <strong>roles<\/strong> required when trying to create a <em>Scheduled run<\/em> on a <code>Managed Notebook Instance<\/code>.<\/p>\n<ul>\n<li><strong>Notebook Admin<\/strong> - For access of the notebook instance and open it through Jupyter. User will be able to run written codes in the Notebook as well.<\/li>\n<li><strong>Vertex AI User<\/strong> - So that the user can <strong>create schedule run<\/strong> on the notebook instance since the creation of the scheduled run is under the Vertex AI API itself.<\/li>\n<li><strong>Storage Admin<\/strong> - Creation of scheduled run will require a Google Cloud Storage bucket location where the job will be saved<\/li>\n<\/ul>\n<p>Posting the answer as <em>community wiki<\/em> for the benefit of the community that might encounter this use case in the future.<\/p>\n<p>Feel free to edit this answer for additional information.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":17.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":200.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to use Execute function"
    },
    {
        "Answerer_created_time":1375058329287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":762.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":171.1678108333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Summary<\/strong>: I am trying to define a <code>dvc<\/code> step using <code>dvc-run<\/code> where the command depends on some environment variables (for instance <code>$HOME<\/code>). The problem is that when I'm defining the step on machine A, then the variable is expanded when stored in the <code>.dvc<\/code> file. In this case, it won't be possible to reproduce the step on machine B. Did I hit a limitation of <code>dvc<\/code>? If that's not the case, what's the right approach?<\/p>\n\n<p><strong>More details<\/strong>: I faced the issue when trying to define a step for which the command is a <code>docker run<\/code>. Say that:<\/p>\n\n<ul>\n<li>on machine A <code>myrepo<\/code> is located at <code>\/Users\/user\/myrepo<\/code> and <\/li>\n<li>on machine B it is to be found at <code>\/home\/ubuntu\/myrepo<\/code>. <\/li>\n<\/ul>\n\n<p>Furthermore, assume I have a script <code>myrepo\/script.R<\/code> which processes a data file to be found at <code>myrepo\/data\/mydata.txt<\/code>. Lastly, assume that my step's command is something like: <\/p>\n\n<pre><code>docker run -v $HOME\/myrepo\/:\/prj\/ my_docker_image \/prj\/script.R \/prj\/data\/mydata.txt\n<\/code><\/pre>\n\n<p>If I'm running <code>dvc run -f step.dvc -d ... -d ... [cmd]<\/code> where <code>cmd<\/code> is the <code>docker<\/code> execution above, then in <code>step.dvc<\/code> the environment variable <code>$HOME<\/code> will be expanded. In this case, the step will be broken on machine B.<\/p>",
        "Challenge_closed_time":1563988872483,
        "Challenge_comment_count":6,
        "Challenge_created_time":1563703426437,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while defining a dvc step using dvc-run where the command depends on environment variables. When defining the step on one machine, the variable is expanded and stored in the .dvc file, making it impossible to reproduce the step on another machine. The issue arises when the command is a docker run and the environment variable $HOME is expanded in the .dvc file, causing the step to break on another machine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57132106",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":8.6,
        "Challenge_reading_time":18.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":79.2905683334,
        "Challenge_title":"Expanding environment variables in the command part of a dvc run",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":203,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1300789717227,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":11410.0,
        "Poster_view_count":1782.0,
        "Solution_body":"<p>From <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/run\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n\n<blockquote>\n  <p>Use single quotes ' instead of \" to wrap the command if there are environment variables in it, that you want to be evaluated dynamically. E.g. dvc run -d script.sh '.\/myscript.sh $MYENVVAR'<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1564319630556,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":4.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"DVC",
        "Challenge_type":"anomaly",
        "Challenge_summary":"expanded environment variable in .dvc file"
    },
    {
        "Answerer_created_time":1587281590603,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":473.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":84.1130386111,
        "Challenge_answer_count":1,
        "Challenge_body":"<h2>The problem<\/h2>\n<p>I have a jupyter notebook with an <code>ipydatagrid<\/code> widget that displays a dataframe. This notebook works correctly when run locally, but not when run in AWS SageMaker Studio. When run in SageMaker Studio, instead of showing the widget it simply shows the text <code>Loading widget...<\/code><\/p>\n<p>How does one use a <code>ipydatagrid<\/code> widget in the SageMaker Studio environment?<\/p>\n<h2>Details<\/h2>\n<p>Python version:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ python --version\nPython 3.7.10\n<\/code><\/pre>\n<p>Run at start:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip install -r requirements.txt\n$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n$ jupyter nbextension install --py --symlink --sys-prefix ipydatagrid\n$ jupyter nbextension enable --py --sys-prefix ipydatagrid\n<\/code><\/pre>\n<p>File <code>requirements.txt<\/code>:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>ipydatagrid==1.1.11\npandas==1.0.1\n<\/code><\/pre>\n<p>Notebook contents:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># %%\nimport pandas as pd\nfrom ipydatagrid import DataGrid\nfrom IPython.display import display\nimport ipywidgets\n\n# %%\ndata = [\n    (&quot;potato&quot;, 1.2, True),\n    (&quot;sweet potato&quot;, 0.8, False),\n    (&quot;french fries&quot;, 4.5, True),\n    (&quot;waffle fries&quot;, 4.9, True)\n]\ndf = pd.DataFrame(\n    data,\n    columns=[&quot;food&quot;, &quot;stars&quot;, &quot;is_available&quot;]\n)\n\n# %%\ngrid = DataGrid(df)\n\n# %%\ndisplay(grid)\n\n# %%\n# SANITY CHECK:\nbutton = ipywidgets.Button(\n    description=&quot;Button&quot;,\n    disabled=False\n)\ndef on_click(b):\n    print(&quot;CLICK&quot;)\nbutton.on_click(on_click)\ndisplay(button)\n<\/code><\/pre>\n<h3>Error messages<\/h3>\n<p>If I use the Google Chrome developer tools, I can see more logs in the browser that give some error messages, most of which are repeated:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>manager.js:305 Uncaught (in promise) Error: Module ipydatagrid, semver range ^1.1.11 is not registered as a widget module\n    at C.loadClass (manager.js:305:19)\n    at C.&lt;anonymous&gt; (manager-base.js:263:46)\n    at l (manager-base.js:44:23)\n    at Object.next (manager-base.js:25:53)\n    at manager-base.js:19:71\n    at new Promise (&lt;anonymous&gt;)\n    at Rtm6.k (manager-base.js:15:12)\n    at C.e._make_model (manager-base.js:257:16)\n    at C.&lt;anonymous&gt; (manager-base.js:246:45)\n    at l (manager-base.js:44:23)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>utils.js:119 Error: Could not create a model.\n    at n (utils.js:119:27)\n    at async C._handleCommOpen (manager.js:61:51)\n    at async v._handleCommOpen (default.js:994:100)\n    at async v._handleMessage (default.js:1100:43)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>manager-base.js:273 Could not instantiate widget\n<\/code><\/pre>\n<p>However, there is no overt error message that's immediately obvious to the user, including in the log where <code>print<\/code> statements send their output.<\/p>",
        "Challenge_closed_time":1648549758096,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648246951157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where the ipydatagrid widget is not displaying in AWS SageMaker Studio, despite working correctly when run locally. The user has provided details of the Python version, the requirements, and the notebook contents. The user has also shared error messages that appear in the browser console, but there is no overt error message visible to the user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71623732",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":39.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":84.1130386111,
        "Challenge_title":"ipydatagrid widget does not display in SageMaker Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":304,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1560606498248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Provo, UT, USA",
        "Poster_reputation_count":528.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab<\/em>), and per the <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid#Installation\" rel=\"nofollow noreferrer\">ipydatagrid installation instructions<\/a>, current\/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.<\/p>\n<p>I had a quick look at the past releases of <code>ipydatagrid<\/code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/0.2.16\" rel=\"nofollow noreferrer\">v0.2.16<\/a> and <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/1.0.1\" rel=\"nofollow noreferrer\">v1.0.1<\/a> (which are adjacent on GitHub).<\/p>\n<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package<\/code> and also note that versions &lt;1.0 don't seem to be present <a href=\"https:\/\/libraries.io\/pypi\/ipydatagrid\/versions\" rel=\"nofollow noreferrer\">on PyPI<\/a>.<\/p>\n<p>So unfortunately I think (unless\/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":10.3,
        "Solution_reading_time":18.2,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ipydatagrid not displaying"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7837736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Would like to upload Jupyter notebooks from different sources like GitHub into my workspace either directly or through my local machine (download locally first and then upload) but I would like to do it programmatically. Either with the AzureML SDK or azure cli  <\/p>",
        "Challenge_closed_time":1651104442392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651101620807,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to upload Jupyter notebooks from various sources like GitHub to their AzureML workspace programmatically using either AzureML SDK or Azure CLI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/829311\/how-could-i-upload-notebooks-to-my-azureml-workspa",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.7837736111,
        "Challenge_title":"How could I upload notebooks to my AzureML workspace programatically",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. You can use compute instance <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal\">terminal<\/a> in AML notebooks to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/samples-notebooks#get-samples-on-azure-machine-learning-compute-instance\">clone<\/a> the GitHub repo. There's currently no option to upload notebooks to your workspace programmatically using sdk or cli.<\/p>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":7.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"upload notebooks programmatically"
    },
    {
        "Answerer_created_time":1444758849803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11962.0,
        "Answerer_view_count":960.0,
        "Challenge_adjusted_solved_time":4.5755286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What is the command to check the version of Gremlin Python client running on a AWS Sagemaker jupyter notebook? I would like to run the command on the jupyter notebook cell.<\/p>",
        "Challenge_closed_time":1610839775480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610823303577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to check the version of Gremlin Python client running on an AWS Sagemaker Jupyter notebook and wants to know the command to run on the Jupyter notebook cell.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65753455",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":4.5755286111,
        "Challenge_title":"How to get the version of gremlin python client on AWS SageMaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":198.0,
        "Challenge_word_count":42,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1475704783950,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":473.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>From a notebook cell you should be able to just ask Pip which version is being used<\/p>\n<pre><code>!pip list | grep gremlinpython\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":1.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"check Gremlin Python version"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1.03971,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When I try to run the experiment defined in <a href=\"https:\/\/github.com\/MicrosoftLearning\/mslearn-dp100\/blob\/main\/06%20-%20Work%20with%20Data.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> in  notebook, I encountered an error when it is creating the conda env. The error occurs when the below cell is executed:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Experiment, ScriptRunConfig, Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.widgets import RunDetails\n\n\n# Create a Python environment for the experiment\nsklearn_env = Environment(&quot;sklearn-env&quot;)\n\n# Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\nsklearn_env.python.conda_dependencies = packages\n\n# Get the training dataset\ndiabetes_ds = ws.datasets.get(&quot;diabetes dataset&quot;)\n\n# Create a script config\nscript_config = ScriptRunConfig(source_directory=experiment_folder,\n                              script='diabetes_training.py',\n                              arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n                                           '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n                              environment=sklearn_env)\n\n# submit the experiment\nexperiment_name = 'mslearn-train-diabetes'\nexperiment = Experiment(workspace=ws, name=experiment_name)\nrun = experiment.submit(config=script_config)\nRunDetails(run).show()\nrun.wait_for_completion() \n<\/code><\/pre>\n<p>Everytime I run this, I always faced the issue of creating the conda env as below:<\/p>\n<pre><code>Creating conda environment...\nRunning: ['conda', 'env', 'create', '-p', '\/home\/azureuser\/.azureml\/envs\/azureml_000000000000', '-f', 'azureml-environment-setup\/mutated_conda_dependencies.yml']\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n\nInstalling pip dependencies: ...working... \n\nAttempting to clean up partially built conda environment: \/home\/azureuser\/.azureml\/envs\/azureml_000000000000\nRemove all packages in environment \/home\/azureuser\/.azureml\/envs\/azureml_000000000000:\nCreating conda environment failed with exit code: -15\n<\/code><\/pre>\n<p>I could not find anything useful on the internet and this is not the only script where it fail. When I am try to run other experiments I have sometimes faced this issue. One solution which worked in the above case is I moved the pandas from pip to conda and it was able to create the coonda env. Example below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\n\n<\/code><\/pre>\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip','pandas'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep'])\n\n<\/code><\/pre>\n\n<p>The error message (or the logs from Azure) is also not much help. Would apprecite if a proper solution is available.<\/p>\n<p>Edit: I have recently started learning to use Azure for Machine learning and so if I am not sure if I am missing something? I assume the example notebooks should work as is hence raised this question.<\/p>",
        "Challenge_closed_time":1621615054950,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621610368967,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user encountered an error while creating a conda environment in Azure ML when running an experiment defined in a notebook. The error occurred when executing a specific cell of code, and the error message was not helpful. The user found a solution by moving pandas from pip to conda in the CondaDependencies.create function. The user is unsure if they are missing something and expects the example notebooks to work as is.",
        "Challenge_last_edit_time":1621615281776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67639665",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.8,
        "Challenge_reading_time":48.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":1.3016619445,
        "Challenge_title":"Azure ML not able to create conda environment (exit code: -15)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2373.0,
        "Challenge_word_count":373,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1488207114692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":493.0,
        "Poster_view_count":9.0,
        "Solution_body":"<h2>short answer<\/h2>\n<p>Totally been in your shoes before. This code sample seems a smidge out of date. Using <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> as a reference, can you try the following?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>packages = CondaDependencies.create(\n    pip_packages=['azureml-defaults','scikit-learn']\n)\n<\/code><\/pre>\n<h2>longer  answer<\/h2>\n<p><a href=\"https:\/\/www.anaconda.com\/blog\/using-pip-in-a-conda-environment\" rel=\"nofollow noreferrer\">Using pip with Conda<\/a> is not always smooth sailing. In this instance, conda isn't reporting up the issue that pip is having. The solution is to create and test this environment locally where we can get more information, which will at least will give you a more informative error message.<\/p>\n<ol>\n<li>Install anaconda  or miniconda (or use an Azure ML Compute Instance which has conda pre-installed)<\/li>\n<li>Make a  file called environment.yml that looks like this<\/li>\n<\/ol>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - azureml-defaults\n    - azureml-dataprep[pandas]\n    - scikit-learn==0.24.1\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create this environment with the command <code>conda env create -f environment.yml<\/code>.<\/li>\n<li>respond to any discovered error message<\/li>\n<li>If there' no error, use this new <code>environment.yml<\/code> with Azure ML like so<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = '.\/environment.yml')\n<\/code><\/pre>\n<h2>more context<\/h2>\n<p>the error I'm guessing that's happening is when you reference a pip requirements file from a conda environment file. In this scenario, conda calls <code>pip install -r  requirements.txt<\/code> and if that command errors out, conda can't report the error.<\/p>\n<h3><code>requirements.txt<\/code><\/h3>\n<pre><code>scikit-learn==0.24.1\nazureml-dataprep[pandas]\n<\/code><\/pre>\n<h3><code>environment.yml<\/code><\/h3>\n<pre><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - -rrequirements.txt\n<\/code><\/pre>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1621619024732,
        "Solution_link_count":2.0,
        "Solution_readability":13.2,
        "Solution_reading_time":30.09,
        "Solution_score_count":3.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":225.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"conda environment creation error"
    },
    {
        "Answerer_created_time":1544524371740,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cologne Germany",
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":3.6875811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running into a ModuleNotFoundError for pandas while using the following code to orchestrate my Azure Machine Learning Pipeline:<\/p>\n<pre><code># Loading run config\nprint(&quot;Loading run config&quot;)\ntask_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \n\ntask_1_script_run_config = ScriptRunConfig(\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    run_config=task_1_run_config    \n)\n\ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name=task_1_script_run_config.script,\n    source_directory=task_1_script_run_config.source_directory,\n    compute_target=compute_target\n)\n\npipeline_run_config = Pipeline(workspace=workspace, steps=[task_1_py_script_step])#, task_2])\n\npipeline_run = Experiment(workspace, 'Test_Run_New_Pipeline').submit(pipeline_run_config)\npipeline_run.wait_for_completion()\n<\/code><\/pre>\n<p>The environment.yml<\/p>\n<pre><code>name: phinmo_pipeline_env\ndependencies:\n- python=3.8\n- pip:\n  - pandas\n  - azureml-core==1.43.0\n  - azureml-sdk\n  - scipy\n  - scikit-learn\n  - numpy\n  - pyyaml==6.0\n  - datetime\n  - azure\nchannels:\n  - conda-forge\n<\/code><\/pre>\n<p>The loaded RunConfiguration in T01_Test_Task.yml looks like this:<\/p>\n<pre><code># The script to run.\nscript: T01_Test_Task.py\n# The arguments to the script file.\narguments: [\n  &quot;--test&quot;, False,\n  &quot;--date&quot;, &quot;2022-07-26&quot;\n]\n# The name of the compute target to use for this run.\ncompute_target: phinmo-compute-cluster\n# Framework to execute inside. Allowed values are &quot;Python&quot;, &quot;PySpark&quot;, &quot;CNTK&quot;, &quot;TensorFlow&quot;, and &quot;PyTorch&quot;.\nframework: Python\n# Maximum allowed duration for the run.\nmaxRunDurationSeconds: 6000\n# Number of nodes to use for running job.\nnodeCount: 1\n\n#Environment details.\nenvironment:\n  # Environment name\n  name: phinmo_pipeline_env\n  # Environment version\n  version:\n  # Environment variables set for the run.\n  #environmentVariables:\n  #  EXAMPLE_ENV_VAR: EXAMPLE_VALUE\n  # Python details\n  python:\n    # user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.\n    userManagedDependencies: false\n    # The python interpreter path\n    interpreterPath: python\n    # Path to the conda dependencies file to use for this run. If a project\n    # contains multiple programs with different sets of dependencies, it may be\n    # convenient to manage those environments with separate files.\n    condaDependenciesFile: environment.yml\n    # The base conda environment used for incremental environment creation.\n    baseCondaEnvironment: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n  # Docker details\n  \n# History details.\nhistory:\n  # Enable history tracking -- this allows status, logs, metrics, and outputs\n  # to be collected for a run.\n  outputCollection: true\n  # Whether to take snapshots for history.\n  snapshotProject: true\n  # Directories to sync with FileWatcher.\n  directoriesToWatch:\n  - logs\n# data reference configuration details\ndataReferences: {}\n# The configuration details for data.\ndata: {}\n# Project share datastore reference.\nsourceDirectoryDataStore:\n<\/code><\/pre>\n<p>I already tried a few things like overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml. But I am at a loss at what else to try. the T01_Test_Task.py runs without issues on its own. But putting it into a pipeline just does not seem to work.<\/p>",
        "Challenge_closed_time":1658937635368,
        "Challenge_comment_count":1,
        "Challenge_created_time":1658922798813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleNotFoundError for pandas while using Azure Machine Learning Pipeline with a yml file based RunConfiguration and environment.yml. The T01_Test_Task.py runs without issues on its own, but putting it into a pipeline is not working. The user has tried overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object and assigning a version number to pandas in the environment.yml, but the issue persists.",
        "Challenge_last_edit_time":1658924360076,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73137433",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":47.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":4.1212652778,
        "Challenge_title":"ModuleNotFoundError while using AzureML pipeline with yml file based RunConfiguration and environment.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":366,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544524371740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cologne Germany",
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Okay I found the issue.\nI am unnecessarily using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. I was able to see that only in the Task description in the Azure Machine Learning Studio UI.<\/p>\n<p>I was able to just remove that part and now it works:<\/p>\n<pre><code>task_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name='T01_Test_Task.py',\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    runconfig=task_1_run_config, \n    compute_target=compute_target\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ModuleNotFoundError for pandas"
    },
    {
        "Answerer_created_time":1531231343652,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"St. Louis, MO, USA",
        "Answerer_reputation_count":676.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":1504.1879255556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed an AWS endpoint using a Docker container (I followed <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers.html\" rel=\"nofollow noreferrer\">this<\/a>).<\/p>\n<p>Everything is working perfectly but now I need to put it in production and define an auto scaling strategy.<\/p>\n<p>I tried 2 things:<\/p>\n<ol>\n<li><p>AWS console but the auto scaling button is greyed\nout.<\/p>\n<\/li>\n<li><p>The method described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-code-apply.html\" rel=\"nofollow noreferrer\">here<\/a>. My endpoint name\nis <code>EmbeddingEndpoint<\/code> and my variant name is <code>SimpleVariant<\/code>. So my\nfinal command is<\/p>\n<\/li>\n<\/ol>\n<pre><code>aws application-autoscaling put-scaling-policy \\\n--policy-name scalable_policy_for_embedding \\\n--policy-type TargetTrackingScaling \\\n--resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant \\\n--service-namespace sagemaker \\\n--scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n--target-tracking-scaling-policy-configuration file:\/\/policy_config.json\n<\/code><\/pre>\n<p>but I get this result :<\/p>\n<pre><code>An error occurred (ObjectNotFoundException) when calling the PutScalingPolicy operation: \nNo scalable target registered for service namespace: sagemaker, resource ID: \nendpoint\/EmbeddingEndpoint\/variant\/SimpleVariant, scalable dimension: \nsagemaker:variant:DesiredInstanceCount\n<\/code><\/pre>\n<p>Does someone have another solution, or is it that I didn't set the variable well ?\nThank you in advance !<\/p>",
        "Challenge_closed_time":1630466729772,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625051653240,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has deployed an AWS endpoint using a Docker container and is now trying to define an auto scaling strategy. However, the auto scaling button in the AWS console is greyed out and the method described in the documentation is not working. The user is receiving an error message stating that there is no scalable target registered for the service namespace. The user is seeking another solution or advice on setting the variable correctly.",
        "Challenge_last_edit_time":1630503140507,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68193708",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":19.2,
        "Challenge_reading_time":21.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1504.1879255556,
        "Challenge_title":"Unable to Define Auto Scaling for SageMaker Endpoint",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":336.0,
        "Challenge_word_count":149,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570620624976,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Your <code>sagemaker<\/code> service-namespace does not have any registered scaling targets. You need to first run <code>register-scalable-target<\/code> before running <code>put-scaling-policy<\/code>.<\/p>\n<pre><code>aws application-autoscaling register-scalable-target \\\n    --service-namespace sagemaker \\\n    --scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n    --resource-id endpoint\/EmbeddingEndpoint\/variant\/SimpleVariant\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":29.5,
        "Solution_reading_time":6.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"greyed out auto scaling button"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":1.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2608333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Challenge_closed_time":1593108137000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593107198000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in changing instance types for SageMaker Studio as it seems to only support a limited number of types. They are seeking a way to change to other instance types, similar to how it can be done for SageMaker Notebook Instances.",
        "Challenge_last_edit_time":1668530553628,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sagemaker-studio",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":7.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.2608333333,
        "Challenge_title":"Notebook Instance Types for SageMaker Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":864.0,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The instance types you are seeing are Fast Launch Instances ( which are instance types designed to launch in under two minutes).\n\nIn order to see all the types of instances, click on the switch on top of the instance type list that says \"Fast Launch\", that should display the rest of available instances.\n\nHere is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html\n\nHope it helps!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925564584,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":5.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"change SageMaker Studio instance type"
    },
    {
        "Answerer_created_time":1483548930012,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1875.0,
        "Answerer_view_count":146.0,
        "Challenge_adjusted_solved_time":0.4867144444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>[UPDATED] We are currently working on creating a Multi-Arm Bandit model for sign up optimization using the Build Your Own workflow that can be found here (basically substituting the model for our own):<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/p>\n<p>Our project directory is set up as:\n<a href=\"https:\/\/i.stack.imgur.com\/QwaIQ.png\" rel=\"nofollow noreferrer\">Project Directory<\/a><\/p>\n<p>The issue is that I added some code including the dataclasses library that is only available since Python 3.7, and our project seems to keep using 3.6, causing a failure when running the Cloud Formation set up. The error in our Cloudwatch Logs is:<\/p>\n<pre><code>2021-03-31T11:04:11.077-05:00 Copy\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import predictor as myapp\n  File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt;\n    from model_contents.model import MultiArmBandit, BanditParameters\n  File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt;\n    from dataclasses import dataclass, field, asdict\nTraceback (most recent call last): File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker worker.init_process() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process self.load_wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi self.wsgi = self.app.wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi self.callable = self.load() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load return self.load_wsgiapp() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp return util.import_app(self.app_uri) File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app mod = importlib.import_module(module) File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt; import predictor as myapp File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt; from model_contents.model import MultiArmBandit, BanditParameters File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt; from dataclasses import dataclass, field, asdict\n\n    2021-03-31T11:04:11.077-05:00\n\nCopy\nModuleNotFoundError: No module named 'dataclasses'\nModuleNotFoundError: No module named 'dataclasses'\n<\/code><\/pre>\n<p>Our updated Dockerfile is:<\/p>\n<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:18.04\n\n# Retrieves information about what packages can be installed\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python3-pip \\\n         python3.8 \\\n         python3-setuptools \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Set python 3.8 as default\nRUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n\n# Here we get all python packages.\nRUN pip --no-cache-dir install numpy boto3 flask gunicorn\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\nENV PYTHONPATH \/model_contents\n\n# Set up the program in the image\nCOPY bandit\/ \/opt\/program\/\nWORKDIR \/opt\/program\/\n\nRUN chmod +x \/opt\/program\/serve &amp;&amp; chmod +x \/opt\/program\/train\nLABEL git_tag=$GIT_TAG\n<\/code><\/pre>\n<p>I'm not sure if the nginx.conf file defaults to Py 3.6 so I want to make sure that it's not a big hassle to upgrade to Py 3.7 or 3.8 without many changes.<\/p>",
        "Challenge_closed_time":1617311610732,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617309858560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while creating a Multi-Arm Bandit model for sign-up optimization using the Build Your Own workflow in Sagemaker Endpoint. The issue is caused by the use of the dataclasses library, which is only available in Python 3.7 and above, while the project is using Python 3.6, resulting in a failure when running the Cloud Formation set up. The user has updated the Dockerfile to include Python 3.8 and wants to ensure that upgrading to Python 3.7 or 3.8 will not cause any issues with the nginx.conf file.",
        "Challenge_last_edit_time":1617383727407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66911321",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.5,
        "Challenge_reading_time":88.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":77,
        "Challenge_solved_time":0.4867144444,
        "Challenge_title":"Upgrading Python version for running and creating custom container for Sagemaker Endpoint",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1398.0,
        "Challenge_word_count":615,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526288719836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation_count":33.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>You can update the Dockerfile after it install Python3.8 using <code>apt-get<\/code> with the following <code>RUN<\/code> commands<\/p>\n<pre><code>RUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n<\/code><\/pre>\n<p>The first <code>RUN<\/code> command will link <code>\/usr\/bin\/python<\/code> to <code>\/usr\/bin\/python3.8<\/code> and the second one will link <code>\/usr\/bin\/python3<\/code> to <code>\/usr\/bin\/python3.8<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1617312029390,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"dataclasses not available in Python 3.6"
    },
    {
        "Answerer_created_time":1429811498812,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":1910.0,
        "Answerer_view_count":316.0,
        "Challenge_adjusted_solved_time":17.1826736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a pre-trained ML model (saved as .h5 file) to Azure ML. I have created an AKS cluster and trying to deploy the model as shown below:<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\n\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\n\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\nenv = Environment.get(workspace, name='AzureML-TensorFlow-1.13-GPU')\n\n# Installing packages present in my requirements file\nwith open('requirements.txt') as f:\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\ndependencies.append(&quot;azureml-defaults&gt;=1.0.45&quot;)\n\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=dependencies)\n\n# Including the source folder so that all helper scripts are included in my deployment\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# Deployment with suitable config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=4, memory_gb=32)\nmodel = Model(workspace, 'sketch-inference')\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<p>My main entry script requires some additional helper scripts, which I include by mentioning the source folder in my inference config.<\/p>\n<p>I was expecting that the helper scripts I add should be able to access the packages installed while setting up the environment during deployment, but I get ModuleNotFoundError.<\/p>\n<p>Here is the error output, along with the a couple of environment variables I printed while executing entry script:<\/p>\n<pre><code>    AZUREML_MODEL_DIR ----  azureml-models\/sketch-inference\/1\n    PYTHONPATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages:\/var\/azureml-server:\n    PATH ----  \/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/bin:\/opt\/miniconda\/bin:\/usr\/local\/nvidia\/bin:\/usr\/local\/cuda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/opt\/intel\/compilers_and_libraries\/linux\/mpi\/bin64\n    Exception in worker process\n    Traceback (most recent call last):\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n        worker.init_process()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process\n        self.load_wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi\n        self.wsgi = self.app.wsgi()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n        self.callable = self.load()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load\n        return self.load_wsgiapp()\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp\n        return util.import_app(self.app_uri)\n    File &quot;\/azureml-envs\/azureml_6dc005c11e151f8d9427c0c6091a1bb9\/lib\/python3.6\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app\n        __import__(module)\n    File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n        import create_app\n    File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n        from app import main\n    File &quot;\/var\/azureml-server\/app.py&quot;, line 32, in &lt;module&gt;\n        from aml_blueprint import AMLBlueprint\n    File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 25, in &lt;module&gt;\n        import main\n    File &quot;\/var\/azureml-app\/main.py&quot;, line 12, in &lt;module&gt;\n        driver_module_spec.loader.exec_module(driver_module)\n    File &quot;\/structure\/azureml-app\/ProcessImage\/app.py&quot;, line 16, in &lt;module&gt;\n        from ProcessImage.samples.coco.inference import run as infer\n    File &quot;\/var\/azureml-app\/ProcessImage\/samples\/coco\/inference.py&quot;, line 1, in &lt;module&gt;\n        import skimage.io\n    ModuleNotFoundError: No module named 'skimage'\n<\/code><\/pre>\n<p>The existing answers related to this aren't of much help. I believe there must be a simpler way to fix this, since AzureML specifically provides the feature to setup environment with pip\/conda packages installed either by supplying requirements.txt file or individually.<\/p>\n<p>What am I missing here? Kindly help.<\/p>",
        "Challenge_closed_time":1606249620652,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606187763027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a pre-trained ML model to Azure ML using an AKS cluster. They have created an environment with required packages installed and included helper scripts in the deployment. However, the helper scripts are unable to access the installed packages, resulting in a ModuleNotFoundError. The user is seeking a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64979752",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":67.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":17.1826736111,
        "Challenge_title":"Unable to access python packages installed in Azure ML",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1381.0,
        "Challenge_word_count":396,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1429811498812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Boston, MA, USA",
        "Poster_reputation_count":1910.0,
        "Poster_view_count":316.0,
        "Solution_body":"<p>So, after some trial and error, creating a fresh environment and then adding the packages solved the problem for me. I am still not clear on why this didn't work when I tried to use <a href=\"http:\/\/from%20azureml.core%20import%20Workspace%20from%20azureml.core.model%20import%20Model%20from%20azureml.core.environment%20import%20Environment,%20DEFAULT_GPU_IMAGE%20from%20azureml.core.conda_dependencies%20import%20CondaDependencies%20from%20azureml.core.model%20import%20InferenceConfig%20from%20azureml.core.webservice%20import%20AksWebservice,%20LocalWebservice%20from%20azureml.core.compute%20import%20ComputeTarget%20%20%20#%201.%20Instantiate%20the%20workspace%20workspace%20=%20Workspace.from_config(path=%22config.json%22)%20%20#%202.%20Setup%20the%20environment%20env%20=%20Environment(%27sketchenv%27)%20with%20open(%27requirements.txt%27)%20as%20f:%20#%20Fetch%20all%20dependencies%20as%20a%20list%20%20%20%20%20dependencies%20=%20f.readlines()%20dependencies%20=%20%5Bx.strip()%20for%20x%20in%20dependencies%20if%20%27#%20%27%20not%20in%20x%5D%20env.docker.base_image%20=%20DEFAULT_GPU_IMAGE%20env.python.conda_dependencies%20=%20CondaDependencies.create(conda_packages=%5B%27numpy==1.17.4%27,%20%27Cython%27%5D,%20pip_packages=dependencies)%20%20#%203.%20Inference%20Config%20inference_config%20=%20InferenceConfig(entry_script=%27app.py%27,%20environment=env,%20source_directory=%27.\/ProcessImage%27)%20%20#%204.%20Compute%20target%20(using%20existing%20cluster%20from%20the%20workspacke)%20aks_target%20=%20ComputeTarget(workspace=workspace,%20name=%27sketch-ppt-vm%27)%20%20#%205.%20Deployment%20config%20deployment_config%20=%20AksWebservice.deploy_configuration(cpu_cores=6,%20memory_gb=100)%20%20#%206.%20Model%20deployment%20model%20=%20Model(workspace,%20%27sketch-inference%27)%20#%20Registered%20model%20(which%20contains%20model%20files\/folders)%20service%20=%20Model.deploy(workspace,%20%22process-sketch-dev%22,%20%5Bmodel%5D,%20inference_config,%20deployment_config,%20deployment_target=aks_target,%20overwrite=True)%20service.wait_for_deployment(show_output%20=%20True)%20print(service.state)\" rel=\"nofollow noreferrer\">Environment.from_pip_requirements()<\/a>. A detailed answer in this regard would be interesting to read.<\/p>\n<p>My primary task was inference - object detection given an image, and we have our own model developed by our team. There are two types of imports I wanted to have:<\/p>\n<p><strong>1. Standard python packages (installed through pip)<\/strong><br \/>\nThis was solved by creating conda dependencies and add it to env object (Step 2)<\/p>\n<p><strong>2. Methods\/vars from helper scripts<\/strong> (if you have pre\/post processing to be done during model inference):<br \/>\nThis was done by mentioning <code>source_directory<\/code> in InferenceConfig (step 3)<\/p>\n<p>Here is my updated script which combines Environment creation, Inference and Deployment configs and using existing compute in the workspace (created through portal).<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.webservice import AksWebservice, LocalWebservice\nfrom azureml.core.compute import ComputeTarget\n\n\n# 1. Instantiate the workspace\nworkspace = Workspace.from_config(path=&quot;config.json&quot;)\n\n# 2. Setup the environment\nenv = Environment('sketchenv')\nwith open('requirements.txt') as f: # Fetch all dependencies as a list\n    dependencies = f.readlines()\ndependencies = [x.strip() for x in dependencies if '# ' not in x]\nenv.docker.base_image = DEFAULT_GPU_IMAGE\nenv.python.conda_dependencies = CondaDependencies.create(conda_packages=['numpy==1.17.4', 'Cython'], pip_packages=dependencies)\n\n# 3. Inference Config\ninference_config = InferenceConfig(entry_script='app.py', environment=env, source_directory='.\/ProcessImage')\n\n# 4. Compute target (using existing cluster from the workspacke)\naks_target = ComputeTarget(workspace=workspace, name='sketch-ppt-vm')\n\n# 5. Deployment config\ndeployment_config = AksWebservice.deploy_configuration(cpu_cores=6, memory_gb=100)\n\n# 6. Model deployment\nmodel = Model(workspace, 'sketch-inference') # Registered model (which contains model files\/folders)\nservice = Model.deploy(workspace, &quot;process-sketch-dev&quot;, [model], inference_config, deployment_config, deployment_target=aks_target, overwrite=True)\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n<\/code><\/pre>\n<hr \/>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":25.5,
        "Solution_reading_time":63.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":271.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"ModuleNotFoundError in helper scripts"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.9663888889,
        "Challenge_answer_count":1,
        "Challenge_body":"What is value and use case for Deep Learning AMI (DLAMI)?\n\nIt seems that customers often pack ML dependencies at the docker level (themselves, or with DL containers or with SageMaker containers), instead of the AMI level. So what is the value and use-case of DL AMI ?",
        "Challenge_closed_time":1594216705000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594209626000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning the value and use case of Deep Learning AMI (DLAMI) as customers tend to pack ML dependencies at the docker level instead of the AMI level.",
        "Challenge_last_edit_time":1668530670747,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUQInSlgeCS6mIe4DJv3KwnQ\/what-is-value-and-use-case-for-deep-learning-ami-dlami",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":3.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.9663888889,
        "Challenge_title":"What is value and use case for Deep Learning AMI (DLAMI)?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":57,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The value of the DLAMI (https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/what-is-dlami.html) is ease of use and saving time to get up to speed in a development environment.  If you are developing code for ML there is a huge variety of frameworks and software that you might need to install. The DLAMI includes the more popular ones, so you may quickly deploy a machine complete with common dependencies. This results in a reduction of the time needed for installing and configuring things. It speeds up experimentation and evaluation. If you want to try a new framework, it is already there.\n\nThe second reason is that AWS keeps the AMI up to date, so you may just deploy a new AMI periodically rather than having to patch.  Again, this saves you time and lets you concentrate on the underlying development and business activities.\n\nAll that said, for running in production and at volume you might want to use a different tool, I would imagine that for most cases creating docker images to your specific requirements would make a lot of sense. No need to go over the good and bad points of containers here.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612481183580,
        "Solution_link_count":1.0,
        "Solution_readability":8.7,
        "Solution_reading_time":13.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"value of DLAMI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1808333333,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to connect a Sagemaker notebook to Glue Catalog, but is not allowed to use developer endpoints because of security constraints.\n\nI can't seem to find documentation on the Glue Catalog API that would allow this, or examples of how this might be done.  Any links or pointers would be greatly appreciated.",
        "Challenge_closed_time":1594232347000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594231696000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting a Sagemaker notebook to Glue Catalog due to security constraints that prevent the use of developer endpoints. They are seeking documentation or examples on how to achieve this.",
        "Challenge_last_edit_time":1668594120152,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUoiI3L85FT6OmPewooCH4lQ\/how-to-connect-a-sagemaker-notebook-to-glue-catalog",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":4.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1808333333,
        "Challenge_title":"How to connect a Sagemaker Notebook to Glue Catalog",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1061.0,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"So there is the catalog API which allows you to describe databases, tables, etc. Documentation regarding the calls and data structures can be found here:\n\n- https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-api-catalog-tables.html\n\nBoto3 for get_table\n- https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/glue.html#Glue.Client.get_table\n\nIf they have a restrictive security posture (as suggested by the avoidance of Dev Endpoints) you may also suggest a Glue VPC-E:  https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/vpce-interface.html\n\nI would ask what are they accessing the catalog for, as the Dev Endpoint isn't entirely about the Glue Catalog, but about the compute resources andSparkMagic.\n\nAlso, think about steering them towards AWS Data Wrangler for interacting with Glue Catalog if they are using Pandas. Helpful snippets can be found here: \n\n- https:\/\/github.com\/awslabs\/aws-data-wrangler\/blob\/master\/tutorials\/005%20-%20Glue%20Catalog.ipynb",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1598510602164,
        "Solution_link_count":4.0,
        "Solution_readability":15.0,
        "Solution_reading_time":12.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"connect Sagemaker to Glue Catalog"
    },
    {
        "Answerer_created_time":1552934828727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":254.0,
        "Answerer_view_count":62.0,
        "Challenge_adjusted_solved_time":0.5847786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Challenge_closed_time":1618410091900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1618407986697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" issue while trying to run an object detection code in AWS SageMaker. The error message indicates that the \"cv2\" module is missing, even though it is listed in the requirement file. The user has tried installing \"imgaug\" and \"opencv-python headless\" but the issue persists. The error occurs during the training job and causes it to fail.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":30.4,
        "Challenge_reading_time":154.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":56,
        "Challenge_solved_time":0.5847786111,
        "Challenge_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1218.0,
        "Challenge_word_count":510,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558009697176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cergy-Pontoise, Cergy, France",
        "Poster_reputation_count":53.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":1.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing cv2 module"
    },
    {
        "Answerer_created_time":1306431192840,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Rafael, CA, USA",
        "Answerer_reputation_count":5369.0,
        "Answerer_view_count":637.0,
        "Challenge_adjusted_solved_time":27.9719833334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Thanks for getting back to me.<\/p>\n\n<p>Basically I subscribed to a Cluster API service (cortana analytics). This is the sample application as per Microsoft Machine Learning site<\/p>\n\n<p><a href=\"http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx\" rel=\"nofollow\">http:\/\/microsoftazuremachinelearning.azurewebsites.net\/ClusterModel.aspx<\/a><\/p>\n\n<p>As you could see there are 2 arguments to be passed on<\/p>\n\n<p>Input<\/p>\n\n<p>K<\/p>\n\n<p>Where input could be 10;5;2,18;1;6,7;5;5,22;3;4,12;2;1,10;3;4 (each row is separated by semi colon)<\/p>\n\n<p>And K is cluster number: 5 (for example)<\/p>\n\n<p>So to consume this API I use PowerBI Edit Query, <\/p>\n\n<p>So go to Get Data > More > Azure > Microsoft Data MarketPlace, I can see the list of APIs I subscribed to, one of them is the one I referred to in the link above.<\/p>\n\n<p>So I load that as Function lets called it \"Score\"<\/p>\n\n<p>Then I got energy table which I loaded in from a csv file, I want to cluster energy consumption into 5 clusters.<\/p>\n\n<p>So my data layout is<\/p>\n\n<p>Year   Energy<\/p>\n\n<p>2001   6.28213<\/p>\n\n<p>2002  14.12845<\/p>\n\n<p>2003   5.55851<\/p>\n\n<p>and so on, lets say I got 100 rows of the data.<\/p>\n\n<p>So I tried to pass \"6.28213;14.12845;5.55851\", \"5\" to Score function but I dont know how to <\/p>\n\n<ol>\n<li><p>Convert my table into records<\/p><\/li>\n<li><p>pass 2 argument records and constant value 5 as K.<\/p><\/li>\n<\/ol>\n\n<p>Hope this makes sense.<\/p>\n\n<p>Please help! :)<\/p>\n\n<p>Thank you in advance.<\/p>",
        "Challenge_closed_time":1476896300823,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476795601683,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble consuming a Cluster API service from Cortana Analytics using PowerBI Edit Query. They have loaded the API as a function and have a table of energy consumption data that they want to cluster into 5 clusters. However, they are unsure how to convert their table into records and pass two argument records and a constant value of 5 as K to the Score function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40108999",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":19.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":27.9719833334,
        "Challenge_title":"Consume Microsoft Cluster API using PowerBI",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":137.0,
        "Challenge_word_count":215,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427899203630,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":403.0,
        "Poster_view_count":200.0,
        "Solution_body":"<p>To convert a column of numbers into a semicolon delimited text, do this to your table:<\/p>\n\n<ol>\n<li>Convert your Energy column is type text.<\/li>\n<li>Add <code>[Energy]<\/code> after the name of your table, which gives you a list of the numbers.<\/li>\n<li>Use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/mt253358.aspx\" rel=\"nofollow\"><code>Text.Combine<\/code><\/a> to turn the list into a text value seperated by <code>;<\/code><\/li>\n<\/ol>\n\n<p>Here's a mashup that does that:<\/p>\n\n<pre><code>let\n    Source = Table.FromRows(Json.Document(Binary.Decompress(Binary.FromText(\"NcjBCQAgDAPAXfKWYqKR7iLdfw1F8J63N9Q70bBCKQ5Ue6VbnEHl9L9xz2GniaoD\", BinaryEncoding.Base64), Compression.Deflate)), let _t = ((type text) meta [Serialized.Text = true]) in type table [Year = _t, Energy = _t]),\n    #\"Changed Type\" = Table.TransformColumnTypes(Source,{{\"Year\", Int64.Type}, {\"Energy\", type text}}),\n    Custom1 = #\"Changed Type\"[Energy],\n    Custom2 = Text.Combine(Custom1, \";\")\nin\n    Custom2\n<\/code><\/pre>\n\n<hr>\n\n<p>Once you have a function, you'll invoke it like <code>YourFunction(Custum2, 5)<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":14.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":108.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"trouble consuming Cluster API"
    },
    {
        "Answerer_created_time":1522794798772,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":157.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":7.1225861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>when I create a run using <code>mlflow.start_run()<\/code> ,even if my script is interrupted before executing <code>mlflow.end_run()<\/code>, the run gets tagged as finished instead of unfinished in Status?<\/p>",
        "Challenge_closed_time":1618223603527,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618197962217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where an MLflow experiment run, created using \"mlflow.start_run()\", is getting tagged as finished even if the script is interrupted before executing \"mlflow.end_run()\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67052295",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.8,
        "Challenge_reading_time":3.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7.1225861111,
        "Challenge_title":"MLflow unfinished experiment saved as finished",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":32,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578750761196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>When your notebook stops the run gets the status finished. However, if you want to continue logging metrics or artifacts to that run, you just need to use <code>mlflow.start_run(run_id=&quot;YourRunIDYouCanGetItFromUI&quot;)<\/code>. This is explained in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":5.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":38.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"run tagged as finished prematurely"
    },
    {
        "Answerer_created_time":1614882423070,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":1.5591075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For some reason the jupyter notebooks on my VM are in the wrong environment (ie stuck in <code>(base)<\/code>). Furthermore, I can change the environment in the terminal but not in the notebook. Here is what happens when I attempt <code>!conda activate desired_env<\/code> in the notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n# conda environments:\n#\nbase                  *  \/anaconda\nazureml_py36             \/anaconda\/envs\/azureml_py36\nazureml_py38             \/anaconda\/envs\/azureml_py38\nazureml_py38_pytorch     \/anaconda\/envs\/azureml_py38_pytorch\nazureml_py38_tensorflow     \/anaconda\/envs\/azureml_py38_tensorflow\n<\/code><\/pre>\n<p>I tried the answers <a href=\"https:\/\/stackoverflow.com\/questions\/61915607\/commandnotfounderror-your-shell-has-not-been-properly-configured-to-use-conda\">here<\/a> (e.g., first running <code>!source \/anaconda\/etc\/profile.d\/conda.sh<\/code>).<\/p>\n<p>I also tried activating the environment using <code>source<\/code> rather than 'conda activate': <code>!source \/anaconda\/envs\/desired_env\/bin\/activate<\/code>. This runs but doesn't actually do anything when I see the current environment in <code>conda env list<\/code><\/p>\n<p>Edit: also adding that if I install a package in the <code>(base)<\/code> environment in the terminal, I still don't have access to it in jupyter notebook.<\/p>",
        "Challenge_closed_time":1636652640790,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636646694700,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to change the virtual environment within Azure ML notebook and is stuck in the wrong environment. The user tried changing the environment in the terminal but encountered an error message. The user also tried activating the environment using 'source' but it did not work. Additionally, the user is unable to access packages installed in the (base) environment in the notebook.",
        "Challenge_last_edit_time":1636647028003,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69931411",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.7,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1.6516916667,
        "Challenge_title":"Can't change virtual environment within Azure ML notebook",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":672.0,
        "Challenge_word_count":188,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1586979502910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":361.0,
        "Poster_view_count":47.0,
        "Solution_body":"<p>I'm the PM that released AzureML Notebooks, you can't activate a Conda env from a cell, you have to create a new kernel will the Conda Env. Here are the instructions: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels<\/a><\/p>",
        "Solution_comment_count":8.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.1,
        "Solution_reading_time":5.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":35.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to change environment"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2249388889,
        "Challenge_answer_count":1,
        "Challenge_body":"So we are trying in AWS SageMaker console code to detect images and to configure it we need gcc compiler. which we added in cloud console, refreshed the studio but after running in terminal python setup.py install\" it says that the gcc isnt installed. How should we proceed?",
        "Challenge_closed_time":1684327148088,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684322738308,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to install gcc compiler using \"pip install gcc\" in AWS SageMaker console. Even after adding the compiler in the cloud console and refreshing the studio, the terminal shows an error message stating that gcc is not installed. The user is seeking guidance on how to proceed.",
        "Challenge_last_edit_time":1684670304944,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUuk7qOE4pQrqKFTXZq6FYuQ\/problem-with-pip-install-gcc-in-terminal",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":3.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.2249388889,
        "Challenge_title":"problem with \"pip install gcc\" in terminal",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":54,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"gcc doesn't install via pip. You should install  this way\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1890",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1684327148088,
        "Solution_link_count":1.0,
        "Solution_readability":13.7,
        "Solution_reading_time":1.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"gcc not installed"
    },
    {
        "Answerer_created_time":1505166133223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":43.793195,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to pull the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pre-built-docker-containers-frameworks.html\" rel=\"noreferrer\">pre-built docker images<\/a> for SageMaker. I am able to successfully <code>docker login<\/code> to ECR (my AWS credentials). When I try to pull the image I get the standard <code>no basic auth credentials<\/code>.<\/p>\n\n<p>Maybe I'm misunderstanding... I assumed those ECR URLs were public.<\/p>\n\n<pre><code>$(aws ecr get-login --region us-west-2 --no-include-email)\n\ndocker pull 246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn\n<\/code><\/pre>",
        "Challenge_closed_time":1557174090812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557016435310,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is having trouble pulling pre-built docker images for SageMaker despite successfully logging in to ECR with their AWS credentials. They receive a \"no basic auth credentials\" error when attempting to pull the image and are unsure if the ECR URLs are public.",
        "Challenge_last_edit_time":1557175542127,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55987935",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":43.793195,
        "Challenge_title":"How do I pull the pre-built docker images for SageMaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2738.0,
        "Challenge_word_count":68,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343237570416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California",
        "Poster_reputation_count":1325.0,
        "Poster_view_count":131.0,
        "Solution_body":"<p>Could you show your ECR login command and pull command in the question?<\/p>\n\n<p>For SageMaker pre-built image 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/p>\n\n<p>What I do is:<\/p>\n\n<ol>\n<li>Log in ECR<\/li>\n<\/ol>\n\n<p><code>$(aws ecr get-login --no-include-email --registry-ids 520713654638 --region us-west-2)<\/code><\/p>\n\n<ol start=\"2\">\n<li>Pull the image<\/li>\n<\/ol>\n\n<p><code>docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/code><\/p>\n\n<p>These images are public readable so you can pull them from any AWS account. I guess the reason you failed is that you did not specify --registry-ids in your login. But it's better if you can provide your scripts for others to identify what's wrong.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":9.83,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no auth credentials for pulling images"
    },
    {
        "Answerer_created_time":1613062428296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":14.9379061111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a conda environment called <code>Foo<\/code>. After activating this environment I installed Kedro with <code>pip<\/code>, since <code>conda<\/code> was giving me a conflict. Even though I'm inside the <code>Foo<\/code> environment, when I run:<\/p>\n<pre><code>kedro jupyter lab\n<\/code><\/pre>\n<p>It picks up the modules from my <code>base<\/code> environment, not the <code>Foo<\/code> environment. Any idea, why this is happening, and how I can change what modules my notebook detect?<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>By mangling with my code I found out that on the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> it was calling the python from the base environment, not the <code>Foo<\/code> environment. I changed it manually, but is there a mode automatic way of setting the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> to use the current environment I'm on?<\/p>",
        "Challenge_closed_time":1652879216830,
        "Challenge_comment_count":1,
        "Challenge_created_time":1652795556263,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a conda environment called \"Foo\" and installed Kedro with pip while being inside the \"Foo\" environment. However, when running \"kedro jupyter lab\", the modules are being picked up from the \"base\" environment instead of \"Foo\". The user found out that the kernel.json file was calling the python from the base environment and manually changed it. The user is looking for an automatic way to set the kernel.json file to use the current environment.",
        "Challenge_last_edit_time":1652825440368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72275283",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":23.2390463889,
        "Challenge_title":"Kedro using wrong conda environment",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":115.0,
        "Challenge_word_count":119,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423164285360,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Solution_body":"<p>The custom Kedro kernel spec is a feature that I recently added to Kedro. When you run <code>kedro jupyter lab\/notebook<\/code> it should automatically pick up on the conda environment without you needing to manually edit the kernel.json file. I tested this myself to check that it worked so I'm very interested in understanding what's going on here!<\/p>\n<p>The function <a href=\"https:\/\/github.com\/kedro-org\/kedro\/blob\/58c57c384f5257b998edebb99d94bff46574ae1e\/kedro\/framework\/cli\/jupyter.py#L99\" rel=\"nofollow noreferrer\"><code>_create_kernel<\/code><\/a> is what makes the the Kedro kernel spec. The docstring for that explains what's going on, but in short we delegate to <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L92\" rel=\"nofollow noreferrer\"><code>ipykernel.kernelspec.install<\/code><\/a>. This generates a kernelspec that points towards the Python path given by <code>sys.executable<\/code> (see <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L27\" rel=\"nofollow noreferrer\"><code>make_ipkernel_cmd<\/code><\/a>). In theory this should already point towards the correct Python path, which takes account of the conda environment.<\/p>\n<p>It's worth checking <code>which kedro<\/code> to see which conda environment that points to, and if we need to debug further then please do raise an issue on our <a href=\"https:\/\/github.com\/kedro-org\/kedro\" rel=\"nofollow noreferrer\">Github repo<\/a>. I'd definitely like to get to the bottom of this and understand where the problem is.<\/p>\n<p>P.S. you can also do a plain <code>jupyter lab\/notebook<\/code> to launch a kernel with the right conda environment and then run <code>%load_ext kedro.extras.extensions.ipython<\/code> in the first cell. This is basically equivalent to using the Kedro kernelspec, which loads the Kedro IPython extension automatically.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.8,
        "Solution_reading_time":25.37,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":217.0,
        "Tool":"Kedro",
        "Challenge_type":"inquiry",
        "Challenge_summary":"set kernel.json to current environment"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":34.4616136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am from R background where we can use Plumber kind tool which provide visualization\/graph as Image via end points so we can integrate in our Java application.<\/p>\n\n<p>Now I want to integrate my Python\/Juypter visualization graph with my Java application but not sure how to host it and make it as endpoint. Right now I using AWS sagemaker to host Juypter notebook<\/p>",
        "Challenge_closed_time":1526073355692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525949293883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is from an R background and is looking for a way to integrate Python\/Jupyter visualization graphs with their Java application. They are currently using AWS Sagemaker to host their Jupyter notebook but are unsure how to make it an endpoint for their visualization.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50271174",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":5.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":34.4616136111,
        "Challenge_title":"How to make a Python Visualization as service | Integrate with website | specially sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":76.0,
        "Challenge_word_count":75,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1501403168107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Solution_body":"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. <\/p>\n\n<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn\u2019t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. <\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":9.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":123.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"integrate Python\/Jupyter visualization"
    },
    {
        "Answerer_created_time":1395235213383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Norway",
        "Answerer_reputation_count":46807.0,
        "Answerer_view_count":3021.0,
        "Challenge_adjusted_solved_time":6.7343452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I recently (and sceptically) started messing around with <strong>Azure Machine Learning Studio<\/strong>. When I stumbled accross the menu option for a machine learning work-flow <strong>Open in a new Notebook<\/strong> (For Python 3, 2 or R) I thought it was too good to be true:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AYDbo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AYDbo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And it most likely is, since this option is seemingly only available for the first step of the process. The option still exists in the <strong>right-click menu<\/strong> elsewhere, but it's greyed out:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cy6MG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cy6MG.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does anyone know why it is like this? Do I have to activate something in the menus, or buy some sort of a premium license? Is the functionality only available for <em>some<\/em> of the machine learning algorithms? Or is it just not supposed to be an available option in the menus?<\/p>\n\n<p>By the way, if you click <kbd>Python 3<\/kbd> 3 in the first step, you get a corresponding Python 3 code snippet in a Jupyter Notebook where you immediately can start messing around with the dataset:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/gTHGF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gTHGF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I realize that making this functionality available for each step in each and every model that anyone chooses to design would be an extremely difficult and maybe even impossible thing to do. But again, why is the option still in the menu?<\/p>",
        "Challenge_closed_time":1529068177343,
        "Challenge_comment_count":1,
        "Challenge_created_time":1529043933700,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to open a new Python or R notebook in Azure Machine Learning Studio, as the option is greyed out in the right-click menu. The user is unsure if this is due to a premium license requirement or if the functionality is only available for certain machine learning algorithms. The option is available for the first step of the process, but not for subsequent steps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50870166",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":9.6,
        "Challenge_reading_time":22.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.7343452778,
        "Challenge_title":"Can not open new Python or R notebook in Azure Machine Learning Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1386.0,
        "Challenge_word_count":243,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395235213383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":46807.0,
        "Poster_view_count":3021.0,
        "Solution_body":"<p>Following the suggestion from @Jon in the comment section as well as a suggestion on <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/windowsdesktop\/en-US\/84a33ecc-1db2-4d11-83d2-3e96f0bcfaa7\/why-open-in-a-new-notebook-is-invalid?forum=MachineLearning\" rel=\"nofollow noreferrer\">microsoft.com<\/a>, I added a <strong>Convert to CSV Module<\/strong> at the end. After running the experiment, <strong>Open in a new Notebook<\/strong> is available when you right clik the <strong>Convert to CSV Module<\/strong>:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Bz7nJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Bz7nJ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What you get by clicking <kbd>Python 3<\/kbd> is this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ywbHz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ywbHz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The functionality is certainly not as magnificent as I was hoping, but it's still pretty cool. If anyone knows <em>anything<\/em> about other possibilites or plans for future development, please don't hesitate to contribute with an answer!<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.2,
        "Solution_reading_time":15.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":112.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"greyed out notebook option"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.4081172222,
        "Challenge_answer_count":1,
        "Challenge_body":"The CloudWatch logs show following error\n```\n    Traceback (most recent call last):\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_scheduler\/handlers.py\", line 194, in post\n        job_id = await ensure_async(self.scheduler.create_job(CreateJob(**payload)))\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_server\/utils.py\", line 182, in ensure_async\n        result = await obj\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 109, in wrapper\n        raise excep\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 105, in wrapper\n        return await func(*args, **kwargs)\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 210, in create_job\n        s3_file_uploader = await self._prepare_job_artifacts(\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 168, in _prepare_job_artifacts\n        input_uri = S3URI(runtime_environment_parameters.s3_input)\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/runtime_environment_parameters.py\", line 40, in s3_input\n        return self.parameters.get(RuntimeEnvironmentParameterName.S3_INPUT.value)\n    AttributeError: 'NoneType' object has no attribute 'get'\n[E 2023-03-16 15:36:01.070 SchedulerApp] 'NoneType' object has no attribute 'get' Traceback (most recent call last): File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_scheduler\/handlers.py\", line 194, in post job_id = await ensure_async(self.scheduler.create_job(CreateJob(**payload))) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_server\/utils.py\", line 182, in ensure_async result = await obj File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 109, in wrapper raise excep File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 105, in wrapper return await func(*args, **kwargs) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 210, in create_job s3_file_uploader = await self._prepare_job_artifacts( File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 168, in _prepare_job_artifacts input_uri = S3URI(runtime_environment_parameters.s3_input) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/runtime_environment_parameters.py\", line 40, in s3_input return self.parameters.get(RuntimeEnvironmentParameterName.S3_INPUT.value) AttributeError: 'NoneType' object has no attribute 'get'\n```\nAlso all the \"advanced options\" are missing in the \"create notebook job\" dialogue.\n\nThe setting is isolated VPC with permissions updated accordingly: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/scheduled-notebook-policies.html\n\nThe VPC endpoints for S3, SageMaker API and Runtime, SSM, STS, Metrics, Logs, ECR API and ECR DKR are deployed in the VPC. Notebooks are working fine.\n\nAny idea what could be wrong?",
        "Challenge_closed_time":1678987295300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678982226078,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create a notebook job in Sagemaker Studio in an isolated VPC. The CloudWatch logs show an AttributeError: 'NoneType' object has no attribute 'get'. Additionally, the \"advanced options\" are missing in the \"create notebook job\" dialogue. The VPC endpoints for S3, SageMaker API and Runtime, SSM, STS, Metrics, Logs, ECR API and ECR DKR are deployed in the VPC.",
        "Challenge_last_edit_time":1679330199392,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbLVvWJbeS42j0e1nymPYcg\/sagemaker-studio-in-vpc-only-fails-to-create-notebook-job-with-unexpected-error-occurred-during-creation-of-job",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.2,
        "Challenge_reading_time":42.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":1.4081172222,
        "Challenge_title":"Sagemaker Studio in VPC Only fails to create Notebook job with \"Unexpected error occurred during creation of job.\"",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":235,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for reporting the issue. \n\nCan you try also configuring following two vpc endpoints? \n1. Amazon EC2\n2. Amazon EventBridge\nAlso, if you have s3 vpc gateway endpoint policy with fine control of allowed s3 bucet, please allow s3 access for sagemakerheadlessexecution-prod-* like below. \nFYI - you can find more reference link from https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-notebook-auto-execution-advanced.html\n\n```\n{\n   \"Action\":[\n      \"s3:*\"\n   ],\n   \"Resource\":[\n      \"arn:aws:s3:::sagemakerheadlessexecution-prod-*\",\n      \"arn:aws:s3:::sagemakerheadlessexecution-prod-*\/*\"\n   ],\n   \"Effect\":\"Allow\",\n   \"Sid\":\"SCTASK14554266\"\n}\n```",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1678987295300,
        "Solution_link_count":1.0,
        "Solution_readability":15.5,
        "Solution_reading_time":8.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"AttributeError in CloudWatch logs"
    },
    {
        "Answerer_created_time":1356359759000,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Dubai - United Arab Emirates",
        "Answerer_reputation_count":436.0,
        "Answerer_view_count":73.0,
        "Challenge_adjusted_solved_time":0.4312602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The code below works fine in a sagemaker notebook in the cloud. Locally I also have aws credentials created via the aws cli. Personally, I do not like notebooks (unless I do some EDA or the like). So I wonder if one can also fire this code up from a local machine (e.g. in visual studio code) as it only tells sagemaker what to do anyway? I guess it is only a question of authenticating and getting the session object? Thanks!<\/p>\n<pre><code>import boto3\nimport os\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\nfrom sagemaker import image_uris\n\nregion_name = boto3.session.Session().region_name\ns3_bucket_name = 'bucket_name'\n# this image cannot be used below !!! there must be an issue with sagemaker ?\ntraining_image_name = image_uris.retrieve(framework='xgboost', region=region_name, version='latest')\nrole = get_execution_role()\ns3_prefix = 'my_model'\ntrain_file_name = 'sagemaker_train.csv'\nval_file_name = 'sagemaker_val.csv'\nsagemaker_session = sagemaker.Session()\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;10&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>",
        "Challenge_closed_time":1650001048760,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649999496223,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is wondering if it is possible to run the code for a Sagemaker notebook locally in Visual Studio Code. They have AWS credentials created via the AWS CLI and are looking to authenticate and get the session object. The code works fine in a Sagemaker notebook in the cloud, but the user prefers not to use notebooks and is looking for an alternative.",
        "Challenge_last_edit_time":1651138032863,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71880336",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.6,
        "Challenge_reading_time":28.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":0.4312602778,
        "Challenge_title":"can one run sagemaker notebook code locally in visual studio code",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":425.0,
        "Challenge_word_count":182,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>On a local machine,<\/p>\n<ul>\n<li>Make sure to install AWS CLI: <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/getting-started-install.html<\/a><\/li>\n<li>Create an access key id and secret access key to access Sagemaker services locally: <a href=\"https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/id_credentials_access-keys.html<\/a><\/li>\n<li>Set these credentials using <code>aws configure<\/code> command locally.<\/li>\n<li>The code should work fine except getting the execution role. You can either hard code the Sagemaker role in the code (not best practice) or store it in the Parameter store and access it from there.<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.9,
        "Solution_reading_time":11.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":78.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"run Sagemaker code locally"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":0.4295236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently trying to open parquet files using Azure Jupyter Notebooks. I have tried both Python kernels (2 and 3).\nAfter the installation of <em>pyarrow<\/em> I can import the module only if the Python kernel is 2 (not working with Python 3)<\/p>\n\n<p>Here is what I've done so far (for clarity, I am not mentioning all my various attempts, such as using <em>conda<\/em> instead of <em>pip<\/em>, as it also failed):<\/p>\n\n<pre><code>!pip install --upgrade pip\n!pip install -I Cython==0.28.5\n!pip install pyarrow\n\nimport pandas  \nimport pyarrow\nimport pyarrow.parquet\n\n#so far, so good\n\nfilePath_parquet = \"foo.parquet\"\ntable_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n<\/code><\/pre>\n\n<p>This works well if I'm doing that off-line (using Spyder, Python v.3.7.0). But it fails using an Azure Notebook.<\/p>\n\n<pre><code> AttributeErrorTraceback (most recent call last)\n&lt;ipython-input-54-2739da3f2d20&gt; in &lt;module&gt;()\n      6 \n      7 #table_parquet_raw = pd.read_parquet(filePath_parquet, engine='pyarrow')\n----&gt; 8 table_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n\nAttributeError: 'module' object has no attribute 'read_parquet'\n<\/code><\/pre>\n\n<p>Any idea please?<\/p>\n\n<p>Thank you in advance !<\/p>\n\n<p>EDIT:<\/p>\n\n<p>Thank you very much for your reply Peter Pan !\nI have typed these  statements, here is what I got:<\/p>\n\n<p>1.<\/p>\n\n<pre><code>    print(pandas.__dict__)\n<\/code><\/pre>\n\n<p>=> read_parquet does not appear<\/p>\n\n<p>2.<\/p>\n\n<pre><code>    print(pandas.__file__)\n<\/code><\/pre>\n\n<p>=> I get:<\/p>\n\n<pre><code>    \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/pandas\/__init__.py\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li><p>import sys; print(sys.path) => I get:<\/p>\n\n<pre><code>['', '\/home\/nbuser\/anaconda3_23\/lib\/python34.zip',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/plat-linux',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/lib-dynload',\n'\/home\/nbuser\/.local\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/Sphinx-1.3.1-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/IPython\/extensions',\n'\/home\/nbuser\/.ipython']\n<\/code><\/pre><\/li>\n<\/ol>\n\n<p>Do you have any idea please ?<\/p>\n\n<p>EDIT 2:<\/p>\n\n<p>Dear @PeterPan, I have typed both <code>!conda update conda<\/code> and  <code>!conda update pandas<\/code> : when checking the Pandas version (<code>pandas.__version__<\/code>), it is still <code>0.19.2<\/code>.<\/p>\n\n<p>I have also tried with <code>!conda update pandas -y -f<\/code>, it returns:\n`Fetching package metadata ...........\nSolving package specifications: .<\/p>\n\n<p>Package plan for installation in environment \/home\/nbuser\/anaconda3_23:<\/p>\n\n<p>The following NEW packages will be INSTALLED:<\/p>\n\n<pre><code>pandas: 0.19.2-np111py34_1`\n<\/code><\/pre>\n\n<p>When typing:\n<code>!pip install --upgrade pandas<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Requirement already up-to-date: pandas in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\nRequirement already up-to-date: pytz&gt;=2011k in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: numpy&gt;=1.9.0 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: python-dateutil&gt;=2 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: six&gt;=1.5 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from python-dateutil&gt;=2-&gt;pandas)<\/code><\/p>\n\n<p>Finally, when typing:<\/p>\n\n<p><code>!pip install --upgrade pandas==0.24.0<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Collecting pandas==0.24.0\n  Could not find a version that satisfies the requirement pandas==0.24.0 (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0rc1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0rc2, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0rc1, 0.19.0, 0.19.1, 0.19.2, 0.20.0rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0rc1, 0.21.0, 0.21.1, 0.22.0)\nNo matching distribution found for pandas==0.24.0<\/code><\/p>\n\n<p>Therefore, my guess is that the problem comes from the way the packages are managed in Azure. Updating a package (here Pandas), should lead to an update to the latest version available, shouldn't it?<\/p>",
        "Challenge_closed_time":1545730275287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1545322944093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to read \".parquet\" files in Azure Jupyter Notebook using Python 2 and 3 kernels. After installing pyarrow, the module can only be imported with Python 2 kernel and not with Python 3. The user has tried various attempts such as using conda instead of pip, but it failed. The user has also tried updating pandas, but it did not work. The user suspects that the problem may be due to the way packages are managed in Azure.",
        "Challenge_last_edit_time":1547188516027,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53872444",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":60.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":113.1475538889,
        "Challenge_title":"Cannot read \".parquet\" files in Azure Jupyter Notebook (Python 2 and 3)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1879.0,
        "Challenge_word_count":455,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545322329030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I tried to reproduce your issue on my Azure Jupyter Notebook, but failed. There was no any issue for me without doing your two steps <code>!pip install --upgrade pip<\/code> &amp; <code>!pip install -I Cython==0.28.5<\/code> which I think not matter.<\/p>\n\n<p>Please run some codes below to check your import package <code>pandas<\/code> whether be correct.<\/p>\n\n<ol>\n<li>Run <code>print(pandas.__dict__)<\/code> to check whether has the description of <code>read_parquet<\/code> function in the output.<\/li>\n<li>Run <code>print(pandas.__file__)<\/code> to check whether you imported a different <code>pandas<\/code> package.<\/li>\n<li>Run <code>import sys; print(sys.path)<\/code> to check the order of paths whether there is a same named file or directory under these paths.<\/li>\n<\/ol>\n\n<p>If there is a same file or directory named <code>pandas<\/code>, you just need to rename it and restart your <code>ipynb<\/code> to re-run. It's a common issue which you can refer to these SO threads <a href=\"https:\/\/stackoverflow.com\/questions\/35341363\/attributeerror-module-object-has-no-attribute-reader\">AttributeError: &#39;module&#39; object has no attribute &#39;reader&#39;<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/36250353\/importing-installed-package-from-script-raises-attributeerror-module-has-no-at\">Importing installed package from script raises &quot;AttributeError: module has no attribute&quot; or &quot;ImportError: cannot import name&quot;<\/a>.<\/p>\n\n<p>In Other cases, please update your post for more details to let me know.<\/p>\n\n<hr>\n\n<p>The latest <code>pandas<\/code> version should be <code>0.23.4<\/code>, not <code>0.24.0<\/code>.<\/p>\n\n<p>I tried to find out the earliest version of <code>pandas<\/code> which support the <code>read_parquet<\/code> feature via search the function name <code>read_parquet<\/code> in the documents of different version from <code>0.19.2<\/code> to <code>0.23.3<\/code>. Then, I found <code>pandas<\/code> supports <code>read_parquet<\/code> feature after the version <code>0.21.1<\/code>, as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The new features shown in the <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/whatsnew.html\" rel=\"nofollow noreferrer\"><code>What's New<\/code><\/a> of version <code>0.21.1<\/code>\n<a href=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>According to your <code>EDIT 2<\/code> description, it seems that you are using Python 3.4 in Azure Jupyter Notebook. Not all <code>pandas<\/code> versions support Python 3.4 version.<\/p>\n\n<p>The versions <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.21.1<\/code><\/a> &amp; <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.22\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.22.0<\/code><\/a> offically support Python 2.7,3.5, and 3.6, as below.\n<a href=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the <a href=\"https:\/\/pypi.org\/project\/pandas\/\" rel=\"nofollow noreferrer\">PyPI page for <code>pandas<\/code><\/a> also requires the Python version as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6613J.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6613J.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So you can try to install the <code>pandas<\/code> versions <code>0.21.1<\/code> &amp; <code>0.22.0<\/code> in the current notebook of Python 3.4. if failed, please create a new notebook in Python <code>2.7<\/code> or <code>&gt;=3.5<\/code> to install <code>pandas<\/code> version <code>&gt;= 0.21.1<\/code> to use the function <code>read_parquet<\/code>.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1547190062312,
        "Solution_link_count":14.0,
        "Solution_readability":10.9,
        "Solution_reading_time":52.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":51.0,
        "Solution_word_count":383.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to read parquet files"
    },
    {
        "Answerer_created_time":1469978721363,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":48.0,
        "Challenge_adjusted_solved_time":73.1672544445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We can now publish Docker images to AWS ECR directly from <strong>SageMaker Studio<\/strong> using this code <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli<\/a>\nI did follow the easy installation instructions:<\/p>\n<pre><code>!pip install sagemaker-studio-image-build\nsm-docker build .\n<\/code><\/pre>\n<p>Also Trust policy and permissions have been set as described in the instructions.\nBut I'm getting the error &quot;<strong>Command did not exit successfully docker push<\/strong>&quot; at the stage where it is pushing the Docker image to AWS ECR. Any idea why? Here are the details print as output:<\/p>\n<pre><code>[Container] 2021\/05\/04 06:57:20 Running command echo Pushing the Docker image...\nPushing the Docker image...\n\n[Container] 2021\/05\/04 06:57:20 Running command docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG\nThe push refers to repository [752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml]\nAn image does not exist locally with the tag: 752731038471.dkr.ecr.eu-central-1.amazonaws.com\/sagemaker-studio-d-tfbogtriaiml\n\n[Container] 2021\/05\/04 06:57:20 Command did not exit successfully docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG exit status 1\n[Container] 2021\/05\/04 06:57:20 Phase complete: POST_BUILD State: FAILED\n[Container] 2021\/05\/04 06:57:20 Phase context status code: COMMAND_EXECUTION_ERROR Message: Error while executing command: docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\/$IMAGE_REPO_NAME:$IMAGE_TAG. Reason: exit status 1\n<\/code><\/pre>",
        "Challenge_closed_time":1620378926343,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620115524227,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while pushing a Docker image to AWS ECR from SageMaker Studio using AWS CLI. The error message states that the command did not exit successfully while pushing the Docker image and an image does not exist locally with the tag. The user has followed the installation instructions and set trust policy and permissions as described.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67380942",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":23.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":73.1672544445,
        "Challenge_title":"Pushing docker image in AWS ECR from SageMaker Studio using AWS CLI",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1046.0,
        "Challenge_word_count":173,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1469978721363,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":415.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>In the Dockerfile, there was a reference to another file that was not present in the directory from where the command <code>sm-docker build .<\/code> was launched.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":2.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"failed Docker image push"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.6107855556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have the following error while running <a href=\"http:\/\/wandb.me\/prompts-quickstart\" rel=\"noopener nofollow ugc\">W&amp;B_Prompts_Quickstart notebook<\/a> <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/8\/88da9eb34b5ef62baf4fcd367d69c08d5579c825.png\" data-download-href=\"\/uploads\/short-url\/jwFkh0P5adLx7fGnRM3YtxxLn4p.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/8\/88da9eb34b5ef62baf4fcd367d69c08d5579c825.png\" alt=\"image\" data-base62-sha1=\"jwFkh0P5adLx7fGnRM3YtxxLn4p\" width=\"690\" height=\"289\" data-dominant-color=\"F4F4F4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">774\u00d7325 8.32 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Challenge_closed_time":1684232017936,
        "Challenge_comment_count":0,
        "Challenge_created_time":1684222619108,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running the W&B_Prompts_Quickstart notebook, as shown in the attached image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/error-in-w-b-prompts-quickstart-notebook\/4411",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":28.9,
        "Challenge_reading_time":15.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.6107855556,
        "Challenge_title":"Error in W&B_Prompts_Quickstart notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":50,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tinsae\">@tinsae<\/a> thanks for reporting this issue. There\u2019s a breaking change with newer LangChain version, and the Growth team is working on a fix.<\/p>\n<p>You could run the Prompts Quickstart notebook for now by pinning a previous LangChain version which I just tested and seems to be working fine. Could you please change the installation section as follows:<\/p>\n<pre><code class=\"lang-auto\">!pip install \"wandb&gt;=0.15.2\" -qqq\n!pip install \"langchain==v0.0.158\" openai\n<\/code><\/pre>\n<p>Would this work for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":7.04,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error in notebook"
    },
    {
        "Answerer_created_time":1611181716003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":2450.9835063889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am currently developing an Azure ML pipeline that as one of its outputs is maintaining a SQL table holding all of the unique items that are fed into it. There is no way to know in advance if the data fed into the pipeline is new unique items or repeats of previous items, so before updating the table that it maintains it pulls the data already in that table and drops any of the new items that already appear.<\/p>\n<p>However, due to this there are cases where this self-reference results in zero new items being found, and as such there is nothing to export to the SQL table. When this happens Azure ML throws an error, as it is considered an error for there to be zero lines of data to export. In my case, however, this is expected behaviour, and as such absolutely fine.<\/p>\n<p>Is there any way for me to suppress this error, so that when it has zero lines of data to export it just skips the export module and moves on?<\/p>",
        "Challenge_closed_time":1630895031176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622071490553,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is developing an Azure ML pipeline that maintains a SQL table holding unique items. The pipeline drops any new items that already appear in the table before updating it. However, in cases where there are zero new items, Azure ML throws an error as it is considered an error for there to be zero lines of data to export. The user is looking for a way to suppress this error and skip the export module when there are zero lines of data to export.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67713876",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":12.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2450.9835063889,
        "Challenge_title":"Is there a way to stop Azure ML throwing an error when exporting zero lines of data?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":192,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611181716003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>This issue has been resolved by an update to Azure Machine Learning; You can now run pipelines with a flag set to &quot;Continue on Failure Step&quot;, which means that steps following the failed data export will continue to run.<\/p>\n<p>This does mean you will need to design your pipeline to be able to handles upstream failures in its downstream modules; this must be done very carefully.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.8,
        "Solution_reading_time":4.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error with zero data export"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":164.9280080556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using MS Azure ML and have found that when I start a Notebook (from the Azure ML Studio) it is executing in a a different environment than if I create a Python script and run it from the studio. I want to be able to create a specific environment and have the Notebook use that. The environment that the Notebook seems to run does not contain the packages I need and I want to preserve different environments.<\/p>",
        "Challenge_closed_time":1641553768432,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641247442447,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with different environments in Azure ML Studio while running a Notebook and a Python script. The Notebook is executing in a different environment and does not contain the required packages, which is causing issues for the user. The user wants to create a specific environment and use it in the Notebook to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70571948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":85.0905513889,
        "Challenge_title":"Why is env different in an Azure ML notbook and an Azure ML terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":225.0,
        "Challenge_word_count":91,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1639768329608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Eden Prarie, MN",
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>First open a terminal, using the same compute target as you want to use with your Notebook afterwards, and to use and <strong>existing environment<\/strong> you can do:<\/p>\n<pre><code>conda activate existing_env\nconda install ipykernel\npython -m ipykernel install --user --name existing_env --display-name &quot;Python 3.8 - Existing Environment&quot;   \n<\/code><\/pre>\n<p>However, to create a <strong>new environment<\/strong> and use it in you AzureML Notebook, you have to do the following commands:<\/p>\n<pre><code>conda create --name new_env python=3.8\nconda activate new_env\nconda install pip\nconda install ipykernel\npython -m ipykernel install --user --name new_env --display-name &quot;Python 3.8 - New Environment&quot;\n<\/code><\/pre>\n<p>And then last, but not least, you have to edit the Jupyter Kernel display names:<\/p>\n<p><strong>IMPORTANT<\/strong> Please ensure you are comfortable running all these steps:<\/p>\n<pre><code>jupyter kernelspec list\ncd &lt;folder-that-matches-the-kernel-of-your-environment&gt;\nsudo nano kernel.json\n<\/code><\/pre>\n<p>Then edit the name to match what you want and save the file.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1641841183276,
        "Solution_link_count":0.0,
        "Solution_readability":16.6,
        "Solution_reading_time":14.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing packages in Notebook"
    },
    {
        "Answerer_created_time":1600461527240,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1051.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":7.3123536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am planning to start a Jupyter Notebook instance and execute each notebook file in AWS Sagemaker using AWS Step Functions. Can this be achieved?<\/p>",
        "Challenge_closed_time":1606148997440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606118086040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to start and execute a Jupyter Notebook instance in AWS Sagemaker using AWS Step Functions.",
        "Challenge_last_edit_time":1606122672967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64964435",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":2.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":8.5865,
        "Challenge_title":"Can you start and execute a Jupyter Notebook in Sagemaker using Step Functions?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":671.0,
        "Challenge_word_count":37,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461162996723,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":882.0,
        "Poster_view_count":97.0,
        "Solution_body":"<p>The AWS Step Functions Data Science SDK is an open source library that allows data scientists to easily create workflows that process and publish machine learning models using SageMaker and Step Functions.<\/p>\n<p>The following <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-nbexamples.html\" rel=\"nofollow noreferrer\">Example notebooks<\/a>, which are available in Jupyter notebook instances in the <a href=\"https:\/\/console.aws.amazon.com\/sagemaker\/\" rel=\"nofollow noreferrer\">SageMaker console<\/a> and the related <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/step-functions-data-science-sdk\" rel=\"nofollow noreferrer\">GitHub project<\/a>:<\/p>\n<ul>\n<li>hello_world_workflow.ipynb<\/li>\n<li>machine_learning_workflow_abalone.ipynb<\/li>\n<li>training_pipeline_pytorch_mnist.ipynb<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":21.4,
        "Solution_reading_time":11.33,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"start Jupyter in Step Functions"
    },
    {
        "Answerer_created_time":1446841002932,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Trondheim, Norway",
        "Answerer_reputation_count":226.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":0.2383613889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an Azure ML Web service which outputs JSON response on request, and the structure of the sample request is as following:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"gender\",\n        \"age\",\n        \"income\"\n      ],\n      \"Values\": [\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ],\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>And the input parameters are supposedly like this:<\/p>\n\n<p>gender  String<br>\nage Numeric<br>\nincome  Numeric     <\/p>\n\n<p>My Post method looks like this:<\/p>\n\n<pre><code>    [HttpPost]\n        public ActionResult GetPredictionFromWebService()\n        {\n            var gender = Request.Form[\"gender\"];\n            var age = Request.Form[\"age\"];\n\n\n            if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n            {\n                var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = Int32.Parse(result[0, 2])\n                    };\n                }\n            }\n\n\n\n\n            return RedirectToAction(\"index\");\n        }\n<\/code><\/pre>\n\n<p>But for whatever reason; the Azure ML Webservice doesn\u2019t seem to respond anything to my request.\nDoes anyone know what the reason might be? I see no error or anything, just an empty response.<\/p>",
        "Challenge_closed_time":1456314151448,
        "Challenge_comment_count":0,
        "Challenge_created_time":1456313293347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an Azure ML Web service that outputs a JSON response on request. The user has created a post method to get predictions from the web service, but the web service is not responding to the request, and the user is not receiving any error messages.",
        "Challenge_last_edit_time":1456839908307,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35600907",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":16.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.2383613889,
        "Challenge_title":"Azure ML Web Service request not working in C#",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":480.0,
        "Challenge_word_count":143,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456309738852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":39.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>The answer to your problem is that the \u201cNumeric\u201d datatype which is written in the input parameters in Azure ML is in fact a float and not an integer for your income measure. So when trying to request a response from Azure ML, you are not providing it the \u201cadequate\u201d information needed in the right format for it to respond correctly, resulting in it not giving you any response.<\/p>\n\n<p>I believe your model would look something similar to this based on your input parameters:<\/p>\n\n<pre><code>public class Person\n    {\n        public string Gender { get; set; }\n        public int Age { get; set; }\n        public int Income { get; set; }\n\n\n        public override string ToString()\n        {\n            return Gender + \",\" + Age + \",\" + Income;\n        }\n    }\n<\/code><\/pre>\n\n<p>You would have to change your Income datatype into float like so:<\/p>\n\n<pre><code>public class Person\n{\n    public string Gender { get; set; }\n    public int Age { get; set; }\n    public float Income { get; set; }\n\n    public override string ToString()\n    {\n        return Gender + \",\" + Age + \",\" + Income;\n    }\n}\n<\/code><\/pre>\n\n<p>And then your post-method would look something like this:<\/p>\n\n<pre><code>    [HttpPost]\n    public ActionResult GetPredictionFromWebService()\n    {\n        var gender = Request.Form[\"gender\"];\n        var age = Request.Form[\"age\"];\n\n        if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n        {\n            var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n                };\n            }\n        }\n\n        ViewBag.myData = PersonResult.Income.ToString();\n        return View(\"Index\");\n    }\n<\/code><\/pre>\n\n<p>The key here is simply:<\/p>\n\n<pre><code>Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n<\/code><\/pre>\n\n<p>Rather than your legacy <\/p>\n\n<pre><code>Income = Int32.Parse(result[0, 2])\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":24.94,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":221.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"no response from web service"
    },
    {
        "Answerer_created_time":1393579668636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Challenge_adjusted_solved_time":0.6604344445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to overwrite properties taken from the parameters.yaml file within a Kedro notebook?<\/p>\n\n<p>I am trying to dynamically change parameter values within a notebook. I would like to be able to give users the ability to run a standard pipeline but with customizable parameters. I don't want to change the YAML file, I just want to change the parameter for the life of the notebook.<\/p>\n\n<p>I have tried editing the params within the context but this has no affect.<\/p>\n\n<pre><code>context.params.update({\"test_param\": 2})\n<\/code><\/pre>\n\n<p>Am I missing something or is this not an intended use case?<\/p>",
        "Challenge_closed_time":1582114523347,
        "Challenge_comment_count":1,
        "Challenge_created_time":1582112145783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to dynamically change parameter values within a Kedro notebook without changing the YAML file. They have attempted to edit the parameters within the context but it did not work. They are seeking clarification on whether this is an intended use case or if they are missing something.",
        "Challenge_last_edit_time":1583420995670,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60299478",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":8.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.6604344445,
        "Challenge_title":"Setting parameters in Kedro Notebook",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":586.0,
        "Challenge_word_count":100,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582111403048,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>Kedro supports specifying <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/03_configuration.html#specifying-extra-parameters\" rel=\"nofollow noreferrer\">extra parameters<\/a> from the command line by running<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>kedro run --params \"key1:value1,key2:value2\"\n<\/code><\/pre>\n\n<p>which solves your second use case.<\/p>\n\n<p>As for the notebook use case, updating <code>context.params<\/code> does not have any effect since the context does not store the parameters on <code>self<\/code> but rather pulls them from the config every time the property is being called.<\/p>\n\n<p>However you can still add extra parameters to the context object after it being instantiated:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>extra_params = context._extra_params or {}\nextra_params.update({\"test_param\": 2})\ncontext._extra_params = extra_params\n<\/code><\/pre>\n\n<p>This will update extra parameters that are applied on top of regular parameters coming from the config.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":14.4,
        "Solution_reading_time":13.31,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":106.0,
        "Tool":"Kedro",
        "Challenge_type":"inquiry",
        "Challenge_summary":"dynamically change Kedro parameters"
    },
    {
        "Answerer_created_time":1351534968768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":823.0,
        "Answerer_view_count":114.0,
        "Challenge_adjusted_solved_time":43.0164288889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know how to access packages in <code>R<\/code> scripts in <code>Azure machine<\/code> learning by either using the <code>Azure<\/code> supported ones or by zipping up the packages.<\/p>\n\n<p>My problem now is that <code>Azure<\/code> machine learning does not support the <code>h2o package<\/code> and when I tried using the zipped file - it gave an <code>error<\/code>. <\/p>\n\n<p>Has anyone figured out how to use <code>h2o<\/code> in <code>R<\/code> in <code>Azure machine<\/code> learning?<\/p>",
        "Challenge_closed_time":1487237805672,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487082559403,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in running the h2o package in R script in Azure machine learning. Azure machine learning does not support the h2o package and using the zipped file resulted in an error. The user is seeking help to figure out how to use h2o in R in Azure machine learning.",
        "Challenge_last_edit_time":1487082946528,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42228715",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":43.1239636111,
        "Challenge_title":"Running h2o in R script in Azure machine learning",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":361.0,
        "Challenge_word_count":75,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351534968768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":823.0,
        "Poster_view_count":114.0,
        "Solution_body":"<p>So since there was no reply to my question, I made some research and came up with the following:<\/p>\n\n<p>H2O cannot be ran in a straightforward manner in Azure machine learning embedded R scripts. A workaround the problem is to consider using an Azure created environment - specially for H2O. The options available are:<\/p>\n\n<ol>\n<li>Spinning up an H2O Artificial Intelligence Virtual Machine solution<\/li>\n<li>Using an H2O application for HDInsight<\/li>\n<\/ol>\n\n<p>For more reading, you can go to: <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html\" rel=\"nofollow noreferrer\">http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/azure.html<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.8,
        "Solution_reading_time":8.43,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"h2o package not supported"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.4182138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to retrieve notebooks that were hosted on notebooks.azure.com? If so, how? The service is now discontinued but I would like to retrieve files that were hosted on the service.<\/p>",
        "Challenge_closed_time":1615975649020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615959743450,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on how to retrieve notebooks that were previously hosted on notebooks.azure.com, which is now discontinued.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/317875\/retrieve-notebooks-azure-files",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":2.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.4182138889,
        "Challenge_title":"Retrieve Notebooks Azure Files",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=e2b81f66-f91b-46e3-964b-29275f3216c7\">@Sean  <\/a> I am afraid that the option to retrieve this data is not possible. Please refer this <a href=\"https:\/\/github.com\/microsoft\/AzureNotebooks\/issues\/838\">thread<\/a> for information and the options that were available before the last day to migrate them. Thanks!!    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":4.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":38.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"retrieve discontinued notebooks"
    },
    {
        "Answerer_created_time":1425426748316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":91.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":1787.9299130556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am doing the following steps to install R Hash_2.2.6.zip package on to Azure ML<\/p>\n\n<ol>\n<li>Upload the .zip file as a dataset<\/li>\n<li>Create a new experiment and Add \"Execute R Script\" to experiment<\/li>\n<li>Drag and drop .zip file dataset to experiment.<\/li>\n<li>Connect the Dataset in step3 to \"Execute R Script\" of step2<\/li>\n<li>Run the experiment to install the package<\/li>\n<\/ol>\n\n<p>However I am getting this error: <code>zip file src\/hash_2.2.6.zip not found<\/code><\/p>\n\n<p>Just so that its very clear, I am following steps mentioned in this article: <a href=\"http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx\" rel=\"nofollow\">http:\/\/blogs.technet.com\/b\/saketbi\/archive\/2014\/08\/20\/microsoft-azure-ml-amp-r-language-extensibility.aspx<\/a>.<\/p>\n\n<p>Any help in this regard is greatly appreciated.<\/p>",
        "Challenge_closed_time":1425437860360,
        "Challenge_comment_count":0,
        "Challenge_created_time":1419001312673,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install the R Hash_2.2.6.zip package on Azure ML. The user has followed the steps mentioned in a blog post but is still getting an error stating that the zip file is not found. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1483481029407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/27568624",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1787.9299130556,
        "Challenge_title":"Installing additional R Package on Azure ML",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2765.0,
        "Challenge_word_count":103,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419000630800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":159.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>To install a package this way, you have to create a .zip of a .zip. The outer layer of packaging will get unzipped into the src\/ folder when the dataset is passed in to the module, and you'll be able to install the inner package from there.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1425439161243,
        "Solution_link_count":0.0,
        "Solution_readability":5.9,
        "Solution_reading_time":2.95,
        "Solution_score_count":5.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"zip file not found"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":0.8399936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am attempting to submit an experiment to the Azure Machine Learning Service using a custom docker image.  Everything works ok when I provide the docker image, but fails if I choose to provide a dockerfile.<\/p>\n\n<p>The use of a base_dockerfile in the DockerSection object is documented <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.dockersection?view=azure-ml-py\" rel=\"nofollow noreferrer\">here<\/a> and was added in v1.0.53 of the sdk (as noted <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/service\/azure-machine-learning-release-notes.md\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>\n\n<p>Example code:<\/p>\n\n<pre><code>ds = DockerSection()\nds.enabled = True\nds.base_dockerfile = \"FROM ubuntu:latest RUN echo 'Hello world!'\"\nds.base_image = None\n<\/code><\/pre>\n\n<p>The rest of the code is the same as when running with a predefined image from the registry (e.g. setting base_image in the above code).<\/p>\n\n<p>Example error from ML service is:<\/p>\n\n<blockquote>\n  <p>raise ActivityFailedException(error_details=json.dumps(error,\n  indent=4))\n  azureml.exceptions._azureml_exception.ActivityFailedException:\n  ActivityFailedException:\n          Message: Activity Failed: {\n      \"error\": {\n          \"code\": \"ServiceError\",\n          \"message\": \"InternalServerError\",\n          \"details\": []\n      },\n      \"correlation\": {\n          \"operation\": null,\n          \"request\": \"K\/C4FSnEz74=\"\n      },\n      \"environment\": \"southcentralus\",\n      \"location\": \"southcentralus\",\n      \"time\": \"2019-08-20T16:33:17.130928Z\" }\n          InnerException None\n          ErrorResponse {\"error\": {\"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"ServiceError\\\",\\n<br>\n  \\\"message\\\": \\\"InternalServerError\\\",\\n        \\\"details\\\": []\\n<br>\n  },\\n    \\\"correlation\\\": {\\n        \\\"operation\\\": null,\\n<br>\n  \\\"request\\\": \\\"K\/C4FSnEz74=\\\"\\n  },\\n    \\\"environment\\\":\n  \\\"southcentralus\\\",\\n    \\\"location\\\": \\\"southcentralus\\\",\\n<br>\n  \\\"time\\\": \\\"2019-08-20T16:33:17.130928Z\\\"\\n}\"}}<\/p>\n<\/blockquote>\n\n<p>I've used an example dockerfile in the code above (taken from the SDK documentation) but get the same error if I use the dockerfile that created the base image that works ok from the registry.<\/p>\n\n<p>Any ideas - or pointers to samples where this actually works - appreciated!<\/p>",
        "Challenge_closed_time":1566323058620,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566320034643,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while submitting an experiment to the Azure Machine Learning Service using a custom docker image. The experiment fails when a base_dockerfile is used instead of a base_image. The error message received is an ActivityFailedException with an InternalServerError. The user has tried using an example dockerfile from the SDK documentation but still faces the same error. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57578332",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":29.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.8399936111,
        "Challenge_title":"Failure of experiment when using base_dockerfile instead of base_image",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":307.0,
        "Challenge_word_count":222,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355436865376,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Thanks for reporting this issue! This appears to be a bug that our team is investigating.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.2,
        "Solution_reading_time":1.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"experiment fails with base_dockerfile"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":44.4506683334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to docker and environments. This could be basics but i have been trying to install packages in my pyproject.toml file in Dockerfile without success.   <\/p>\n<p>I have tried using poetry to export requirements.txt file  and using it with the   <br \/>\nEnvironment.from_pip_requirements('requirements.txt') function and a Dockerfile.   <\/p>\n<p>But could there be any elegant solution to use toml file directly for creating a custom environment ?  <\/p>",
        "Challenge_closed_time":1638981076963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638821054557,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having trouble installing packages in their pyproject.toml file in a Dockerfile using poetry. They have tried exporting a requirements.txt file and using it with the Environment.from_pip_requirements function, but are looking for a more elegant solution to use the toml file directly for creating a custom environment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/653688\/custom-dockerfile-on-azure-environment-with-python",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":6.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":44.4506683334,
        "Challenge_title":"Custom Dockerfile on Azure Environment with python poetry",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for the response, <a href=\"\/users\/na\/?userid=1cea772e-bffd-0003-0000-000000000000\">@Ram R  <\/a>     <br \/>\nUsing  the Dockerfile :     <\/p>\n<pre><code>FROM python:3.8-slim-buster  \nENV PYTHONUNBUFFERED=1 \\  \n    PYTHONDONTWRITEBYTECODE=1 \\  \n    PIP_NO_CACHE_DIR=1 \\  \n    PIP_DISABLE_PIP_VERSION_CHECK=1 \\  \n    POETRY_VERSION=1.1.7 \\  \n    PYLINT_VERSION=2.9.4  \n  \nRUN pip install pylint==$PYLINT_VERSION \\  \n    &amp;&amp; pip install &quot;poetry==$POETRY_VERSION&quot;   \n  \nCOPY pyproject.toml .\/  \nRUN poetry config virtualenvs.create false   \n<\/code><\/pre>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":6.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"trouble installing packages"
    },
    {
        "Answerer_created_time":1655773889523,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":390.0,
        "Answerer_view_count":240.0,
        "Challenge_adjusted_solved_time":7.7833411111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am porting custom job training from gcp AI Platform to Vertex AI.\nI am able to start a job, but can't find how to to get the status and how to stream the logs to my local client.<\/p>\n<p>For AI Platform I was using this to get the state:<\/p>\n<pre><code>from google.oauth2 import service_account\nfrom googleapiclient import discovery\nscopes = ['https:\/\/www.googleapis.com\/auth\/cloud-platform']\ncredentials = service_account.Credentials.from_service_account_file(keyFile, scopes=scopes)\nml_apis = discovery.build(&quot;ml&quot;,&quot;v1&quot;, credentials=credentials, cache_discovery=False)\nx = ml_apis.projects().jobs().get(name=&quot;projects\/%myproject%\/jobs\/&quot;+job_id).execute()  # execute http request\nreturn x['state']\n<\/code><\/pre>\n<p>And this to stream the logs:<\/p>\n<pre><code>cmd = 'gcloud ai-platform jobs stream-logs ' + job_id\n<\/code><\/pre>\n<p>This does not work for Vertex AI job. What is the replacement code?<\/p>",
        "Challenge_closed_time":1657854766996,
        "Challenge_comment_count":1,
        "Challenge_created_time":1657835239540,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while porting a custom job from GCP AI Platform to Vertex AI. They are unable to find a way to get the status and stream logs to their local client. The code they were using for AI Platform is not working for Vertex AI, and they are looking for a replacement code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72986981",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.1,
        "Challenge_reading_time":13.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":5.4242933333,
        "Challenge_title":"Porting custom job from GCP AI Platform to Vertex AI - how to get state and logs of job?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":118,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1254829817772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2595.0,
        "Poster_view_count":357.0,
        "Solution_body":"<p>Can you try this command for streaming logs :<\/p>\n<pre><code>gcloud ai custom-jobs stream-logs 123 --region=europe-west4\n<\/code><\/pre>\n<p>123 is the <strong>ID<\/strong> of the custom job for this case, you can add glcoud wide flags such as --format as well.<\/p>\n<p>You can visit this <a href=\"https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/ai\/custom-jobs\/stream-logs\" rel=\"nofollow noreferrer\">link<\/a> for more details about this command and additional flags available.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1657863259568,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":6.18,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":56.0,
        "Tool":"Vertex AI",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to get job status and logs"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4565.0625,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently, the `silnlp.nmt.translate` script always creates a ClearML task. This should be optional. By default, it should just execute locally.",
        "Challenge_closed_time":1657980432000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1641546207000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a \"comet-ml not installed\" error despite having installed comet-ml. The error occurs when trying to instantiate Trainer with `report_to='comet_ml'` in the TrainingArguments. The error message suggests that the CometCallback requires comet-ml to be installed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/120",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.7,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":166.0,
        "Challenge_repo_star_count":23.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4565.0625,
        "Challenge_title":"Execute translate script without creating ClearML task",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":26,
        "Discussion_body":"I think that this error might be preventing me from using the Translate script locally.\r\n\r\nWhen I try I get the following error:\r\nInstalling the current project: silnlp (1.0.0)\r\n(silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate BT-English\/cba-en\/cba-en_cp01 --src-project cba --trg-iso en --books EXO --output-usfm BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT --checkpoint best\r\n2022-06-28 21:02:08,808 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-06-28 21:02:09,149 - silnlp.common.utils - INFO - Git commit: f46a63c3b3\r\nRetrying (Retry(total=239, connect=239, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4556fa0>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\nRetrying (Retry(total=238, connect=238, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569190>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n^CRetrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569340>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n\r\nprint(args):\r\nNamespace(books=['EXO'], checkpoint='best', clearml_queue=None, eager_execution=False, end_seq=None, experiment='BT-English\/cba-en\/cba-en_cp01', memory_growth=False, output_usfm='BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT', src=None, src_prefix=None, src_project='cbaNT', start_seq=None, trg=None, trg_iso='en', trg_prefix=None)\r\n Tested this for translating and it worked fine.   (silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate --checkpoint last --src-project tl-TCB --src \/home\/david\/disk2\/gutenberg\/Paratext\/projects\/TCB\/091SAtlASD15.SFM --trg-iso blx --output-usfm \/home\/david\/disk2\/gutenberg\/BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\/results\/091SAAMIU_last.sfm BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\r\n2022-07-16 15:03:40,452 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-07-16 15:03:40,828 - silnlp.common.utils - INFO - Git commit: 8cd5b9c649\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 231, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 225, in main\r\n    translator.translate_text_file(args.src, args.trg_iso, args.trg)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 151, in translate_text_file\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{os.path.basename(src_file_path)}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 79, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 9, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml_connection.py\", line 24, in __post_init__\r\n    self.name = self.name.replace(\"\\\\\", \"\/\")\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"ClearML",
        "Challenge_type":"anomaly",
        "Challenge_summary":"comet-ml not recognized"
    },
    {
        "Answerer_created_time":1459625723203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Grenoble, France",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":2210.9752852778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My final purpose is to run experiment from  an Api.<\/p>\n\n<p>the experiment come from :\n<a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2<\/a>\nbut export the file in my custom git where I clone it, in the image below -><\/p>\n\n<p>I have 2 images in my docker compose :\ntree project : <\/p>\n\n<pre><code>|_app\/\n| |_Dockerfile\n|\n|_mlflow\/\n| |_Dockerfile\n|\n|_docker-compose.yml\n\n<\/code><\/pre>\n\n<p>app\/Dockerfile<\/p>\n\n<pre><code>FROM continuumio\/anaconda3\n\nENV APP_HOME .\/\nWORKDIR ${APP_HOME}\nRUN conda config --append channels conda-forge\nRUN conda install --quiet --yes \\\n    'mlflow' \\\n    'psycopg2' \\\n    'tensorflow'\nRUN pip install pylint\nRUN pwd;ls \\\n&amp;&amp; git clone https:\/\/github.com\/MChrys\/QuickSign.git \nRUN pwd;ls \\\n    &amp;&amp; cd QuickSign \\\n    &amp;&amp; pwd;ls\n\nCOPY . .\n\n#RUN conda install jupyter \n#CMD jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root --no-browser\nCMD cd QuickSign &amp;&amp; mlflow run .\n<\/code><\/pre>\n\n<p>mlflow\/Dockerfile<\/p>\n\n<pre><code>FROM python:3.7.0\n\nRUN pip install mlflow\n\nRUN mkdir \/mlflow\/\n\nCMD mlflow server \\\n    --backend-store-uri \/mlflow \\\n    --host 0.0.0.0\n<\/code><\/pre>\n\n<p>docker-compose.yml<\/p>\n\n<pre><code>version: '3'\nservices:\n  notebook:\n    build:\n      context: .\/app\n    ports:\n      - \"8888:8888\"\n    depends_on: \n      - mlflow\n    environment: \n      MLFLOW_TRACKING_URI: 'http:\/\/mlflow:5000'\n  mlflow:\n    build:\n      context: .\/mlflow\n    expose: \n      - \"5000\"\n    ports:\n      - \"5000:5000\"\n<\/code><\/pre>\n\n<p>when I <code>docker-compose up<\/code> the image I obtain  :<\/p>\n\n<pre><code>notebook_1_74059cdc20ce |     response = requests.request(**kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/api.py\", line 60, in request\nnotebook_1_74059cdc20ce |     return session.request(method=method, url=url, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\nnotebook_1_74059cdc20ce |     resp = self.send(prep, **send_kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\nnotebook_1_74059cdc20ce |     r = adapter.send(request, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\nnotebook_1_74059cdc20ce |     raise ConnectionError(e, request=request)\nnotebook_1_74059cdc20ce | requests.exceptions.ConnectionError: HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/create (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fd5db4edc50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\n<\/code><\/pre>\n\n<p>The problem look like that I run a project which is not found in the server images, as I run it in the app image, but I don't know how figure it out I have to trigger  the experiment from a futur flask app <\/p>",
        "Challenge_closed_time":1586436837590,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578475698630,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to connect their Mlflow server to their Mlflow project image. They have two images in their Docker compose and are trying to run an experiment from an API. The error message suggests that the project is not found in the server images, and the user is unsure how to trigger the experiment from a future Flask app.",
        "Challenge_last_edit_time":1578477326563,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59642900",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":11.0,
        "Challenge_reading_time":38.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":2211.4274888889,
        "Challenge_title":"Unable to connect Mlflow server to my mlflow project image",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2937.0,
        "Challenge_word_count":289,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459625723203,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Grenoble, France",
        "Poster_reputation_count":56.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>The problem came from  docker for windows, I was unable to make working docker compose on it but there are no problem to build it when I run it on virtual machine with ubuntu.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":2.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":34.0,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"project not found in server"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.3472294445,
        "Challenge_answer_count":1,
        "Challenge_body":"![Studio encountered an error when creating your project](https:\/\/repost.aws\/media\/postImages\/original\/IMWYkHCNADT7ihgQuoRgh7nQ)\nI trying tutorial on \"MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline\". But I am getting  error as shown in image",
        "Challenge_closed_time":1658994240283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658949790257,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to create a project using the MLOps template for model building, training, and deployment with third-party Git repositories using CodePipeline in Sagemaker Studio. The error message is shown in the attached image.",
        "Challenge_last_edit_time":1668585903416,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOCKdskABQumCC7OnzBZR4g\/sagemaker-studio-encountered-an-error-when-creating-your-project-github-and-codepipeline-template",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.9,
        "Challenge_reading_time":5.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":12.3472294445,
        "Challenge_title":"Sagemaker Studio encountered an error when creating your project(github and codepipeline template)",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":169.0,
        "Challenge_word_count":46,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello. It seems like you are having permission problems according to the snapshot you provided. \nIf you head to the Cloudformation service, you will probably get a better understanding of where the tamplate is failing. \nMake sure to have followed the prerequisites and check out this [section](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-projects-templates-sm.html#sagemaker-projects-templates-update).",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1658994240283,
        "Solution_link_count":1.0,
        "Solution_readability":15.3,
        "Solution_reading_time":5.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error creating MLOps project"
    },
    {
        "Answerer_created_time":1441938169003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":8.0,
        "Challenge_adjusted_solved_time":71.1239344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>SageMaker model containers I believe are flask apps that run inside a container. I've deployed a few custom containers, but I'm wondering if I can see the actual code (docker file and all) for their exsisting algorithms? Would be helpful to understand in more detail what's happening inside during training\/inference<\/p>",
        "Challenge_closed_time":1629473611812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629216101217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to access the actual code, including the docker file, for existing algorithms in SageMaker model containers. This would help them better understand the processes happening during training and inference.",
        "Challenge_last_edit_time":1629217565648,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68820617",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":4.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":71.5307208333,
        "Challenge_title":"See the actual code for SageMaker model containers?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":57,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421343783700,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Solution_body":"<p>If you want to have a look at the containers for the different Frameworks as they are supported by Amazon SageMaker, you can check the GitHub:<\/p>\n<ul>\n<li>Deep Learning Containers: <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\" rel=\"nofollow noreferrer\">Github Link<\/a><\/li>\n<li>XGBoost Container: <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">Github Link<\/a><\/li>\n<li>SKLearn Container: <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\" rel=\"nofollow noreferrer\">Github Link<\/a><\/li>\n<li>Spark Container: <a href=\"https:\/\/github.com\/aws\/sagemaker-spark-container\" rel=\"nofollow noreferrer\">Github Link<\/a><\/li>\n<\/ul>\n<p>This should answer your question regarding dockerfiles, training and deployment.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">built-in algorithms<\/a> in Amazon SageMaker (also called 1P algorithms) are part of AWS PI, so they are not publicly available.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":16.6,
        "Solution_reading_time":13.53,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":91.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"access SageMaker algorithm code"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":2.4104166667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was able to run an experiment by allowing mcr.microsoft.com. We wants to use the curated environment AS-IS, no custom packages will be installed.<\/p>\n<p>For the security reason I don't want to whitelist the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-azureml-behind-firewall?tabs=ipaddress%2Cpublic#outbound-configuration\" rel=\"nofollow noreferrer\">application firewall<\/a> rules:<\/p>",
        "Challenge_closed_time":1650856278807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650847601307,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use a curated environment without installing any custom packages and wants to know if an internet outbound connection is required to download the packages. They are concerned about security and do not want to whitelist the application firewall rules.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71993445",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":15.6,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.4104166667,
        "Challenge_title":"Does a curated environment needs an internet outbound connection to download the packages",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":69.0,
        "Challenge_word_count":52,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1632461310820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":107.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>We are in the process of migration from Vienna Global ACR to MCR  you need below to use curated env.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/OdcJU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OdcJU.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.9,
        "Solution_reading_time":3.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"internet required for package download?"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.3284266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to run the notebook in studio, but can I export the studio as notebook and import it in another place?<\/p>",
        "Challenge_closed_time":1667268149163,
        "Challenge_comment_count":1,
        "Challenge_created_time":1667248966827,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if they can build their ML pipeline\/experiment in ML studio designer and export it as a Python and Jupyter notebook to be used in another place. They are able to run the notebook in studio but want to know if it can be exported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1069926\/can-i-build-my-ml-pipeline-experiment-in-ml-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":5.3284266667,
        "Challenge_title":"can I build my ML pipeline\/experiment in ML studio designer, and export it as a python and jupyter notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=a7efa3c2-0ff5-4cc8-8b14-63a031eb0713\">@usui gina  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A. Sorry, this is not support at this moment. But Azure Machine Learning Studio already has Notebook function just for your reference.     <\/p>\n<p>I will forward your feedback to product team and at the same time, I would highly recommend you provide your feedback in Azure Machine Learning portal to raise more visability - top right side as below screenshot    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/255817-image.png?platform=QnA\" alt=\"255817-image.png\" \/>    <\/p>\n<p>I hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":9.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"export ML Studio pipeline"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.83,
        "Challenge_answer_count":0,
        "Challenge_body":"logs: \r\n\r\n```\r\nRun papermill notebooks\/sklearn\/train-diabetes-mlproject.ipynb out.ipynb -k python\r\nInput Notebook:  notebooks\/sklearn\/train-diabetes-mlproject.ipynb\r\nOutput Notebook: out.ipynb\r\n\r\nExecuting:   0%|          | 0\/7 [00:00<?, ?cell\/s]Executing notebook with kernel: python\r\n\r\nExecuting:  14%|\u2588\u258d        | 1\/7 [00:01<00:07,  1.33s\/cell]\r\nExecuting:  29%|\u2588\u2588\u258a       | 2\/7 [00:02<00:07,  1.43s\/cell]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4\/7 [00:05<00:03,  1.32s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:07<00:01,  1.34s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:08<00:01,  1.40s\/cell]\r\nTraceback (most recent call last):\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/bin\/papermill\", line 8, in <module>\r\n    sys.exit(papermill())\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/cli.py\", line 240, in papermill\r\n    execute_notebook(\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 110, in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 222, in raise_for_execution_errors\r\n    raise error\r\npapermill.exceptions.PapermillExecutionError: \r\n---------------------------------------------------------------------------\r\nException encountered at \"In [5]\":\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-5-ef514d3992f5> in <module>\r\n----> 1 run = mlflow.projects.run(\r\n      2     uri=str(project_uri),\r\n      3     parameters=***\"alpha\": 0.3***,\r\n      4     backend=\"azureml\",\r\n      5     backend_config=backend_config,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in run(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\r\n    271     )\r\n    272 \r\n--> 273     submitted_run_obj = _run(\r\n    274         uri=uri,\r\n    275         experiment_id=experiment_id,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in _run(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, use_conda, storage_dir, synchronous)\r\n     98         backend = loader.load_backend(backend_name)\r\n     99         if backend:\r\n--> 100             submitted_run = backend.run(\r\n    101                 uri,\r\n    102                 entry_point,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/mlflow\/_internal\/projects.py in run(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id)\r\n    240         if compute and compute != _LOCAL and compute != _LOCAL.upper():\r\n    241             remote_environment = _load_remote_environment(mlproject)\r\n--> 242             remote_environment.register(workspace=workspace)\r\n    243             cpu_cluster = _load_compute_target(workspace, backend_config)\r\n    244             src.run_config.target = cpu_cluster.name\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/environment.py in register(self, workspace)\r\n    803         environment_client = EnvironmentClient(workspace.service_context)\r\n    804         environment_dict = Environment._serialize_to_dict(self)\r\n--> 805         response = environment_client._register_environment_definition(environment_dict)\r\n    806         env = Environment._deserialize_and_add_to_object(response)\r\n    807 \r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/_restclient\/environment_client.py in _register_environment_definition(self, environment_dict)\r\n     75             message = \"Error registering the environment definition. Code: ***\\n: ***\".format(response.status_code,\r\n     76                                                                                             response.text)\r\n---> 77             raise Exception(message)\r\n     78 \r\n     79     def _get_image_details(self, name, version=None):\r\n\r\nException: Error registering the environment definition. Code: 409\r\n: ***\r\n  \"error\": ***\r\n    \"code\": \"TransientError\",\r\n    \"severity\": null,\r\n    \"message\": \"Etag conflict on 0e149764-3720-4610-b0f3-3e3f974544ac\/8f54aa7d6c05b2722ba149d8ea3185c263ecf5310eb2d7271569d1918c736972 with etag .\",\r\n    \"messageFormat\": null,\r\n    \"messageParameters\": null,\r\n    \"referenceCode\": null,\r\n    \"detailsUri\": null,\r\n    \"target\": null,\r\n    \"details\": [],\r\n    \"innerError\": null,\r\n    \"debugInfo\": null\r\n  ***,\r\n  \"correlation\": ***\r\n    \"operation\": \"db22e6e6bfa07f499f1749f708b798c9\",\r\n    \"request\": \"f470e9430c5ed842\"\r\n  ***,\r\n  \"environment\": \"eastus\",\r\n  \"location\": \"eastus\",\r\n  \"time\": \"2020-10-01T20:17:52.8383774+00:00\",\r\n  \"componentName\": \"environment-management\"\r\n***\r\n\r\nError: Process completed with exit code 1.\r\n```",
        "Challenge_closed_time":1601658581000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601583593000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an MlflowException while running a double ensemble on Google Colab. They were able to solve some issues by cloning the repo and reinstalling numpy, but they are unsure how to solve this particular issue. The error message indicates an invalid value for metric 'IC' and a value error related to duplicate bin edges. The user followed instructions to download cn data and provided a yaml file with configuration details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1170",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":20.3,
        "Challenge_reading_time":69.38,
        "Challenge_repo_contributor_count":58.0,
        "Challenge_repo_fork_count":2387.0,
        "Challenge_repo_issue_count":1906.0,
        "Challenge_repo_star_count":3704.0,
        "Challenge_repo_watch_count":2001.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":20.83,
        "Challenge_title":"mlflow.projects.run failing consistently with etag error ",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":338,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_type":"anomaly",
        "Challenge_summary":"invalid metric value"
    },
    {
        "Answerer_created_time":1594906911350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41528.0,
        "Answerer_view_count":6164.0,
        "Challenge_adjusted_solved_time":138.9138230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are using AWS Sagemaker feature, bring your own docker, where we have inference model written in R. As I understood, batch transform job runs container in a following way:<\/p>\n<pre><code>docker run image serve\n<\/code><\/pre>\n<p>Also, on docker we have a logic to determine which function to invoke:<\/p>\n<pre><code>args &lt;- commandArgs()\nif (any(grepl('train', args))) {\n    train()}\nif (any(grepl('serve', args))) {\n    serve()}\n<\/code><\/pre>\n<p>Is there a way, to override default container invocation so we can pass some additional parameters?<\/p>",
        "Challenge_closed_time":1599691026140,
        "Challenge_comment_count":6,
        "Challenge_created_time":1599220820873,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is using AWS Sagemaker with a docker container running an inference model in R. They want to know if there is a way to override the default container invocation to pass additional parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63740792",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":16.6,
        "Challenge_reading_time":7.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":130.6125741667,
        "Challenge_title":"Provide additional input to docker container running inference model",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":702.0,
        "Challenge_word_count":84,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473837223712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgrade",
        "Poster_reputation_count":353.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>As you said, and is indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, Sagemaker will run your container with the following command:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>docker run image serve\n<\/code><\/pre>\n<p>By issuing this command Sagemaker will overwrite any <code>CMD<\/code> that you provide in your container Dockerfile, so you cannot use <code>CMD<\/code> to provide dynamic arguments to your program.<\/p>\n<p>We can think in use the Dockerfile <code>ENTRYPOINT<\/code> to consume some environment variables, but the documentation of AWS dictates that it is preferable use the <code>exec<\/code> form of the <code>ENTRYPOINT<\/code>. Somethink like:<\/p>\n<pre><code>ENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;]\n<\/code><\/pre>\n<p>I think that, for analogy with <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">model training<\/a>, they need this kind of container execution to enable the container to receive termination signals:<\/p>\n<blockquote>\n<p>The exec form of the <code>ENTRYPOINT<\/code> instruction starts the executable directly, not as a child of <code>\/bin\/sh<\/code>. This enables it to receive signals like <code>SIGTERM<\/code> and <code>SIGKILL<\/code> from SageMaker APIs.<\/p>\n<\/blockquote>\n<p>To allow variable expansion, we need to use the <code>ENTRYPOINT<\/code> <code>shell<\/code> form. Imagine:<\/p>\n<pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;, &quot;$ENV_VAR1&quot;]\n<\/code><\/pre>\n<p>If you try to do the same with the <code>exec<\/code> form the variables provided will be treated as a literal and will not be sustituited for their actual values.<\/p>\n<p>Please, see the approved answer of <a href=\"https:\/\/stackoverflow.com\/questions\/37904682\/how-do-i-use-docker-environment-variable-in-entrypoint-array\">this<\/a> stackoverflow question for a great explanation of this subject.<\/p>\n<p>But, one thing you can do is obtain the value of these variables in your R code, similar as when you process <code>commandArgs<\/code>:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>ENV_VAR1 &lt;- Sys.getenv(&quot;ENV_VAR1&quot;)\n<\/code><\/pre>\n<p>To pass environment variables to the container, as indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\"><code>CreateModel<\/code><\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\"><code>CreateTransformJob<\/code><\/a> requests on your container.<\/p>\n<p>You probably will need to include in your Dockerfile <code>ENV<\/code> definitions for every required environment variable on your container, and provide for these definitions default values with <code>ARG<\/code>:<\/p>\n<pre><code>ARG ENV_VAR1_DEFAULT_VALUE=VAL1\nENV_VAR1=$ENV_VAR1_DEFAULT_VALUE\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1599720910636,
        "Solution_link_count":6.0,
        "Solution_readability":17.2,
        "Solution_reading_time":43.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":313.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"override container invocation parameters"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":145.9862022222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am quite new to docker. My problem in short is to create a docker file that contains python with sklearn and pandas which can be used on aws sagemaker.<\/p>\n<p>My current docker file looks like the following:<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>However when i try to create this image I get an error at line <code>pip3 install sagemaker-training<\/code>. The error is the following:<\/p>\n<pre><code>error: command 'gcc' failed: No such file or directory\n\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-o5rzjscd\/install-record.txt --single-version-externally-managed --compile --install-headers \/opt\/conda\/include\/python3.9\/sagemaker-training Check the logs for full command output.\n\nThe command '\/bin\/bash -o pipefail -c pip3 install sagemaker-training' returned a non-zero code: 1\n<\/code><\/pre>\n<p>If there is a more suitable base image can someone point that out to me? I am generally trying to follow this page <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit<\/a>.<\/p>\n<p>Note: I realise I can use some sagemaker pre-built containers without using my own docker file. However I am trying to do this for my own learning so I know what to do for projects that can't utilise them.<\/p>",
        "Challenge_closed_time":1635439092896,
        "Challenge_comment_count":5,
        "Challenge_created_time":1634912008467,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a docker file that contains Python with sklearn and pandas which can be used on AWS Sagemaker. However, they are encountering an error at line \"pip3 install sagemaker-training\" which says \"error: command 'gcc' failed: No such file or directory\". The user is seeking advice on a suitable base image and is trying to learn how to create their own docker file for future projects.",
        "Challenge_last_edit_time":1634913542568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69678393",
        "Challenge_link_count":2,
        "Challenge_participation_count":6,
        "Challenge_readability":11.5,
        "Challenge_reading_time":29.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":146.4123413889,
        "Challenge_title":"Creating docker file which installs python with sklearn and pandas that can be used on sagemaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":161.0,
        "Challenge_word_count":227,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1521737124656,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Milton Keynes",
        "Poster_reputation_count":738.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>I adjusted your Dockerfile and it builds successfully for me.<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\nARG defaultuser=jovyan\nUSER root\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update &amp;&amp; \\\n    apt-get -y install gcc mono-mcs &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\nUSER $defaultuser\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>(I had to adjust for the fact that the default user from the base container isn't root, when installing GCC)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":6.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"gcc command failed"
    },
    {
        "Answerer_created_time":1515548304623,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vietnam",
        "Answerer_reputation_count":549.0,
        "Answerer_view_count":68.0,
        "Challenge_adjusted_solved_time":2.1902408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to do a neural network for regression analysis using optuna based on <a href=\"https:\/\/dreamer-uma.com\/pytorch-optuna-hyperparameter-tuning\/\" rel=\"nofollow noreferrer\">this site<\/a>.\nI would like to create a model with two 1D data as input and one 1D data as output in batch learning.<\/p>\n<p><code>x<\/code> is the training data and <code>y<\/code> is the teacher data.<\/p>\n<pre><code>class Model(nn.Module):\n    # \u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf(\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210\u6642\u306e\u521d\u671f\u5316)\n    def __init__(self,trial, mid_units1, mid_units2):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(2, mid_units1)\n        self.bn1 = nn.BatchNorm1d(mid_units1)\n        self.linear2 = nn.Linear(mid_units1, mid_units2)\n        self.bn2 = nn.BatchNorm1d(mid_units2)\n        self.linear3 = nn.Linear(mid_units2, 1)\n        self.activation = trial_activation(trial)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\nEPOCH = 100\nx = torch.from_numpy(a[0].astype(np.float32)).to(device)\ny = torch.from_numpy(a[1].astype(np.float32)).to(device)\n\ndef train_epoch(model, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()    # \u52fe\u914d\u60c5\u5831\u30920\u306b\u521d\u671f\u5316\n    y_pred = model(x)                                               # \u4e88\u6e2c\n    loss = criterion(y_pred.reshape(y.shape), y)          # \u640d\u5931\u3092\u8a08\u7b97(shape\u3092\u63c3\u3048\u308b)\n    loss.backward()                                                       # \u52fe\u914d\u306e\u8a08\u7b97\n    optimizer.step()                                                      # \u52fe\u914d\u306e\u66f4\u65b0\n    return loss.item()\n\ndef trial_activation(trial):\n    activation_names = ['ReLU','logsigmoid']\n    activation_name = trial.suggest_categorical('activation', activation_names)\n    if activation_name == activation_names[0]:\n        activation = F.relu\n    else:\n        activation = F.logsigmoid\n    return activation\n\ndef objective(trial):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # \u4e2d\u9593\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u306e\u8a66\u884c\n    mid_units1 = int(trial.suggest_discrete_uniform(&quot;mid_units1&quot;, 1024*2,1024*4, 64*2))\n    mid_units2 = int(trial.suggest_discrete_uniform(&quot;mid_units2&quot;, 1024, 1024*2, 64*2))\n\n    net = Model(trial, mid_units1, mid_units2).to(device)\n\n    criterion = nn.MSELoss() \n    # \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a66\u884c\n    optimizer = trial_optimizer(trial, net)\n    train_loss = 0\n    for epoch in range(EPOCH):\n        train_loss = train_epoch(net, optimizer, criterion, device)\n    torch.save(net.state_dict(), str(trial.number) + &quot;new1.pth&quot;)\n    return train_loss\n\nstrage_name = &quot;a.sql&quot;\nstudy_name = 'a'\n\nstudy = optuna.create_study(\n    study_name = study_name,\n    storage='sqlite:\/\/\/'  + strage_name, \n    load_if_exists=True,\n    direction='minimize')\nTRIAL_SIZE = 100\n\nstudy.optimize(objective, n_trials=TRIAL_SIZE)\n<\/code><\/pre>\n<p>error message<\/p>\n<pre><code>---&gt; 28     loss = criterion(y_pred.reshape(y.shape), y)          # \u640d\u5931\u3092\u8a08\u7b97(shape\u3092\u63c3\u3048\u308b)\n     29     loss.backward()                                                       # \u52fe\u914d\u306e\u8a08\u7b97\n     30     optimizer.step()                                                      # \u52fe\u914d\u306e\u66f4\u65b0\n\nAttributeError: 'NoneType' object has no attribute 'reshape'\n<\/code><\/pre>\n<p>Because of the above error, I checked the value of <code>y_pred<\/code> and found it to be <code>None<\/code>.<\/p>\n<pre><code>    model.train()\n    optimizer.zero_grad()\n<\/code><\/pre>\n<p>I am thinking that these two lines may be wrong, but I don't know how to solve this problem.<\/p>",
        "Challenge_closed_time":1646568057230,
        "Challenge_comment_count":1,
        "Challenge_created_time":1646559560287,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a PyTorch training model for regression analysis using Optuna, but encounters an error message stating that 'NoneType' object has no attribute 'reshape'. The user suspects that the issue may be with the lines of code related to model training and optimizer initialization, but is unsure how to resolve the problem.",
        "Challenge_last_edit_time":1646560172363,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71369132",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":39.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":2.3602619445,
        "Challenge_title":"The pytorch training model cannot be created successfully",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":267,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643702319420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>With PyTorch, when you call <code>y_pred = model(x)<\/code> that will call the <code>forward<\/code> function which is defined in the <code>Model<\/code> class.<\/p>\n<p>So, <code>y_pred <\/code> will get the result of the <code>forward<\/code> function, in your case, it returns nothing, that's why you get a <code>None<\/code> value. You can change the <code>forward<\/code> function as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(self, x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        return x\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":7.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":69.0,
        "Tool":"Optuna",
        "Challenge_type":"anomaly",
        "Challenge_summary":" 'NoneType' attribute error"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":320.9530555556,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi, I am trying to install some libraries in Studio Lab which requires root privileges. \r\n\r\nBelow I have run `whoami` to check if I am root user. (I am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nBelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\nI followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\nOn running `su -` , It asks for the password, but we don't have any password for Studio Lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\nCan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nPlease let me know if my query is not clear. ",
        "Challenge_closed_time":1655933766000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1654778335000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to open a database file and is encountering an unexpected error while saving a file. They have deleted some unwanted notebooks from the studio lab's files and are now unable to install libraries with pip, create new files, or start the kernel.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/118",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.06,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":113.0,
        "Challenge_repo_issue_count":218.0,
        "Challenge_repo_star_count":408.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":320.9530555556,
        "Challenge_title":"How to get root access in SageMaker Studio Lab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":122,
        "Discussion_body":"Thank you for trying Studio Lab. Now Studio Lab does not allow `sudo` (similar issue: https:\/\/github.com\/aws\/studio-lab-examples\/issues\/40#issuecomment-1005305538). We can use `pip` and `conda` instead. Please refer the following issue.\r\n\r\nWhat software do you try to install? Some libraries will be available in `conda-forge` . Here is the sample of search.\r\n\r\nhttps:\/\/anaconda.org\/search?q=gym Thanks for the reply, I will try to explain the issue.\r\n I am trying to run bipedal robot from Open-ai gym. \r\n```\r\n!pip install gym\r\n!apt-get update\r\n!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> \/dev\/null\r\n!apt-get install xvfb\r\n!pip install pyvirtualdisplay \r\n!pip -q install pyglet\r\n!pip -q install pyopengl\r\n!apt-get install swig\r\n!pip install box2d box2d-kengz\r\n!pip install pybullet\r\n```\r\nThese are the libraries which I need to install for the code to work. \r\nIt works fine on google-colab: (screenshot below)\r\n![colab_gym_ss](https:\/\/user-images.githubusercontent.com\/91401599\/173034039-1973ee00-6c0b-4f49-adad-9bb324c59b8c.png)\r\n\r\nBut it throws error when I run it on SageMaker Studio Lab: (screenshot below)\r\n![sagemaker1](https:\/\/user-images.githubusercontent.com\/91401599\/173034679-f49340dc-de46-42bb-be1b-7c7e3616dac4.png)\r\n\r\nError Log (I have made gym_install.sh file which installs everything described above, I am running it below.): \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ sh gym_install.sh\r\nRequirement already satisfied: gym in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (0.24.1)\r\nRequirement already satisfied: gym-notices>=0.0.4 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (0.0.7)\r\nRequirement already satisfied: numpy>=1.18.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (1.22.4)\r\nRequirement already satisfied: cloudpickle>=1.2.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (2.1.0)\r\nReading package lists... Done\r\nE: List directory \/var\/lib\/apt\/lists\/partial is missing. - Acquire (13: Permission denied)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nRequirement already satisfied: pyvirtualdisplay in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.0)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nCollecting box2d\r\n  Using cached Box2D-2.3.2.tar.gz (427 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting box2d-kengz\r\n  Using cached Box2D-kengz-2.3.3.tar.gz (425 kB)\r\n  Preparing metadata (setup.py) ... done\r\nBuilding wheels for collected packages: box2d, box2d-kengz\r\n  Building wheel for box2d (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d\r\n  Running setup.py clean for box2d\r\n  Building wheel for box2d-kengz (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d-kengz\r\n  Running setup.py clean for box2d-kengz\r\nFailed to build box2d box2d-kengz\r\nInstalling collected packages: box2d-kengz, box2d\r\n  Running setup.py install for box2d-kengz ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Running setup.py install for box2d-kengz did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [18 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running install\r\n      \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n        warnings.warn(\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: legacy-install-failure\r\n\r\n\u00d7 Encountered error while trying to install package.\r\n\u2570\u2500> box2d-kengz\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for output from the failure.\r\nRequirement already satisfied: pybullet in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.2.5)\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\n### To be specific I am getting error in this line:\r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ apt-get install xvfb\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\n```\r\nSo as mentioned by you I tried to find xvfb in conda-forge, but couldn't find it. \r\nThere are some wrapper xvfb on conda-forge but installing them didn't help with the error. \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ python check.py\r\nTraceback (most recent call last):\r\n  File \"\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/GM_\/ARS_src\/check.py\", line 9, in <module>\r\n    display = Display(visible=0, size=(1024, 768))\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/display.py\", line 54, in __init__\r\n    self._obj = cls(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/xvfb.py\", line 44, in __init__\r\n    AbstractDisplay.__init__(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/abstractdisplay.py\", line 85, in __init__\r\n    helptext = get_helptext(program)\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/util.py\", line 13, in get_helptext\r\n    p = subprocess.Popen(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 966, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 1842, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\nTo reproduce above error run below code (check.py) :->\r\n```\r\nimport os\r\nimport numpy as np\r\nimport gym\r\nfrom gym import wrappers\r\nimport pyvirtualdisplay\r\nfrom pyvirtualdisplay import Display\r\n\r\nif __name__ == \"__main__\":\r\n    display = Display(visible=0, size=(1024, 768))\r\n```\r\n Thank you for sharing the error message. Studio Lab does not allow `apt install` so that the `gym_install.sh` does not work straightly. We have to prepare the environment by `conda` and `pip`.\r\n\r\nAt first we can install `swig` from `conda`. We can not install `xvfb` from `conda`, is this necessary to run the code?  Basically I have to capture the video output using `Display` of `pyvirtualdisplay` library. Everything else works. \r\nAs you can see `display = Display(visible=0, size=(1024, 768))` this line throws error  `FileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'` , so I am trying to install `Xvfb`.\r\n\r\nThanks, @ar8372 Sorry for the late reply. I raised the #124 to work OpenAI Gym in Studio Lab. Please comment to #124 if you have additional information. We need your insight to solve the issue. If you do not mind, please close this issue to suppress the duplication of issues.",
        "Discussion_score_count":2.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to save file"
    },
    {
        "Answerer_created_time":1317676233236,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":2348.0,
        "Answerer_view_count":206.0,
        "Challenge_adjusted_solved_time":71.3002738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?<\/p>",
        "Challenge_closed_time":1555007316016,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554750635030,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know how to update their SageMaker notebook's Jupyter environment to the latest alpha release and restart the process.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55580232",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":2.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":71.3002738889,
        "Challenge_title":"Update SageMaker Jupyterlab environment",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1720.0,
        "Challenge_word_count":22,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>Hi and thank you for using SageMaker!<\/p>\n\n<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait<\/code>.<\/p>\n\n<p>Best,\nKevin<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":2.95,
        "Solution_score_count":8.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"update SageMaker Jupyter environment"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.843735,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I am trying to create an Azure ML Environment using a Dockerfile but it contains the 'COPY' instruction.    <\/p>\n<p>From the documentation of Environment.from_dockerfile ( <a href=\"https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-\">https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-<\/a> ), I can not find a way to give it some files along with the Dockerfile itself.    <\/p>\n<p>So, how to pass context to enable using COPY in the Dockerfile ?    <\/p>\n<p>Thank you for your time !<\/p>",
        "Challenge_closed_time":1634774742763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634739305317,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges while creating an Azure ML Environment using a Dockerfile that contains the 'COPY' instruction. The user is unable to find a way to give files along with the Dockerfile to enable using COPY in the Dockerfile.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/597612\/(azure)(ml)(python-sdk)(environment)(docker)-docke",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.843735,
        "Challenge_title":"[Azure][ML][Python SDK][Environment][Docker] Docker copy missing context",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Docker context is not supported with AzureML Python SDK at the moment. Context support will added later this year<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"unable to use COPY instruction"
    },
    {
        "Answerer_created_time":1587069846163,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"France",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.0910288889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have to build yolact++ in docker enviromment (i'm using sagemaker notebook). Like this<\/p>\n<pre><code>ARG PYTORCH=&quot;1.3&quot;\nARG CUDA=&quot;10.1&quot;\nARG CUDNN=&quot;7&quot;\n \nFROM pytorch\/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel\n<\/code><\/pre>\n<p>And i want to run this<\/p>\n<pre><code>COPY yolact\/external\/DCNv2\/setup.py \/opt\/ml\/code\/external\/DCNv2\/setup.py\nRUN cd \/opt\/ml\/code\/external\/DCNv2 &amp;&amp; \\\npython setup.py build develop\n<\/code><\/pre>\n<p>But i got this error :<\/p>\n<pre><code>No CUDA runtime is found, using CUDA_HOME='\/usr\/local\/cuda'\nTraceback (most recent call last):\nFile &quot;setup.py&quot;, line 64, in &lt;module&gt;\next_modules=get_extensions(),\nFile &quot;setup.py&quot;, line 41, in get_extensions\nraise NotImplementedError('Cuda is not available')\nNotImplementedError: Cuda is not available\n<\/code><\/pre>\n<p>But the enviromment supports CUDA. Anyone have an idea where is the problem ?<\/p>\n<p>Thank you.<\/p>",
        "Challenge_closed_time":1621519129332,
        "Challenge_comment_count":4,
        "Challenge_created_time":1621258222833,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to build YOLACT++ in a Docker environment using Sagemaker notebook. They are encountering an error that says \"CUDA is not available\" even though the environment supports CUDA. The user is seeking help to identify the problem.",
        "Challenge_last_edit_time":1621519151352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67570694",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":12.9,
        "Challenge_reading_time":13.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":72.4740275,
        "Challenge_title":"How to build YOLACT++ using Docker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":486.0,
        "Challenge_word_count":104,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587069846163,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"France",
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>SOLUTION :<\/p>\n<p>i edit the \/etc\/docker\/daemon.json with content:<\/p>\n<pre><code>{\n&quot;runtimes&quot;: {\n    &quot;nvidia&quot;: {\n        &quot;path&quot;: &quot;\/usr\/bin\/nvidia-container-runtime&quot;,\n        &quot;runtimeArgs&quot;: []\n     } \n},\n&quot;default-runtime&quot;: &quot;nvidia&quot; \n}\n<\/code><\/pre>\n<p>Then i Restart docker daemon:<\/p>\n<pre><code>sudo system restart docker\n<\/code><\/pre>\n<p>it solved my problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1621519479056,
        "Solution_link_count":0.0,
        "Solution_readability":18.8,
        "Solution_reading_time":5.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"CUDA not available"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.9523508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am learning about Kubernetes online endpoints and my objective is to monitor my resources which are deployed using Kubernetes endpoints. Is there any provision to get out-of-the-box monitoring to Kubernetes online endpoints to check the performance. I am new to this domain. Any help is appreciated.<\/p>",
        "Challenge_closed_time":1652337837120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652334408657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to monitor resources deployed using Kubernetes online endpoints and is looking for out-of-the-box monitoring options. They are new to this domain and are seeking assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72210628",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.9523508333,
        "Challenge_title":"How to manage out of the box monitoring using Kubernetes online endpoints",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":58,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652333709927,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>The general monitoring is available and supportive in AKS, but the out of the box implementation was not supportive unfortunately. Check the below documentation and screenshot to refer supportive formats of monitoring in AKS<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0uxW5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0uxW5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.8,
        "Solution_reading_time":8.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"monitor Kubernetes endpoints"
    },
    {
        "Answerer_created_time":1526889513900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":24.1450022222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to write a small program using the AzureML Python SDK (v1.0.85) to register an Environment in AMLS and use that definition to construct a local Conda environment when experiments are being run (for a pre-trained model). The code works fine for simple scenarios where all dependencies are loaded from Conda\/ public PyPI, but when I introduce a private dependency (e.g. a utils library) I am getting a InternalServerError with the message \"Error getting recipe specifications\".<\/p>\n\n<p>The code I am using to register the environment is (after having authenticated to Azure and connected to our workspace):<\/p>\n\n<pre><code>environment_name = config['environment']['name']\npy_version = \"3.7\"\nconda_packages = [\"pip\"]\npip_packages = [\"azureml-defaults\"]\nprivate_packages = [\".\/env-wheels\/utils-0.0.3-py3-none-any.whl\"]\n\nprint(f\"Creating environment with name {environment_name}\")\nenvironment = Environment(name=environment_name)\nconda_deps = CondaDependencies()\n\nprint(f\"Adding Python version: {py_version}\")\nconda_deps.set_python_version(py_version)\n\nfor conda_pkg in conda_packages:\n    print(f\"Adding Conda denpendency: {conda_pkg}\")\n    conda_deps.add_conda_package(conda_pkg)\n\nfor pip_pkg in pip_packages:\n    print(f\"Adding Pip dependency: {pip_pkg}\")\n    conda_deps.add_pip_package(pip_pkg)\n\nfor private_pkg in private_packages:\n    print(f\"Uploading private wheel from {private_pkg}\")\n    private_pkg_url = Environment.add_private_pip_wheel(workspace=ws, file_path=Path(private_pkg).absolute(), exist_ok=True)\n    print(f\"Adding private Pip dependency: {private_pkg_url}\")\n    conda_deps.add_pip_package(private_pkg_url)\n\nenvironment.python.conda_dependencies = conda_deps\nenvironment.register(workspace=ws)\n<\/code><\/pre>\n\n<p>And the code I am using to create the local Conda environment is:<\/p>\n\n<pre><code>amls_environment = Environment.get(ws, name=environment_name, version=environment_version)\n\nprint(f\"Building environment...\")\namls_environment.build_local(workspace=ws)\n<\/code><\/pre>\n\n<p>The exact error message being returned when <code>build_local(...)<\/code> is called is:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 814, in build_local\n    raise error\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 807, in build_local\n    recipe = environment_client._get_recipe_for_build(name=self.name, version=self.version, **payload)\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\_restclient\\environment_client.py\", line 171, in _get_recipe_for_build\n    raise Exception(message)\nException: Error getting recipe specifications. Code: 500\n: {\n  \"error\": {\n    \"code\": \"ServiceError\",\n    \"message\": \"InternalServerError\",\n    \"detailsUri\": null,\n    \"target\": null,\n    \"details\": [],\n    \"innerError\": null,\n    \"debugInfo\": null\n  },\n  \"correlation\": {\n    \"operation\": \"15043e1469e85a4c96a3c18c45a2af67\",\n    \"request\": \"19231be75a2b8192\"\n  },\n  \"environment\": \"westeurope\",\n  \"location\": \"westeurope\",\n  \"time\": \"2020-02-28T09:38:47.8900715+00:00\"\n}\n\nProcess finished with exit code 1\n<\/code><\/pre>\n\n<p>Has anyone seen this error before or able to provide some guidance around what the issue may be?<\/p>",
        "Challenge_closed_time":1583243758048,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583156836040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while building a local AMLS environment with a private wheel. The user is trying to register an environment in AMLS and use that definition to construct a local Conda environment when experiments are being run. The code works fine for simple scenarios where all dependencies are loaded from Conda\/public PyPI, but when the user introduces a private dependency, they are getting an InternalServerError with the message \"Error getting recipe specifications\". The user is seeking guidance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60490195",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":42.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":24.1450022222,
        "Challenge_title":"Unable to build local AMLS environment with private wheel",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":239.0,
        "Challenge_word_count":291,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526889513900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The issue was with out firewall blocking the required requests between AMLS and the storage container (I presume to get the environment definitions\/ private wheels).<\/p>\n\n<p>We resolved this by updating the firewall with appropriate ALLOW rules for the AMLS service to contact and read from the attached storage container.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":4.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"InternalServerError with private dependency"
    },
    {
        "Answerer_created_time":1542217357387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Raleigh, NC, USA",
        "Answerer_reputation_count":126.0,
        "Answerer_view_count":12.0,
        "Challenge_adjusted_solved_time":255.9776175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am deploying a custom pytorch model on AWS sagemaker, Following <a href=\"https:\/\/github.com\/abiodunjames\/MachineLearning\/blob\/master\/DeployYourModelToSageMaker\/inference.py\" rel=\"nofollow noreferrer\">this<\/a> tutorial.\nIn my case I have few dependencies to install some modules.<\/p>\n<p>I need pycocotools in my inference.py script. I can easily install pycocotool inside a separate notebook using this bash command,<\/p>\n<p><code>%%bash<\/code><\/p>\n<p><code>pip -g install pycocotools<\/code><\/p>\n<p>But when I create my endpoint for deployment, I get this error that pycocotools in not defined.\nI need pycocotools inside my inference.py script. How I can install this inside a .py file<\/p>",
        "Challenge_closed_time":1618965088403,
        "Challenge_comment_count":3,
        "Challenge_created_time":1618953881933,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a custom PyTorch model on AWS Sagemaker and needs to install pycocotools in their inference.py script. They have tried installing it in a separate notebook but are encountering an error when creating an endpoint for deployment. They are seeking a solution to install pycocotools inside a .py file.",
        "Challenge_last_edit_time":1618961452447,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67186515",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":12.4,
        "Challenge_reading_time":9.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":3.1129083334,
        "Challenge_title":"Installing modules inside python .py file",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":167.0,
        "Challenge_word_count":90,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567880532003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Lahore, Pakistan",
        "Poster_reputation_count":137.0,
        "Poster_view_count":100.0,
        "Solution_body":"<p>At the beginning of inference.py add these lines:<\/p>\n<pre><code>from subprocess import check_call, run, CalledProcessError\nimport sys\nimport os\n\n# Since it is likely that you're going to run inference.py multiple times, this avoids reinstalling the same package:\nif not os.environ.get(&quot;INSTALL_SUCCESS&quot;):\n    \n    try:\n        check_call(\n        [ sys.executable, &quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    except CalledProcessError:\n        run(\n        [&quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    os.environ[&quot;INSTALL_SUCCESS&quot;] = &quot;True&quot;\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1619882971870,
        "Solution_link_count":0.0,
        "Solution_readability":15.1,
        "Solution_reading_time":7.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"error in endpoint creation"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4348.4297222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Challenge_closed_time":1586730089000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1571075742000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while using PyTorch MNIST Notebook with SageMaker Studio. They had to add two commands to the notebook, but even after that, SageMaker local mode still showed some errors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":16.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":91.0,
        "Challenge_repo_star_count":96.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4348.4297222222,
        "Challenge_title":"Can not compile SageMaker examples",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":19,
        "Discussion_body":"This is reported by user and the problem is kubeflow pipeline has some breaking changes on parameters but we always install latest KFP pipeline which is not compatible. \r\n\r\nShort term. use lower kfp version\r\n```\r\n!pip install https:\/\/storage.googleapis.com\/ml-pipeline\/release\/0.1.29\/kfp.tar.gz --upgrade\r\n```\r\n\r\nLong term, update examples and make sure it leverages latest features of KFP.  Will check on the [SageMaker example](https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/01438d181f502504056eac89bfc0eb091733e9a8\/notebooks\/05_Kubeflow_Pipeline\/05_04_Pipeline_SageMaker.ipynb) and file a PR to make it leverage the latest features of KFP. And the master example of [SageMaker Kubeflow Pipeline](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/samples\/contrib\/aws-samples\/mnist-kmeans-sagemaker), will try to use [master yaml file](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/aws\/sagemaker). After so, will try to use latest version 2.05 of kfp to make it compatible. Potential SageMaker example issues with users: [1st](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1401) and [2nd](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1642). But the issue description is not that informative. Will talk with users if necessary. Let's not put time on this one. I will ask SM team to fix Op issue and we can concentrate on others. Since the updated SageMaker example has been merged, let's close this issue.",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"errors in local mode"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2302777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Does SageMaker pipe mode serve as a cost saving measure? Or is is just faster than file mode but generally not much cheaper? The cost savings of it might be 1. no need to copy data to training instances and 2. training instances need less space. Are these savings generally significant for customers?",
        "Challenge_closed_time":1590165887000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590161458000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether SageMaker pipe mode is a cost-saving measure or just faster than file mode. They are considering the potential cost savings of not needing to copy data to training instances and requiring less space on training instances, but are unsure if these savings are significant for customers.",
        "Challenge_last_edit_time":1668521878124,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QURbsBp9m5TsqKWWDdP8VJyw\/sagemaker-pipe-mode",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.2302777778,
        "Challenge_title":"SageMaker Pipe Mode",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":55,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To the best of my understanding, pipe mode decreases startup times, but frequently increases the bill.\n\nThe SageMaker billing starts after the data has been copied onto the container in File mode and control is transferred to the user script. \n\nReading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time. \n\nFurther the data is, to the best of my knowledge, not hitting the disk (EBS). This is fast, but also means that if you pass over your data multiple times, you have to re-read it again, on your dime (S3 requests and container wait times).\n\nPipe mode is still a good idea. For example if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n\nAlso, in PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925574687,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":11.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"SageMaker pipe mode cost-saving?"
    },
    {
        "Answerer_created_time":1376606307612,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, BC, Canada",
        "Answerer_reputation_count":1567.0,
        "Answerer_view_count":159.0,
        "Challenge_adjusted_solved_time":8936.4835138889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I try to train a pytorch model on amazon sagemaker studio.<\/p>\n\n<p>It's working when I use an EC2 for training with:<\/p>\n\n<pre><code>estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                sagemaker_session = sess,\n                train_instance_count=1,\n                train_instance_type='ml.c5.xlarge',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>and it's work on local mode in classic sagemaker notebook (non studio) with:<\/p>\n\n<pre><code> estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                train_instance_count=1,\n                train_instance_type='local',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>But when I use it the same code (with train_instance_type='local') on sagemaker studio it doesn't work and I have the following error: No such file or directory: 'docker': 'docker'<\/p>\n\n<p>I tried to install docker with pip install but the docker command is not found if use it in terminal<\/p>",
        "Challenge_closed_time":1596561017227,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588239469173,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error \"No such file or directory: 'docker': 'docker'\" while trying to train a PyTorch model on Amazon SageMaker Studio in local mode. The same code works on EC2 and classic SageMaker notebook in local mode. The user tried to install docker with pip install, but the docker command is not found in the terminal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61520346",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.8,
        "Challenge_reading_time":14.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2311.5411261111,
        "Challenge_title":"No such file or directory: 'docker': 'docker' when running sagemaker studio in local mode",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1884.0,
        "Challenge_word_count":121,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576136255052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>This indicates that there is a problem finding the Docker service.<\/p>\n<p>By default, the Docker is not installed in the SageMaker Studio  (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/656#issuecomment-632170943\" rel=\"nofollow noreferrer\">confirming github ticket response<\/a>).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1620410809823,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":3.98,
        "Solution_score_count":7.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"docker command not found"
    },
    {
        "Answerer_created_time":1445975382243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":6831.0,
        "Answerer_view_count":653.0,
        "Challenge_adjusted_solved_time":3.9043655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>SageMaker seems to give examples of using two different serving stacks for serving custom docker images:<\/p>\n\n<ol>\n<li>NGINX + Gunicorn + Flask<\/li>\n<li>NGINX + TensorFlow Serving<\/li>\n<\/ol>\n\n<p>Could someone explain to me at a very high level (I have very little knowledge of network engineering) what responsibilities these different components have? And since the second stack has only two components instead of one, can I rightly assume that TensorFlow Serving does the job (whatever that may be) of both Gunicorn and Flask? <\/p>\n\n<p>Lastly, I've read that it's possible to use Flask and TensorFlow serving at the same time. Would this then be NGINX -> Gunicorn -> Flask -> TensorFlow Serving? And what are there advantages of this?<\/p>",
        "Challenge_closed_time":1547801639500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547670814743,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is seeking an explanation of the two different serving stacks for serving custom docker images in SageMaker, namely NGINX + Gunicorn + Flask and NGINX + TensorFlow Serving. They are also curious about the responsibilities of these different components and whether TensorFlow Serving does the job of both Gunicorn and Flask. Additionally, the user is interested in knowing the advantages of using Flask and TensorFlow serving at the same time and whether it would be NGINX -> Gunicorn -> Flask -> TensorFlow Serving.",
        "Challenge_last_edit_time":1547802223467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54224934",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":9.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":36.3402102778,
        "Challenge_title":"SageMaker TensorFlow serving stack comparisons",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":741.0,
        "Challenge_word_count":116,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. <\/p>\n\n<p>I'll go over the different components from bottom to top:<\/p>\n\n<p><strong>TensorFlow Serving<\/strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and\/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.<\/p>\n\n<p><strong>Flask<\/strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. <\/p>\n\n<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1<\/strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2<\/strong>)(<strong>3<\/strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4<\/strong>). You could visualize this as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/67EXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/67EXW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Gunicorn<\/strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.<\/p>\n\n<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. <\/p>\n\n<p><strong>Nginx<\/strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. <\/p>\n\n<p><a href=\"https:\/\/serverfault.com\/questions\/331256\/why-do-i-need-nginx-and-something-like-gunicorn\">This answer<\/a> might be helpful as well. <\/p>\n\n<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1547816279183,
        "Solution_link_count":3.0,
        "Solution_readability":10.2,
        "Solution_reading_time":36.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":444.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"serving stacks and responsibilities"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.1180555556,
        "Challenge_answer_count":0,
        "Challenge_body":"The DVC evaluation is crashing. After investigation, the bug was introduced by #348.\r\n\r\nThe bug:\r\n```\r\nTraceback (most recent call last):\r\n  File \"eval.py\", line 111, in <module>\r\n    main()\r\n  File \"eval.py\", line 107, in main\r\n    json.dump(all_metrics_dict, f)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/__init__.py\", line 179, in dump\r\n    for chunk in iterable:\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 431, in _iterencode\r\n    yield from _iterencode_dict(o, _current_indent_level)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type int64 is not JSON serializable\r\n```\r\n\r\nTo reproduce:\r\n\r\n```\r\n# For the bug introduced by #348, use 0bb500551b1b7c6f5bb9228335aa4df30a654e9c.\r\n# For the working code __before__ #348, use b9c886966ca4d893b41457a17262e198e3ba7f03.\r\nexport COMMIT=...\r\n\r\ngit clone https:\/\/github.com\/BlueBrain\/Search\r\ncd Search\/\r\n\r\n# Change <image> and <container>.\r\ndocker build -f data_and_models\/pipelines\/ner\/Dockerfile --build-arg BBS_REVISION=$COMMIT -t <image> .\r\ndocker run -it --rm -v \/raid:\/raid --name <container> <image>\r\n\r\ngit checkout $COMMIT\r\ngit checkout -- data_and_models\/pipelines\/ner\/dvc.lock\r\n\r\ncd data_and_models\/pipelines\/ner\/\r\ndvc pull --with-deps evaluation@organism\r\ndvc repro -fs evaluation@organism\r\n```\r\n\r\n_Originally posted by @pafonta in https:\/\/github.com\/BlueBrain\/Search\/issues\/335#issuecomment-833506692_",
        "Challenge_closed_time":1620393602000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620385977000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where using only \"zn.Method\" without \"zn.params\" in a Node does not add parameters to the \"dvc.yaml\" file, causing it to not depend on the \"params.yaml\" file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/361",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":647.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":2.1180555556,
        "Challenge_title":"DVC eval crashes \"int64 not JSON serializable\"",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":166,
        "Discussion_body":"",
        "Discussion_score_count":0.0,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_type":"anomaly",
        "Challenge_summary":"missing parameters in dvc.yaml"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1158333333,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nWhen I schedule a tensorboard from the UI does it always schedule on the polyaxon \"core nodes\", or does it schedule on any cpu machine that is available?\n\nWe have an issue sometimes with cpu nodes not scaling down, and I am wondering if it is because tensorboards are running on them?",
        "Challenge_closed_time":1649335625000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649335208000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing an issue where CPU nodes are not scaling down and suspects that Tensorboard services may be the cause. They are unsure if Tensorboard always schedules on the core nodes or any available CPU machine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1482",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.1158333333,
        "Challenge_title":"Are Tensorboad services preventing CPU nodes to scale down?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"By default, Tensorboard service does not use any node scheduling, so it probably uses the default nodes.\nIf you need to use a different node selectors or attach presets. When you click start tensorboard or run downstreamOp, the modal shows an editor which is basically prefilled a polyaxonfile:\n\nyou can just assign a preset if you have one:\n\npresets: [tensorboard-cpu-preset]\n\nOr patch the run with a node selector:\n\nrunPatch:\n  environment:\n    nodeSelector:\n      polyaxon_pool_type: polyaxon-tensorboard\n\nNote that for Tensorboad, by default it loads the component from here: Tensorboard Component\nYou can create your own Tensorboad component by following this tutorial\nIn that case you will use:\n\nhubRef: ORG\/tensorboard:VERSION\n\nFinally you can add this preset if you think that you might forget about a services running forever: https:\/\/polyaxon.com\/docs\/core\/scheduling-presets\/services-timeout\/\n\nOr adding it before clicking start:\n\ntermination:\n  timeout: 86400 # 24 hours",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.7,
        "Solution_reading_time":12.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":136.0,
        "Tool":"Polyaxon",
        "Challenge_type":"anomaly",
        "Challenge_summary":"CPU nodes not scaling down"
    },
    {
        "Answerer_created_time":1546431264350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Norway",
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":55.8339541667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are trying to develop an MLflow pipeline. We have our developing environment in a series of dockers (no local python environment &quot;whatsoever&quot;). This means that we have set up a docker container with MLflow and all requirements necessary to run pipelines. The issue we have is that when we write our MLflow project file we need to use &quot;docker_env&quot; to specify the environment. This figure illustrates what we want to achieve:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tLuw.jpg\" rel=\"nofollow noreferrer\">MLflow run dind<\/a><\/p>\n<p>MLflow inside the docker needs to access the docker daemon\/service so that it can either use the &quot;docker-image&quot; in the MLflow project file or pull it from docker hub. We are aware of the possibility of using &quot;conda_env&quot; in the MLflow project file but wish to avoid this.<\/p>\n<p>Our question is,<\/p>\n<p>Do we need to set some sort of &quot;docker in docker&quot; solution to achieve our goal?<\/p>\n<p>Is it possible to set up the docker container in which MLflow is running so that it can access the &quot;host machine&quot; docker daemon?<\/p>\n<p>I have been all over Google and MLflow's documentation but I can seem to find anything that can guide us. Thanks a lot in advance for any help or pointers!<\/p>",
        "Challenge_closed_time":1640385689187,
        "Challenge_comment_count":4,
        "Challenge_created_time":1640160788793,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to develop an MLflow pipeline within a docker container and needs to use \"docker_env\" to specify the environment in the MLflow project file. However, MLflow inside the docker needs to access the docker daemon\/service to use the \"docker-image\" in the project file or pull it from docker hub. The user is wondering if they need to set up a \"docker in docker\" solution or if it's possible to set up the docker container to access the host machine's docker daemon.",
        "Challenge_last_edit_time":1640184686952,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70445997",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":9.0,
        "Challenge_reading_time":17.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1,
        "Challenge_self_closed":1,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":62.4723316667,
        "Challenge_title":"MLflow run within a docker container - Running with \"docker_env\" in MLflow project file",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":779.0,
        "Challenge_word_count":211,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546431264350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I managed to create my pipeline using docker and docker_env in MLflow. It is not necessary to run d-in-d, the &quot;sibling approach&quot; is sufficient. This approach is described here:<\/p>\n<p><a href=\"https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/\" rel=\"nofollow noreferrer\">https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/<\/a><\/p>\n<p>and it is the preferred method to avoid d-in-d.<\/p>\n<p>One needs to be very careful when mounting volumes within the primary and secondary docker environments: all volume mounts happen in the host machine.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.87,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"MLflow",
        "Challenge_type":"inquiry",
        "Challenge_summary":"access host docker daemon"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.9835497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>According to the adoption plan, we need to rebuild everything, there is no quick way to push <a href=\"https:\/\/github.com\/Azure\/Azure-Machine-Learning-Adoption-Framework\">https:\/\/github.com\/Azure\/Azure-Machine-Learning-Adoption-Framework<\/a>    <\/p>\n<p>Am I correct?     <\/p>\n<p>It\u2019s not user friendly if I am not wrong.<\/p>",
        "Challenge_closed_time":1656630131776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656615790997,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering challenges with the Azure Machine Learning Adoption Framework, as they need to rebuild everything according to the adoption plan and there is no quick way to push changes. The user also finds the framework not user-friendly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/909961\/azure-machine-learning-adoption-framework",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.9835497222,
        "Challenge_title":"Azure-Machine-Learning-Adoption-Framework",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ce84de18-8973-44f3-8101-f191f9216b1f\">@Alexandre  <\/a>    <\/p>\n<p>Thanks for reaching out to us, I have answered this question as well in your other post. I think you are talking about move from Studio classc to Designer, please refer to below document:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview<\/a>    <\/p>\n<p>Basically yes for your other thread, you need to rebuild the whole pipeline since we can not copy - paste your orignal structure to Designer.    <\/p>\n<p>I am sorry for the inconveniences since the new studio has a disfferent structure to make this migration not that easy. Please let me know if you have any question during this process, we will provide help.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.3,
        "Solution_reading_time":12.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":126.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"challenges with adoption framework"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":47.7326625,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi Wandb Community!<\/p>\n<p>I have a laptop in my office that I am able to run <code>wandb local<\/code> on and sync my ML experiments to my account email address. My company gave me another laptop to work from on the road and I would like to set up <code>wandb local<\/code> on that laptop to streamline ML experiments I do in office and on the road.<\/p>\n<p>On my laptop, I have <code>wandb<\/code> and <code>docker<\/code> installed successfully. I can also run <code>wandb local<\/code> successfully. However, I\u2019m not sure if I need to copy the same api key and license over for the single account to work on both machines. Is there a smart way to do this?<\/p>\n<p>Thanks in advance!<\/p>\n<p>wand: 0.12.18<br>\nOS: Ubuntu 22.04<br>\ndocker: 20.10.17<\/p>",
        "Challenge_closed_time":1655499106272,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655327268687,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to set up wandb local on a second laptop provided by their company to streamline their ML experiments. They have successfully installed wandb and docker on the second laptop but are unsure if they need to copy the same API key and license over to use the same account on both machines. They are seeking advice on how to do this efficiently.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/coordinate-wandb-local-across-two-laptops\/2620",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":6.3,
        "Challenge_reading_time":9.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":47.7326625,
        "Challenge_title":"Coordinate wandb local across two laptops",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":171.0,
        "Challenge_word_count":130,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a class=\"mention\" href=\"\/u\/aclifton314\">@aclifton314<\/a> ,<\/p>\n<p>Thank you for writing in with your question. Local is explicitly an \u201con-device\u201d service.If you want to share data across devices,  you would want to host you instance on a server to be able to reach it from anywhere, otherwise your two laptops wont share data. You can however still use the individual machine to sync data back\/forth to W&amp;B cloud,  pull experiments\/runs\/metrics\/ ect. to the individual machines. If this is your intended approach then you can copy the same API and Local License key to both machines.  <a href=\"https:\/\/docs.wandb.ai\/guides\/self-hosted\/local#login\">Here<\/a> is a quick reference on how to switch between a private instance and the wandb cloud when you need to sync the data. Please let us know if you have additional questions.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.8,
        "Solution_reading_time":10.87,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":128.0,
        "Tool":"Weights & Biases",
        "Challenge_type":"inquiry",
        "Challenge_summary":"use wandb on second laptop"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":7804.7933702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried connecting through Sagemaker notebook to RDS. However, to connect to RDS, my public IP needs to be allowed for security reasons. I can see when I run this command: &quot;curl ifconfig.me&quot; on Sagemaker Notebook instance that public IP keeps changing from time to time.<\/p>\n<p>What is the correct way to connect to RDS with notebook on sagemaker? Do I need to crawl the RDS with AWS Glue and then use Athena on crawled tables and then take the query results from S3 with Sagemaker notebook?<\/p>",
        "Challenge_closed_time":1627578308223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599481052090,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting Sagemaker Jupyter Notebook to RDS due to the changing public IP address. The user is seeking advice on the correct way to connect to RDS with the notebook on Sagemaker. The user is also considering using AWS Glue to crawl RDS and then using Athena on crawled tables to take query results from S3 with Sagemaker notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63777462",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7804.7933702778,
        "Challenge_title":"Sagemaker Jupyter Notebook Cannot Connect to RDS",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":865.0,
        "Challenge_word_count":94,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509559597047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.<\/p>\n<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":7.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"connect Sagemaker to RDS"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.6287383334,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello. I created a Notebook Job Definition, scheduled to run every hour. According to the status it is \"Active\". Whenever I manually trigger the job with the \"Run Job\" button, it creates a new Notebook Job and runs successfully. However, the notebook never runs on the schedule. It should execute every hour, but instead only executes when manually triggered.\n\nIs there anything I need to do for the notebook to obey the schedule? Thanks,",
        "Challenge_closed_time":1675405386807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675370723349,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a Notebook Job Definition scheduled to run every hour, but the notebook never runs on the schedule and only executes when manually triggered. The user is seeking advice on how to make the notebook obey the schedule.",
        "Challenge_last_edit_time":1675718147630,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUjxD8Cf8lTc2Tknbt4MAzFQ\/sagemaker-notebook-doesn-t-run-on-job-schedule",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.6287383334,
        "Challenge_title":"Sagemaker notebook doesn't run on Job schedule",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":90.0,
        "Challenge_word_count":80,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Are there any problems with IAM role settings, etc.?\n\nPlease check this document for reference only.\nhttps:\/\/aws.amazon.com\/jp\/blogs\/machine-learning\/operationalize-your-amazon-sagemaker-studio-notebooks-as-scheduled-notebook-jobs\/",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1675405386807,
        "Solution_link_count":1.0,
        "Solution_readability":21.9,
        "Solution_reading_time":3.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"anomaly",
        "Challenge_summary":"notebook not running on schedule"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":46.5198344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to train <strong>Azure Machine Learning model<\/strong> on azure using <strong>Azure Machine Learning Service.<\/strong> But I want to use the <strong>custom Docker image<\/strong> for deploying the model on azure. I am not able to understand how to deploy Machine Learning models using Custom Docker Image.<\/p>\n\n<p>Please share me if there is any tutorial or blog about the deploy ml models using a custom image.<\/p>\n\n<p>Please check the below Docker file commands:-<\/p>\n\n<pre><code># Set locale\nRUN apt-get update\nRUN apt-get install locales\nRUN locale-gen en_US.UTF-8\nRUN update-locale LANG=en_US.UTF-8\n\n# Install MS SQL v13 driver for PyOdbc\nRUN apt-get install -y curl\nRUN apt-get install apt-transport-https\nRUN curl https:\/\/packages.microsoft.com\/keys\/microsoft.asc | apt-key add - \nRUN curl https:\/\/packages.microsoft.com\/config\/ubuntu\/16.04\/prod.list &gt; \/etc\/apt\/sources.list.d\/mssql-release.list\nRUN exit\nRUN apt-get update\n\nRUN ACCEPT_EULA=Y apt-get install -y msodbcsql\nRUN apt-get install -y unixodbc-dev\n<\/code><\/pre>\n\n<p>I want to use the <strong>Azure Container Registry<\/strong> for push the docker image and use the <strong>Custom Docker Image.<\/strong> Please let me know if there is any way.<\/p>\n\n<p><strong>Is there any way to Deploy Azure ML Models using Custom docker images?<\/strong><\/p>",
        "Challenge_closed_time":1569245799796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569236101160,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to deploy an Azure Machine Learning model on Azure using a custom Docker image but is unsure how to do so. They are seeking guidance on how to deploy Machine Learning models using a custom image and are interested in using Azure Container Registry to push the Docker image. The user has provided a Docker file with commands and is looking for any tutorials or blogs on the topic.",
        "Challenge_last_edit_time":1569244969488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58060865",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2.6940655556,
        "Challenge_title":"Deploy Azure Machine Learning models using Custom Docker Image on Azure Container Regisrty",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":864.0,
        "Challenge_word_count":184,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554466050936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":219.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>You can do following:<\/p>\n\n<ol>\n<li>Create an [Environment][1] with the coordinates of your custom Docker image specified in Docker section.<\/li>\n<li>Create [InferenceConfig][2] with that Environment as argument, and use it when deploying the model.<\/li>\n<\/ol>\n\n<p>For example, assuming you have a model already and eliding other arguments:<\/p>\n\n<pre><code>from azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment(name=\"myenv\")\nenv.docker.base_image = \"mybaseimage\"\nenv.docker.base_image_registry.address = \"ip-address\"\nenv.docker.base_image_registry.username = \"my-username\"\nenv.docker.base_image_registry.password = \"my-password\"\n\nic = InferenceConfig(\u2026,environment = env)\nmodel.deploy(\u2026,inference_config = ic)\n<\/code><\/pre>\n\n<pre><code>\n  [1]: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py\n  [2]: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1569412440892,
        "Solution_link_count":2.0,
        "Solution_readability":20.2,
        "Solution_reading_time":14.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"deploy ML model with custom image"
    },
    {
        "Answerer_created_time":1376680978667,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":1454.0,
        "Answerer_view_count":109.0,
        "Challenge_adjusted_solved_time":4809.9866886111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am fairly new to kubernetes - I have developed web UI\/API that automates model deployment using Azure Machine Learning Services to Azure Kubernetes Services (AKS). As a hardening measure, I am tying to set up managed identity for deployed pods in AKS using <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-azure-ad-identity\" rel=\"nofollow noreferrer\">this documentation<\/a>. One of the step is to edit the deployment to add identity-feature label at <code>\/spec\/template\/metadata\/labels<\/code> for the deployment (see para starting like <code>Edit the deployment to add ...<\/code> in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-azure-ad-identity#assign-azure-identity-to-machine-learning-web-service\" rel=\"nofollow noreferrer\">this section<\/a>). <\/p>\n\n<p>I wish to automate this step using python kubernetes client (<a href=\"https:\/\/github.com\/kubernetes-client\/python\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kubernetes-client\/python<\/a>). Browsing the available API, I was wondering that perhaps <code>patch_namespaced_deployment<\/code> will allow me to edit deployment and add label at <code>\/spec\/template\/metadata\/labels<\/code>. I was looking for some example code using the python client for the same - any help to achieve above will be appreciated.<\/p>",
        "Challenge_closed_time":1590493308888,
        "Challenge_comment_count":5,
        "Challenge_created_time":1590392663393,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to Kubernetes and is trying to set up managed identity for deployed pods in AKS using a specific documentation. One of the steps is to edit the deployment to add an identity-feature label at \/spec\/template\/metadata\/labels for the deployment. The user wishes to automate this step using python kubernetes client and is looking for some example code to achieve the same.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61997914",
        "Challenge_link_count":4,
        "Challenge_participation_count":6,
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":27.9570819444,
        "Challenge_title":"How to edit\/patch kubernetes deployment to add label using python",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":6338.0,
        "Challenge_word_count":149,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1281518863807,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mumbai, India",
        "Poster_reputation_count":45542.0,
        "Poster_view_count":3034.0,
        "Solution_body":"<p>Have a look at this example:<\/p>\n<p><a href=\"https:\/\/github.com\/kubernetes-client\/python\/blob\/master\/examples\/deployment_crud.py#L62-L70\" rel=\"nofollow noreferrer\">https:\/\/github.com\/kubernetes-client\/python\/blob\/master\/examples\/deployment_crud.py#L62-L70<\/a><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def update_deployment(api_instance, deployment):\n    # Update container image\n    deployment.spec.template.spec.containers[0].image = &quot;nginx:1.16.0&quot;\n    # Update the deployment\n    api_response = api_instance.patch_namespaced_deployment(\n        name=DEPLOYMENT_NAME,\n        namespace=&quot;default&quot;,\n        body=deployment)\n    print(&quot;Deployment updated. status='%s'&quot; % str(api_response.status))\n<\/code><\/pre>\n<p>The Labels are on the deployment object, from the App v1 API,<\/p>\n<pre><code>kind: Deployment\nmetadata:\n  name: deployment-example\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app: nginx\n<\/code><\/pre>\n<p>which means you need to update the following:<\/p>\n<p><code>deployment.spec.template.metadata.labels.app = &quot;nginx&quot;<\/code><\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1607708615472,
        "Solution_link_count":2.0,
        "Solution_readability":24.4,
        "Solution_reading_time":14.66,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":71.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"inquiry",
        "Challenge_summary":"automate identity label addition"
    },
    {
        "Answerer_created_time":1405786822143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":210.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":4.3662702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to restrict sudo access for users in the jupyterserver kernel app when running sagemaker studio? or is it easier to just configure the vpc to prevent outbound traffice?<\/p>",
        "Challenge_closed_time":1657545273716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657529555143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how to restrict sudo access for users in the jupyterserver kernel app when running Sagemaker studio. They are also considering configuring the VPC to prevent outbound traffic as an alternative solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72935918",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":4.3662702778,
        "Challenge_title":"Restricting Sagemaker studio jupyterkenel app sudo access for user",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":39,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Configuring VPC to restrict outbound traffic is quite easy. You can start from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-notebooks-and-internet-access.html\" rel=\"nofollow noreferrer\">here<\/a>. There are lot of AWS Official blogs\/samples written on this topic but you can start with these:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/securing-amazon-sagemaker-studio-connectivity-using-a-private-vpc\/\" rel=\"nofollow noreferrer\">Securing Amazon SageMaker Studio connectivity using a private VPC<\/a>\n<a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-studio-vpc-networkfirewall\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio in a private VPC with NAT Gateway and Network Firewall<\/a><\/p>\n<p>on the topic of sudo access, Studio uses <code>run-as<\/code> POSIX user\/group to manage the <code>JupyterServer app<\/code> and <code>KernelGateWay app<\/code>. The JupyterServer app user is run as <code>sagemaker-user<\/code>, which has <code>sudo<\/code> permission to enable installation of yum packages, whereas the <code>KernelGateway app<\/code> user is run as <code>root<\/code> and can perform pip\/conda installs, but <strong>neither<\/strong> can access the host instance. Apart from the default <code>run-as<\/code> user, the user inside the container is mapped to a <code>non-privileged user ID range<\/code> on the notebook instances. This is to ensure that the user can\u2019t escalate privileges to come out of the container and perform any restricted operations in the EC2 instance.<\/p>\n<p>In addition, SageMaker adds specific route rules to block requests to Amazon EFS and the <code>instance metadata service (IMDS)<\/code> from the container, and users can\u2019t change these rules. All the inter-network traffic in Studio is TLS 1.2 encrypted, barring some intra-node traffic like communication between nodes in a distributed training or processing job and communication between a service control plane and training instances. Check out this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/dive-deep-into-amazon-sagemaker-studio-notebook-architecture\/\" rel=\"nofollow noreferrer\">blog<\/a> to understand better on How Studio runs<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.0,
        "Solution_reading_time":28.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":245.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"restrict sudo access"
    },
    {
        "Answerer_created_time":1275416023832,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation_count":12639.0,
        "Answerer_view_count":932.0,
        "Challenge_adjusted_solved_time":34.5765611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting the below error when I'm trying to run the following line of code to load en_core_web_sm in the Azure Machine Learning instance.<\/p>\n<p>I debugged the issue and found out that once I install scrubadub_spacy, that seems is the issue causing the error.<\/p>\n<pre><code>spacy.load(&quot;en_core_web_sm&quot;)\n<\/code><\/pre>\n<pre><code>OSError                                   Traceback (most recent call last)\n&lt;ipython-input-2-c6e652d70518&gt; in &lt;module&gt;\n     1 # Load English tokenizer, tagger, parser and NER\n----&gt; 2 nlp = spacy.load(&quot;en_core_web_sm&quot;)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/__init__.py in load(name, vocab, disable, exclude, config)\n    50     &quot;&quot;&quot;\n    51     return util.load_model(\n---&gt; 52         name, vocab=vocab, disable=disable, exclude=exclude, config=config\n    53     )\n    54 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model(name, vocab, disable, exclude, config)\n   418             return get_lang_class(name.replace(&quot;blank:&quot;, &quot;&quot;))()\n   419         if is_package(name):  # installed as package\n--&gt; 420             return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]\n   421         if Path(name).exists():  # path to model data directory\n   422             return load_model_from_path(Path(name), **kwargs)  # type: ignore[arg-type]\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model_from_package(name, vocab, disable, exclude, config)\n   451     &quot;&quot;&quot;\n   452     cls = importlib.import_module(name)\n--&gt; 453     return cls.load(vocab=vocab, disable=disable, exclude=exclude, config=config)  # type: ignore[attr-defined]\n   454 \n   455 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/en_core_web_sm\/__init__.py in load(**overrides)\n    10 \n    11 def load(**overrides):\n---&gt; 12     return load_model_from_init_py(__file__, **overrides)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config)\n   619         disable=disable,\n   620         exclude=exclude,\n--&gt; 621         config=config,\n   622     )\n   623 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config)\n   485     config_path = model_path \/ &quot;config.cfg&quot;\n   486     overrides = dict_to_dot(config)\n--&gt; 487     config = load_config(config_path, overrides=overrides)\n   488     nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)\n   489     return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_config(path, overrides, interpolate)\n   644     else:\n   645         if not config_path or not config_path.exists() or not config_path.is_file():\n--&gt; 646             raise IOError(Errors.E053.format(path=config_path, name=&quot;config.cfg&quot;))\n   647         return config.from_disk(\n   648             config_path, overrides=overrides, interpolate=interpolate\n\nOSError: [E053] Could not read config.cfg from \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/en_core_web_sm\/en_core_web_sm-2.3.1\/config.cfg\n<\/code><\/pre>\n<p>I installed the packages using the below three lines codes from <a href=\"https:\/\/spacy.io\/usage\" rel=\"nofollow noreferrer\">Spacy<\/a><\/p>\n<pre><code>pip install -U pip setuptools wheel\npip install -U spacy\npython -m spacy download en_core_web_sm\n<\/code><\/pre>\n<p>How should I fix this issue? thanks in advance.<\/p>",
        "Challenge_closed_time":1644122814723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643912383590,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an OSError when trying to load en_core_web_sm in Azure Machine Learning instance, and has found that the issue is caused by installing scrubadub_spacy package. The user has installed the packages using the recommended Spacy commands. The user is seeking guidance on how to resolve the issue.",
        "Challenge_last_edit_time":1643998339103,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70976353",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":46.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":58.4530925,
        "Challenge_title":"After installing scrubadub_spacy package, spacy.load(\"en_core_web_sm\") not working OSError: [E053] Could not read config.cfg",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":201.0,
        "Challenge_word_count":299,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573605534107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Saint Louis, MO, USA",
        "Poster_reputation_count":25.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Taking the path from your error message:<\/p>\n<pre><code>en_core_web_sm-2.3.1\/config.cfg\n<\/code><\/pre>\n<p>You have a model for v2.3, but it's looking for a <code>config.cfg<\/code>, which is only a thing in v3 of spaCy. It looks like you upgraded spaCy without realizing it.<\/p>\n<p>There are two ways to fix this. One is to reinstall the model with <code>spacy download<\/code>, which will get a version that matches your current spaCy version. If you are just starting something that is probably the best idea. Based on the release date of scrubadub, it seems to be intended for use with spaCy v3.<\/p>\n<p>However, note that v2 and v3 are pretty different - if you have a project with v2 of spaCy you might want to downgrade instead.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":9.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":119.0,
        "Tool":"Azure Machine Learning",
        "Challenge_type":"anomaly",
        "Challenge_summary":"OSError loading package"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":13.1620658333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently struggling to decide on what situations in which a glue job is preferable over a sagemaker processing job and vice versa? Some advice on this topic would be greatly appreciated.<\/p>\n<p>I can do the same on both, so why should I bother with the difference?<\/p>",
        "Challenge_closed_time":1660951371720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660903988283,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty deciding when to use a Glue job or a Sagemaker Processing job for an ETL task and is seeking advice on the topic. They are unsure of the differences between the two and why they should choose one over the other.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73415182",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":4.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2,
        "Challenge_self_closed":0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":13.1620658333,
        "Challenge_title":"When do I use a glue job or a Sagemaker Processing job for an etl?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":62,
        "Discussion_body":null,
        "Discussion_score_count":null,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<ul>\n<li>if you want to use a specific EC2 instance, use SageMaker<\/li>\n<li>Pricing: SageMaker is pro-rated per-second while Glue has minimum charge amount (1min or 10min depending on versions). You should measure how much would a workload cost you on each platform<\/li>\n<li>customization: in SageMaker Processing you can customize the execution environment, as you provide a Docker image (you could run more than Spark\/Python, such as C++ or R)<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.9,
        "Solution_reading_time":5.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker",
        "Challenge_type":"inquiry",
        "Challenge_summary":"Glue vs Sagemaker for ETL"
    }
]