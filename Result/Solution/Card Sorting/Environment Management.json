[
    {
        "Answerer_created_time":1312912826288,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation_count":5843.0,
        "Answerer_view_count":153.0,
        "Challenge_adjusted_solved_time":0.9586563889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have deployed a build of mlflow to a pod in my kubernetes cluster. I'm able to port forward to the mlflow ui, and now I'm attempting to test it. To do this, I am running the following test on a jupyter notebook that is running on another pod in the same cluster.<\/p>\n<pre><code>import mlflow\n\nprint(&quot;Setting Tracking Server&quot;)\ntracking_uri = &quot;http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000&quot;\n\nmlflow.set_tracking_uri(tracking_uri)\n\nprint(&quot;Logging Artifact&quot;)\nmlflow.log_artifact('\/home\/test\/mlflow-example-artifact.png')\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p>When I run this though, I get<\/p>\n<pre><code>ConnectionError: HTTPConnectionPool(host='mlflow-tracking-server.default.svc.cluster.local', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get? (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>\n<p>The way I have deployed the mlflow pod is shown below in the yaml and docker:<\/p>\n<p>Yaml:<\/p>\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: &lt;ECR_IMAGE&gt;\n        ports:\n        - containerPort: 5000\n        env:\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;S3_BUCKET&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\n  annotations:\n    service.beta.kubernetes.io\/aws-load-balancer-type: nlb\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  selector:\n    app: mlflow-tracking-server\n  ports:\n    - name: http\n      port: 5000\n      targetPort: http\n<\/code><\/pre>\n<p>While the dockerfile calls a script that executes the mlflow server command: <code>mlflow server --default-artifact-root ${AWS_MLFLOW_BUCKET} --host 0.0.0.0 --port 5000<\/code>, I cannot connect to the service I have created using that mlflow pod.<\/p>\n<p>I have tried using the tracking uri <code>http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000<\/code>, I've tried using the service EXTERNAL-IP:5000, but everything I tried cannot connect and log using the service. Is there anything that I have missed in deploying my mlflow server pod to my kubernetes cluster?<\/p>",
        "Challenge_closed_time":1587498737656,
        "Challenge_comment_count":6,
        "Challenge_created_time":1587495286493,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has deployed an MLflow build to a pod in their Kubernetes cluster and is attempting to test it by running a Jupyter notebook on another pod in the same cluster. However, they are encountering a connection error when trying to set the tracking server using the mlflow library. The user has provided the YAML and Docker files used to deploy the MLflow pod and has tried various tracking URIs but cannot connect to the service. The user is seeking assistance in identifying any missed steps in deploying the MLflow server pod to their Kubernetes cluster.",
        "Challenge_last_edit_time":1599477403816,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61351024",
        "Challenge_link_count":2,
        "Challenge_participation_count":8,
        "Challenge_readability":14.0,
        "Challenge_reading_time":34.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":0.9586563889,
        "Challenge_title":"Kubernetes MLflow Service Pod Connection",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":855.0,
        "Challenge_word_count":276,
        "Platform":"Stack Overflow",
        "Poster_created_time":1417012835812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":945.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>Your <strong>mlflow-tracking-server<\/strong> service should have <em>ClusterIP<\/em> type, not <em>LoadBalancer<\/em>. <\/p>\n\n<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer<\/em> Service type.<\/p>\n\n<blockquote>\n  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that\u2019s outside of your cluster.\n  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.<\/p>\n  \n  <p>Type values and their behaviors are:<\/p>\n  \n  <ul>\n  <li><p><strong>ClusterIP<\/strong>: Exposes the Service on a cluster-internal IP. Choosing this\n  value makes the Service only reachable from within the cluster. This\n  is the default ServiceType. <\/p><\/li>\n  <li><p><strong>NodePort<\/strong>: Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll > be able to contact the NodePort Service, from outside the cluster, by\n  requesting :. <\/p><\/li>\n  <li><strong>LoadBalancer<\/strong>: Exposes the Service\n  externally using a cloud provider\u2019s load balancer. NodePort and\n  ClusterIP Services, to which the external load balancer routes, are\n  automatically created. <\/li>\n  <li><strong>ExternalName<\/strong>: Maps the Service to the contents\n  of the externalName field (e.g. foo.bar.example.com), by returning a\n  CNAME record with its value. No proxying of any kind is set up.<\/li>\n  <\/ul>\n  \n  <p><a href=\"https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#publishing-services-service-types\" rel=\"nofollow noreferrer\">kubernetes.io<\/a><\/p>\n<\/blockquote>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1587499353943,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":21.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":206.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":129.76,
        "Challenge_answer_count":0,
        "Challenge_body":"Describe the bug\n\nAttempting to deploy the docker image build yaml file available under https:\/\/github.com\/polyaxon\/polyaxon-examples\/blob\/master\/in_cluster\/build_image\/build-ml.yaml using the polyaxon cli returns the following error:\n\n>polyaxon run -f build_docker_image.yaml -l\nPolyaxonfile is not valid.\nError message: The Polyaxonfile's version specified is not supported by your current CLI.Your CLI support Polyaxonfile versions between: 1.1 <= v <= 1.1.You can run `polyaxon upgrade` and check documentation for the specification..\n\nTo reproduce\n\nThe contents of build_docker_image.yaml\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: python:3.8.8-buster\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nExpected behavior\n\nA build operation should begin that results in an image being saved to the docker registry connected under destination: connection:\n\nEnvironment\n\nPolyaxon 1.8.0\nPolyaxon CLI 1.8.0",
        "Challenge_closed_time":1618580258000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618113122000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while attempting to deploy a docker image build yaml file using the Polyaxon CLI. The error message indicates that the Polyaxonfile's version specified is not supported by the current CLI version. The user is using Polyaxon 1.8.0 and Polyaxon CLI 1.8.0. The expected behavior is to begin a build operation resulting in an image being saved to the docker registry.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1303",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":11.9,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":129.76,
        "Challenge_title":"Issue building docker image using example build config",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":140,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Very strange, and I could not reproduce. Did you try other files in the same examples folder https:\/\/github.com\/polyaxon\/polyaxon-examples\/tree\/master\/in_cluster\/build_image ?\nNot sure if quoting all values would work on your system:\n\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: localreg\n    value: \"polyaxon-examples:ml\"\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"python:3.8.8-buster\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\"]'\n      - 'pip3 install scikit-learn xgboost matplotlib vega-datasets joblib lightgbm xgboost'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\n\nFinally you may try with polyaxon -v ... to trigger some additional debug information.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":8.77,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":78.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":164.9280080556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using MS Azure ML and have found that when I start a Notebook (from the Azure ML Studio) it is executing in a a different environment than if I create a Python script and run it from the studio. I want to be able to create a specific environment and have the Notebook use that. The environment that the Notebook seems to run does not contain the packages I need and I want to preserve different environments.<\/p>",
        "Challenge_closed_time":1641553768432,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641247442447,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with different environments in Azure ML Studio while running a Notebook and a Python script. The Notebook is executing in a different environment and does not contain the required packages, which is causing issues for the user. The user wants to create a specific environment and use it in the Notebook to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70571948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":85.0905513889,
        "Challenge_title":"Why is env different in an Azure ML notbook and an Azure ML terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":225.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1639768329608,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Eden Prarie, MN",
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>First open a terminal, using the same compute target as you want to use with your Notebook afterwards, and to use and <strong>existing environment<\/strong> you can do:<\/p>\n<pre><code>conda activate existing_env\nconda install ipykernel\npython -m ipykernel install --user --name existing_env --display-name &quot;Python 3.8 - Existing Environment&quot;   \n<\/code><\/pre>\n<p>However, to create a <strong>new environment<\/strong> and use it in you AzureML Notebook, you have to do the following commands:<\/p>\n<pre><code>conda create --name new_env python=3.8\nconda activate new_env\nconda install pip\nconda install ipykernel\npython -m ipykernel install --user --name new_env --display-name &quot;Python 3.8 - New Environment&quot;\n<\/code><\/pre>\n<p>And then last, but not least, you have to edit the Jupyter Kernel display names:<\/p>\n<p><strong>IMPORTANT<\/strong> Please ensure you are comfortable running all these steps:<\/p>\n<pre><code>jupyter kernelspec list\ncd &lt;folder-that-matches-the-kernel-of-your-environment&gt;\nsudo nano kernel.json\n<\/code><\/pre>\n<p>Then edit the name to match what you want and save the file.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1641841183276,
        "Solution_link_count":0.0,
        "Solution_readability":16.6,
        "Solution_reading_time":14.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":140.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":66.2517636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>Is there a way to specify the disk storage type for Compute instances?     <br \/>\nBoth the Azure portal and ARM templates do not have an option to define the disk storage type, which defaults to the P10 disks (Premium SSD).     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/148883-azureml-compute.png?platform=QnA\" alt=\"148883-azureml-compute.png\" \/>Thanks    <\/p>",
        "Challenge_closed_time":1636952401092,
        "Challenge_comment_count":1,
        "Challenge_created_time":1636713894743,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in specifying the disk storage type for Compute instances in Azure Machine Learning. The default disk storage type is P10 disks (Premium SSD) and there is no option to define it in both the Azure portal and ARM templates.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/625035\/azure-machine-learning-specify-disk-storage-type",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":5.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":66.2517636111,
        "Challenge_title":"Azure Machine Learning - Specify disk storage type",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":54,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=41b4924d-c8f5-4ca4-9844-0c0af46eb5d5\">@Simon Magrin  <\/a>  Thanks, Currently There's no way to change the disk storage type for CIs or compute clusters. We have added this to our product backlog item to support in the near future.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":13.8,
        "Solution_reading_time":17.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433746746023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":22140.0,
        "Answerer_view_count":1710.0,
        "Challenge_adjusted_solved_time":6.9395733333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been trying to install a machine learning package that I can use in my R script.<\/p>\n\n<p>I have done placed the tarball of the installer inside a zip file and am doing <\/p>\n\n<pre><code>install.packages(\"src\/packagename_2.0-3.tar.gz\", repos = NULL, type=\"source\") \n<\/code><\/pre>\n\n<p>from within the R script. However, the progress indicator just circles indefinitely, and it's not installed in environment.<\/p>\n\n<p>How can I install this package?<\/p>\n\n<p><code>ada<\/code> is the package I'm trying to install and <code>ada_2.0-3.tar.gz<\/code> is the file I'm using.<\/p>",
        "Challenge_closed_time":1443095996316,
        "Challenge_comment_count":0,
        "Challenge_created_time":1443064053390,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in installing an R library in Azure ML. They have tried to install the package using a tarball of the installer inside a zip file, but the progress indicator just circles indefinitely, and the package is not installed in the environment. The package they are trying to install is \"ada\" using the file \"ada_2.0-3.tar.gz\".",
        "Challenge_last_edit_time":1443071013852,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32752659",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":7.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":8.873035,
        "Challenge_title":"unable to install R library in azure ml",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":982.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1426899441168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2024.0,
        "Poster_view_count":298.0,
        "Solution_body":"<p>You cannot use the tarball packages. If you are on windows you need to do the following:<\/p>\n\n<p>Once you install a package (+ it's dependencies) it will download the packages in a directory <\/p>\n\n<blockquote>\n  <p>C:\\Users\\xxxxx\\AppData\\Local\\Temp\\some directory\n  name\\downloaded_packages<\/p>\n<\/blockquote>\n\n<p>These will be in a zip format. These are the packages you need. <\/p>\n\n<p>Or download the windows binaries from cran.<\/p>\n\n<p>Next you need to put all the needed packages in one total zip-file and upload this to AzureML as a new dataset.<\/p>\n\n<p>in AzureML load the data package connected to a r-script<\/p>\n\n<pre><code>install.packages(\"src\/ada.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(ada, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>Be sure to check that all dependent packages are available in Azure. Rpart is available.<\/p>\n\n<p>For a complete overview, look at this <a href=\"http:\/\/blogs.msdn.com\/b\/benjguin\/archive\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning.aspx\" rel=\"nofollow\">msdn blog<\/a> explaining it a bit better with some visuals.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.3,
        "Solution_reading_time":13.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":135.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":430.5058333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently the customer is shown an error message but also has the option to report an issue which is misleading, we should remove the report issue button for this scenario.",
        "Challenge_closed_time":1655835283000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654285462000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to add the AzureML extension on an OpenShift cluster. The error message indicates that the operation failed due to an inability to get the status from the local CRD with the error: {Error: Retry for given duration didn't get any results with err {status not populated}}.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1588",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":12.1,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":430.5058333333,
        "Challenge_title":"Improve the error message when trying to execute a YAML that is not Azure ML realted",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":45,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405786822143,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":210.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":4.3662702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to restrict sudo access for users in the jupyterserver kernel app when running sagemaker studio? or is it easier to just configure the vpc to prevent outbound traffice?<\/p>",
        "Challenge_closed_time":1657545273716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657529555143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on how to restrict sudo access for users in the jupyterserver kernel app when running Sagemaker studio. They are also considering configuring the VPC to prevent outbound traffic as an alternative solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72935918",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":4.3662702778,
        "Challenge_title":"Restricting Sagemaker studio jupyterkenel app sudo access for user",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":46.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Configuring VPC to restrict outbound traffic is quite easy. You can start from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-notebooks-and-internet-access.html\" rel=\"nofollow noreferrer\">here<\/a>. There are lot of AWS Official blogs\/samples written on this topic but you can start with these:\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/securing-amazon-sagemaker-studio-connectivity-using-a-private-vpc\/\" rel=\"nofollow noreferrer\">Securing Amazon SageMaker Studio connectivity using a private VPC<\/a>\n<a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-studio-vpc-networkfirewall\" rel=\"nofollow noreferrer\">Amazon SageMaker Studio in a private VPC with NAT Gateway and Network Firewall<\/a><\/p>\n<p>on the topic of sudo access, Studio uses <code>run-as<\/code> POSIX user\/group to manage the <code>JupyterServer app<\/code> and <code>KernelGateWay app<\/code>. The JupyterServer app user is run as <code>sagemaker-user<\/code>, which has <code>sudo<\/code> permission to enable installation of yum packages, whereas the <code>KernelGateway app<\/code> user is run as <code>root<\/code> and can perform pip\/conda installs, but <strong>neither<\/strong> can access the host instance. Apart from the default <code>run-as<\/code> user, the user inside the container is mapped to a <code>non-privileged user ID range<\/code> on the notebook instances. This is to ensure that the user can\u2019t escalate privileges to come out of the container and perform any restricted operations in the EC2 instance.<\/p>\n<p>In addition, SageMaker adds specific route rules to block requests to Amazon EFS and the <code>instance metadata service (IMDS)<\/code> from the container, and users can\u2019t change these rules. All the inter-network traffic in Studio is TLS 1.2 encrypted, barring some intra-node traffic like communication between nodes in a distributed training or processing job and communication between a service control plane and training instances. Check out this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/dive-deep-into-amazon-sagemaker-studio-notebook-architecture\/\" rel=\"nofollow noreferrer\">blog<\/a> to understand better on How Studio runs<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.0,
        "Solution_reading_time":28.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":245.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1445975382243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":6831.0,
        "Answerer_view_count":653.0,
        "Challenge_adjusted_solved_time":3.9043655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>SageMaker seems to give examples of using two different serving stacks for serving custom docker images:<\/p>\n\n<ol>\n<li>NGINX + Gunicorn + Flask<\/li>\n<li>NGINX + TensorFlow Serving<\/li>\n<\/ol>\n\n<p>Could someone explain to me at a very high level (I have very little knowledge of network engineering) what responsibilities these different components have? And since the second stack has only two components instead of one, can I rightly assume that TensorFlow Serving does the job (whatever that may be) of both Gunicorn and Flask? <\/p>\n\n<p>Lastly, I've read that it's possible to use Flask and TensorFlow serving at the same time. Would this then be NGINX -> Gunicorn -> Flask -> TensorFlow Serving? And what are there advantages of this?<\/p>",
        "Challenge_closed_time":1547801639500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547670814743,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is seeking an explanation of the two different serving stacks for serving custom docker images in SageMaker, namely NGINX + Gunicorn + Flask and NGINX + TensorFlow Serving. They are also curious about the responsibilities of these different components and whether TensorFlow Serving does the job of both Gunicorn and Flask. Additionally, the user is interested in knowing the advantages of using Flask and TensorFlow serving at the same time and whether it would be NGINX -> Gunicorn -> Flask -> TensorFlow Serving.",
        "Challenge_last_edit_time":1547802223467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54224934",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":9.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":36.3402102778,
        "Challenge_title":"SageMaker TensorFlow serving stack comparisons",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":741.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1361339272692,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"NYC",
        "Poster_reputation_count":6281.0,
        "Poster_view_count":958.0,
        "Solution_body":"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. <\/p>\n\n<p>I'll go over the different components from bottom to top:<\/p>\n\n<p><strong>TensorFlow Serving<\/strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and\/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.<\/p>\n\n<p><strong>Flask<\/strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. <\/p>\n\n<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1<\/strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2<\/strong>)(<strong>3<\/strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4<\/strong>). You could visualize this as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/67EXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/67EXW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Gunicorn<\/strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.<\/p>\n\n<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. <\/p>\n\n<p><strong>Nginx<\/strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. <\/p>\n\n<p><a href=\"https:\/\/serverfault.com\/questions\/331256\/why-do-i-need-nginx-and-something-like-gunicorn\">This answer<\/a> might be helpful as well. <\/p>\n\n<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1547816279183,
        "Solution_link_count":3.0,
        "Solution_readability":10.2,
        "Solution_reading_time":36.77,
        "Solution_score_count":2.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":444.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":54.3333333333,
        "Challenge_answer_count":4,
        "Challenge_body":"Hello,\n\nI am running into an issue with timeout value for environment specific webhooks in Dialogflow CX.\n\nWe have extended the timeout value to 15 seconds for the webhook and we have multiple environment specific webhook URLs set as well.\u00a0 You can see precise setup as part of this issue posted by another user facing the exact same issue 3 months back :\u00a0https:\/\/issuetracker.google.com\/issues\/261683010\n\nIt seems like the webhook timeout value is not being considered for environment specific URLs.\u00a0 I see it does work for the agent level URL which is not what I would like to use as we have for obvious reasons different URL for non-prod and production apps.\n\nIs there any workaround or possible solution to this problem?\n\nCheers, jags",
        "Challenge_closed_time":1678347120000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678151520000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the timeout value for environment-specific webhooks in Dialogflow CX. Despite extending the timeout value to 15 seconds, the webhook timeout value is not being considered for environment-specific URLs. The user is looking for a possible solution or workaround to this problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-CX-environment-specific-webhook-timeout-is-not\/m-p\/529553#M1383",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.5,
        "Challenge_reading_time":9.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":54.3333333333,
        "Challenge_title":"Dialogflow CX environment specific webhook timeout is not considered",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":183.0,
        "Challenge_word_count":130,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Found a suitable workaround:\n\nUpdate the default webhook URL to production with timeout 15 seconds.\u00a0 Remove the environment specific webhook URL.\nRelease a version of the flow and use it with the production environment.\nUpdate the default webhook URL back to test endpoint with timeout 15 seconds.\nRepeat the steps before releasing the new version for production usage.\n\nWhen you release the version of a flow, Dialogflow CX would create the snapshot of webhook configuration as well along with flow.\u00a0 And when you update the default webhook URL again, it would only update for unpublished\/draft version and it does not affect the webhook URL in the released version of the flow.\u00a0 It's verified and working approach!\n\nCheers, jags\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":9.3,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":5.5099236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Background<\/strong><\/p>\n\n<p>I have created an ML Workspace environment from a conda <code>environment.yml<\/code> plus some docker config and environment variables. I can access it from within a Python notebook:<\/p>\n\n<pre><code>env = Environment.get(workspace=ws, name='my-environment', version='1')\n<\/code><\/pre>\n\n<p>I can use this successfully to run a Python script as an experiment, i.e.<\/p>\n\n<pre><code>runconfig = ScriptRunConfig(source_directory='script\/', script='my-script.py', arguments=script_params)\nrunconfig.run_config.target = compute_target\nrunconfig.run_config.environment = env\nrun = exp.submit(runconfig)\n<\/code><\/pre>\n\n<p><strong>Problem<\/strong><\/p>\n\n<p>I would now like to run this same script as a Pipeline, so that I can trigger multiple runs with different parameters. I have created the Pipeline as follows:<\/p>\n\n<pre><code>pipeline_step = PythonScriptStep(\n    source_directory='script', script_name='my-script.py',\n    arguments=['-a', param1, '-b', param2],\n    compute_target=compute_target,\n    runconfig=runconfig\n)\nsteps = [pipeline_step]\npipeline = Pipeline(workspace=ws, steps=steps)\npipeline.validate()\n<\/code><\/pre>\n\n<p>When I then try to run the Pipeline:<\/p>\n\n<pre><code>pipeline_run = Experiment(ws, 'my_pipeline_run').submit(\n    pipeline, pipeline_parameters={...}\n)\n<\/code><\/pre>\n\n<p>I get the following error: <code>Response status code does not indicate success: 400 (Conda dependencies were not specified. Please make sure that all conda dependencies were specified i).<\/code><\/p>\n\n<p>When I view the pipeline run in the Azure Portal it seems that the environment has not been picked up: none of my conda dependencies are configured, hence the code does not run. What am I doing wrong?<\/p>",
        "Challenge_closed_time":1583255270992,
        "Challenge_comment_count":0,
        "Challenge_created_time":1583235435267,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created an ML Workspace environment from a conda environment.yml and wants to run a Python script as a pipeline. However, when attempting to run the pipeline, the user receives an error message indicating that the conda dependencies were not specified. The environment does not seem to have been picked up, and none of the conda dependencies are configured, causing the code to fail.",
        "Challenge_last_edit_time":1583277273840,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60506398",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.0,
        "Challenge_reading_time":22.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":5.5099236111,
        "Challenge_title":"How do I use an environment in an ML Azure Pipeline",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1972.0,
        "Challenge_word_count":195,
        "Platform":"Stack Overflow",
        "Poster_created_time":1403698315160,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":1534.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>You're almost there, but you need to use <code>RunConfiguration<\/code> instead of <code>ScriptRunConfig<\/code>. More info <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-getting-started.ipynb\" rel=\"noreferrer\">here<\/a><\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.runconfig import RunConfiguration\n\nenv = Environment.get(workspace=ws, name='my-environment', version='1')\n# create a new runconfig object\nrunconfig = RunConfiguration()\nrunconfig.environment = env\n\npipeline_step = PythonScriptStep(\n    source_directory='script', script_name='my-script.py',\n    arguments=['-a', param1, '-b', param2],\n    compute_target=compute_target,\n    runconfig=runconfig\n)\n\npipeline = Pipeline(workspace=ws, steps=[pipeline_step])\n\npipeline_run = Experiment(ws, 'my_pipeline_run').submit(pipeline)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.3,
        "Solution_reading_time":12.47,
        "Solution_score_count":8.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":375.8113888889,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThank you for creating such a helpful tool!\r\nThe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the MLFlow experiment artifacts dir. I think the issue is with inconsistent naming for the saved png for certain plot types.\r\nThank you for your help!\r\n<!--\r\n-->\r\n\r\n**To Reproduce**\r\n<!--\r\nAdd a Minimal, Complete, and Verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nIf the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=False, \r\n      silent=True,\r\n      verbose=False,\r\n      # for MLFlow logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = True, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**Expected behavior**\r\n<!--\r\n-->\r\nI expect ALL of the plot types to be logged under the MLFlow artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nHowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\r\nI think the issue is with inconsistent naming of the file. Here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:INFO:Saving 'calibration.png'\r\n2021-10-11 19:03:20,064:INFO:Visual Rendered Successfully\r\n2021-10-11 19:03:20,213:INFO:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:WARNING:[Errno 2] No such file or directory: 'Calibration Curve.png'\r\n```\r\nSo you can see that it is looking for 'Calibration Curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**Versions**\r\nPython 3.8.11\r\n\r\n<!--\r\nPlease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\nPycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1635405096000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1634052175000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a bug while trying to convert an experiment using the aim convert mlflow command with the experiment ID. The process failed with an error message. However, using the experiment name instead of the ID worked successfully. The expected behavior was to convert the experiment by ID. The user's environment included Aim Version 3.6, Python 3.8.1, pip3, and Ubuntu 20.04.3 LTS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1674",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":27.37,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":375.8113888889,
        "Challenge_title":"[BUG] some types plot types are not getting saved to the MLFlow experiment artifacts dir",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":268,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@ejohnson-amerilife Thank you so much for bringing this up. Would you like to submit a PR for this? ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.2,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.9523508333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am learning about Kubernetes online endpoints and my objective is to monitor my resources which are deployed using Kubernetes endpoints. Is there any provision to get out-of-the-box monitoring to Kubernetes online endpoints to check the performance. I am new to this domain. Any help is appreciated.<\/p>",
        "Challenge_closed_time":1652337837120,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652334408657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to monitor resources deployed using Kubernetes online endpoints and is looking for out-of-the-box monitoring options. They are new to this domain and are seeking assistance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72210628",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.9523508333,
        "Challenge_title":"How to manage out of the box monitoring using Kubernetes online endpoints",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652333709927,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>The general monitoring is available and supportive in AKS, but the out of the box implementation was not supportive unfortunately. Check the below documentation and screenshot to refer supportive formats of monitoring in AKS<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0uxW5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0uxW5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.8,
        "Solution_reading_time":8.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5089333333,
        "Challenge_answer_count":7,
        "Challenge_body":"<p>When I finished the training with the offline mode, I use  the following command to upload the trained results to the cloud service.<\/p>\n<pre><code class=\"lang-auto\">wandb  sync   MY_RUN_DIRECTORY\n<\/code><\/pre>\n<p>But I got the KeyError: \u2018run_url\u2019<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044.jpeg\" data-download-href=\"\/uploads\/short-url\/oQmDb7gGEFmjb7DrMnTKtSiKweM.jpeg?dl=1\" title=\"Screenshot 2023-04-24 202643\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_690x240.jpeg\" alt=\"Screenshot 2023-04-24 202643\" data-base62-sha1=\"oQmDb7gGEFmjb7DrMnTKtSiKweM\" width=\"690\" height=\"240\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_690x240.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_1035x360.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae1f879a48417b9728aa910bc9e8bb757627f044_2_1380x480.jpeg 2x\" data-dominant-color=\"181818\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2023-04-24 202643<\/span><span class=\"informations\">1396\u00d7487 192 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>How to solve this question?<\/p>",
        "Challenge_closed_time":1682341145608,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682339313448,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a KeyError: 'run_url' while trying to upload trained results to a cloud service using the 'wandb sync' command after finishing training in offline mode. The user is seeking a solution to this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/sync-error\/4267",
        "Challenge_link_count":5,
        "Challenge_participation_count":7,
        "Challenge_readability":27.6,
        "Challenge_reading_time":23.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.5089333333,
        "Challenge_title":"Sync error",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lee086824\">@lee086824<\/a> thanks for reporting this issue. There was a regression in wandb <code>v0.14.1<\/code> that would throw this <code>KeyError: 'run_url'<\/code>. Is this your current version, and if so could you please upgrade to our most recent client\/SDK version and try to sync your runs again? Would it work for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.0,
        "Solution_reading_time":4.6,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1569518464147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation_count":432.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":0.1342155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Pip installation is stuck in an infinite loop if there are unresolvable conflicts in dependencies. To reproduce, <code>pip==20.3.0<\/code> and:<\/p>\n<pre><code>pip install pyarrow==2.0.0 azureml-defaults==1.18.0\n<\/code><\/pre>",
        "Challenge_closed_time":1606928528563,
        "Challenge_comment_count":6,
        "Challenge_created_time":1606928045387,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with pip installation getting stuck in an infinite loop when there are unresolvable conflicts in dependencies. The issue can be reproduced by installing pyarrow==2.0.0 and azureml-defaults==1.18.0 with pip version 20.3.0.",
        "Challenge_last_edit_time":1608739608112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65112585",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":11.3,
        "Challenge_reading_time":4.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":13.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1342155556,
        "Challenge_title":"Pip installation stuck in infinite loop if unresolvable conflicts in dependencies",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2146.0,
        "Challenge_word_count":34,
        "Platform":"Stack Overflow",
        "Poster_created_time":1568658032912,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Redmond, WA, USA",
        "Poster_reputation_count":133.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>Workarounds:<\/p>\n<p>Local environment:\nDowngrade pip to &lt; 20.3<\/p>\n<p>Conda environment created from yaml:\nThis will be seen only if conda-forge is highest priority channel, anaconda channel doesn't have pip 20.3 (as of now). To mitigate the issue please explicitly specify pip&lt;20.3 (!=20.3 or =20.2.4 pin to other version) as a conda dependency in the conda specification file<\/p>\n<p>AzureML experimentation:\nFollow the case above to make sure pinned pip resulted as a conda dependency in the environment object, either from yml file or programmatically<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.0,
        "Solution_reading_time":7.12,
        "Solution_score_count":12.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.3988888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Challenge_closed_time":1606993756000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606945520000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing difficulty in opening a terminal on an m5.8xlarge instance in SageMaker Studio, as the terminal only shows 2 CPUs instead of the expected 32. They are seeking guidance on how to open a terminal on the correct instance.",
        "Challenge_last_edit_time":1668512453971,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sagemaker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.6,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":13.3988888889,
        "Challenge_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1054.0,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Where do you launch the terminal from? If you use the launcher window, it would start on the t2.medium as you are experiencing.\n\nHowever, if you use the **launch terminal button** in the toolbar that is displayed at the top of your notebook, it will launch the image terminal on the new instance the notebook's kernel is running on (your m5.8xlarge instance).",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1610011918846,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":4.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.0516666667,
        "Challenge_answer_count":1,
        "Challenge_body":"How does one choose or tune the hardware backend of a Sagemaker Studio Notebook?",
        "Challenge_closed_time":1576708404000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1576675818000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to select or adjust the hardware configuration of a SageMaker Studio Notebook.",
        "Challenge_last_edit_time":1667955285675,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUp5wKTB0URcCPyBgUcAWMww\/how-to-tune-sagemaker-studio-notebooks-hardware-config",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":1.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":9.0516666667,
        "Challenge_title":"How to tune SageMaker Studio Notebooks hardware config?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":21,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"At the top right of a notebook (near the kernel ) there will be a resource configurations button, you'll be able to choose the instance you want to run the notebook on.\n\nA nice feature of that is that all the instances shares the same EFS mount (SageMaker studio uses EFS for notebook storage) if you save a dataframe to the local disk (EFS) you can change instance type during your work and continue from the place you've been in (Move from a GPU instance to a CPU instance for cost effectiveness \/ back to GPU for performance)",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925589624,
        "Solution_link_count":0.0,
        "Solution_readability":18.5,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":96.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1502815666600,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Memphis, TN, USA",
        "Answerer_reputation_count":5028.0,
        "Answerer_view_count":957.0,
        "Challenge_adjusted_solved_time":0.08215,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm working on a Window 10 machine and trying to pip install mlflow but I'm getting the following error message.<\/p>\n\n<pre><code>THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\nmlflow from https:\/\/files.pythonhosted.org\/packages\/01\/ec\/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270\/mlflow-1.0.0-py3-none-any.whl#sha256=0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4:\n    Expected sha256 0f2f116a377b9da538642eaf688caa0a7166ee1ede30c8734830eb9e789574b4\n         Got        eb34ea16ecfe02d474ce50fd1f88aba82d56dcce9e8fdd30193ab39edf32ac9e\n<\/code><\/pre>",
        "Challenge_closed_time":1560545869140,
        "Challenge_comment_count":1,
        "Challenge_created_time":1560545573400,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install mlflow using pip install on a Windows 10 machine. The error message indicates that the packages do not match the hashes from the requirements file, and suggests that someone may have tampered with them.",
        "Challenge_last_edit_time":1560799785056,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56604989",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":10.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.08215,
        "Challenge_title":"How to install mlflow using pip install",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":365.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1355343131932,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5655.0,
        "Poster_view_count":629.0,
        "Solution_body":"<p>It is trying to check cache for packages. They were likely compiled in linux or some other OS and you are trying to install them in Windows.<\/p>\n\n<p>This should fix your issue:<\/p>\n\n<pre><code>pip install --no-cache-dir mlflow\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":3.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1456986606312,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":757.0,
        "Answerer_view_count":80.0,
        "Challenge_adjusted_solved_time":7933.4568986111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>There's probably something very obvious I'm missing or Sagemaker just doesn't support these kinds of extensions, but I've been trying to enable toc2 (Table of Contents) jupyter extension for my Sagemaker notebook via lifecycle configurations, but for whatever reason it still isn't showing up.<\/p>\n\n<p>I built my script out combining a sample AWS script and a quick article on the usual ways of enabling extensions:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh<\/a><\/p>\n\n<p><a href=\"https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231<\/a><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;EOF\n\n--Activate notebook environment\nsource activate JupyterSystemEnv\n\n--Install extensions\npip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib\nnbextension install\njupyter nbextension enable toc2 --py --sys-prefix\n\nsource deactivate\n\n\nEOF\n<\/code><\/pre>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1610748471692,
        "Challenge_comment_count":2,
        "Challenge_created_time":1586792396013,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to install the toc2 (Table of Contents) Jupyter extension for their AWS Sagemaker notebook via lifecycle configurations, despite following a sample AWS script and an article on enabling extensions. The user is unsure if Sagemaker supports these types of extensions.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61191412",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":20.5,
        "Challenge_reading_time":18.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6654.4654663889,
        "Challenge_title":"Unable to install toc2 notebook extension for AWS Sagemaker Instance (Lifecycle Configurations)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1027.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456986606312,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":757.0,
        "Poster_view_count":80.0,
        "Solution_body":"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user<\/code> to copy the JS\/CSS files into Jupyter's search directory and some config updates (<a href=\"https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\" rel=\"nofollow noreferrer\">https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions<\/a>).<\/p>\n<p>Corrected statement<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable toc2\/main\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n\nEOF\n\n##Below may be unnecessary, but other user needed to run to see success\ninitctl restart jupyter-server --no-wait\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1615352840848,
        "Solution_link_count":2.0,
        "Solution_readability":18.1,
        "Solution_reading_time":11.02,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":13.7880555556,
        "Challenge_answer_count":1,
        "Challenge_body":"hi there,  \n  \nI am setting up a jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. But it shows the following error messages.  \n\": com.microsoft.sqlserver.jdbc.SQLServerException: The TCP\/IP connection to the host xxx.xxx.xxx.xxx, port 21000 has failed. Error: \"connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP\/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\"  \n  \nI used the exactly the same VPC, subnet, security group as what i used in a glue job to extract the data from the local db. While the glue job works but the SageMaker notebook failed. I am sure the firewalls are opened.  \nCould anyone tell me how to solve it?  \nI also came across the following articles, but i am not sure if it is the root cause.  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/",
        "Challenge_closed_time":1557180251000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557130614000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while setting up a Jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. The error message shows that the TCP\/IP connection to the host has failed, and the user is unsure about the root cause. The user has tried using the same VPC, subnet, and security group as used in a glue job, which works, but the SageMaker notebook failed. The user is seeking help to solve the issue.",
        "Challenge_last_edit_time":1668629494374,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUUUm8LxKZTzixOCI_IovK1A\/sagemaker-notebook-instance-in-vpc-failed-to-connect-to-local-database",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.9,
        "Challenge_reading_time":13.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":13.7880555556,
        "Challenge_title":"SageMaker notebook instance in VPC failed to connect to local database",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":547.0,
        "Challenge_word_count":166,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,  \nThe principle here is that there much be network connectivity between the Notebook Instance and the DB Instance, and the security groups on the DB Instance should allow in-bound traffic from the Notebook Instance  \n  \nOne example of such as setup is  \n1. RDS DB Instance is VPC vpc-a and Subnet subnet-b.  \n2. SageMaker Notebook is launched in VPC vpc-a, Subnet subnet-b, with Security Group sg-c with DirectIntenetAccess \"Disabled\"  \n3. In the RDS DB Instance's Security Group rules, you can add an Inbound Rule to allow inbound traffic from the SageMaker Notebook security group \"sg-c\"  \n_-- Type - Protocol - Port Range - Source_  \n_-- MYSQL\/Aurora - TCP - 3306 - sg-c_  \n  \n  \n-----  \nSample Code:\n\n```\n! pip install mysql-connector\r\n\r\nimport mysql.connector\r\nmydb = mysql.connector.connect(\r\nhost=\"$RDS_ENDPOINT\",\r\nuser=\"$RDS_USERNAME\",\r\npasswd=\"$RDS_PASSWORD\"\r\n)\r\ncursor = mydb.cursor()\r\ncursor.execute(\"SHOW DATABASES\") \n```\n\nThanks for using Amazon SageMaker and let us know if there's anything else we can help with!  \n  \nEdited by: JaipreetS-AWS on May 6, 2019 3:04 PM",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1557180305000,
        "Solution_link_count":0.0,
        "Solution_readability":10.9,
        "Solution_reading_time":12.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":148.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341161196310,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tokyo",
        "Answerer_reputation_count":1057.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When running <code>kedro install<\/code> I get the following error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>Attempting uninstall: terminado\n    Found existing installation: terminado 0.8.3\nERROR: Cannot uninstall 'terminado'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n<\/code><\/pre>\n<p>This github <a href=\"https:\/\/github.com\/jupyter\/notebook\/issues\/4543\" rel=\"nofollow noreferrer\">issue<\/a> suggests the following fix:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install terminado --user --ignore-installed\n<\/code><\/pre>\n<p>But it does not work for me as I keep having the same error.<\/p>\n<p><strong>Note:<\/strong>\nThis question is similar to <a href=\"https:\/\/stackoverflow.com\/questions\/61770369\/docker-ubuntu-20-04-cannot-uninstall-terminado-and-problems-with-pip\">this<\/a> but different enough that I think it is worth asking separately.<\/p>",
        "Challenge_closed_time":1605936149736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605936149737,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running 'kedro install' due to the inability to uninstall 'terminado', which is a distutils installed project. The suggested fix of 'pip install terminado --user --ignore-installed' did not work for the user.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64940102",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":14.3,
        "Challenge_reading_time":13.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro install - Cannot uninstall `terminado`",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2331.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1341161196310,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tokyo",
        "Poster_reputation_count":1057.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>The problem is the version that the kedro template project requires see <code>src\/requiremetns.txt<\/code><\/p>\n<p>In my project it is <code>terminado==0.9.1<\/code>, hence the following solves the problem:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install terminado==0.9.1  --user --ignore-installed\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":4.33,
        "Solution_score_count":10.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1546431264350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Norway",
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":55.8339541667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We are trying to develop an MLflow pipeline. We have our developing environment in a series of dockers (no local python environment &quot;whatsoever&quot;). This means that we have set up a docker container with MLflow and all requirements necessary to run pipelines. The issue we have is that when we write our MLflow project file we need to use &quot;docker_env&quot; to specify the environment. This figure illustrates what we want to achieve:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1tLuw.jpg\" rel=\"nofollow noreferrer\">MLflow run dind<\/a><\/p>\n<p>MLflow inside the docker needs to access the docker daemon\/service so that it can either use the &quot;docker-image&quot; in the MLflow project file or pull it from docker hub. We are aware of the possibility of using &quot;conda_env&quot; in the MLflow project file but wish to avoid this.<\/p>\n<p>Our question is,<\/p>\n<p>Do we need to set some sort of &quot;docker in docker&quot; solution to achieve our goal?<\/p>\n<p>Is it possible to set up the docker container in which MLflow is running so that it can access the &quot;host machine&quot; docker daemon?<\/p>\n<p>I have been all over Google and MLflow's documentation but I can seem to find anything that can guide us. Thanks a lot in advance for any help or pointers!<\/p>",
        "Challenge_closed_time":1640385689187,
        "Challenge_comment_count":4,
        "Challenge_created_time":1640160788793,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to develop an MLflow pipeline within a docker container and needs to use \"docker_env\" to specify the environment in the MLflow project file. However, MLflow inside the docker needs to access the docker daemon\/service to use the \"docker-image\" in the project file or pull it from docker hub. The user is wondering if they need to set up a \"docker in docker\" solution or if it's possible to set up the docker container to access the host machine's docker daemon.",
        "Challenge_last_edit_time":1640184686952,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70445997",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":9.0,
        "Challenge_reading_time":17.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":62.4723316667,
        "Challenge_title":"MLflow run within a docker container - Running with \"docker_env\" in MLflow project file",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":779.0,
        "Challenge_word_count":211,
        "Platform":"Stack Overflow",
        "Poster_created_time":1546431264350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I managed to create my pipeline using docker and docker_env in MLflow. It is not necessary to run d-in-d, the &quot;sibling approach&quot; is sufficient. This approach is described here:<\/p>\n<p><a href=\"https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/\" rel=\"nofollow noreferrer\">https:\/\/jpetazzo.github.io\/2015\/09\/03\/do-not-use-docker-in-docker-for-ci\/<\/a><\/p>\n<p>and it is the preferred method to avoid d-in-d.<\/p>\n<p>One needs to be very careful when mounting volumes within the primary and secondary docker environments: all volume mounts happen in the host machine.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":7.87,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.6138888889,
        "Challenge_answer_count":0,
        "Challenge_body":"as the title suggests\r\n",
        "Challenge_closed_time":1641229646000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1641220236000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to configure their profile with AWS CLI for using AWS Built-in sagemaker algorithms. They are encountering a ValueError that requires them to set up local AWS configuration with a region supported by SageMaker. The user is unsure if it is possible to link access AWS resources in Studiolab.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/38",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.0,
        "Challenge_reading_time":0.98,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.6138888889,
        "Challenge_title":"I just wonder if i can initialize my sagemaker studio lab? ",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":15,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello - if you have any specific technical issues please use our issue template here to describe it in detail. \r\n\r\nhttps:\/\/github.com\/aws\/studio-lab-examples\/issues\/new?assignees=&labels=bug&template=bug-report-for-sagemaker-studio-lab.md&title=\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":26.4,
        "Solution_reading_time":3.25,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1263294862568,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":183045.0,
        "Answerer_view_count":13691.0,
        "Challenge_adjusted_solved_time":0.4228347222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Challenge_closed_time":1618568305232,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618566783027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is triggering a Step Function execution via a Python cell in a SageMaker Notebook and wants to automate the process of checking if the execution has been completed before starting post-analysis custom logic. The user is currently manually executing a cell to check the status of the execution and wants to know if there is a simpler\/more accurate way to automate this process without creating any new AWS resource. The user cannot make any changes to MyPackage and the Step Function definition.",
        "Challenge_last_edit_time":1618585069787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":25.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.4228347222,
        "Challenge_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1216.0,
        "Challenge_word_count":251,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369257942212,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":70285.0,
        "Poster_view_count":13121.0,
        "Solution_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":10.19,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1538134316367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Ahmedabad, Gujarat, India",
        "Answerer_reputation_count":1009.0,
        "Answerer_view_count":102.0,
        "Challenge_adjusted_solved_time":0.7625188889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to dockerize <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">mlflow<\/a> with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP).<\/p>\n<p>Before deploying anything to GCP however, I wanted to get a local deployment working. I found <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">this<\/a> guide that details the process of setting up the environment. Having followed the guide (excluding the SQL part), I can see the  mlflow UI on <code>localhost:80<\/code> as nginx redirects traffic on port 80 to 5000. To add authentication, I found <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-set-up-password-authentication-with-nginx-on-ubuntu-14-04\" rel=\"nofollow noreferrer\">here<\/a> that I can do it using <code>sudo htpasswd -c .htpasswd &lt;username&gt;<\/code> in the <code>etc\/nginx\/<\/code> directory and then adding<\/p>\n<pre><code>location \\ {\n   auth_basic &quot;Private Property&quot;;\n   auth_basic_user_file .htpasswd;\n}\n<\/code><\/pre>\n<p>to the <code>nginx.conf<\/code> (or <code>mlflow.conf<\/code> in this case) to make it appear online. Trouble is, when I go to <code>localhost:80<\/code> <em>now<\/em> and enter in my username\/password, I continue to see<\/p>\n<pre><code>[error] 6#6: *1 open() &quot;\/etc\/nginx\/.htpasswd&quot; failed (2: No such file or directory)\n<\/code><\/pre>\n<p>in the <code>docker-compose up<\/code> logs as they are printed to the terminal, and as such <em>I'm not able to see the mlflow UI<\/em> on <code>localhost:80<\/code> (either a blank screen or nginx 403 error).<\/p>\n<p>Now, I've looked at several other posts (such as <a href=\"https:\/\/stackoverflow.com\/questions\/2010677\/nginx-and-auth-basic\">this one<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/16510374\/403-forbidden-nginx-using-correct-credentials\">this one<\/a>) and it seems to me that nginx doesn't have the right permissions to read the <code>.htpasswd<\/code> in the <code>etc\/nginx\/<\/code> directory file or that the path of the file isn't correct, i.e. the path has to be in reference to the <code>nginx.conf<\/code> file.<\/p>\n<p>Even though I made these corrections to the above towards-data-science files, the problem still persists.  I've been stuck for a while on this. Any particular reasons why this may be happening?<\/p>\n<p>Edit:\nHere is my directory structure in case it may help:<\/p>\n<pre><code>mlflow-docker\/:\n  mlflow\/:\n    Dockerfile\n  nginx\/:\n    Dockerfile\n    mlflow.conf\n    nginx.conf\n  docker-compose.yml\n<\/code><\/pre>",
        "Challenge_closed_time":1601618748112,
        "Challenge_comment_count":2,
        "Challenge_created_time":1601594496277,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to dockerize mlflow with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP). They followed a guide to set up the environment and added authentication using htpasswd, but they are unable to see the mlflow UI on localhost:80 due to an error that says nginx doesn't have the right permissions to read the .htpasswd file in the etc\/nginx\/ directory or the path of the file isn't correct. The user has made corrections to the files, but the problem still persists.",
        "Challenge_last_edit_time":1601616621328,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64164367",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":34.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":6.7366208333,
        "Challenge_title":"Nginx authentication issues when building mlflow through docker-compose",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":841.0,
        "Challenge_word_count":305,
        "Platform":"Stack Overflow",
        "Poster_created_time":1524443788580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>You need to add the .htpasswd file inside your container's file system.<\/p>\n<p>Generate the password file in your project's nginx folder.<\/p>\n<pre><code>sudo htpasswd -c .htpasswd sammy\n<\/code><\/pre>\n<p>Copy the password file to the nginx container's directory. Add following line in nginx dockerfile.<\/p>\n<pre><code>COPY .htpasswd \/etc\/nginx\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1601619366396,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":4.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":46.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1639972620503,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Challenge_adjusted_solved_time":107.9469136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to GCP's Vertex AI and suspect I am running into an error from my lack of experience, but Googling the answer has brought me no fruitful information.<\/p>\n<p>I created a Jupyter Notebook in AI Platform but wanted to schedule it to run at a set period of time. So I was hoping to use Vertex AI's Execute function. At first when I tried accessing Vertex I was unable to do so because the API had not been enabled in GCP. My IT team then enabled the Vertex AI API and I can now utilize Vertex. Here is a picture showing it is enabled. <a href=\"https:\/\/i.stack.imgur.com\/pUSRO.png\" rel=\"nofollow noreferrer\">Enabled API Picture<\/a><\/p>\n<p>I uploaded my notebook to a JupyterLab instance in Vertex, and when I click on the Execute button, I get an <a href=\"https:\/\/i.stack.imgur.com\/jnUDv.png\" rel=\"nofollow noreferrer\">error message<\/a> saying I need to &quot;Enable necessary APIs&quot;, specifically for Vertex AI API. I'm not sure why this is considering it's already been enabled. I try to click Enable, but it just spins and spins, and then I can only get out of it by closing or reloading the tab.<\/p>\n<p>One other thing I want to call out in case it's a settings issue is that currently my <a href=\"https:\/\/i.stack.imgur.com\/6UxKb.png\" rel=\"nofollow noreferrer\">Managed Notebooks tab says &quot;PREVIEW&quot;<\/a> in the Workbench. I started thinking maybe this was an indicator that there was a separate feature that needed to be enabled to use Managed Notebooks (which is where I can access the Execute button from). When I click on the User-Managed Notebooks and open JupyterLab from there, I don't have the Execute button.<\/p>\n<p>The GCP account I'm using does have billing enabled.<\/p>\n<p>Can anyone point me in the right direction to getting the Execute button to work?<\/p>",
        "Challenge_closed_time":1648544476136,
        "Challenge_comment_count":8,
        "Challenge_created_time":1648067877907,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with GCP Vertex AI where they are unable to use the Execute function even though the Vertex AI API has been enabled. When they try to enable the necessary APIs, the process keeps spinning and they are unable to proceed. Additionally, the user is unsure if the \"PREVIEW\" status in the Managed Notebooks tab is related to the issue. The user is seeking guidance to resolve the issue.",
        "Challenge_last_edit_time":1648155867247,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71593747",
        "Challenge_link_count":3,
        "Challenge_participation_count":9,
        "Challenge_readability":9.0,
        "Challenge_reading_time":22.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":132.3883969444,
        "Challenge_title":"GCP Vertex AI \"Enable necessary APIs\" when already enabled",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":518.0,
        "Challenge_word_count":299,
        "Platform":"Stack Overflow",
        "Poster_created_time":1646671161350,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Based on @JamesS comments, the issue was solved by adding necessary permissions on his individual account since it is the account configured on OP's <code>Managed Notebook Instance<\/code> in which has an access mode of <code>Single user only<\/code>.<\/p>\n<p>Based on my testing when I tried to replicate the scenario, &quot;Enable necessary APIs&quot; message box will continue to show when the user has no <em>&quot;Vertex AI User&quot;<\/em> role assigned to it. And in conclusion of my testing, below are the minimum <strong>roles<\/strong> required when trying to create a <em>Scheduled run<\/em> on a <code>Managed Notebook Instance<\/code>.<\/p>\n<ul>\n<li><strong>Notebook Admin<\/strong> - For access of the notebook instance and open it through Jupyter. User will be able to run written codes in the Notebook as well.<\/li>\n<li><strong>Vertex AI User<\/strong> - So that the user can <strong>create schedule run<\/strong> on the notebook instance since the creation of the scheduled run is under the Vertex AI API itself.<\/li>\n<li><strong>Storage Admin<\/strong> - Creation of scheduled run will require a Google Cloud Storage bucket location where the job will be saved<\/li>\n<\/ul>\n<p>Posting the answer as <em>community wiki<\/em> for the benefit of the community that might encounter this use case in the future.<\/p>\n<p>Feel free to edit this answer for additional information.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":17.32,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":200.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1476175026763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1241.0,
        "Answerer_view_count":68.0,
        "Challenge_adjusted_solved_time":0.9190636111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to building a docker container with mlflow server inside, with poetry toml file for dependency.(the two toml are exactly the same, it was just a way to try  to figure out)<br>\ntree:<\/p>\n\n<p>\u251c\u2500\u2500 docker-entrypoint.sh <br>\n\u251c\u2500\u2500 Dockerfile<br>\n\u251c\u2500\u2500 files<br>\n\u2502   \u2514\u2500\u2500 pyproject.toml<br>\n\u251c\u2500\u2500 git.sh<br>\n\u251c\u2500\u2500 pyproject.toml<br>\n\u2514\u2500\u2500 README.md<br><\/p>\n\n<p>as you can see, my toml file is next to Dockerfile <code>COPY pyproject.toml .\/<\/code> don't work nevertheless<\/p>\n\n<p><strong>Dockerfile<\/strong><\/p>\n\n<pre><code>FROM python:3.6.10-alpine3.10 as base\nLABEL maintainer=\"\"\n\nENV PYTHONFAULTHANDLER 1 \nENV    PYTHONHASHSEED random \nENV    PYTHONUNBUFFERED 1\n\nENV MLFLOW_HOME .\/ \nENV SERVER_PORT 5000   \nENV    MLFLOW_VERSION 0.7.0 \nENV    SERVER_HOST 0.0.0.0  \nENV    FILE_STORE ${MLFLOW_HOME}\/fileStore  \nENV    ARTIFACT_STORE ${MLFLOW_HOME}\/artifactStore \nENV PIP_DEFAULT_TIMEOUT 100\nENV    PIP_DISABLE_PIP_VERSION_CHECK on\nENV    PIP_NO_CACHE_DIR  off \nENV    POETRY_VERSION  1.0.0 \n\nWORKDIR ${MLFLOW_HOME}\n\nFROM base as builder\n\nRUN apk update  \\\n    &amp;&amp; apk add --no-cache make gcc musl-dev python3-dev libffi-dev openssl-dev subversion\n#download project file from github  repo \nRUN    svn export https:\/\/github.com\/MChrys\/QuickSign\/trunk\/  \\\n    &amp;&amp; pip install poetry==${POETRY_VERSION} \\\n    &amp;&amp; mkdir -p ${FILE_STORE}  \\\n    &amp;&amp; mkdir -p ${ARTIFACT_STORE}\\\n    &amp;&amp; python -m venv \/venv\n\nCOPY  pyproject.toml .\/\nRUN poetry export -f requirements.txt | \/venv\/bin\/pip install -r  --allow-root-install \/dev\/stdin \n\nCOPY . .\nRUN poetry build &amp;&amp; \/venv\/bin\/pip install dist\/*.whl\n\nFROM base as final\n\nRUN apk add --no-cache libffi libpq\nCOPY --from=builder \/venv \/venv\nCOPY docker-entrypoint.sh .\/\n\nEXPOSE $SERVER_PORT\n\nVOLUME [\"${FILE_STORE}\", \"${ARTIFACT_STORE}\"]\n\nCMD [\".\/docker-entrypoint.sh\"]\n<\/code><\/pre>\n\n<p>the build command :<\/p>\n\n<pre><code>docker build - &lt; Dockerfile\n<\/code><\/pre>\n\n<p>I get this error  :<\/p>\n\n<pre><code>Step 21\/32 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder335195979\/pyproject.toml: no such file or   directory\n<\/code><\/pre>\n\n<p><strong>pyproject.toml<\/strong><\/p>\n\n<pre><code>requires = [\"poetry&gt;=1.0.0\", \"mlflow&gt;=0.7.0\", \"python&gt;=3.6\"]\nbuild-backend = \"poetry.masonry.api\"\n\n[tool.poetry]\nname = \"Sign\"\ndescription = \"\"\nversion = \"1.0.0\"\nreadme = \"README.md\"\nauthors = [\n  \"\"\n]\n\nlicense = \"MIT\"\n\n\n[tool.poetry.dependencies]\npython = \"3.6\"\nnumpy = \"1.14.3\"\nscipy = \"*\"\npandas = \"0.22.0\"\nscikit-learn = \"0.19.1\"\ncloudpickle = \"*\"\nmlflow =\"0.7.0\"\ntensorflow = \"^2.0.0\"\n\n\n[tool.poetry.dev-dependencies]\n\npylint = \"*\"\ndocker-compose = \"^1.25.0\"\ndocker-image-size-limit = \"^0.2.0\"\ntomlkit = \"^0.5.8\"\n\n<\/code><\/pre>\n\n<p><strong>docker-entrypoint.sh<\/strong><\/p>\n\n<pre><code>#!\/bin\/sh\n\nset -e\n\n. \/venv\/bin\/activate\n\nmlflow server \\\n    --file-store $FILE_STORE \\\n    --default-artifact-root $ARTIFACT_STORE \\\n    --host $SERVER_HOST \\\n    --port $SERVER_PORT\n<\/code><\/pre>\n\n<hr>\n\n<hr>\n\n<p>if i add <code>RUN pwd; ls<\/code> just befor the first <code>COPY<\/code> I obtain :<\/p>\n\n<pre><code>Step 20\/31 : RUN pwd; ls\n ---&gt; Running in e8ec36dd6ca8\n\/\nartifactStore\nbin\ndev\netc\nfileStore\nhome\nlib\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsrv\nsys\ntmp\ntrunk\nusr\nvar\nvenv\nRemoving intermediate container e8ec36dd6ca8\n ---&gt; d7bba641bd7c\nStep 21\/31 : COPY  pyproject.toml .\/\nCOPY failed: stat \/var\/lib\/docker\/tmp\/docker-builder392824737\/pyproject.toml: no such file or directory\n\n<\/code><\/pre>",
        "Challenge_closed_time":1577175350592,
        "Challenge_comment_count":5,
        "Challenge_created_time":1577166859160,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to build a Docker container with an mlflow server inside, with poetry toml file for dependency. The user has placed the toml file next to the Dockerfile, but the \"COPY pyproject.toml .\/\" command does not work and blocks the docker build. The user has tried to add \"RUN pwd; ls\" before the first \"COPY\" command, but it still fails.",
        "Challenge_last_edit_time":1577172041963,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59464404",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":10.4,
        "Challenge_reading_time":44.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":2.3587311111,
        "Challenge_title":"COPY files - next to Dockerfile - don't work and block docker build",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":940.0,
        "Challenge_word_count":358,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459625723203,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Grenoble, France",
        "Poster_reputation_count":56.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>Try \n<code>docker build -t test .<\/code><\/p>\n\n<p>instead of\n<code>docker build - &lt; Dockerfile<\/code><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":0.4295236111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently trying to open parquet files using Azure Jupyter Notebooks. I have tried both Python kernels (2 and 3).\nAfter the installation of <em>pyarrow<\/em> I can import the module only if the Python kernel is 2 (not working with Python 3)<\/p>\n\n<p>Here is what I've done so far (for clarity, I am not mentioning all my various attempts, such as using <em>conda<\/em> instead of <em>pip<\/em>, as it also failed):<\/p>\n\n<pre><code>!pip install --upgrade pip\n!pip install -I Cython==0.28.5\n!pip install pyarrow\n\nimport pandas  \nimport pyarrow\nimport pyarrow.parquet\n\n#so far, so good\n\nfilePath_parquet = \"foo.parquet\"\ntable_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n<\/code><\/pre>\n\n<p>This works well if I'm doing that off-line (using Spyder, Python v.3.7.0). But it fails using an Azure Notebook.<\/p>\n\n<pre><code> AttributeErrorTraceback (most recent call last)\n&lt;ipython-input-54-2739da3f2d20&gt; in &lt;module&gt;()\n      6 \n      7 #table_parquet_raw = pd.read_parquet(filePath_parquet, engine='pyarrow')\n----&gt; 8 table_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n\nAttributeError: 'module' object has no attribute 'read_parquet'\n<\/code><\/pre>\n\n<p>Any idea please?<\/p>\n\n<p>Thank you in advance !<\/p>\n\n<p>EDIT:<\/p>\n\n<p>Thank you very much for your reply Peter Pan !\nI have typed these  statements, here is what I got:<\/p>\n\n<p>1.<\/p>\n\n<pre><code>    print(pandas.__dict__)\n<\/code><\/pre>\n\n<p>=> read_parquet does not appear<\/p>\n\n<p>2.<\/p>\n\n<pre><code>    print(pandas.__file__)\n<\/code><\/pre>\n\n<p>=> I get:<\/p>\n\n<pre><code>    \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/pandas\/__init__.py\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li><p>import sys; print(sys.path) => I get:<\/p>\n\n<pre><code>['', '\/home\/nbuser\/anaconda3_23\/lib\/python34.zip',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/plat-linux',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/lib-dynload',\n'\/home\/nbuser\/.local\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/Sphinx-1.3.1-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/IPython\/extensions',\n'\/home\/nbuser\/.ipython']\n<\/code><\/pre><\/li>\n<\/ol>\n\n<p>Do you have any idea please ?<\/p>\n\n<p>EDIT 2:<\/p>\n\n<p>Dear @PeterPan, I have typed both <code>!conda update conda<\/code> and  <code>!conda update pandas<\/code> : when checking the Pandas version (<code>pandas.__version__<\/code>), it is still <code>0.19.2<\/code>.<\/p>\n\n<p>I have also tried with <code>!conda update pandas -y -f<\/code>, it returns:\n`Fetching package metadata ...........\nSolving package specifications: .<\/p>\n\n<p>Package plan for installation in environment \/home\/nbuser\/anaconda3_23:<\/p>\n\n<p>The following NEW packages will be INSTALLED:<\/p>\n\n<pre><code>pandas: 0.19.2-np111py34_1`\n<\/code><\/pre>\n\n<p>When typing:\n<code>!pip install --upgrade pandas<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Requirement already up-to-date: pandas in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\nRequirement already up-to-date: pytz&gt;=2011k in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: numpy&gt;=1.9.0 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: python-dateutil&gt;=2 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: six&gt;=1.5 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from python-dateutil&gt;=2-&gt;pandas)<\/code><\/p>\n\n<p>Finally, when typing:<\/p>\n\n<p><code>!pip install --upgrade pandas==0.24.0<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Collecting pandas==0.24.0\n  Could not find a version that satisfies the requirement pandas==0.24.0 (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0rc1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0rc2, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0rc1, 0.19.0, 0.19.1, 0.19.2, 0.20.0rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0rc1, 0.21.0, 0.21.1, 0.22.0)\nNo matching distribution found for pandas==0.24.0<\/code><\/p>\n\n<p>Therefore, my guess is that the problem comes from the way the packages are managed in Azure. Updating a package (here Pandas), should lead to an update to the latest version available, shouldn't it?<\/p>",
        "Challenge_closed_time":1545730275287,
        "Challenge_comment_count":0,
        "Challenge_created_time":1545322944093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to read \".parquet\" files in Azure Jupyter Notebook using Python 2 and 3 kernels. After installing pyarrow, the module can only be imported with Python 2 kernel and not with Python 3. The user has tried various attempts such as using conda instead of pip, but it failed. The user has also tried updating pandas, but it did not work. The user suspects that the problem may be due to the way packages are managed in Azure.",
        "Challenge_last_edit_time":1547188516027,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53872444",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":60.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":113.1475538889,
        "Challenge_title":"Cannot read \".parquet\" files in Azure Jupyter Notebook (Python 2 and 3)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1879.0,
        "Challenge_word_count":455,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545322329030,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I tried to reproduce your issue on my Azure Jupyter Notebook, but failed. There was no any issue for me without doing your two steps <code>!pip install --upgrade pip<\/code> &amp; <code>!pip install -I Cython==0.28.5<\/code> which I think not matter.<\/p>\n\n<p>Please run some codes below to check your import package <code>pandas<\/code> whether be correct.<\/p>\n\n<ol>\n<li>Run <code>print(pandas.__dict__)<\/code> to check whether has the description of <code>read_parquet<\/code> function in the output.<\/li>\n<li>Run <code>print(pandas.__file__)<\/code> to check whether you imported a different <code>pandas<\/code> package.<\/li>\n<li>Run <code>import sys; print(sys.path)<\/code> to check the order of paths whether there is a same named file or directory under these paths.<\/li>\n<\/ol>\n\n<p>If there is a same file or directory named <code>pandas<\/code>, you just need to rename it and restart your <code>ipynb<\/code> to re-run. It's a common issue which you can refer to these SO threads <a href=\"https:\/\/stackoverflow.com\/questions\/35341363\/attributeerror-module-object-has-no-attribute-reader\">AttributeError: &#39;module&#39; object has no attribute &#39;reader&#39;<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/36250353\/importing-installed-package-from-script-raises-attributeerror-module-has-no-at\">Importing installed package from script raises &quot;AttributeError: module has no attribute&quot; or &quot;ImportError: cannot import name&quot;<\/a>.<\/p>\n\n<p>In Other cases, please update your post for more details to let me know.<\/p>\n\n<hr>\n\n<p>The latest <code>pandas<\/code> version should be <code>0.23.4<\/code>, not <code>0.24.0<\/code>.<\/p>\n\n<p>I tried to find out the earliest version of <code>pandas<\/code> which support the <code>read_parquet<\/code> feature via search the function name <code>read_parquet<\/code> in the documents of different version from <code>0.19.2<\/code> to <code>0.23.3<\/code>. Then, I found <code>pandas<\/code> supports <code>read_parquet<\/code> feature after the version <code>0.21.1<\/code>, as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The new features shown in the <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/whatsnew.html\" rel=\"nofollow noreferrer\"><code>What's New<\/code><\/a> of version <code>0.21.1<\/code>\n<a href=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>According to your <code>EDIT 2<\/code> description, it seems that you are using Python 3.4 in Azure Jupyter Notebook. Not all <code>pandas<\/code> versions support Python 3.4 version.<\/p>\n\n<p>The versions <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.21.1<\/code><\/a> &amp; <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.22\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.22.0<\/code><\/a> offically support Python 2.7,3.5, and 3.6, as below.\n<a href=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the <a href=\"https:\/\/pypi.org\/project\/pandas\/\" rel=\"nofollow noreferrer\">PyPI page for <code>pandas<\/code><\/a> also requires the Python version as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6613J.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6613J.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So you can try to install the <code>pandas<\/code> versions <code>0.21.1<\/code> &amp; <code>0.22.0<\/code> in the current notebook of Python 3.4. if failed, please create a new notebook in Python <code>2.7<\/code> or <code>&gt;=3.5<\/code> to install <code>pandas<\/code> version <code>&gt;= 0.21.1<\/code> to use the function <code>read_parquet<\/code>.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1547190062312,
        "Solution_link_count":14.0,
        "Solution_readability":10.9,
        "Solution_reading_time":52.59,
        "Solution_score_count":1.0,
        "Solution_sentence_count":51.0,
        "Solution_word_count":383.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1290013527427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Berlin",
        "Answerer_reputation_count":15929.0,
        "Answerer_view_count":1202.0,
        "Challenge_adjusted_solved_time":1080.3870858333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. This works fine in other images I have used till now, but in this one, the command fails with a segfault:<\/p>\n<pre><code>htop \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop) \nSegmentation fault (core dumped)\n<\/code><\/pre>\n<p>More info :<\/p>\n<pre><code>htop --version\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop: \/opt\/conda\/lib\/libncursesw.so.6: no version information available (required by htop)\nhtop 2.2.0 - (C) 2004-2019 Hisham Muhammad\nReleased under the GNU GPL.\n<\/code><\/pre>",
        "Challenge_closed_time":1655459795692,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651570402183,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a segfault error while trying to launch the htop command in the Pytorch 1.10 - Python 3.8 CPU optimized AWS Sagemaker container. The error message indicates that there is no version information available for libncursesw.so.6, which is required by htop. The user has successfully used htop in other images before, but is facing issues with this particular image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72097417",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1080.3870858333,
        "Challenge_title":"Segfault using htop on AWS Sagemaker pytorch-1.10-cpu-py38 app",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":147.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Poster_created_time":1551431163127,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Paris, France",
        "Poster_reputation_count":494.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>I fixed this with<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code># Note: add sudo if needed:\nln -fs \/lib\/x86_64-linux-gnu\/libncursesw.so.6 \/opt\/conda\/lib\/libncursesw.so.6\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.0,
        "Solution_reading_time":2.61,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1658300099523,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":0.83368,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>l am running a notebook instance as usual.<\/p>\n<p>But suddenly something was wrong. The notebook failed to give out anything. So l use a simple line of code to test it, which returns the following result:\n<a href=\"https:\/\/i.stack.imgur.com\/o6h2t.png\" rel=\"nofollow noreferrer\">my test result<\/a><\/p>\n<p>The notebook even failed to run so simple a code, and the kernel is showing a 'busy' state.(right-up corner)<\/p>\n<p>Simultaneously I'm running another .py file through the terminal, which works well all along.<\/p>\n<p>Anyone gets an idea? Thanks for any possible reason told.<\/p>\n<p>instance <a href=\"https:\/\/i.stack.imgur.com\/AW09e.png\" rel=\"nofollow noreferrer\">instance<\/a><\/p>\n<p>My log console\n<a href=\"https:\/\/i.stack.imgur.com\/TxKhy.png\" rel=\"nofollow noreferrer\">log console<\/a><\/p>",
        "Challenge_closed_time":1659356687900,
        "Challenge_comment_count":7,
        "Challenge_created_time":1659266020643,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user's Sagemaker notebook instance failed to run any code, including a simple test code, and the kernel is stuck in a 'busy' state. The user is also running another .py file through the terminal, which is working fine. The user is seeking possible reasons for this issue.",
        "Challenge_last_edit_time":1659353686652,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73182806",
        "Challenge_link_count":3,
        "Challenge_participation_count":8,
        "Challenge_readability":7.7,
        "Challenge_reading_time":11.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":25.1853491667,
        "Challenge_title":"sagemaker notebook instance failed to run even a single line of code",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1658300099523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>if anyone encounters the condition in which the kernel is showing 'busy' and your can't run any code, just uninstall this kerenel and reinstall one.<\/p>\n<p>Using 'conda'-related command about which you can easily find information on the Internet<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":3.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":168.6647222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Challenge_closed_time":1657139268000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656532075000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to initialize the kedro-mlflow project as the CLI commands are not available. The user has tried to create a Kedro project using the starter `pandas-iris` and installing kedro-mlflow, but the `mlflow` command is unknown to Kedro inside the project folder. The user is seeking advice on how to fix this issue. The bug also happens with the last version on master.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.77,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":168.6647222222,
        "Challenge_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1407449881432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":228.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":732.1807,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Challenge_closed_time":1539818808056,
        "Challenge_comment_count":3,
        "Challenge_created_time":1538879533133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with AWS NoCredentials in training while attempting to run example code for Amazon Sagemaker on a local GPU. The script fails on m.fit in Docker container with the error \"Unable to locate credentials\". The user suspects that the issue has to do with passing the AWS credentials to the Docker container and is unsure if the AWS credentials should be passed in as environmental variables. The user has checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though they have an ~\/.aws\/config and ~\/.aws\/credentials file. The user has updated their sagemaker install and checked their .aws\/config and .aws\/credentials files, but the issue persists. The",
        "Challenge_last_edit_time":1539893383687,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Challenge_link_count":3,
        "Challenge_participation_count":7,
        "Challenge_readability":16.6,
        "Challenge_reading_time":96.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":63,
        "Challenge_solved_time":260.9097008333,
        "Challenge_title":"AWS NoCredentials in training",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1374.0,
        "Challenge_word_count":621,
        "Platform":"Stack Overflow",
        "Poster_created_time":1380496984176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gloucester, VA, USA",
        "Poster_reputation_count":544.0,
        "Poster_view_count":85.0,
        "Solution_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1542529234207,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":12.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.9785680556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Since the latest Azure ML release, we have been unable to submit any job using a private docker registry. Same jobs were working before the new release.  <br \/>\nWe configure the job as follows (all of this is automated and the code has not changed):<\/p>\n<p>base_image_name = 'REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_1601582281'<\/p>\n<pre><code># Set the container registry information  \n\nmyenv = Environment(name=&amp;#34;lb&amp;#34;)  \n\nmyenv.docker.enabled = True  \nmyenv.docker.base_image = base_image_name  \nmyenv.docker.base_image_registry.address = &amp;#39;REDACTED.azurecr.io\/lb\/&amp;#39;  \nmyenv.docker.base_image_registry.username, myenv.docker.base_image_registry.password = get_docker_secrets()  \nmyenv.python.user_managed_dependencies = True  \n\nmyenv.python.interpreter_path = &amp;#34;\/opt\/miniconda\/bin\/python&amp;#34;  \n<\/code><\/pre>\n<p>Instead of successful job submission, we are instead getting:  <br \/>\n{  <br \/>\n&quot;error&quot;: {  <br \/>\n&quot;message&quot;: &quot;Activity Failed:\\n{\\n \\&quot;error\\&quot;: {\\n \\&quot;code\\&quot;: \\&quot;UserError\\&quot;,\\n \\&quot;message\\&quot;: \\&quot;Unable to get image details : Specified base docker image REDACTED.azurecr.io\/lb\/learning_box_azure_compute:0.1.15_16\\&quot;,\\n \\&quot;details\\&quot;: []\\n },\\n \\&quot;correlation\\&quot;: {\\n \\&quot;operation\\&quot;: null,\\n \\&quot;request\\&quot;: \\&quot;c41448d429f9c80b\\&quot;\\n },\\n \\&quot;environment\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;location\\&quot;: \\&quot;eastus\\&quot;,\\n \\&quot;time\\&quot;: \\&quot;2020-11-09T21:40:39.699533Z\\&quot;,\\n \\&quot;componentName\\&quot;: \\&quot;execution-worker\\&quot;\\n}&quot;  <br \/>\n}  <br \/>\n}  <br \/>\nThe image has not changed (we tried a few different ones from prior successful jobs) and the use of the SDK has not changed.  <br \/>\nHas anybody else encountered a similar problem since the Nov 5 upgrade (<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes<\/a>)?  <br \/>\nThis is a major block as we cannot proceed with any project that depend on Azure ML at this time.<\/p>",
        "Challenge_closed_time":1605016029612,
        "Challenge_comment_count":4,
        "Challenge_created_time":1604958506767,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to submit any job using a private docker registry since the latest Azure ML release. The error message received is \"Unable to get image details\". The image and the use of the SDK have not changed. The user is seeking help from others who may have encountered a similar problem since the Nov 5 upgrade.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/157021\/unable-to-use-private-docker-registry-with-latest",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":14.5,
        "Challenge_reading_time":29.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15.9785680556,
        "Challenge_title":"Unable to use private docker registry with latest Azure ML release",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":191,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d02d6aeb-d51f-460e-9be5-e8da649952cc\">@Fabien Campagne  <\/a>  Thanks for the details, with fully qualified base image name you do not need to specify container registry address. container registry address itself should be just a host name.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":3.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":8.0431913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Azure ML, I'm trying to execute a Python module that needs to import the module pyxdameraulevenshtein (<a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>).<\/p>\n\n<p>I followed the usual way, which is to create a zip file and then import it; however for this specific module, it seems to never be able to find it. The error message is as usual:<\/p>\n\n<p><em>ImportError: No module named 'pyxdameraulevenshtein'<\/em><\/p>\n\n<p>Has anyone included this pyxdameraulevenshtein module in Azure ML with success ?<\/p>\n\n<p>(I took the package from <a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>.)<\/p>\n\n<p>Thanks for any help you can provide,<\/p>\n\n<p>PH<\/p>",
        "Challenge_closed_time":1496307870776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496235911927,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to execute a Python module that needs to import the module pyxdameraulevenshtein in Azure ML. Despite following the usual way of creating a zip file and importing it, the module seems to never be found, resulting in an ImportError. The user is seeking help to include this module in Azure ML successfully.",
        "Challenge_last_edit_time":1496278915287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44285641",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":11.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":19.9885691667,
        "Challenge_title":"Azure ML Python with Script Bundle cannot import module",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2395.0,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1496233557192,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I viewed the <code>pyxdameraulevenshtein<\/code> module page, there are two packages you can download which include a wheel file for MacOS and a source code tar file. I don't think you can directly use the both on Azure ML, because the MacOS one is just a share library <code>.so<\/code> file for darwin which is not compatible with Azure ML, and the other you need to first compile it.<\/p>\n\n<p>So my suggestion is as below for using <code>pyxdameraulevenshtein<\/code>.<\/p>\n\n<ol>\n<li>First, compile the source code of <code>pyxdameraulevenshtein<\/code> to a DLL file on Windows, please refer to the document for Python <a href=\"https:\/\/docs.python.org\/2\/extending\/windows.html\" rel=\"nofollow noreferrer\">2<\/a>\/<a href=\"https:\/\/docs.python.org\/3\/extending\/windows.html\" rel=\"nofollow noreferrer\">3<\/a> or search for doing this.<\/li>\n<li>Write a Python script using the DLL you compiled to implement your needs, please refer to the SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/252417\/how-can-i-use-a-dll-file-from-python\">How can I use a DLL file from Python?<\/a> for how to use DLL from Python and refer to the Azure offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts\" rel=\"nofollow noreferrer\">tutorial<\/a> to write your Python script<\/li>\n<li>Package your Python script and DLL file as a zip file, then to upload the zip file to use it in <code>Execute Python script<\/code> model of Azure ML.<\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.5,
        "Solution_reading_time":19.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":192.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1384530039387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Ljubljana, Slovenia",
        "Answerer_reputation_count":2470.0,
        "Answerer_view_count":285.0,
        "Challenge_adjusted_solved_time":18.7397944444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Docker-compose seems to have stopped working on Sagemaker Notebook instances. When running <code>docker-compose up<\/code> I encounter the following error:<\/p>\n<pre><code>During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/docker-compose&quot;, line 8, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 81, in main\n    command_func()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 200, in perform_command\n    project = project_from_options('.', options)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 70, in project_from_options\n    enabled_profiles=get_profiles_from_options(options, environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 153, in get_project\n    verbose=verbose, version=api_version, context=context, environment=environment\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 43, in get_client\n    environment=environment, tls_version=get_tls_version(environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 170, in docker_client\n    client = APIClient(use_ssh_client=not use_paramiko_ssh, **kwargs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 197, in __init__\n    self._version = self._retrieve_server_version()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 222, in _retrieve_server_version\n    'Error while fetching server API version: {0}'.format(e)\ndocker.errors.DockerException: Error while fetching server API version: Timeout value connect was Timeout(connect=60, read=60, total=None), but it must be an int, float or None\n<\/code><\/pre>\n<p>I can start Docker containers as usual.<\/p>\n<pre><code>sh-4.2$ docker version\nClient:\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.15.14\n Git commit:        f0df350\n Built:             Tue Sep 28 19:55:40 2021\n OS\/Arch:           linux\/amd64\n Context:           default\n Experimental:      true\n\nServer:\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.15.14\n  Git commit:       b0f5bc3\n  Built:            Tue Sep 28 19:57:35 2021\n  OS\/Arch:          linux\/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0\n  GitCommit:        %runc_commit\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n<\/code><\/pre>\n<p>But <code>docker-compose<\/code> wouldn't work...<\/p>\n<pre><code>sh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build unknown\ndocker-py version: 5.0.0\nCPython version: 3.6.13\nOpenSSL version: OpenSSL 1.1.1l  24 Aug 2021\n<\/code><\/pre>",
        "Challenge_closed_time":1636114943820,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636114943820,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with Docker-compose not working on Sagemaker Notebook instances. When running \"docker-compose up\", the user encounters a DockerException error related to the server API version timeout. However, the user is able to start Docker containers as usual.",
        "Challenge_last_edit_time":1636115811803,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69853177",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.8,
        "Challenge_reading_time":42.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":null,
        "Challenge_title":"Docker-compose wouldn't start on Sagemaker's Notebook instance",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":245,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384530039387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ljubljana, Slovenia",
        "Poster_reputation_count":2470.0,
        "Poster_view_count":285.0,
        "Solution_body":"<p>For those of you who (might) have encountered the same issue, here's the fix:<\/p>\n<p>1). Install the newest version of docker-compose:<\/p>\n<pre><code>sh-4.2$ sudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/download\/1.29.2\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsh-4.2$ sudo chmod +x \/usr\/local\/bin\/docker-compose\n<\/code><\/pre>\n<p>2). Change your <code>PATH<\/code> accordingly (since docker-compose is installed using <code>conda<\/code> and is picked up first) or use <code>\/usr\/local\/bin\/docker-compose<\/code> onwards:<\/p>\n<pre><code>sh-4.2$ PATH=\/usr\/local\/bin:$PATH\nsh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build 5becea4c\ndocker-py version: 5.0.0\nCPython version: 3.7.10\nOpenSSL version: OpenSSL 1.1.0l  10 Sep 2019\n<\/code><\/pre>\n<p>Perhaps, the issue is related to this:<\/p>\n<blockquote>\n<p>On August 9, 2021 the Jupyter Notebook and Jupyter Lab open source software projects announced 2 security concerns that could impact Amazon Sagemaker Notebook Instance customers.<\/p>\n<p>Sagemaker has deployed updates to address these concerns, and we recommend customers with existing notebook sessions to stop and restart their notebook instance(s) to benefit from these updates. Notebook instances launched after August 10, 2021, when updates were deployed, are not impacted by this issue and do not need to be restarted.<\/p>\n<\/blockquote>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1636183275063,
        "Solution_link_count":1.0,
        "Solution_readability":10.3,
        "Solution_reading_time":18.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":167.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1459625723203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Grenoble, France",
        "Answerer_reputation_count":56.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":2210.9752852778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My final purpose is to run experiment from  an Api.<\/p>\n\n<p>the experiment come from :\n<a href=\"https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples\/tensorflow\/tf2<\/a>\nbut export the file in my custom git where I clone it, in the image below -><\/p>\n\n<p>I have 2 images in my docker compose :\ntree project : <\/p>\n\n<pre><code>|_app\/\n| |_Dockerfile\n|\n|_mlflow\/\n| |_Dockerfile\n|\n|_docker-compose.yml\n\n<\/code><\/pre>\n\n<p>app\/Dockerfile<\/p>\n\n<pre><code>FROM continuumio\/anaconda3\n\nENV APP_HOME .\/\nWORKDIR ${APP_HOME}\nRUN conda config --append channels conda-forge\nRUN conda install --quiet --yes \\\n    'mlflow' \\\n    'psycopg2' \\\n    'tensorflow'\nRUN pip install pylint\nRUN pwd;ls \\\n&amp;&amp; git clone https:\/\/github.com\/MChrys\/QuickSign.git \nRUN pwd;ls \\\n    &amp;&amp; cd QuickSign \\\n    &amp;&amp; pwd;ls\n\nCOPY . .\n\n#RUN conda install jupyter \n#CMD jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root --no-browser\nCMD cd QuickSign &amp;&amp; mlflow run .\n<\/code><\/pre>\n\n<p>mlflow\/Dockerfile<\/p>\n\n<pre><code>FROM python:3.7.0\n\nRUN pip install mlflow\n\nRUN mkdir \/mlflow\/\n\nCMD mlflow server \\\n    --backend-store-uri \/mlflow \\\n    --host 0.0.0.0\n<\/code><\/pre>\n\n<p>docker-compose.yml<\/p>\n\n<pre><code>version: '3'\nservices:\n  notebook:\n    build:\n      context: .\/app\n    ports:\n      - \"8888:8888\"\n    depends_on: \n      - mlflow\n    environment: \n      MLFLOW_TRACKING_URI: 'http:\/\/mlflow:5000'\n  mlflow:\n    build:\n      context: .\/mlflow\n    expose: \n      - \"5000\"\n    ports:\n      - \"5000:5000\"\n<\/code><\/pre>\n\n<p>when I <code>docker-compose up<\/code> the image I obtain  :<\/p>\n\n<pre><code>notebook_1_74059cdc20ce |     response = requests.request(**kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/api.py\", line 60, in request\nnotebook_1_74059cdc20ce |     return session.request(method=method, url=url, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\nnotebook_1_74059cdc20ce |     resp = self.send(prep, **send_kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\nnotebook_1_74059cdc20ce |     r = adapter.send(request, **kwargs)\nnotebook_1_74059cdc20ce |   File \"\/opt\/conda\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\nnotebook_1_74059cdc20ce |     raise ConnectionError(e, request=request)\nnotebook_1_74059cdc20ce | requests.exceptions.ConnectionError: HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/create (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fd5db4edc50&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n\n<\/code><\/pre>\n\n<p>The problem look like that I run a project which is not found in the server images, as I run it in the app image, but I don't know how figure it out I have to trigger  the experiment from a futur flask app <\/p>",
        "Challenge_closed_time":1586436837590,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578475698630,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to connect their Mlflow server to their Mlflow project image. They have two images in their Docker compose and are trying to run an experiment from an API. The error message suggests that the project is not found in the server images, and the user is unsure how to trigger the experiment from a future Flask app.",
        "Challenge_last_edit_time":1578477326563,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59642900",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":11.0,
        "Challenge_reading_time":38.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":2211.4274888889,
        "Challenge_title":"Unable to connect Mlflow server to my mlflow project image",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2937.0,
        "Challenge_word_count":289,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459625723203,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Grenoble, France",
        "Poster_reputation_count":56.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>The problem came from  docker for windows, I was unable to make working docker compose on it but there are no problem to build it when I run it on virtual machine with ubuntu.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":2.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":34.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":39.8832972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to create a tabular dataset in a notebook with R kernel. The following code works with python kernel but how to do the same thing with R kernel ? Can anyone please help me ? Any help would be appreciated.     <\/p>\n<pre><code>from azureml.core import Workspace, Dataset  \n from azureml.core.dataset import Dataset  \n      \n subscription_id = 'abc'  \n resource_group = 'abcd'  \n workspace_name = 'xyz'  \n      \n workspace = Workspace(subscription_id, resource_group, workspace_name)  \n      \n dataset = Dataset.get_by_name(workspace, name='test')  \n      \n      \n # create tabular dataset from all parquet files in the directory  \n tabular_dataset_3 = Dataset.Tabular.from_parquet_files(path=(datastore,'\/UI\/09-17-2022_125003_UTC\/userdata1.parquet'))  \n<\/code><\/pre>",
        "Challenge_closed_time":1663571695407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663428115537,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking help to create a tabular dataset in a notebook with R kernel. They have provided a code that works with python kernel but are unsure how to do the same thing with R kernel.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1012184\/how-to-create-tabular-dataset-in-notebook-with-r-k",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":9.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":39.8832972222,
        "Challenge_title":"How to create tabular dataset in notebook with R kernel",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":85,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=0274aa41-1ea9-4fdc-8434-c9c13c43307c\">@Ankit19 Gupta  <\/a> The Azure Machine Learning SDK for R was deprecated at the end of 2021 to make way for an improved R training and deployment experience using Azure Machine Learning CLI 2.0    <br \/>\nPlease refer the azureml SDK <a href=\"https:\/\/github.com\/Azure\/azureml-sdk-for-r\">repo<\/a> for more details which was deprecated at the end of last year. You can use CLI to register the dataset using specification file.    <\/p>\n<pre><code>az ml dataset register [--file]  \n                       [--output-metadata-file]  \n                       [--path]  \n                       [--resource-group]  \n                       [--show-template]  \n                       [--skip-validation]  \n                       [--subscription-id]  \n                       [--workspace-name]  \n<\/code><\/pre>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.9,
        "Solution_reading_time":13.75,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":106.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":11.7562972223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running CI\/CD pipelines in Azure Dev Ops in order to deploy my machine learning workloads on components of a Azure Machine Learning Workspace (e.g. AzureML pipelines, endpoints, etc). I recently came across an issue in my pipelines and wanted to validate that the versions of the Azure CLI and Azure CLI ml extension didn't cause this issue.<\/p>\n<p>Because the Azure CLI is a tool that is developed in python, it is relatively easy to install an older version of the cli. I can just run <code>pip install azure-cli==2.44.1<\/code>. However, I'm having trouble figuring out how to install a specific version of the ml extension. According to this documentation, all extensions are packaged as python wheels, so theoretically if I had the URL of the build wheel, I could just target that. But I'm having trouble finding where the ml extension code is hosted. I found the pypi package for <a href=\"https:\/\/pypi.org\/project\/azure-cli-ml\/\">v1 of the extension<\/a>. Does anyone know where v2 is hosted?<\/p>\n<p>The version of the extension I would like to install is <code>2.13.0<\/code>.<\/p>",
        "Challenge_closed_time":1677220157550,
        "Challenge_comment_count":3,
        "Challenge_created_time":1677177834880,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges in installing an older version of the Azure CLI ml extension. While it is easy to install an older version of the Azure CLI, the user is having trouble finding the ml extension code for a specific version. The user is looking for version 2.13.0 of the ml extension.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1183753\/how-do-i-install-an-older-version-of-the-azure-cli",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":7.7,
        "Challenge_reading_time":14.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":11.7562972223,
        "Challenge_title":"How do I install an older version of the Azure CLI ml extension?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":186,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=1837dca2-1954-4413-9ba8-ffcc3afb6ce4\">Claire Salling<\/a> You can install a specific version of <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-cli?tabs=private\">Azure ML cli extension<\/a> using these commands.<\/p>\n<p>Remove any existing extensions of azure-cli-ml(v1) or ml(v2)<\/p>\n<p><code>az extension remove -n azure-cli-ml <\/code>  <br \/>\n<code>az extension remove -n ml<\/code><\/p>\n<p>Add the required version:<\/p>\n<p><code>az extension add --name ml --version 2.13.0<\/code><\/p>\n<p>List the extension to confirm the version.<\/p>\n<p><code>az extension list<\/code><\/p>\n<pre><code> {\n    &quot;experimental&quot;: false,\n    &quot;extensionType&quot;: &quot;whl&quot;,\n    &quot;name&quot;: &quot;ml&quot;,\n    &quot;path&quot;: &quot;C:\\\\Users\\\\user\\\\.azure\\\\cliextensions\\\\ml&quot;,\n    &quot;preview&quot;: false,\n    &quot;version&quot;: &quot;2.13.0&quot;\n  }\n<\/code><\/pre>\n<p>If this answers your query, do click <code>Accept Answer<\/code> and <code>Yes<\/code> for was this answer helpful. And, if you have any further query do let us know.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":14.98,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1370286859500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":16981.0,
        "Answerer_view_count":1733.0,
        "Challenge_adjusted_solved_time":21792.3181611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I need the sacred package for a new code base I downloaded. It requires sacred. \n<a href=\"https:\/\/pypi.python.org\/pypi\/sacred\" rel=\"noreferrer\">https:\/\/pypi.python.org\/pypi\/sacred<\/a><\/p>\n\n<p>conda install sacred fails with \nPackageNotFoundError: Package missing in current osx-64 channels: \n  - sacred<\/p>\n\n<p>The instruction on the package site only explains how to install with pip. What do you do in this case?<\/p>",
        "Challenge_closed_time":1501767830263,
        "Challenge_comment_count":2,
        "Challenge_created_time":1501710168027,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to install the 'sacred' package using conda and is receiving a 'PackageNotFoundError'. The user is seeking guidance on how to install the package in this scenario as the package's website only provides instructions for installation using pip.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45471477",
        "Challenge_link_count":2,
        "Challenge_participation_count":4,
        "Challenge_readability":9.2,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":16.0172877778,
        "Challenge_title":"python package can be installed by pip but not conda",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":7845.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1321905325720,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California, United States",
        "Poster_reputation_count":3278.0,
        "Poster_view_count":399.0,
        "Solution_body":"<p>That package is not available as a conda package at all. You can search for packages on anaconda.org: <a href=\"https:\/\/anaconda.org\/search?q=sacred\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=sacred<\/a> You can see the type of package in the 4th column. Other Python packages may be available as conda packages, for instance, NumPy: <a href=\"https:\/\/anaconda.org\/search?q=numpy\" rel=\"nofollow noreferrer\">https:\/\/anaconda.org\/search?q=numpy<\/a><\/p>\n\n<p>As you can see, the conda package numpy is available from a number of different channels (the channel is the name before the slash). If you wanted to install a package from a different channel, you can add the option to the install\/create command with the <code>-c<\/code>\/<code>--channel<\/code> option, or you can add the channel to your configuration <code>conda config --add channels channel-name<\/code>.<\/p>\n\n<p>If no conda package exists for a Python package, you can either install via pip (if available) or <a href=\"https:\/\/docs.conda.io\/projects\/conda-build\/en\/latest\/user-guide\/tutorials\/building-conda-packages.html\" rel=\"nofollow noreferrer\">build your own conda package<\/a>. This isn't usually too difficult to do for pure Python packages, especially if one can use <code>skeleton<\/code> to build a recipe from a package on PyPI.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1580162513407,
        "Solution_link_count":5.0,
        "Solution_readability":11.0,
        "Solution_reading_time":16.88,
        "Solution_score_count":5.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":163.0,
        "Tool":"Sacred"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":91.5109113889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>During the preparation for a training (in <code>prepare_data<\/code> in pytorch lightning) I either create or update local data (download, prepare different encodings). I then create a W&amp;B artifact and wait for the upload to be complete. Later in the code (in <code>setup()<\/code> in pytorch lightning) I use the data. Strictly speaking, this is not necessary, because I have the files locally, but I want to track the usage of the data (and the IDs of the data used for training, validation, \u2026). I added the <code>wait()<\/code> statement, because wandb would download the previous version (v=n-1) of the data \/without the enoding just added). In mode <code>ONLINE<\/code> this works nicely. However, in mode <code>DISABLED<\/code> I get this error: <code>ValueError: Cannot call wait on an artifact before it has been logged or in offline mode<\/code>. How am I supposed to handle <code>wait()<\/code>in order to have it work in all modes? (it would be nice if <code>wait()<\/code> would do it).<\/p>\n<p>This is the sample code:<\/p>\n<pre><code class=\"lang-python\"># Upload the data\nartifact = wandb.Artifact(name=..., type=...)\nartifact.description = ...\nartifact.metadata = ...\nartifact.add_file(local_path=...)\nwandb.run.log_artifact(artifact)\nartifact.save()  # I think I don't need this, playing around because of this issue\nartifact.wait()\n<\/code><\/pre>\n<pre><code class=\"lang-python\"># Use (Download) the data\nartifact = wandb.run.use_artifact(artifact_or_name=... + \":latest\")\nartifact_entry = artifact.get_path(...)\nartifact_entry.download(root=...)\n<\/code><\/pre>",
        "Challenge_closed_time":1655462923780,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655133484499,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the use of artifact.wait() in mode \"DISABLED\" while preparing for a training in PyTorch Lightning. The user wants to track the usage of data and IDs used for training, validation, etc. and added the wait() statement to ensure that the latest version of the data is downloaded. However, in mode \"DISABLED\", the user is getting a ValueError and is seeking a solution to handle wait() to make it work in all modes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/how-to-deal-with-artifact-wait-when-running-in-mode-disabled\/2607",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.1,
        "Challenge_reading_time":20.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":91.5109113889,
        "Challenge_title":"How to deal with artifact.wait() when running in mode \"DISABLED\"",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":121.0,
        "Challenge_word_count":210,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>, sorry about the late response. Runs have a <code>disabled<\/code> attribute. Here is a code snippet you can use:<\/p>\n<pre><code class=\"lang-auto\">run = wandb.init(mode=\"disabled\")\nif run.disabled:\n    \/\/ your code\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":3.6,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1428454496052,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":35.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":61.6148286111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created a docker container that is using Sagemaker via the java sdk. This container is deployed on a k8s cluster with several replicas. <\/p>\n\n<p>The container is doing simple requests to Sagemaker to list some models that we have trained and deployed. However we are now having issues with some java certificate. I am quite novice with k8s and certificates so I will appreciate if you could provide some help to fix the issue.<\/p>\n\n<p>Here are some traces from the log when it tries to list the endpoints:<\/p>\n\n<pre><code>org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:394)\n    at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:353)\n    at com.amazonaws.http.conn.ssl.SdkTLSSocketFactory.connectSocket(SdkTLSSocketFactory.java:132)\n    at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:141)\n    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n    at com.amazonaws.http.conn.$Proxy67.connect(Unknown Source)\n    at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n    at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1236)\n    at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1056)\n    ... 70 common frames omitted\nCaused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:397)\n    at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:302)\n    at sun.security.validator.Validator.validate(Validator.java:262)\n    at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324)\n    at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229)\n    at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:124)\n    at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1621)\n    ... 97 common frames omitted\nCaused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n    at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)\n    at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)\n    at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280)\n    at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:392)\n    ... 103 common frames omitted \n<\/code><\/pre>",
        "Challenge_closed_time":1544024416203,
        "Challenge_comment_count":2,
        "Challenge_created_time":1543802602820,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a docker container that uses Sagemaker via the Java SDK and deployed it on a k8s cluster with several replicas. However, the container is facing issues with a Java certificate when making simple requests to Sagemaker to list some models that have been trained and deployed. The user is seeking help to fix the issue. The log traces show that the issue is related to the inability to find a valid certification path to the requested target.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53586515",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":37.1,
        "Challenge_reading_time":51.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":61.6148286111,
        "Challenge_title":"Sagemaker certificate issue with Kubernetes",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":488.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1428454496052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>I think I have found the answer to my problem. I have set up another k8s cluster and deployed the container there as well. They are working fine and the certificate issues does not happen. When investigating more I have noticed that they were some issues with DNS resolution on the first k8s cluster. In fact the containers with certificate issues could not ping google.com for example.\nI fixed the DNS issue by not relying on core-dns and setting the DNS configuration in the deployment.yaml file. I am not sure to understand why exactly but this seems to have fixed the certificate issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":7.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":103.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1559888978768,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":569.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":21.5677077778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I created the following <code>environment.yml<\/code> file from my local Anaconda that contains an openjdk package.<\/p>\n<pre><code>name: venv\nchannels:\n  - defaults\ndependencies:\n  - openjdk=11.0.6\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6AlVr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6AlVr.png\" alt=\"Anaconda openjdk\" \/><\/a><\/p>\n<p>However, Azure Machine Learning couldn't install the openjdk package from the <code>environment.yml<\/code> file as module is not found.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gxuS6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gxuS6.png\" alt=\"ResolvePackageNotFound\" \/><\/a><\/p>\n<p>Backstory:<\/p>\n<p>I'm building a machine learning model using H2O.ai Python library. Unfortunately, H2O.ai is written in Java so it requires Java to run. I've installed openjdk to my local Anaconda venv for running H2O.ai locally - it runs perfectly. However, I couldn't deploy this model to Azure Machine Learning because it couldn't install openjdk from requirements.txt or environment.yml as module not found.<\/p>",
        "Challenge_closed_time":1619880273880,
        "Challenge_comment_count":2,
        "Challenge_created_time":1619503641653,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in installing the OpenJDK library in Azure Machine Learning. The user has created an environment.yml file containing the OpenJDK package, but Azure Machine Learning is unable to install it as the module is not found. The user needs OpenJDK to run H2O.ai, a machine learning library written in Java, and has successfully installed it in their local Anaconda environment. However, they are unable to deploy the model to Azure Machine Learning due to the installation issue.",
        "Challenge_last_edit_time":1619802630132,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67277764",
        "Challenge_link_count":4,
        "Challenge_participation_count":4,
        "Challenge_readability":10.6,
        "Challenge_reading_time":14.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":104.6200630556,
        "Challenge_title":"How to install OpenJDK library?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1389.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559888978768,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":569.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>Solution:<\/p>\n<p>Install openjdk through conda but specify conda-forge as the channel to install the package from.<\/p>\n<pre><code>name: venv\nchannels:\n  - defaults\n  - conda-forge\ndependencies:\n  - conda-forge::openjdk=11.0.9.1\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/AHCye.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AHCye.png\" alt=\"Conda Forge\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.0,
        "Solution_reading_time":5.21,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":32.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.724685,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I need some help trying to understand why I can't see any GIT options (left panel and top selection drop down menu) in my Azure machine learning JupyterLab.<\/p>\n<p>I did the following steps:<\/p>\n<pre><code> jupyter labextension install @jupyterlab\/git\n pip install --upgrade jupyterlab-git\n jupyter serverextension enable --py jupyterlab_git\n jupyter lab build\n<\/code><\/pre>\n<p>I've restarted my jupyterLab a couple of times, if I check the command:<\/p>\n<pre><code> jupyter labextension list\n<\/code><\/pre>\n<p>I get that @jupyterlab\/git v0,20,0 is enabled and ok.  <br \/>\nWhat am I doing wrong?<\/p>\n<p>Thank you in advance,  <br \/>\nCarla<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/12015-issue1.png?platform=QnA\" alt=\"12015-issue1.png\" \/><\/p>",
        "Challenge_closed_time":1594762360103,
        "Challenge_comment_count":5,
        "Challenge_created_time":1594716551237,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Azure Machine Learning and jupyterlab git extension as they are unable to see any GIT options in their JupyterLab despite following the necessary steps to install the extension. The user has restarted JupyterLab multiple times and confirmed that the extension is enabled, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/46614\/azure-machine-learning-and-jupyterlab-git-extensio",
        "Challenge_link_count":1,
        "Challenge_participation_count":9,
        "Challenge_readability":11.3,
        "Challenge_reading_time":10.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":12.724685,
        "Challenge_title":"Azure Machine Learning and jupyterlab git extension not working",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":103,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a>@ramr-msft<\/a> ,<\/p>\n<p>I did the steps mention in the link you gave me (<a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab-git\">https:\/\/github.com\/jupyterlab\/jupyterlab-git<\/a>) but still I can't open the Git extension from the Git tab on the left panel because it still doesn't exists.<\/p>\n<p>You mentioned we can still manage git repositories using the command line. Do you have any useful documentation on this approach?<\/p>\n<p>Once again, thank you in advance.  <br \/>\nCarla<\/p>\n",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":6.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554060427012,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2433.0,
        "Answerer_view_count":228.0,
        "Challenge_adjusted_solved_time":68.9593552778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have been using a jupyter notebook instance to spin up a training job (on separate instance) and deploy the endpoint (on another instance). I am using sagemaker tensorflow APIs for this as shown below:<\/p>\n<pre><code># create Tensorflow object and provide and entry point script\ntf_estimator = TensorFlow(entry_point='tf-train.py', role='SageMakerRole',\n                      train_instance_count=1, train_instance_type='ml.p2.xlarge',\n                      framework_version='1.12', py_version='py3')\n\n# train model on data on s3 and save model artifacts to s3\ntf_estimator.fit('s3:\/\/bucket\/path\/to\/training\/data')\n\n# deploy model on another instance using checkpoints saved on S3\npredictor = estimator.deploy(initial_instance_count=1,\n                         instance_type='ml.c5.xlarge',\n                         endpoint_type='tensorflow-serving')\n<\/code><\/pre>\n<p>I have been doing all of these steps through a jupyter notebook instance. What AWS services I can use to get rid off the dependency of jupyter notebook instance and automate these tasks of training and deploying the model in serverless fashion?<\/p>",
        "Challenge_closed_time":1595624761856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595377179513,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is currently using a Jupyter notebook instance to train and deploy a model using Sagemaker TensorFlow APIs. They are looking for a way to automate these tasks without the dependency on a Jupyter notebook instance and deploy the model in a serverless fashion.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63024900",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":14.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":68.7728730556,
        "Challenge_title":"How to train and deploy model in script mode on Sagemaker without using jupyter notebook instance (serverless)?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":758.0,
        "Challenge_word_count":138,
        "Platform":"Stack Overflow",
        "Poster_created_time":1378268842847,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, India",
        "Poster_reputation_count":4616.0,
        "Poster_view_count":592.0,
        "Solution_body":"<p>I recommend <code>AWS Step Functions<\/code>.  Been using it to schedule <code>SageMaker Batch Transform<\/code> and preprocessing jobs since it integrates with <code>CloudWatch<\/code> event rules.  It can also train models, perform hpo tuning, and integrates with <code>lambda<\/code>.  There is a SageMaker\/Step Functions SDK as well as you can use Step Functions directly by creating state machines. Some examples and documentation:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/11\/introducing-aws-step-functions-data-science-sdk-amazon-sagemaker\/<\/a><\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/connect-sagemaker.html<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1595625433192,
        "Solution_link_count":4.0,
        "Solution_readability":21.9,
        "Solution_reading_time":12.5,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":145.0984213889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What-If and Individual Conditional Expectation (ICE) plots are not supported in Azure Machine Learning studio under the Explanations tab since the uploaded explanation needs an active compute to recalculate predictions and probabilities of perturbed features. It is currently supported in Jupyter notebooks when run as a widget using the SDK. How can I open the automated ML explanation in Jupyter notebooks?<\/p>",
        "Challenge_closed_time":1614614318550,
        "Challenge_comment_count":5,
        "Challenge_created_time":1614091964233,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a challenge in opening the automated ML explanation in Jupyter notebooks. What-If and Individual Conditional Expectation (ICE) plots are not supported in Azure Machine Learning studio under the Explanations tab, but can be run as a widget using the SDK in Jupyter notebooks.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/285089\/how-can-i-open-the-automated-ml-explanation-in-jup",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":13.8,
        "Challenge_reading_time":6.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":145.0984213889,
        "Challenge_title":"How can I open the automated ML explanation in Jupyter notebooks?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello Cagatay,<\/p>\n<p>In jupyter notebook for AutoML models, you can download the trained model, then compute explanations locally and visualize the explanation results using ExplanationDashboard from interpret-community. Sample code below:-<\/p>\n<pre><code>best_run, fitted_model = remote_run.get_output()\n\nfrom azureml.train.automl.runtime.automl_explain_utilities import AutoMLExplainerSetupClass, automl_setup_model_explanations\nautoml_explainer_setup_obj = automl_setup_model_explanations(fitted_model, X=X_train,\n                                                                                                                         X_test=X_test, y=y_train,\n                                                                                                                         task='regression')\n\nfrom interpret.ext.glassbox import LGBMExplainableModel\nfrom azureml.interpret.mimic_wrapper import MimicWrapper\nexplainer = MimicWrapper(ws, automl_explainer_setup_obj.automl_estimator, LGBMExplainableModel,\n                         init_dataset=automl_explainer_setup_obj.X_transform, run=best_run,\n                         features=automl_explainer_setup_obj.engineered_feature_names,\n                         feature_maps=[automl_explainer_setup_obj.feature_map],\n                         classes=automl_explainer_setup_obj.classes)\n\npip install interpret-community[visualization]\n\nengineered_explanations = explainer.explain(['local', 'global'], eval_dataset=automl_explainer_setup_obj.X_test_transform)\nprint(engineered_explanations.get_feature_importance_dict()),\nfrom interpret_community.widget import ExplanationDashboard\nExplanationDashboard(engineered_explanations, automl_explainer_setup_obj.automl_estimator, datasetX=automl_explainer_setup_obj.X_test_transform)\n\nraw_explanations = explainer.explain(['local', 'global'], get_raw=True, \n                                     raw_feature_names=automl_explainer_setup_obj.raw_feature_names,\n                                     eval_dataset=automl_explainer_setup_obj.X_test_transform)\nprint(raw_explanations.get_feature_importance_dict()),\nfrom interpret_community.widget import ExplanationDashboard\nExplanationDashboard(raw_explanations, automl_explainer_setup_obj.automl_pipeline, datasetX=automl_explainer_setup_obj.X_test_raw)\n<\/code><\/pre>\n<p>The code sample repo please refer to: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/explain-model\/azure-integration\/scoring-time\/train-explain-model-locally-and-deploy.ipynb\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/explain-model\/azure-integration\/scoring-time\/train-explain-model-locally-and-deploy.ipynb<\/a><\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":48.0,
        "Solution_reading_time":32.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":103.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1641102333407,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":67.0,
        "Answerer_view_count":18.0,
        "Challenge_adjusted_solved_time":1.1414036111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I tried importing cv2 from opencv but i got error saying<\/p>\n<blockquote>\n<p>import error: libgthread-2.0.so.0: cannot open shared object file: No such file or directory<\/p>\n<\/blockquote>\n<p>Since Sagemaker Studio Labs doesn't support installation of Ubuntu packages i couldn't use <code>apt-get<\/code> or <code>yum<\/code> to install libglib2.0-0.<\/p>",
        "Challenge_closed_time":1641103613576,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641099504523,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to import cv2 in SageMaker Studio Lab due to the missing shared object file libgthread-2.0.so.0. The user was unable to install the required package libglib2.0-0 as Sagemaker Studio Labs does not support the installation of Ubuntu packages.",
        "Challenge_last_edit_time":1641107240987,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70553701",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":5.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.1414036111,
        "Challenge_title":"Couldn't import cv2 in SageMaker Studio Lab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":534.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589906719620,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":63.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>With this line, you can install the glib dependency for Amazon Sagemaker Studio Lab. Just run it on your notebook cell.<\/p>\n<pre><code>! conda install glib=2.51.0 -y\n<\/code><\/pre>\n<p>You also can create another virtual environment for your session that contains glib:<\/p>\n<pre><code>! conda create -n glib-test -c defaults -c conda-forge python=3 glib=2.51.0` -y\n<\/code><\/pre>\n<p>After that maybe you need albumentations to import cv2:<\/p>\n<pre><code>! pip install albumentations\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.0,
        "Solution_reading_time":6.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":16.0833333333,
        "Challenge_answer_count":5,
        "Challenge_body":"Hi, I am following this tutorial on model deployment (https:\/\/codelabs.developers.google.com\/vertex-image-deploy#6), but I ran into a issue when importing the aiplatform library.\n\nWhen running \"from google.cloud import aiplatform\", I get the following error message:\n\nImportError                               Traceback (most recent call last)\n\/tmp\/ipykernel_22080\/3236611779.py in <module>\n      4 #!python #3.7.12\n      5 \n----> 6 from google.cloud import aiplatform\n      7 \n      8 import tensorflow as tf\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/__init__.py in <module>\n     22 \n     23 \n---> 24 from google.cloud.aiplatform import initializer\n     25 \n     26 from google.cloud.aiplatform.datasets import (\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/initializer.py in <module>\n     24 \n     25 from google.api_core import client_options\n---> 26 from google.api_core import gapic_v1\n     27 import google.auth\n     28 from google.auth import credentials as auth_credentials\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/__init__.py in <module>\n     17 from google.api_core.gapic_v1 import config_async\n     18 from google.api_core.gapic_v1 import method\n---> 19 from google.api_core.gapic_v1 import method_async\n     20 from google.api_core.gapic_v1 import routing_header\n     21 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method_async.py in <module>\n     20 import functools\n     21 \n---> 22 from google.api_core import grpc_helpers_async\n     23 from google.api_core.gapic_v1 import client_info\n     24 from google.api_core.gapic_v1.method import _GapicCallable\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers_async.py in <module>\n     23 \n     24 import grpc\n---> 25 from grpc import aio\n     26 \n     27 from google.api_core import exceptions, grpc_helpers\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/aio\/__init__.py in <module>\n     21 \n     22 import grpc\n---> 23 from grpc._cython.cygrpc import (init_grpc_aio, shutdown_grpc_aio, EOF,     24                                  AbortError, BaseError, InternalError,\n     25                                  UsageError)\n\nImportError: cannot import name 'shutdown_grpc_aio' from 'grpc._cython.cygrpc' (\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_cython\/cygrpc.cpython-37m-x86_64-linux-gnu.so)\n\nThe versions of the concerned libraries are shown below.\n\ngoogle-api-core                       2.10.1\ngoogle-api-python-client              2.55.0\ngoogle-cloud-aiplatform               1.17.0\n\ngrpcio                                1.33.1\ngrpcio-gcp                            0.2.2\ngrpcio-status                         1.47.0\n\nI have tried grpcio versions 1.26, 1.27.2, and even the latest 1.50, but all of them had import errors (concerning importing of aio module for 1.26 and 127.2 and AbortError module for 1.50). Are there any additional steps or libraries that I need to take to avoid these import errors?\n\nThank you!",
        "Challenge_closed_time":1668446100000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1668388200000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue while importing the aiplatform library while following a tutorial on model deployment. The error message shows an ImportError and the concerned libraries' versions are also mentioned. The user has tried different versions of grpcio but still faces import errors. The user is seeking additional steps or libraries to avoid these errors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-with-importing-aiplatform\/m-p\/489087#M771",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":10.5,
        "Challenge_reading_time":34.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":16.0833333333,
        "Challenge_title":"Issues with importing aiplatform",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":0.0,
        "Challenge_word_count":265,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, thank you for your reply. I am running the code on Vertex AI.\n\nI realised I had to restart the kernel to refresh the package after updating grpcio, and I could then import aiplatform without any issues as shown below:\n\nfrom google.cloud import aiplatform\nprint(\"aiplatform version: \", aiplatform.__version__)\n\naiplatform version:  1.17.0\n\nThanks again for your help!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.5,
        "Solution_reading_time":4.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":61.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1376606307612,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, BC, Canada",
        "Answerer_reputation_count":1567.0,
        "Answerer_view_count":159.0,
        "Challenge_adjusted_solved_time":8936.4835138889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I try to train a pytorch model on amazon sagemaker studio.<\/p>\n\n<p>It's working when I use an EC2 for training with:<\/p>\n\n<pre><code>estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                sagemaker_session = sess,\n                train_instance_count=1,\n                train_instance_type='ml.c5.xlarge',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>and it's work on local mode in classic sagemaker notebook (non studio) with:<\/p>\n\n<pre><code> estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                train_instance_count=1,\n                train_instance_type='local',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>But when I use it the same code (with train_instance_type='local') on sagemaker studio it doesn't work and I have the following error: No such file or directory: 'docker': 'docker'<\/p>\n\n<p>I tried to install docker with pip install but the docker command is not found if use it in terminal<\/p>",
        "Challenge_closed_time":1596561017227,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588239469173,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error \"No such file or directory: 'docker': 'docker'\" while trying to train a PyTorch model on Amazon SageMaker Studio in local mode. The same code works on EC2 and classic SageMaker notebook in local mode. The user tried to install docker with pip install, but the docker command is not found in the terminal.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61520346",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":14.8,
        "Challenge_reading_time":14.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2311.5411261111,
        "Challenge_title":"No such file or directory: 'docker': 'docker' when running sagemaker studio in local mode",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1884.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1576136255052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Solution_body":"<p>This indicates that there is a problem finding the Docker service.<\/p>\n<p>By default, the Docker is not installed in the SageMaker Studio  (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/656#issuecomment-632170943\" rel=\"nofollow noreferrer\">confirming github ticket response<\/a>).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1620410809823,
        "Solution_link_count":1.0,
        "Solution_readability":14.2,
        "Solution_reading_time":3.98,
        "Solution_score_count":7.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":29.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.2260158334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.    <\/p>\n<p>Features in <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python<\/a> are neat, but that of having API docs\/swagger for the webservice seems missing.    <\/p>\n<p>Having some documentation is useful especially if the model takes in input several features of different type.    <\/p>\n<p>To overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.     <\/p>\n<p>Can I do something equivalent to this with AKS in AzureML instead? If so, how?<\/p>",
        "Challenge_closed_time":1600930445547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600897231890,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to deploy a machine learning service using AzureML on AKS and add OpenAPI specification for it. The user finds that the API docs\/swagger feature is missing in AzureML on AKS and currently uses Docker containers with FastAPI library to build the API and OpenAPI\/Swagger specs. The user is seeking guidance on how to add OpenAPI specification to a webservice deployed with AzureML in AKS.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":12.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":9.2260158334,
        "Challenge_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":123,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=9ced4628-b03a-4169-99b4-e42b0955c045\">@Davide Fiocco  <\/a> The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.akswebservice?view=azure-ml-py\">here<\/a>. You can print your <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service\">swagger_uri<\/a> of the web service and check if it confirms with the specifications you are creating currently.     <\/p>\n<p>If the above response helps, please accept the response as answer. Thanks!!    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.9,
        "Solution_reading_time":8.84,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1498123491552,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"\u4e2d\u56fdJiangsu Sheng",
        "Answerer_reputation_count":22369.0,
        "Answerer_view_count":3121.0,
        "Challenge_adjusted_solved_time":112.0978552778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would really like to get access to some of the updated functions in pandas 0.19, but Azure ML studio uses pandas 0.18 as part of the Anaconda 4.0 bundle. Is there a way to update the version that is used within the \"Execute Python Script\" components?<\/p>",
        "Challenge_closed_time":1505456513227,
        "Challenge_comment_count":2,
        "Challenge_created_time":1505401641617,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user wants to update pandas to version 0.19 in Azure ML Studio, but the platform currently uses pandas 0.18 as part of the Anaconda 4.0 bundle. The user is seeking a way to update the version used within the \"Execute Python Script\" components.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46222606",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":5.7,
        "Challenge_reading_time":3.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":15.2421138889,
        "Challenge_title":"Updating pandas to version 0.19 in Azure ML Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1903.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1421081882987,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":588.0,
        "Poster_view_count":64.0,
        "Solution_body":"<p>I offer the below steps for you to show how to update the version of pandas  library in <code>Execute Python Script<\/code>.<\/p>\n\n<p><strong><em>Step 1<\/em><\/strong> : Use the <code>virtualenv<\/code> component to create an independent python runtime environment in your system.Please install it first with command <code>pip install virtualenv<\/code> if you don't have it.<\/p>\n\n<p>If you installed it successfully ,you could see it in your python\/Scripts file.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step2<\/em><\/strong> : Run the commad to create independent python runtime environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 3<\/em><\/strong> : Then go into the created directory's Scripts folder and activate it (this step is important , don't miss it)<\/p>\n\n<p>Please don't close this command window and use <code>pip install pandas==0.19<\/code> to download external libraries in this command window.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Wj857.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Wj857.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 4<\/em><\/strong> : Compress all of the files in the Lib\/site-packages folder into a zip package (I'm calling it pandas - package here)<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 5<\/em><\/strong> \uff1aUpload the zip package into the Azure Machine Learning WorkSpace DataSet.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/efRkK.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/efRkK.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>specific steps please refer to the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/\" rel=\"noreferrer\">Technical Notes<\/a>.<\/p>\n\n<p>After success, you will see the uploaded package in the DataSet List<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 6<\/em><\/strong> \uff1a Before the defination of method <code>azureml_main<\/code> in the Execute Python Script module, you need to remove the old <code>pandas<\/code> modules &amp; its dependencies, then to import <code>pandas<\/code> again, as the code below.<\/p>\n\n<pre><code>import sys\nimport pandas as pd\nprint(pd.__version__)\ndel sys.modules['pandas']\ndel sys.modules['numpy']\ndel sys.modules['pytz']\ndel sys.modules['six']\ndel sys.modules['dateutil']\nsys.path.insert(0, '.\\\\Script Bundle')\nfor td in [m for m in sys.modules if m.startswith('pandas.') or m.startswith('numpy.') or m.startswith('pytz.') or m.startswith('dateutil.') or m.startswith('six.')]:\n    del sys.modules[td]\nimport pandas as pd\nprint(pd.__version__)\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n<\/code><\/pre>\n\n<p>Then you can see the result from logs as below, first print the old version <code>0.14.0<\/code>, then print the new version <code>0.19.0<\/code> from the uploaded zip file.<\/p>\n\n<pre><code>[Information]         0.14.0\n[Information]         0.19.0\n<\/code><\/pre>\n\n<p>You could also refer to these threads: <a href=\"https:\/\/stackoverflow.com\/questions\/45749479\/access-blob-file-using-time-stamp-in-azure\/45814318#45814318\">Access blob file using time stamp in Azure<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/12669546\/reload-with-reset\">reload with reset<\/a>.<\/p>\n\n<p>Hope it helps you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1505805193896,
        "Solution_link_count":15.0,
        "Solution_readability":10.2,
        "Solution_reading_time":51.18,
        "Solution_score_count":6.0,
        "Solution_sentence_count":43.0,
        "Solution_word_count":375.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1985.0091666667,
        "Challenge_answer_count":0,
        "Challenge_body":"If you run any command that uses azureml (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message:\r\n\r\n```\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), Requirement.parse('flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"')).\r\n```\r\n\r\n**Expected Behavior**\r\nNo warning message should be printed.\r\n\r\n**Steps to Reproduce the Issue**\r\n1. From latest master branch in a fresh virtualenv run: `make build install`\r\n2. `cd \/path\/to\/azure\/a2ml-project`\r\n3. `a2ml experiment leaderboard`\r\n4. Observe the warning message above.\r\n\r\n\r\n**Environment Details:**\r\n - OS: macOS 10.15\r\n - A2ML Version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34\r\n - Python Version: 3.7.7\r\n",
        "Challenge_closed_time":1597072927000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1589926894000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an issue where installing azureml-sdk downgrades pyarrow to 3.0.0, which breaks cudf. The error message shows that the module 'pyarrow.lib' has no attribute 'MonthDayNanoIntervalArray'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/173",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":12.25,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":614.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1985.0091666667,
        "Challenge_title":"Warning message about hyperdrive loading with azureml_run_type_providers",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":99,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"try again pls, I cannot reproduce it with latest azure ml Not able to reproduce now.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.01,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1415906440767,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Kyiv",
        "Answerer_reputation_count":12948.0,
        "Answerer_view_count":363.0,
        "Challenge_adjusted_solved_time":260.2198386111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>We have a notebook instance within Sagemaker which contains many Jupyter Python scripts. I'd like to write a program which downloads these various scripts each day (i.e. so that I could back them up). Unfortunately I don't see any reference to this in the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/index.html\" rel=\"nofollow noreferrer\">AWS CLI API<\/a>.<\/p>\n\n<p>Is this achievable? <\/p>",
        "Challenge_closed_time":1542042145470,
        "Challenge_comment_count":1,
        "Challenge_created_time":1541188759393,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to download Jupyter Python scripts from a Sagemaker notebook instance programmatically to back them up daily. However, they cannot find any reference to this in the AWS CLI API. The user is seeking help to achieve this task.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53125108",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":237.0516880556,
        "Challenge_title":"How do I download files within a Sagemaker notebook instance programatically?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":8552.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1299752760990,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2563.0,
        "Poster_view_count":167.0,
        "Solution_body":"<p>It's not exactly that you want, but looks like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Version_control\" rel=\"nofollow noreferrer\">VCS<\/a> can fit your needs. You can use Github(if you already use it) or CodeCommit(free privat repos) Details and additional ways like <code>sync<\/code> target <code>dir<\/code> with <code>S3<\/code> bucket - <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1542125550812,
        "Solution_link_count":3.0,
        "Solution_readability":20.6,
        "Solution_reading_time":8.33,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":44.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.843735,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I am trying to create an Azure ML Environment using a Dockerfile but it contains the 'COPY' instruction.    <\/p>\n<p>From the documentation of Environment.from_dockerfile ( <a href=\"https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-\">https:\/\/learn.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none-<\/a> ), I can not find a way to give it some files along with the Dockerfile itself.    <\/p>\n<p>So, how to pass context to enable using COPY in the Dockerfile ?    <\/p>\n<p>Thank you for your time !<\/p>",
        "Challenge_closed_time":1634774742763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634739305317,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing challenges while creating an Azure ML Environment using a Dockerfile that contains the 'COPY' instruction. The user is unable to find a way to give files along with the Dockerfile to enable using COPY in the Dockerfile.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/597612\/(azure)(ml)(python-sdk)(environment)(docker)-docke",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":17.8,
        "Challenge_reading_time":11.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.843735,
        "Challenge_title":"[Azure][ML][Python SDK][Environment][Docker] Docker copy missing context",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Docker context is not supported with AzureML Python SDK at the moment. Context support will added later this year<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":56.8840472222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/I8c93.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I8c93.png\" alt=\"enter image description here\"><\/a>I am attempting to install Gluonnlp to a sagemaker jupyter notebook. Im using the command <code>!sudo pip3 install gluonnlp<\/code> to install.  Which is successful.  However on import I get <code>ModuleNotFoundError: No module named 'gluonnlp'<\/code><\/p>\n\n<p>I got the same issue when attempting to install mxnet with pip in the same notebook.  It was resolved when I conda installed mxnet instead.  However conda install has not been working for gluonnlp as it cannot find the package.  I can't seem to find a way to conda install gluonnlp.  Any suggestions would be highly appreciated.<\/p>\n\n<p>Here are some of the commands I have tried<\/p>\n\n<p><code>!sudo pip3 install gluonnlp<\/code><\/p>\n\n<p><code>!conda install gluonnlp<\/code> --> Anaconda cant find the package in any channels<\/p>\n\n<pre><code>!conda install pip --y\n!sudo pip3 install gluonnlp\n\n!sudo pip3 install gluonnlp\n\n!conda install -c conda-forge gluonnlp --y\n<\/code><\/pre>\n\n<p>All these commands on my import \nimport warnings<\/p>\n\n<pre><code>warnings.filterwarnings('ignore')\n\nimport io\nimport random\nimport numpy as np\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom bert import data, model\n<\/code><\/pre>\n\n<p>result in the error<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'gluonnlp'\n<\/code><\/pre>",
        "Challenge_closed_time":1564315813180,
        "Challenge_comment_count":6,
        "Challenge_created_time":1564111030610,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while installing and importing Gluonnlp on a Sagemaker Jupyter notebook. The user has successfully installed Gluonnlp using the command \"!sudo pip3 install gluonnlp\", but on import, the user is getting the error \"ModuleNotFoundError: No module named 'gluonnlp'\". The user has tried various commands, including conda install, but it did not work. The user is seeking suggestions to resolve the issue.",
        "Challenge_last_edit_time":1564413127092,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57212696",
        "Challenge_link_count":2,
        "Challenge_participation_count":7,
        "Challenge_readability":9.6,
        "Challenge_reading_time":18.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":56.8840472222,
        "Challenge_title":"Gluonnlp installation not found on Sagemaker jupyter notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2015.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>this is as simple as creating a Jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing:<\/p>\n\n<pre><code>!pip install gluonnlp\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":2.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1431525955023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cherry Hill, NJ, United States",
        "Answerer_reputation_count":2069.0,
        "Answerer_view_count":145.0,
        "Challenge_adjusted_solved_time":213.5880655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>MLFlow version: 1.4.0\nPython version: 3.7.4<\/p>\n\n<p>I'm running the UI as <code>mlflow server...<\/code> with all the required command line options. <\/p>\n\n<p>I am logging to MLFlow as an MLFlow project, with the appropriate <code>MLproject.yaml<\/code> file. The project is being run on a Docker container, so the CMD looks like this: <\/p>\n\n<p><code>mlflow run . -P document_ids=${D2V_DOC_IDS} -P corpus_path=...  --no-conda --experiment-name=${EXPERIMENT_NAME}<\/code><\/p>\n\n<p>Running the experiment like this results in a blank run_name. I know there's a run_id but I'd also like to see the run_name and set it in my code -- either in the command line, or in my code as <code>mlflow.log....<\/code>.  <\/p>\n\n<p>I've looked at <a href=\"https:\/\/stackoverflow.com\/questions\/57199472\/is-it-possible-to-set-change-mlflow-run-name-after-run-initial-creation\">Is it possible to set\/change mlflow run name after run initial creation?<\/a> but I want to programmatically set the run name instead of changing it manually on the UI.<\/p>",
        "Challenge_closed_time":1573412741883,
        "Challenge_comment_count":0,
        "Challenge_created_time":1572643824847,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set the run_name in mlflow command line while logging to MLFlow as an MLFlow project. They are running the experiment on a Docker container and have tried setting the run_name in the command line but it results in a blank run_name. The user wants to programmatically set the run name instead of changing it manually on the UI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58666136",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":13.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":213.5880655556,
        "Challenge_title":"How can I set run_name in mlflow command line?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2230.0,
        "Challenge_word_count":141,
        "Platform":"Stack Overflow",
        "Poster_created_time":1364599762503,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":48.0,
        "Solution_body":"<p>One of the parameters to <code>mlflow.start_run()<\/code> is <code>run_name<\/code>.  This would give you programmatic access to set the run name with each iteration. See the docs <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Here's an example:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from datetime import datetime\n\n## Define the name of our run\nname = \"this run is gonna be bananas\" + datetime.now()\n\n## Start a new mlflow run and set the run name\nwith mlflow.start_run(run_name = name):\n\n    ## ...train model, log metrics\/params\/model...\n\n    ## End the run\n    mlflow.end_run()\n<\/code><\/pre>\n\n<p>If you want to include set the name as part of an MLflow Project, you'll have to specify it as a parameter in the entry points to the project.  This is located in in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/projects.html#mlproject-file\" rel=\"nofollow noreferrer\">MLproject file<\/a>.  Then you can pass those values into the <code>mlflow.start_run()<\/code> function from the command line.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":6.9,
        "Solution_reading_time":13.68,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":129.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":947.3408333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### What steps did you take:\r\n[A clear and concise description of what the bug is.]\r\n\r\nI am use the re usable Sagemaker Components for building kubeflow pipelines.\r\n\r\nsagemaker_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/train\/component.yaml')\r\nsagemaker_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/model\/component.yaml')\r\nsagemaker_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/deploy\/component.yaml')\r\n\r\nWhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = sagemaker_deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='Endpoint-price-prediction-model',\r\n        endpoint_config_name='EndpointConfig-price-prediction-model',\r\n        update_endpoint=True,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### What happened:\r\nI am getting this error \r\nTypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\nI think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\nTraceback (most recent call last):\r\n--\r\n414 | File \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | File \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = sagemaker_deploy_op(\r\n424 | TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### What did you expect to happen:\r\nto update the endpoint without any issue\r\n### Environment:\r\n<!-- Please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\nsagemaker 2.1.0\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\n<!-- If you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nKFP version: <!-- If you are not sure, build commit shows on bottom of KFP UI left sidenav. -->\r\n\r\nKFP SDK version: <!-- Please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### Anything else you would like to add:\r\n[Miscellaneous information that will assist in solving the issue.]\r\n\r\nPlease help me out \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1611093472000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1607683045000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to create a custom model and perform batch transform in Amazon SageMaker without HPO and training jobs. They have removed HPO and training jobs but are facing issues while compiling kfp. They are getting the desired output in Kubeflow but want to see the custom model output in SageMaker without HPO and batch job. The user also requests for any open source loan data model using KF.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":14.3,
        "Challenge_reading_time":43.94,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":947.3408333333,
        "Challenge_title":"TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":304,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"\/assign @mameshini \r\n\/assign @PatrickXYS \r\n\r\nDo you mind taking a look? Thanks @numerology Thanks!\r\n\r\n@akartsky @RedbackThomson Can you take a look?  Hi @jchaudari, \r\nThanks for reporting the issue, we are taking a look at it. \r\n\r\nThanks,\r\nMeghna Hi @jchaudari, \r\nAre you certain you are using the latest version of the components ? The attached yaml files show that you are using version 0.3.0 of the image which is very old. This feature was added more recently in version 0.9.0 - \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/Changelog.md. \r\n\r\nCould you please try with the newer version and let us know if that fixes your issue ?\r\nThanks,\r\nMeghna Baijal If there aren't any further issues, we'll close this by the end of the week. Otherwise, let us know. \/close @akartsky: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888#issuecomment-763167821):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":9.7,
        "Solution_reading_time":16.34,
        "Solution_score_count":null,
        "Solution_sentence_count":15.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5700419445,
        "Challenge_answer_count":1,
        "Challenge_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Challenge_closed_time":1652688680111,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652686627960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is exploring Sagemaker Built-in algorithms and is having difficulty finding research background and implementation details for specific algorithms. They are seeking help in finding this information.",
        "Challenge_last_edit_time":1668094438488,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":5.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.5700419445,
        "Challenge_title":"Sagemaker Built-in Algorithms",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":66,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\n - **BlazingText**: *[BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs](https:\/\/dl.acm.org\/doi\/10.1145\/3146347.3146354)*, Gupta et Khare\n - **[DeepAR](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/deepar_how-it-works.html)** *[DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks](https:\/\/arxiv.org\/abs\/1704.04110)*, Salinas et al.\n - **[Factorization Machines](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/fact-machines-howitworks.html)**\n - **[IP Insights](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ip-insights-howitworks.html)**\n - **[KMeans](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algo-kmeans-tech-notes.html)**\n - **[KNN](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/kNN_how-it-works.html)**\n - **[LDA](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lda-how-it-works.html)**\n - **[Linear Learner](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/linear-learner.html)**\n - **[NTM](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ntm.html)**\n - **[Object2Vec](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object2vec-howitworks.html)**\n - **[Object Detection](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algo-object-detection-tech-notes.html)** (it's an SSD model)\n - **[PCA](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-pca-works.html)**\n - **[Random Cut Forest](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rcf_how-it-works.html)**: *[Robust Random Cut Forest Based Anomaly Detection On Streams](https:\/\/proceedings.mlr.press\/v48\/guha16.pdf)*, Guha et al\n - **[Semantic Segmentation](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html)**\n - **[Seq2seq](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/seq-2-seq-howitworks.html)**\n - **[XGBoost](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-HowItWorks.html)**",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1652688680112,
        "Solution_link_count":18.0,
        "Solution_readability":37.8,
        "Solution_reading_time":28.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":21.5513888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nThe conda environment for python3.6 in notebooks cannot find `pandas.CSVDataSet`\r\n\r\n## Context\r\nI'm wanting to use sagemaker as my development environment. However, I cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## Steps to Reproduce\r\n\r\n0. Startup a Sagemaker instance with defaults\r\n\r\nTerminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => Success!\r\n\r\nNotebook fail:\r\n1. Create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> The environments for the terminal and notebooks are separate by design in Sagemaker\r\n2. Load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> Note that I've started to use the code below; Without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `Path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # Check it exists first. For some reason this is not an idempotent operation?\r\n    current_dir = Path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. Run `context.catalog.list()`\r\n\r\n## Expected Result\r\nThe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## Actual Result\r\n```\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\nFull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not None)\r\n    418         except StopIteration:\r\n\r\nStopIteration: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except StopIteration:\r\n--> 419             raise DataSetError(\"Class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\nDataSetError: Class `pandas.CSVDataSet` not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = AbstractDataSet.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise DataSetError(\r\n    153                 \"An exception occurred when parsing config \"\r\n--> 154                 \"for DataSet `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\nDataSetError: An exception occurred when parsing config for DataSet `example_iris_data`:\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\n## Investigations so far\r\n\r\n### `CSVLocalDataSet`\r\nUpon changing the yaml type for iris.csv from `pandas.CSVDataSet` to `CSVLocalDataSet`, we get success on both the terminal and the notebook. However, this is not my desired outcome; The transition to using `pandas.CSVDataSet` makes it easier, for me at least, to use both S3 and local datasets.\r\n\r\n### `pip install kedro` output from notebook\r\n```\r\nCollecting kedro\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/6f\/4faaa0e58728a318aeabc490271a636f87f6b9165245ce1d3adc764240cf\/kedro-0.15.8-py3-none-any.whl (12.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.5MB 4.1MB\/s eta 0:00:01\r\nRequirement already satisfied: xlsxwriter<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.0.4)\r\nCollecting azure-storage-file<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c9\/33\/6c611563412ffc409b2413ac50e3a063133ea235b86c137759774c77f3ad\/azure_storage_file-1.4.0-py2.py3-none-any.whl\r\nCollecting fsspec<1.0,>=0.5.1 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6e\/2b\/63420d49d5e5f885451429e9e0f40ad1787eed0d32b1aedd6b10f9c2719a\/fsspec-0.7.1-py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 33.5MB\/s ta 0:00:01\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (0.24.2)\r\nCollecting s3fs<1.0,>=0.3.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b8\/e4\/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675\/s3fs-0.4.2-py3-none-any.whl\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nCollecting tables<3.6,>=3.4.4 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/f7\/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a\/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 10.2MB\/s ta 0:00:01\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (2.20.0)\r\nCollecting toposort<2.0,>=1.5 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e9\/8a\/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4\/toposort-1.5-py2.py3-none-any.whl\r\nRequirement already satisfied: click<8.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (6.7)\r\nCollecting azure-storage-queue<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/72\/94\/4db044f1c155b40c5ebc037bfd9d1c24562845692c06798fbe869fe160e6\/azure_storage_queue-1.4.0-py2.py3-none-any.whl\r\nCollecting cookiecutter<2.0,>=1.6.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/86\/c9\/7184edfb0e89abedc37211743d1420810f6b49ae4fa695dfc443c273470d\/cookiecutter-1.7.0-py2.py3-none-any.whl (40kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 24.6MB\/s ta 0:00:01\r\nCollecting pandas-gbq<1.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c3\/74\/126408f6bdb7b2cb1dcb8c6e4bd69a511a7f85792d686d1237d9825e6194\/pandas_gbq-0.13.1-py3-none-any.whl\r\nCollecting pip-tools<5.0.0,>=4.0.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/94\/8f\/59495d651f3ced9b06b69545756a27296861a6edd6c5709fbe1265ed9032\/pip_tools-4.5.1-py2.py3-none-any.whl (41kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 27.5MB\/s ta 0:00:01\r\nCollecting azure-storage-blob<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/25\/f4\/a307ed89014e9abb5c5cfc8ca7f8f797d12f619f17a6059a6fd4b153b5d0\/azure_storage_blob-1.5.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 35.2MB\/s ta 0:00:01\r\nCollecting pyarrow<1.0.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/10\/93fad5849418eade4a4cd581f8cd27be1bbe51e18968ba1492140c887f3f\/pyarrow-0.16.0-cp36-cp36m-manylinux1_x86_64.whl (62.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.9MB 779kB\/s eta 0:00:01    40% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 25.7MB 56.1MB\/s eta 0:00:01\r\nRequirement already satisfied: SQLAlchemy<2.0,>=1.2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.2.11)\r\nRequirement already satisfied: xlrd<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.1.0)\r\nCollecting python-json-logger<1.0,>=0.1.9 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/80\/9d\/1c3393a6067716e04e6fcef95104c8426d262b4adaf18d7aa2470eab028d\/python-json-logger-0.1.11.tar.gz\r\nCollecting anyconfig<1.0,>=0.9.7 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4c\/00\/cc525eb0240b6ef196b98300d505114339bbb7ddd68e3155483f1eb32050\/anyconfig-0.9.10.tar.gz (103kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 34.4MB\/s ta 0:00:01\r\nCollecting azure-storage-common~=1.4 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/6c\/b2285bf3687768dbf61b6bc085b0c1be2893b6e2757a9d023263764177f3\/azure_storage_common-1.4.2-py2.py3-none-any.whl (47kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 25.9MB\/s ta 0:00:01\r\nCollecting azure-common>=1.1.5 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e5\/4d\/d000fc3c5af601d00d55750b71da5c231fcb128f42ac95b208ed1091c2c1\/azure_common-1.1.25-py2.py3-none-any.whl\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.7.3)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.14.3)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2018.4)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (4.0.1)\r\nRequirement already satisfied: numexpr>=2.6.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (2.6.5)\r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.11.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.23)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.6)\r\nCollecting whichcraft>=0.4.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b5\/a2\/81887a0dae2e4d2adc70d9a3557fdda969f863ced51cd3c47b587d25bce5\/whichcraft-0.6.1-py2.py3-none-any.whl\r\nCollecting future>=0.15.2 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/45\/0b\/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9\/future-0.18.2.tar.gz (829kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 27.8MB\/s ta 0:00:01\r\nCollecting poyo>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/42\/50\/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc\/poyo-0.5.0-py2.py3-none-any.whl\r\nCollecting binaryornot>=0.2.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/7e\/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2\/binaryornot-0.4.4-py2.py3-none-any.whl\r\nCollecting jinja2-time>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6a\/a1\/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4\/jinja2_time-0.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.10)\r\nCollecting google-auth-oauthlib (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7b\/b8\/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b\/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\r\nCollecting google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/b0\/cc391ebf8ebf7855cdcfe0a9a4cdc8dcd90287c90e1ac22651d104ac6481\/google_auth-1.12.0-py2.py3-none-any.whl (83kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 35.5MB\/s ta 0:00:01\r\nCollecting pydata-google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/ed\/9c9f410c032645632de787b8c285a78496bd89590c777385b921eb89433d\/pydata_google_auth-0.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (39.1.0)\r\nCollecting google-cloud-bigquery>=1.11.1 (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/8f\/f7\/b6f55e144da37f38a79552a06103f2df4a9569e2dfc6d741a7e2a63d3592\/google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 39.2MB\/s ta 0:00:01\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.14)\r\nCollecting arrow (from jinja2-time>=0.1.0->cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/fa\/f84896dede5decf284e6922134bf03fd26c90870bbf8015f4e8ee2a07bcc\/arrow-0.15.5-py2.py3-none-any.whl (46kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 26.3MB\/s ta 0:00:01\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.0)\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a3\/12\/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379\/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/95\/de\/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d\/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 32.5MB\/s ta 0:00:01\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/08\/6a\/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425\/cachetools-4.0.0-py3-none-any.whl\r\nCollecting google-api-core<2.0dev,>=1.15.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/63\/7e\/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77\/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 29.9MB\/s ta 0:00:01\r\nCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/89\/3c\/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97\/google_cloud_core-1.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.6.1)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/35\/9e\/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098\/google_resumable_media-0.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.11.5)\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/57\/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704\/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 42.0MB\/s ta 0:00:01\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/46\/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf\/googleapis-common-protos-1.51.0.tar.gz\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.18)\r\nBuilding wheels for collected packages: python-json-logger, anyconfig, future, googleapis-common-protos\r\n  Running setup.py bdist_wheel for python-json-logger ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\r\n  Running setup.py bdist_wheel for anyconfig ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\r\n  Running setup.py bdist_wheel for future ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\r\n  Running setup.py bdist_wheel for googleapis-common-protos ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\r\nSuccessfully built python-json-logger anyconfig future googleapis-common-protos\r\ncookiecutter 1.7.0 has requirement click>=7.0, but you'll have click 6.7 which is incompatible.\r\ngoogle-auth 1.12.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\r\ngoogle-cloud-bigquery 1.24.0 has requirement six<2.0.0dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\r\npip-tools 4.5.1 has requirement click>=7, but you'll have click 6.7 which is incompatible.\r\nInstalling collected packages: azure-common, azure-storage-common, azure-storage-file, fsspec, s3fs, tables, toposort, azure-storage-queue, whichcraft, future, poyo, binaryornot, arrow, jinja2-time, cookiecutter, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, pandas-gbq, pip-tools, azure-storage-blob, pyarrow, python-json-logger, anyconfig, kedro\r\n  Found existing installation: s3fs 0.1.5\r\n    Uninstalling s3fs-0.1.5:\r\n      Successfully uninstalled s3fs-0.1.5\r\n  Found existing installation: tables 3.4.3\r\n    Uninstalling tables-3.4.3:\r\n      Successfully uninstalled tables-3.4.3\r\nSuccessfully installed anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 cookiecutter-1.7.0 fsspec-0.7.1 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 oauthlib-3.1.0 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 s3fs-0.4.2 tables-3.5.2 toposort-1.5 whichcraft-0.6.1\r\n```\r\n\r\n### `pip install kedro` output from terminal\r\n```\r\nCollecting kedro\r\n  Using cached kedro-0.15.8-py3-none-any.whl (12.5 MB)\r\nCollecting pandas<1.0,>=0.24.0\r\n  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4 MB 9.6 MB\/s \r\nCollecting azure-storage-file<2.0,>=1.1.0\r\n  Using cached azure_storage_file-1.4.0-py2.py3-none-any.whl (30 kB)\r\nCollecting click<8.0\r\n  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 1.7 MB\/s \r\nCollecting cookiecutter<2.0,>=1.6.0\r\n  Using cached cookiecutter-1.7.0-py2.py3-none-any.whl (40 kB)\r\nCollecting SQLAlchemy<2.0,>=1.2.0\r\n  Downloading SQLAlchemy-1.3.15.tar.gz (6.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.1 MB 49.2 MB\/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting tables<3.6,>=3.4.4\r\n  Using cached tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\/python_json_logger-0.1.11-py2.py3-none-any.whl\r\nCollecting azure-storage-blob<2.0,>=1.1.0\r\n  Using cached azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\r\nCollecting pandas-gbq<1.0,>=0.12.0\r\n  Using cached pandas_gbq-0.13.1-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: fsspec<1.0,>=0.5.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.6.3)\r\nCollecting xlsxwriter<2.0,>=1.0.0\r\n  Downloading XlsxWriter-1.2.8-py2.py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 65.9 MB\/s \r\nCollecting pip-tools<5.0.0,>=4.0.0\r\n  Using cached pip_tools-4.5.1-py2.py3-none-any.whl (41 kB)\r\nCollecting pyarrow<1.0.0,>=0.12.0\r\n  Downloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63.1 MB 25 kB\/s \r\nCollecting xlrd<2.0,>=1.0.0\r\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 66.5 MB\/s \r\nRequirement already satisfied: s3fs<1.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.4.0)\r\nCollecting azure-storage-queue<2.0,>=1.1.0\r\n  Using cached azure_storage_queue-1.4.0-py2.py3-none-any.whl (23 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\/anyconfig-0.9.10-py2.py3-none-any.whl\r\nCollecting toposort<2.0,>=1.5\r\n  Using cached toposort-1.5-py2.py3-none-any.whl (7.6 kB)\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (2.23.0)\r\nRequirement already satisfied: pytz>=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2019.3)\r\nRequirement already satisfied: numpy>=1.13.3 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.18.1)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.8.1)\r\nCollecting azure-common>=1.1.5\r\n  Using cached azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\r\nCollecting azure-storage-common~=1.4\r\n  Using cached azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\r\nCollecting poyo>=0.1.0\r\n  Using cached poyo-0.5.0-py2.py3-none-any.whl (10 kB)\r\nCollecting jinja2-time>=0.1.0\r\n  Using cached jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\r\nCollecting whichcraft>=0.4.0\r\n  Using cached whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\r\nCollecting binaryornot>=0.2.0\r\n  Using cached binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.11.1)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\/future-0.18.2-cp36-none-any.whl\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (3.0.5)\r\nCollecting numexpr>=2.6.2\r\n  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 66.7 MB\/s \r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.14.0)\r\nCollecting pydata-google-auth\r\n  Using cached pydata_google_auth-0.3.0-py2.py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting google-cloud-bigquery>=1.11.1\r\n  Using cached google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\r\nCollecting google-auth\r\n  Using cached google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (46.1.1.post20200323)\r\nRequirement already satisfied: boto3>=1.9.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.12.27)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: idna<3,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nCollecting arrow\r\n  Using cached arrow-0.15.5-py2.py3-none-any.whl (46 kB)\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.1.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0\r\n  Using cached google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\r\nCollecting google-cloud-core<2.0dev,>=1.1.0\r\n  Using cached google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.11.3)\r\nCollecting google-api-core<2.0dev,>=1.15.0\r\n  Using cached google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.3.3)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.15.2)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.14.0)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\/googleapis_common_protos-1.51.0-cp36-none-any.whl\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.20)\r\nBuilding wheels for collected packages: SQLAlchemy\r\n  Building wheel for SQLAlchemy (PEP 517) ... done\r\n  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.15-cp36-cp36m-linux_x86_64.whl size=1215829 sha256=112167e02a19acada7f367d8aca55bbd1e0c655de9edfabebae5e9d055d9a9a6\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/4a\/1b\/3a\/c73044d7be48baeb47cbee343334f7803726ca1e9ba7b29095\r\nSuccessfully built SQLAlchemy\r\nInstalling collected packages: pandas, azure-common, azure-storage-common, azure-storage-file, click, poyo, arrow, jinja2-time, whichcraft, binaryornot, future, cookiecutter, SQLAlchemy, numexpr, tables, python-json-logger, azure-storage-blob, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery, pandas-gbq, xlsxwriter, pip-tools, pyarrow, xlrd, azure-storage-queue, anyconfig, toposort, kedro\r\n  Attempting uninstall: pandas\r\n    Found existing installation: pandas 0.22.0\r\n    Uninstalling pandas-0.22.0:\r\n      Successfully uninstalled pandas-0.22.0\r\nSuccessfully installed SQLAlchemy-1.3.15 anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 click-7.1.1 cookiecutter-1.7.0 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 numexpr-2.7.1 oauthlib-3.1.0 pandas-0.25.3 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 tables-3.5.2 toposort-1.5 whichcraft-0.6.1 xlrd-1.2.0 xlsxwriter-1.2.8\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n|environment | terminal | notebook|\r\n|----|----|----|\r\n|`kedro -V` | kedro, version 0.15.8 | kedro, version 0.15.8|\r\n|`python -V` | Python 3.6.10 :: Anaconda, Inc. | Python 3.6.5 :: Anaconda, Inc.|\r\n|os |  `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"` | `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"`|\r\n|`pip freeze` | anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==1.3.0<br>attrs==19.3.0<br>autovizwidget==0.12.9<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>backcall==0.1.0<br>bcrypt==3.1.7<br>binaryornot==0.4.4<br>bleach==3.1.0<br>boto3==1.12.27<br>botocore==1.15.27<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.14.0<br>chardet==3.0.4<br>click==7.1.1<br>colorama==0.4.3<br>cookiecutter==1.7.0<br>cryptography==2.8<br>decorator==4.4.2<br>defusedxml==0.6.0<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.15.2<br>entrypoints==0.3<br>environment-kernels==1.1.1<br>fsspec==0.6.3<br>future==0.18.2<br>gitdb==4.0.2<br>GitPython==3.1.0<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>hdijupyterutils==0.12.9<br>idna==2.9<br>importlib-metadata==1.5.0<br>ipykernel==5.1.4<br>ipython==7.13.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.5.1<br>jedi==0.16.0<br>Jinja2==2.11.1<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>json5==0.9.3<br>jsonschema==3.2.0<br>jupyter==1.0.0<br>jupyter-client==6.0.0<br>jupyter-console==6.1.0<br>jupyter-core==4.6.1<br>jupyterlab==1.2.7<br>jupyterlab-git==0.9.0<br>jupyterlab-server==1.0.7<br>kedro==0.15.8<br>MarkupSafe==1.1.1<br>mistune==0.8.4<br>mock==3.0.5<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.3<br>nbconvert==5.6.1<br>nbdime==2.0.0<br>nbexamples==0.0.0<br>nbformat==5.0.4<br>nbserverproxy==0.3.2<br>nose==1.3.7<br>notebook==5.7.8<br>numexpr==2.7.1<br>numpy==1.18.1<br>oauthlib==3.1.0<br>packaging==20.3<br>pandas==0.25.3<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.6.2<br>pexpect==4.8.0<br>pickleshare==0.7.5<br>pid==3.0.0<br>pip-tools==4.5.1<br>plotly==4.5.4<br>poyo==0.5.0<br>prometheus-client==0.7.1<br>prompt-toolkit==3.0.3<br>protobuf==3.11.3<br>protobuf3-to-dict==0.1.5<br>psutil==5.7.0<br>psycopg2==2.8.4<br>ptyprocess==0.6.0<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycparser==2.20<br>pydata-google-auth==0.3.0<br>pygal==2.4.0<br>Pygments==2.6.1<br>pykerberos==1.1.14<br>PyNaCl==1.3.0<br>pyOpenSSL==19.1.0<br>pyparsing==2.4.6<br>pyrsistent==0.15.7<br>PySocks==1.7.1<br>pyspark==2.3.2<br>python-dateutil==2.8.1<br>python-json-logger==0.1.11<br>pytz==2019.3<br>PyYAML==5.3.1<br>pyzmq==18.1.1<br>qtconsole==4.7.1<br>QtPy==1.9.0<br>requests==2.23.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rsa==3.4.2<br>s3fs==0.4.0<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-experiments==0.1.10<br>sagemaker-nbi-agent==1.0<br>sagemaker-pyspark==1.2.8<br>scipy==1.4.1<br>Send2Trash==1.5.0<br>six==1.14.0<br>smdebug-rulesconfig==0.1.2<br>smmap==3.0.1<br>sparkmagic==0.15.0<br>SQLAlchemy==1.3.15<br>tables==3.5.2<br>terminado==0.8.3<br>testpath==0.4.4<br>texttable==1.6.2<br>toposort==1.5<br>tornado==6.0.4<br>traitlets==4.3.3<br>urllib3==1.22<br>wcwidth==0.1.8<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>whichcraft==0.6.1<br>widgetsnbextension==3.5.1<br>xlrd==1.2.0<br>XlsxWriter==1.2.8<br>zipp==2.2.0 | alabaster==0.7.10<br>anaconda-client==1.6.14<br>anaconda-project==0.8.2<br>anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==0.24.0<br>astroid==1.6.3<br>astropy==3.0.2<br>attrs==18.1.0<br>Automat==0.3.0<br>autovizwidget==0.15.0<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>Babel==2.5.3<br>backcall==0.1.0<br>backports.shutil-get-terminal-size==1.0.0<br>bcrypt==3.1.7<br>beautifulsoup4==4.6.0<br>binaryornot==0.4.4<br>bitarray==0.8.1<br>bkcharts==0.2<br>blaze==0.11.3<br>bleach==2.1.3<br>bokeh==1.0.4<br>boto==2.48.0<br>boto3==1.12.27<br>botocore==1.15.27<br>Bottleneck==1.2.1<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.11.5<br>characteristic==14.3.0<br>chardet==3.0.4<br>click==6.7<br>cloudpickle==0.5.3<br>clyent==1.2.2<br>colorama==0.3.9<br>contextlib2==0.5.5<br>cookiecutter==1.7.0<br>cryptography==2.8<br>cycler==0.10.0<br>Cython==0.28.4<br>cytoolz==0.9.0.1<br>dask==0.17.5<br>datashape==0.5.4<br>decorator==4.3.0<br>defusedxml==0.6.0<br>distributed==1.21.8<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.14<br>entrypoints==0.2.3<br>enum34==1.1.9<br>environment-kernels==1.1.1<br>et-xmlfile==1.0.1<br>fastcache==1.0.2<br>filelock==3.0.4<br>Flask==1.0.2<br>Flask-Cors==3.0.4<br>fsspec==0.7.1<br>future==0.18.2<br>gevent==1.3.0<br>glob2==0.6<br>gmpy2==2.0.8<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>greenlet==0.4.13<br>h5py==2.8.0<br>hdijupyterutils==0.15.0<br>heapdict==1.0.0<br>html5lib==1.0.1<br>idna==2.6<br>imageio==2.3.0<br>imagesize==1.0.0<br>importlib-metadata==1.5.0<br>ipykernel==4.8.2<br>ipyparallel==6.2.2<br>ipython==6.4.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.4.0<br>isort==4.3.4<br>itsdangerous==0.24<br>jdcal==1.4<br>jedi==0.12.0<br>Jinja2==2.10<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>jsonschema==2.6.0<br>jupyter==1.0.0<br>jupyter-client==5.2.3<br>jupyter-console==5.2.0<br>jupyter-core==4.4.0<br>jupyterlab==0.32.1<br>jupyterlab-launcher==0.10.5<br>kedro==0.15.8<br>kiwisolver==1.0.1<br>lazy-object-proxy==1.3.1<br>llvmlite==0.23.1<br>locket==0.2.0<br>lxml==4.2.1<br>MarkupSafe==1.0<br>matplotlib==3.0.3<br>mccabe==0.6.1<br>mistune==0.8.3<br>mkl-fft==1.0.0<br>mkl-random==1.0.1<br>mock==4.0.1<br>more-itertools==4.1.0<br>mpmath==1.0.0<br>msgpack==0.6.0<br>msgpack-python==0.5.6<br>multipledispatch==0.5.0<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.2<br>nbconvert==5.4.1<br>nbformat==4.4.0<br>networkx==2.1<br>nltk==3.3<br>nose==1.3.7<br>notebook==5.5.0<br>numba==0.38.0<br>numexpr==2.6.5<br>numpy==1.14.3<br>numpydoc==0.8.0<br>oauthlib==3.1.0<br>odo==0.5.1<br>olefile==0.45.1<br>opencv-python==3.4.2.17<br>openpyxl==2.5.3<br>packaging==20.1<br>pandas==0.24.2<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.2.0<br>partd==0.3.8<br>path.py==11.0.1<br>pathlib2==2.3.2<br>patsy==0.5.0<br>pep8==1.7.1<br>pexpect==4.5.0<br>pickleshare==0.7.4<br>Pillow==5.1.0<br>pip-tools==4.5.1<br>pkginfo==1.4.2<br>plotly==4.5.2<br>pluggy==0.6.0<br>ply==3.11<br>poyo==0.5.0<br>prompt-toolkit==1.0.15<br>protobuf==3.6.1<br>protobuf3-to-dict==0.1.5<br>psutil==5.4.5<br>psycopg2==2.7.5<br>ptyprocess==0.5.2<br>py==1.5.3<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycodestyle==2.4.0<br>pycosat==0.6.3<br>pycparser==2.18<br>pycrypto==2.6.1<br>pycurl==7.43.0.1<br>pydata-google-auth==0.3.0<br>pyflakes==1.6.0<br>pygal==2.4.0<br>Pygments==2.2.0<br>pykerberos==1.2.1<br>pylint==1.8.4<br>PyNaCl==1.3.0<br>pyodbc==4.0.23<br>pyOpenSSL==18.0.0<br>pyparsing==2.2.0<br>PySocks==1.6.8<br>pyspark==2.3.2<br>pytest==3.5.1<br>pytest-arraydiff==0.2<br>pytest-astropy==0.3.0<br>pytest-doctestplus==0.1.3<br>pytest-openfiles==0.3.0<br>pytest-remotedata==0.2.1<br>python-dateutil==2.7.3<br>python-json-logger==0.1.11<br>pytz==2018.4<br>PyWavelets==0.5.2<br>PyYAML==5.3.1<br>pyzmq==17.0.0<br>QtAwesome==0.4.4<br>qtconsole==4.3.1<br>QtPy==1.4.1<br>requests==2.20.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rope==0.10.7<br>rsa==3.4.2<br>ruamel-yaml==0.15.35<br>s3fs==0.4.2<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-pyspark==1.2.8<br>scikit-image==0.13.1<br>scikit-learn==0.20.3<br>scipy==1.1.0<br>seaborn==0.8.1<br>Send2Trash==1.5.0<br>simplegeneric==0.8.1<br>singledispatch==3.4.0.3<br>six==1.11.0<br>smdebug-rulesconfig==0.1.2<br>snowballstemmer==1.2.1<br>sortedcollections==0.6.1<br>sortedcontainers==1.5.10<br>sparkmagic==0.12.5<br>Sphinx==1.7.4<br>sphinxcontrib-websupport==1.0.1<br>spyder==3.2.8<br>SQLAlchemy==1.2.11<br>statsmodels==0.9.0<br>sympy==1.1.1<br>tables==3.5.2<br>TBB==0.1<br>tblib==1.3.2<br>terminado==0.8.1<br>testpath==0.3.1<br>texttable==1.6.2<br>toolz==0.9.0<br>toposort==1.5<br>tornado==5.0.2<br>traitlets==4.3.2<br>typing==3.6.4<br>unicodecsv==0.14.1<br>urllib3==1.23<br>wcwidth==0.1.7<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>Werkzeug==0.14.1<br>whichcraft==0.6.1<br>widgetsnbextension==3.4.2<br>wrapt==1.10.11<br>xlrd==1.1.0<br>XlsxWriter==1.0.4<br>xlwt==1.3.0<br>zict==0.1.3<br>zipp==3.0.0|\r\n",
        "Challenge_closed_time":1585791347000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1585713762000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while running a query on the `aws_sagemaker_notebook_instance` table in Steampipe. The error message indicates that the `hydrate call listAwsSageMakerNotebookInstanceTags` failed with a panic interface conversion. The user is using Steampipe version v0.4.1 and aws plugin version v0.15.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro\/issues\/308",
        "Challenge_link_count":35,
        "Challenge_participation_count":5,
        "Challenge_readability":22.7,
        "Challenge_reading_time":580.3,
        "Challenge_repo_contributor_count":164.0,
        "Challenge_repo_fork_count":740.0,
        "Challenge_repo_issue_count":1942.0,
        "Challenge_repo_star_count":7884.0,
        "Challenge_repo_watch_count":102.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":434,
        "Challenge_solved_time":21.5513888889,
        "Challenge_title":"Sagemaker notebooks raise error for `pandas.CSVDataSet`",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":1973,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Kedro aside there are a couple of things that you can do to ensure that your environments match from the terminal vs notebook.  I am not familiar with the new `pandas.CSVDataSet` as I am just now starting with my first `0.15.8` myself.  We have struggled to get package installs correct through our notebooks, I make sure my team is all using their own environment, created from the terminal.\r\n\r\n## activate python3 from the terminal before install\r\n\r\nNote that the file browser on the left hand side of a SageMaker notebook is really mounted at `~\/SageMaker`.\r\n\r\n``` bash\r\nsource activate python3\r\n# may also be - conda activate python3\r\n# unrelated on windows it was - activate python 3\r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n## install ipykernel in your terminal env\r\n\r\nFor conda environments to show up in the notebook dropdown selection you will need `ipykernel` installed. see [docs](https:\/\/ipython.readthedocs.io\/en\/stable\/install\/kernel_install.html)\r\n\r\n```\r\nconda create -n testing python=3.6\r\npip install ipykernel\r\n# I typically don't have to go this far, but installing ipykernel is recommended by the docs\r\nipykernel install --user \r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n\r\n\r\nDo note that if you shut down your SageMaker notebook you will loose your packages and environments by default.\r\n\r\nI also noticed that you have a difference between pandas.  I have no idea if that changes things, but might be a simple fix. Your second idea worked @WaylonWalker. I slightly adapted it as it didn't work straight up:\r\n```\r\nconda create --yes --name kedroenv python=3.6 ipykernel\r\nsource activate kedroenv\r\npython -m ipykernel install --user --name kedroenv --display-name \"Kedro py3.6\"\r\n\r\ncd ~\/Sagemaker\r\nkedro new # Name testing and example pipeline\r\ncd testing\/\r\nkedro run\r\n```\r\nWith a reasonable solution, I'll call this issue closed. Massive thank you @WaylonWalker for pointing me in the right direction.\r\n\r\nCheers,\r\nTom @tjcuddihy We're working with the AWS team to produce a knowledge document on using Kedro and Sagemaker. Would we be able to talk to you about how you used them together? I'd be keen on learning more about how to make Sagemaker play nicely with kedro so I can still access everything I need from my kedro context. @yetudada I have an alpha version of a kedro plugin that plays nicely with sagemaker and allows you to run processing jobs. @uwaisiqbal then you might be interested in this knowledge article that was just published on AWS: https:\/\/aws.amazon.com\/blogs\/opensource\/using-kedro-pipelines-to-train-amazon-sagemaker-models\/ \ud83d\ude80 ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":32.51,
        "Solution_score_count":null,
        "Solution_sentence_count":20.0,
        "Solution_word_count":397.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":13.1620658333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently struggling to decide on what situations in which a glue job is preferable over a sagemaker processing job and vice versa? Some advice on this topic would be greatly appreciated.<\/p>\n<p>I can do the same on both, so why should I bother with the difference?<\/p>",
        "Challenge_closed_time":1660951371720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660903988283,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having difficulty deciding when to use a Glue job or a Sagemaker Processing job for an ETL task and is seeking advice on the topic. They are unsure of the differences between the two and why they should choose one over the other.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73415182",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":4.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":13.1620658333,
        "Challenge_title":"When do I use a glue job or a Sagemaker Processing job for an etl?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1644981356940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":2.0,
        "Solution_body":"<ul>\n<li>if you want to use a specific EC2 instance, use SageMaker<\/li>\n<li>Pricing: SageMaker is pro-rated per-second while Glue has minimum charge amount (1min or 10min depending on versions). You should measure how much would a workload cost you on each platform<\/li>\n<li>customization: in SageMaker Processing you can customize the execution environment, as you provide a Docker image (you could run more than Spark\/Python, such as C++ or R)<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.9,
        "Solution_reading_time":5.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1262052472408,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Melbourne VIC, Australia",
        "Answerer_reputation_count":13830.0,
        "Answerer_view_count":1372.0,
        "Challenge_adjusted_solved_time":31.96977,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I tried to start a R notebook in Sagemaker and I typed<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>install.packages(\"disk.frame\")\n<\/code><\/pre>\n\n<p>and it gave me the error<\/p>\n\n<pre><code>also installing the dependencies \u2018listenv\u2019, \u2018dplyr\u2019, \u2018rlang\u2019, \u2018furrr\u2019, \n\u2018future.apply\u2019, \u2018fs\u2019, \u2018pryr\u2019, \u2018fst\u2019, \u2018globals\u2019, \u2018future\u2019\n\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018rlang\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018fs\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018pryr\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018fst\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018dplyr\u2019 had non-zero exit status\u201d\nWarning message in install.packages(\"disk.frame\"):\n\u201cinstallation of package \u2018disk.frame\u2019 had non-zero exit status\u201d\nUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n<\/code><\/pre>\n\n<p>How do I install R packages on Sagemaker?<\/p>",
        "Challenge_closed_time":1566562240968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1566445052783,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to install the R package \"disk.frame\" on a SageMaker Notebook instance but is encountering errors related to the installation of dependencies such as \"rlang\", \"fs\", \"pryr\", \"fst\", and \"dplyr\". The user is seeking guidance on how to install R packages on SageMaker.",
        "Challenge_last_edit_time":1566447149796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57601733",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":12.1,
        "Challenge_reading_time":15.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":32.5522736111,
        "Challenge_title":"How do I install R packages on the SageMaker Notebook instance?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2786.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1262052472408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Melbourne VIC, Australia",
        "Poster_reputation_count":13830.0,
        "Poster_view_count":1372.0,
        "Solution_body":"<p>I think you just need to specify a repo. For example, setting the RStudio CRAN repo, I can install perfectly fine.<\/p>\n\n<pre class=\"lang-r prettyprint-override\"><code>install.packages(\"disk.frame\", repo=\"https:\/\/cran.rstudio.com\/\")\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.6,
        "Solution_reading_time":3.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1329.0686111111,
        "Challenge_answer_count":0,
        "Challenge_body":"I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\r\nThe name of the inputs are:\r\n1. `input_image` \r\n2. `input_image_meta` \r\n3. `input_anchors` \r\n\r\n\r\nand the name of outputs are:\r\n1.  `output_detections`\r\n2.  `output_mrcnn_class`\r\n3.  `output_mrcnn_bbox`\r\n4.  `output_mrcnn_mask`\r\n5.  `output_rois`\r\n\r\nI have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting `{'error': \"Missing 'inputs' or 'instances' key\"}` in return.\r\n \r\nI have made a model.tar.gz file which has the following structure:\r\n\r\n    mymodel\r\n        |__1\r\n            |__variables\r\n            |__saved_model.pb\r\n\r\n    code\r\n        |__inference.py\r\n        |__requirements.txt\r\n\r\nAs specified in the documentation, inference.py has input_handler and output handler functions. From the client-side, I pass the S3 link of the image which then transforms to the three inputs for the model. \r\n\r\nThe structure of input_handler is as follows:\r\n\r\n```\r\ndef input_handler(data, context):\r\n     input_data = json.loads(data.read().decode('utf-8'))\r\n\r\n    obj = bucket.Object(input_data['img_link'])\r\n    tmp = tempfile.NamedTemporaryFile()\r\n    \r\n    # download image from AWS S3\r\n    with open(tmp.name, 'wb') as f:\r\n        obj.download_fileobj(f)\r\n        image=mpimg.imread(tmp.name)\r\n    \r\n    # make preprocessing\r\n    image = Image.fromarray(image)\r\n     \r\n     ...... # some more transformations \r\n     return = {\"input_image\": Python list for image,\r\n                    \"input_image_meta: Python list for input image meta,\r\n                    \"input_anchors\": Python list for input anchors}\r\n\r\n```\r\nThe deifinition of output_handler is as follows:\r\n\r\n```\r\ndef output_handler(data, context):\r\n      output_string = data.content.decode('unicode-escape')\r\n      return output_string, context.accept_header\r\n```\r\n\r\nThe sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\r\nOn the client side, I call the predictor using follwoing code:\r\n\r\n```\r\nrequest = {}\r\nrequest[\"img_link\"] = \"image.jpg\"\r\nresult = predictor.predict(request)\r\n```\r\n\r\nBut when I print the result the following gets printed out, `{'error': \"Missing 'inputs' or 'instances' key\"}`\r\nAll the bucket connections for loading the image are in inference.py\r\n      ",
        "Challenge_closed_time":1571957138000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1567172491000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error message stating that no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/73",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":28.7,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":104.0,
        "Challenge_repo_issue_count":229.0,
        "Challenge_repo_star_count":160.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1329.0686111111,
        "Challenge_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":283,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello @janismdhanbad,\r\n\r\nI believe your inference requests will have to follow the TensorFlow serving REST API specifications defined here: https:\/\/www.tensorflow.org\/tfx\/serving\/api_rest#request_format_2\r\n\r\n```\r\nThe request body for predict API must be JSON object formatted as follows:\r\n\r\n{\r\n  \/\/ (Optional) Serving signature to use.\r\n  \/\/ If unspecifed default serving signature is used.\r\n  \"signature_name\": <string>,\r\n\r\n  \/\/ Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\r\n  \/\/ A request can have either of them but NOT both.\r\n  \"instances\": <value>|<(nested)list>|<list-of-objects>\r\n  \"inputs\": <value>|<(nested)list>|<object>\r\n}\r\n``` closing due to inactivity. feel free to reopen if necessary.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.3,
        "Solution_reading_time":8.78,
        "Solution_score_count":null,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":529.4461111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nA SageMaker Notebook-v3 workspace that was working fine on Friday today appears with the status as \"Unknown\". \r\nWhen clicking on connect the new window pop up but is empty, and when going back to the SWB page, we see the message, \"We have a problem! Something went wrong\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to 'Workspaces'\r\n2. Look for the workspace that was expected to be \"Stoped\"\r\n2. Click on 'connect'\r\n4. See error\r\n\r\n**Expected behavior**\r\nThat the workspace was \"Stopped\" and when clicking on Connect we can access to the workspace. \r\n\r\n**Screenshots**\r\n![Screen Shot 2021-09-13 at 1 27 57 PM](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**Versions (please complete the following information):**\r\nRelease Version installed: 3.3.1\r\n\r\n**Additional context**\r\nThe workspace was working fine all previous week, autostop and connect without any issue. Unknown status found today.",
        "Challenge_closed_time":1633460282000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1631554276000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error message \"null is not an object\" while trying to connect to Sagemaker notebook. The error is intermittent and occurs after the workspace has been open for a while. The notebook window is not opened after clicking on 'Connect'. The expected behavior is a new window should open with a Jupyter\/Sagemaker notebook in a new window.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/708",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.9,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":529.4461111111,
        "Challenge_title":"[Bug] SageMaker Notebook-v3 Workspace changed to \"Unknown\" status and cannot connect anymore",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":148,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I think there's a good chance this instance was autostopped, but that information was not propagated to DDB correctly.\r\n\r\nCan you log onto the hosting account for that Sagemaker instance and check if it's currently in the `Stopped` state. If yes, the latest code fixes that issue.\r\nhttps:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/8cb199b8093f5e799d2d87c228930a4929ebebb7 Hi @nguyen102 yes, I can confirm that the Sagemaker instance is  in the Stopped sate. So then, the latest code that you mention should fix the issue. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":6.64,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":41.0340352778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use a Jupyter Notebook instance on Sagemaker to run a code that took around 3 hours to complete. Since I pay for hour use, I would like to automatically \"Close and Halt\" the Notebook as well as stop the \"Notebook instance\" after running that code. Is this possible?<\/p>",
        "Challenge_closed_time":1568364157240,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568216434713,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a Jupyter code that can automatically close and halt the Notebook instance on Sagemaker and stop it after running a code that takes around 3 hours to complete, in order to save on hourly usage costs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57892580",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":4.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":41.0340352778,
        "Challenge_title":"Is there a Jupyter code that I can use \"to stop\" a Notebook Instances on Sagemaker, after running any code?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1393.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509012479112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Jupyter Notebook are predominantly designed for exploration and\n   development. If you want to launch long-running or scheduled jobs on\n   ephemeral hardware, it will be a much better experience to use the\n   training API, such as the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\"><code>create_training_job<\/code><\/a> in boto3 or the\n   <code>estimator.fit()<\/code> of the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">Python SDK<\/a>. The code passed to training jobs\n   can be completely arbitrary - not necessarily ML code - so whatever\n   you write in jupyter could likely be scheduled and ran in those\n   training jobs. See the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">random forest sklearn demo here<\/a> for an\n   example. That being said, if you still want to programmatically shut\n   down a SageMaker notebook instance, you can use that boto3 call:<\/p>\n\n<pre><code>import boto3\n\nsm = boto3.client('sagemaker')\nsm.stop_notebook_instance(NotebookInstanceName='string')\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.9,
        "Solution_reading_time":16.82,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1420765480790,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":340.0,
        "Answerer_view_count":61.0,
        "Challenge_adjusted_solved_time":2142.9330063889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Challenge_closed_time":1549414531500,
        "Challenge_comment_count":3,
        "Challenge_created_time":1541699972677,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to execute python from a conda environment using dvc run. Despite having a conda environment configured with python 3.6 and dvc installed, dvc is calling the python version of the main installation of conda and not finding the installed libraries.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2142.9330063889,
        "Challenge_title":"How to execute python from conda environment by dvc run",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":351.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420765480790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":340.0,
        "Poster_view_count":61.0,
        "Solution_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":0.69,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":73.275,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n\r\nhttps:\/\/github.com\/Azure\/azureml-examples\/runs\/1618089261?check_suite_focus=true\r\n\r\n@trangevi \r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/pull\/3419\/files",
        "Challenge_closed_time":1609558021000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1609294231000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered an exception when running the code in qlib-main\/examples\/workflow_by_code.ipynb, resulting in an invalid experiment id of '.ipynb_checkpoints'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/318",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":30.8,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":646.0,
        "Challenge_repo_issue_count":1964.0,
        "Challenge_repo_star_count":873.0,
        "Challenge_repo_watch_count":2758.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":73.275,
        "Challenge_title":"MLflow 1.13 probably broke deployment",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":8,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"yeah @trangevi the logging statement is guaranteed to bork out: https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/azureml\/__init__.py#L413\r\n\r\n@eedeleon fyi https:\/\/github.com\/mlflow\/mlflow\/pull\/3922\r\n @akshaya-a @eedeleon looks resolved, thanks for investigating! will close this issue ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.1,
        "Solution_reading_time":3.79,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":3.7412430556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>in tutorials like <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/fine-tuning-a-pytorch-bert-model-and-deploying-it-with-amazon-elastic-inference-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Fine-tuning a pytorch bert model and deploying it with sagemaker<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/fine-tune-and-host-hugging-face-bert-models-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">fine-tune and host huggingface models on sagemaker<\/a>, a hugging face estimator is used to call a training script. What would be the difference if I just directly ran the script's code in the notebook itself? is it because the estimator makes it easier to deploy the model?<\/p>",
        "Challenge_closed_time":1646330906792,
        "Challenge_comment_count":0,
        "Challenge_created_time":1646317438317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is asking about the difference between using a hugging face estimator with a training script and directly using a notebook in AWS Sagemaker. They want to know if there is any advantage to using the estimator, or if running the script's code in the notebook would be just as effective.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71338750",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":10.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":3.7412430556,
        "Challenge_title":"what is the difference between using a hugging face estimator with training script and directly using a notebook in AWS sagemaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1597997723910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>You could run the script in the notebook itself but it would not deploy with SageMaker provided capabilities then. The estimator that you are seeing is what specifies to SageMaker what framework you are using and the training script that you are passing in. If you ran the script code in the notebook that would be like training in your local environment. By passing in the script to the Estimator you are running a SageMaker training job. The estimator is meant to encapsulate training on SageMaker.<\/p>\n<p>SageMaker Estimator Overview: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":8.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":93.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.5969425,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>****Do I need to add the .ipynb extension manually to my notebook file ?**  <br \/>\n**I can't find the run button.****<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/85574-capture2.png?platform=QnA\" alt=\"85574-capture2.png\" \/><\/p>",
        "Challenge_closed_time":1617872811416,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617852662423,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to find the run button in their notebook file and is unsure if they need to manually add the .ipynb extension to the file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/348777\/cant-find-the-run-button",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":3.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":5.5969425,
        "Challenge_title":"Can't find the run button",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":28,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=441c29e9-5f3d-4595-9f6c-dec6fcde5e85\">@paul gureghian  <\/a> You can re-name your file with .ipynb extension which should help to display the cells and the available options like run button for cell. Usually while creating new files in your workspace the UI prompts to select the extension of the file. If you can create a new file with .ipynb extension and copy these cells individually that should also work. Thanks.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":5.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":7.3733694445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pip install pyenchant, but It doesn't seem to be working.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/169225-image.png?platform=QnA\" alt=\"169225-image.png\" \/>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/169252-image.png?platform=QnA\" alt=\"169252-image.png\" \/>    <\/p>\n<p>Is there any other way?    <br \/>\n<a href=\"https:\/\/stackoverflow.com\/questions\/21083059\/enchant-c-library-not-found-while-installing-pyenchant-using-pip-on-osx\">https:\/\/stackoverflow.com\/questions\/21083059\/enchant-c-library-not-found-while-installing-pyenchant-using-pip-on-osx<\/a>    <br \/>\nI looked it up but do not know where to put it    <br \/>\nThanks!<\/p>",
        "Challenge_closed_time":1643357543967,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643330999837,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has installed pyenchant using pip but it is not working. They are looking for an alternative solution and have found a Stack Overflow post suggesting to install the enchant C library, but they are unsure where to put it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/713217\/machine-learning-conda-env-package(pyenchant)",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":20.1,
        "Challenge_reading_time":9.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7.3733694445,
        "Challenge_title":"machine learning conda env package(pyenchant)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":48,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c427e306-40a4-4da4-b489-b3f6aae251d7\">@Yongchao Liu (Neusoft America Inc)  <\/a> Based on the error it looks like you also need to ensure the enchant C library is available to use for the package. Based on the pip install <a href=\"https:\/\/pyenchant.github.io\/pyenchant\/install.html#installation\">page<\/a> of pyenchant, the package will not work directly out of the box using pip.    <\/p>\n<blockquote>\n<p>In general, PyEnchant will not work out of the box after having been installed with pip. See the Installation section for more details.    <\/p>\n<\/blockquote>\n<p>Since you are using Linux, this is the guidance on the installation page.    <\/p>\n<blockquote>\n<p>The quickest way is to install libenchant using the package manager of your current distribution. PyEnchant tries to be compatible with a large number of libenchant versions. If you find an incompatibility with your libenchant installation, feel free to open a bug report.    <\/p>\n<p>To detect the libenchant binaries, PyEnchant uses ctypes.util.find_library(), which requires ldconfig, gcc, objdump or ld to be installed. This is the case on most major distributions, however statically linked distributions (like Alpine Linux) might not bring along binutils by default.    <\/p>\n<\/blockquote>\n<p>I believe you are using the ubuntu flavor of the azureml base image, In this case I think adding <a href=\"https:\/\/ubuntu.pkgs.org\/20.04\/ubuntu-main-amd64\/libenchant-2-dev_2.2.8-1_amd64.deb.html\">libenchant-2-dev<\/a> as dependency in your YAML should work.     <\/p>\n<pre><code>-libenchant-2-dev=2.2.8  \n<\/code><\/pre>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.1,
        "Solution_reading_time":25.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":236.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1515266756243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tempe, AZ, USA",
        "Answerer_reputation_count":143.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":2.6999758333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm deploying a Keras model that is failing with the error below. The exception says that I can retrieve the logs by running \"print(service.get_logs())\", but that's giving me empty results. I am deploying the model from my AzureNotebook and I'm using the same \"service\" var to retrieve the logs. <\/p>\n\n<p>Also, how can i retrieve the logs from the container instance? I'm deploying to an AKS compute cluster I created. Sadly, the docs link in the exception also doesnt detail how to retrieve these logs.<\/p>\n\n<pre><code>More information can be found using '.get_logs()' Error: \n<\/code><\/pre>\n\n<pre><code>{   \"code\":\n\"KubernetesDeploymentFailed\",   \"statusCode\": 400,   \"message\":\n\"Kubernetes Deployment failed\",   \"details\": [\n    {\n      \"code\": \"CrashLoopBackOff\",\n      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check\nthe logs for your container instance: my-model-service. From\nthe AML SDK, you can run print(service.get_logs()) if you have service\nobject to fetch the logs. \\nYou can also try to run image\nmlwks.azurecr.io\/azureml\/azureml_3c0c34b65cf18c8644e8d745943ab7d2:latest\nlocally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails\nfor more information.\"\n    }   ] }\n<\/code><\/pre>\n\n<p>UPDATE<\/p>\n\n<p>Here's my code to deploy the model:<\/p>\n\n<pre><code>environment = Environment('my-environment')\nenvironment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\"azureml-defaults\",\"azureml-dataprep[pandas,fuse]\",\"tensorflow\", \"keras\", \"matplotlib\"])\nservice_name = 'my-model-service'\n\n# Remove any existing service under the same name.\ntry:\n    Webservice(ws, service_name).delete()\nexcept WebserviceException:\n    pass\n\ninference_config = InferenceConfig(entry_script='score.py', environment=environment)\ncomp = ComputeTarget(workspace=ws, name=\"ml-inference-dev\")\nservice = Model.deploy(workspace=ws,\n                       name=service_name,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_target=comp \n                      )\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>And my score.py<\/p>\n\n<pre><code>import joblib\nimport numpy as np\nimport os\n\nimport keras\n\nfrom keras.models import load_model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\n\ndef init():\n    global model\n\n    model_path = Model.get_model_path('model.h5')\n    model = load_model(model_path)\n    model = keras.models.load_model(model_path)\n\n\n# The run() method is called each time a request is made to the scoring API.\n#\n# Shown here are the optional input_schema and output_schema decorators\n# from the inference-schema pip package. Using these decorators on your\n# run() method parses and validates the incoming payload against\n# the example input you provide here. This will also generate a Swagger\n# API document for your web service.\n@input_schema('data', NumpyParameterType(np.array([[0.1, 1.2, 2.3, 3.4, 4.5, 5.6, 6.7, 7.8, 8.9, 9.0]])))\n@output_schema(NumpyParameterType(np.array([4429.929236457418])))\ndef run(data):\n\n    return [123] #test\n<\/code><\/pre>\n\n<p><strong>Update 2:<\/strong><\/p>\n\n<p>Here is a screencap of the endpoint page. Is it normal for the CPU to be .1? Also, when i hit the swagger url in the browser, i get the error: \"No ready replicas for service doc-classify-env-service\"<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Uvrfx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Uvrfx.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Update 3<\/strong>\nAfter finally getting to the container logs, it turns out that it was choking  with this error on my score.py<\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError: No module named 'inference_schema'<\/p>\n<\/blockquote>\n\n<p>I then ran a test that commented out the refs for \"input_schema\" and \"output_schema\" and also simplified my pip_packages and the REST endpoint come up! I was also able to get a prediction out of the model. <\/p>\n\n<pre><code>pip_packages=[\"azureml-defaults\",\"tensorflow\", \"keras\"])\n<\/code><\/pre>\n\n<p>So my question is, how should I have my pip_packages for the scoring file to utilize the inference_schema decorators? I'm assuming I need to include azureml-sdk[auotml] pip package, but when i do so, the image creation fails and I see several dependency conflicts.<\/p>",
        "Challenge_closed_time":1588964687420,
        "Challenge_comment_count":2,
        "Challenge_created_time":1588954967507,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy a Keras model on Azure ML, but the deployment is failing with a KubernetesDeploymentFailed error. The user is trying to retrieve the logs using \"print(service.get_logs())\", but it is giving empty results. The user is also unable to retrieve the logs from the container instance. The user has shared the code used to deploy the model and the score.py file. After analyzing the logs, the user found that the error was caused by a missing module \"inference_schema\". The user was able to resolve the issue by commenting out the \"input_schema\" and \"output_schema\" references and simplifying the pip_packages. The user is now trying to figure out how to include the \"inference_schema\" module in",
        "Challenge_last_edit_time":1589409955880,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61683506",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":56.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":42,
        "Challenge_solved_time":2.6999758333,
        "Challenge_title":"Azure ML: how to access logs of a failed Model deployment",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1564.0,
        "Challenge_word_count":495,
        "Platform":"Stack Overflow",
        "Poster_created_time":1330016065408,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1704.0,
        "Poster_view_count":232.0,
        "Solution_body":"<p>Try retrieving your service from the workspace directly <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ws.webservices[service_name].get_logs()\n<\/code><\/pre>\n\n<p>Also, I found deploying an image as an endpoint to be easier than inference+deploy model (depending on your use case) <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_image = Image(ws, name='test', version='26')  \nservice = AksWebservice.deploy_from_image(ws, \"test1\", my_image, deployment_config, aks_target)\n<\/code><\/pre>",
        "Solution_comment_count":13.0,
        "Solution_last_edit_time":1588970402070,
        "Solution_link_count":0.0,
        "Solution_readability":16.5,
        "Solution_reading_time":6.6,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":47.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":195.7456172222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie to aws sagemaker.\nI am trying to setup a model in aws sagemaker using keras with GPU support.\nThe docker base image used to infer the model is given below<\/p>\n\n<pre><code>FROM tensorflow\/tensorflow:1.10.0-gpu-py3\n\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends nginx curl\n...\n<\/code><\/pre>\n\n<p>This is the keras code I'm using to check if a GPU is identified by keras in flask.<\/p>\n\n<pre><code>import keras\n@app.route('\/ping', methods=['GET'])\ndef ping():\n\n    keras.backend.tensorflow_backend._get_available_gpus()\n\n    return flask.Response(response='\\n', status=200,mimetype='application\/json')\n<\/code><\/pre>\n\n<p>When I spin up a notebook instance in a sagemaker using the GPU the keras code shows available GPUs.\nSo, in order to access GPU in the inference phase(model) do I need to install any additional libraries in the docker file apart from the tensorflow GPU base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":1545210167392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1544505483170,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is a newbie to AWS Sagemaker and is trying to set up a model using Keras with GPU support. They are using a Docker base image with TensorFlow 1.10.0 GPU and are trying to check if a GPU is identified by Keras in Flask. The user is asking if they need to install any additional libraries in the Docker file to access the GPU in the inference phase.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53717800",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":12.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":195.7456172222,
        "Challenge_title":"Configuring GPU in aws sagemaker with keras and tensorflow as backend",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1958.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544503799112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":57.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>You shouldn't need to install anything else. Keras relies on TensorFlow for GPU detection and configuration.<\/p>\n\n<p>The only thing worth noting is how to use multiple GPUs during training. I'd recommend passing 'gpu_count' as an hyper parameter, and setting things up like so:<\/p>\n\n<pre><code>from keras.utils import multi_gpu_model\nmodel = Sequential()\nmodel.add(...)\n...\nif gpu_count &gt; 1:\n    model = multi_gpu_model(model, gpus=gpu_count)\nmodel.compile(...)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":6.08,
        "Solution_score_count":5.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":59.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":1.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.8151808334,
        "Challenge_answer_count":2,
        "Challenge_body":"I am running a Sagemaker Notebook instance. How can I tell if my Notebook is frozen or just taking a long time? I am using a 24xlarge and querying from Athena in parallel and it seems to be stuck on the same query for a long time. How can I tell if I need more Memory or more VCPUs?",
        "Challenge_closed_time":1683362949783,
        "Challenge_comment_count":0,
        "Challenge_created_time":1683306015132,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is running a Sagemaker Notebook instance and is unsure if it is frozen or just taking a long time. They are using a 24xlarge and querying from Athena in parallel, but it seems to be stuck on the same query for a long time. The user is unsure if they need more Memory or more VCPUs.",
        "Challenge_last_edit_time":1683644479959,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUK_hTODBBTeWX_c-lcO34mg\/how-can-i-tell-if-my-notebook-instance-is-frozen",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.9,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":15.8151808334,
        "Challenge_title":"How can I tell if my Notebook instance is frozen?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":72.0,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi there,\n\nGreetings for the day!\n\nI understand that you wanted to know how can you determine if you need more VCPU or memory when your SageMaker Notebook Instance is frozen or just taking a long time?\n\nI\u2019d like to inform you that If your Sagemaker Notebook instance is taking a long time, you can check if it is frozen or still running by monitoring the CPU and memory usage. If the CPU usage is low or zero, it may be frozen. \n\nIf the CPU usage is high but the memory usage is low, you may need more VCPUs and If the memory usage is high, you may need more memory. \n\nYou can check it via SageMaker Notebook Instance terminal:\n\nTo see the memory and CPU information in detail , kindly follow the below instructions:-\n\n[1] Start Your Notebook Instance\n[2] Go to the  Jupyter Home Page \n[3] Right hand side ,Click on DropDown Option \u201cNew\u201d \n[4] Select \u201cTerminal\u201d.\n\nIn the Jupyter terminal, Run the below commands to see the information of Memory and CPUs.\n\n[+] To see the memory information:\n\n$ free -h\n\n=> output of \u201cfree -h\u201d will provide the information of total memory, used memory, free memory, shared memory etc in human readable form.\n\n[+] To see the CPU information, you can run any of the commands:\n\n$ mpstat -u\n\n=> Output of \u201cmpstat -u\u201d consists of different fields like %guest, %gnice, %steal etc.\n\n%steal\nShow the percentage of time spent in involuntary wait by the virtual CPU or CPUs while the hyper\u2010\nvisor was servicing another virtual processor.\n\n%guest\nShow the percentage of time spent by the CPU or CPUs to run a virtual processor.\n\n%idle\nShow the percentage of time that the CPU or CPUs were idle and the system did not have an out\u2010\nstanding disk I\/O request.\n\nmany more.. You can find more detail about the each field of mpstat command by visiting the manual page of it. To see the manual page of mpstat command, use \u201c$ man mpstst\u201d.\n\nAlong with \u201cmpstat -u\u201d, you can also try the below listed commands to get information about the cpu:\n\n$ lscpu\n$ cat \/proc\/cpuinfo\n$ top\n\nAdditionally, You can also check the cloudwatch logs for any errors or warnings that may indicate the cause of the issue. Most of the time, Cloud watch logs helps to find out the root cause of the issue.\n\nYou can find the CloudWatch logs under CloudWatch \u2192 Log Groups \u2192 \/aws\/sagemaker\/NotebookInstances -> Notebook Instance Name\n\nBased on the analysis , you can select different Notebook Instance type, You can find more detail of SageMaker Instance Here [1].\n\nI request you to kindly follow the above suggested workarounds. \n\nIf you have any difficulty or run into any issue, Please reach out to AWS Support[+] (Sagemaker), along with your issue\/use case in details and we would be happy to assist you further.\n\n[+] Creating support cases and case management - https:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-casehttps:\/\/docs.aws.amazon.com\/awssupport\/latest\/user\/case-management.html#creating-a-support-case \n\nI hope this information would be useful to you.\n\nThank you!\n\nREFERENCES:\n\n[1] https:\/\/aws.amazon.com\/sagemaker\/pricing\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1683362949783,
        "Solution_link_count":2.0,
        "Solution_readability":10.8,
        "Solution_reading_time":37.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":488.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":674.2494444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I am running a lightly edited version of this pipeline example: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-azureml\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.`\r\n\r\nI am also getting this same warning in other pipelines I make and I cannot figure out what is causing it.\r\n\r\nHere is a slightly reduced MWE for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication\r\nfrom azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\r\nfrom azureml.core.conda_dependencies import CondaDependencies\r\nfrom azureml.core.compute import ComputeTarget, AmlCompute\r\nfrom azureml.core.compute_target import ComputeTargetException\r\nfrom azureml.data import OutputFileDatasetConfig\r\nfrom azureml.pipeline.steps import PythonScriptStep\r\nfrom azureml.pipeline.core import Pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\r\n                                    service_principal_id=os.environ['AML_PRINCIPAL_ID'],\r\n                                    service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = Workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = PythonScriptStep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=RunConfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom azureml.core import Run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = Pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = Experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=True)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nExpected a StepRun object but received <class 'azureml.core.run.Run'> instead.\r\nThis usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\r\nPlease check for package conflicts in your python environment\r\n```\r\n",
        "Challenge_closed_time":1626719342000,
        "Challenge_comment_count":7,
        "Challenge_created_time":1624292044000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to deploy an ML model using the az ml model deploy command with additional files, but encounters an error stating that the Dockerfile instruction COPY is not allowed. The error message suggests that only certain instructions such as ARG, ENV, EXPOSE, LABEL, and RUN are allowed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1517",
        "Challenge_link_count":1,
        "Challenge_participation_count":7,
        "Challenge_readability":15.5,
        "Challenge_reading_time":36.56,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":674.2494444444,
        "Challenge_title":"AzureML Pipelines: Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":249,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@afogarty85 can you share the version of SDK you are using? ```\r\nimport azureml\r\nprint(azureml.core.__version__)\r\n1.31.0\r\n``` @afogarty85, I'm unable to reproduce the error you are seeing. Is the pipeline running despite the error\/warning? It is running\/working anyways and indeed -- on a different workspace, I too cannot reproduce it. I am not sure why it is a symptom of the one I am on. I am opening a bug for investigation and will update you when I have a response.  I am also running into this issue with code that was working previously. Had a weekly pipeline scheduled to run at the start of every Monday. It usually took around a couple of minutes  to finish but looking back at some logs it seems like after June 13  runs were taking 100+ hours and most timed out. I tried to manually run the pipeline and hit the exact same issue with Expecting StepRun object, not sure if there was some sort of update around the middle of June to the SDK?\r\n\r\n\r\n***EDIT Had to update the Azure ML SDK along with the azureml-automl-core, azureml-pipeline-core, and azureml-pipeline packages*** I'm sharing the investigation from engineering below. Since this is expected behavior, we will not be fixing it. Hope this helps. \r\n\r\nThis bug is activated if the user has a package version conflict in their local python environment, the PipelineRun.wait_for_completion() method may fail with an error 'Unexpected keyword argument timeout_seconds'. This is because the run rehydration fails and we receive a run object with the wrong type, which doesn't have this argument.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":18.91,
        "Solution_score_count":null,
        "Solution_sentence_count":17.0,
        "Solution_word_count":257.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":124.1602897223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pkl package saved in my azure devops repository<\/p>\n<p>using below code it searches for package in workspace.\nHow to provide package saved in repository<\/p>\n<pre><code> ws = Workspace.get(\n         name=workspace_name,\n         subscription_id=subscription_id,\n        resource_group=resource_group,\n        auth=cli_auth)\n\nimage_config = ContainerImage.image_configuration(\n    execution_script=&quot;score.py&quot;,\n    runtime=&quot;python-slim&quot;,\n    conda_file=&quot;conda.yml&quot;,\n    description=&quot;Image with ridge regression model&quot;,\n    tags={&quot;area&quot;: &quot;ml&quot;, &quot;type&quot;: &quot;dev&quot;},\n)\n\nimage = Image.create(\n    name=image_name,  models=[model], image_config=image_config, workspace=ws\n)\n\nimage.wait_for_creation(show_output=True)\n\nif image.creation_state != &quot;Succeeded&quot;:\n    raise Exception(&quot;Image creation status: {image.creation_state}&quot;)\n\nprint(\n    &quot;{}(v.{} [{}]) stored at {} with build log {}&quot;.format(\n        image.name,\n        image.version,\n        image.creation_state,\n        image.image_location,\n        image.image_build_log_uri,\n    )\n)\n\n# Writing the image details to \/aml_config\/image.json\nimage_json = {}\nimage_json[&quot;image_name&quot;] = image.name\nimage_json[&quot;image_version&quot;] = image.version\nimage_json[&quot;image_location&quot;] = image.image_location\nwith open(&quot;aml_config\/image.json&quot;, &quot;w&quot;) as outfile:\n    json.dump(image_json, outfile)\n<\/code><\/pre>\n<p>I tried to provide path to models but its fails saying package not found<\/p>\n<p>models = $(System.DefaultWorkingDirectory)\/package_model.pkl<\/p>",
        "Challenge_closed_time":1627276170736,
        "Challenge_comment_count":0,
        "Challenge_created_time":1626831896507,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an Azure Machine Learning scoring image using a local package saved in their Azure DevOps repository. They are encountering an issue where the code is searching for the package in the workspace and they are unsure how to provide the package saved in the repository. They have attempted to provide the path to the models but it fails, stating that the package is not found.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68463080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":18.9,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":123.4095080556,
        "Challenge_title":"how create azure machine learning scoring image using local package",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":121,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567209656790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":417.0,
        "Poster_view_count":233.0,
        "Solution_body":"<p>Register model:\nRegister a file or folder as a model by calling <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-\" rel=\"nofollow noreferrer\">Model.register()<\/a>.<\/p>\n<p>In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace.<\/p>\n<pre><code>model = Model.register(workspace=ws,\n                       model_name='',                # Name of the registered model in your workspace.\n                       model_path='',  # Local file to upload and register as a model.\n                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n                       sample_input_dataset=input_dataset,\n                       sample_output_dataset=output_dataset,\n                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n                       description='Ridge regression model to predict diabetes progression.',\n                       tags={'area': 'diabetes', 'type': 'regression'})\n\nprint('Name:', model.name)\nprint('Version:', model.version)\n<\/code><\/pre>\n<p>Deploy machine learning models to Azure: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a><\/p>\n<p>To Troubleshooting remote model deployment Please follow the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?tabs=azcli#function-fails-get_model_path\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1627278873550,
        "Solution_link_count":6.0,
        "Solution_readability":19.4,
        "Solution_reading_time":29.03,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":159.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1406731060412,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation_count":139.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":7804.7933702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried connecting through Sagemaker notebook to RDS. However, to connect to RDS, my public IP needs to be allowed for security reasons. I can see when I run this command: &quot;curl ifconfig.me&quot; on Sagemaker Notebook instance that public IP keeps changing from time to time.<\/p>\n<p>What is the correct way to connect to RDS with notebook on sagemaker? Do I need to crawl the RDS with AWS Glue and then use Athena on crawled tables and then take the query results from S3 with Sagemaker notebook?<\/p>",
        "Challenge_closed_time":1627578308223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599481052090,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting Sagemaker Jupyter Notebook to RDS due to the changing public IP address. The user is seeking advice on the correct way to connect to RDS with the notebook on Sagemaker. The user is also considering using AWS Glue to crawl RDS and then using Athena on crawled tables to take query results from S3 with Sagemaker notebook.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63777462",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.2,
        "Challenge_reading_time":6.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":7804.7933702778,
        "Challenge_title":"Sagemaker Jupyter Notebook Cannot Connect to RDS",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":865.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Poster_created_time":1509559597047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":313.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>RDS is just a managed database running on an EC2 instance. You can connect to that database in a very same way as you would connect from an application. For example, you can use a python based DB client library (depending on what DB flavor you're using, e.g. Postgres) and configure with the connection string, as you would connect any other application to your RDS instance.<\/p>\n<p>I would not recommend to connect to the RDS instance through the public interface. You can place your Notebook instance to the same VPC where your RDS instance is, thus you can talk to RDS directly through the VPC.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":7.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":35.0730002778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own%20example\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own example<\/a> for product recommendations.<\/p>\n\n<p>I want to use the SVD from <a href=\"https:\/\/pypi.org\/project\/scikit-surprise\/\" rel=\"nofollow noreferrer\">scikit-surprise<\/a> library on Sagemaker.<\/p>\n\n<pre><code>from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n<\/code><\/pre>\n\n<p>I added the scikit-surprise package in the Dockerfile, but i am getting the following errors:<\/p>\n\n<h1>Dockerfile:<\/h1>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \\\n    pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nRUN pip install scikit-surprise\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY products_recommender \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<h1>Docker build and deploy :<\/h1>\n\n<pre><code>fullname:XXXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender:latest\nWARNING! Using --password via the CLI is insecure. Use --password-stdin.\nLogin Succeeded\nSending build context to Docker daemon  67.58kB\nStep 1\/10 : FROM ubuntu:16.04\n ---&gt; 13c9f1285025\nStep 2\/10 : MAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n ---&gt; Using cache\n ---&gt; 44baf3286201\nStep 3\/10 : RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends          wget          python          nginx          ca-certificates     &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n ---&gt; Using cache\n ---&gt; 8983fa906515\nStep 4\/10 : RUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python get-pip.py &amp;&amp;     pip install numpy==1.16.2 scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn &amp;&amp;         (cd \/usr\/local\/lib\/python2.7\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp;         rm -rf \/root\/.cache\n ---&gt; Using cache\n ---&gt; 9dbfedf02b57\nStep 5\/10 : RUN pip install scikit-surprise\n ---&gt; Running in 82295cb0affe\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\nCollecting scikit-surprise\n  Downloading https:\/\/files.pythonhosted.org\/packages\/f5\/da\/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa\/scikit-surprise-1.1.0.tar.gz (6.4MB)\nCollecting joblib&gt;=0.11 (from scikit-surprise)\n  Downloading https:\/\/files.pythonhosted.org\/packages\/28\/5c\/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf\/joblib-0.14.1-py2.py3-none-any.whl (294kB)\nRequirement already satisfied: numpy&gt;=1.11.2 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.16.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.2.1)\nRequirement already satisfied: six&gt;=1.10.0 in \/usr\/local\/lib\/python2.7\/dist-packages (from scikit-surprise) (1.12.0)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (setup.py): started\n  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n  ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-Bb1_iT --python-tag cp27:\n  ERROR: running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\/lib.linux-x86_64-2.7\n  creating build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n  creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running egg_info\n  writing requirements to scikit_surprise.egg-info\/requires.txt\n  writing scikit_surprise.egg-info\/PKG-INFO\n  writing top-level names to scikit_surprise.egg-info\/top_level.txt\n  writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n  writing entry points to scikit_surprise.egg-info\/entry_points.txt\n  reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n  copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n  copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n  running build_ext\n  building 'surprise.similarities' extension\n  creating build\/temp.linux-x86_64-2.7\n  creating build\/temp.linux-x86_64-2.7\/surprise\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n  unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for scikit-surprise\n  Running setup.py clean for scikit-surprise\nFailed to build scikit-surprise\nInstalling collected packages: joblib, scikit-surprise\n  Running setup.py install for scikit-surprise: started\n    Running setup.py install for scikit-surprise: finished with status 'error'\n    ERROR: Complete output from command \/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\/lib.linux-x86_64-2.7\n    creating build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/trainset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dataset.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/__main__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/reader.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/builtin_datasets.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/dump.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/utils.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/accuracy.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    creating build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/search.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/split.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    copying surprise\/model_selection\/validation.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/model_selection\n    creating build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/algo_base.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/predictions.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/baseline_only.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/__init__.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/random_pred.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/knns.py -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running egg_info\n    writing requirements to scikit_surprise.egg-info\/requires.txt\n    writing scikit_surprise.egg-info\/PKG-INFO\n    writing top-level names to scikit_surprise.egg-info\/top_level.txt\n    writing dependency_links to scikit_surprise.egg-info\/dependency_links.txt\n    writing entry points to scikit_surprise.egg-info\/entry_points.txt\n    reading manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    writing manifest file 'scikit_surprise.egg-info\/SOURCES.txt'\n    copying surprise\/similarities.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/similarities.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\n    copying surprise\/prediction_algorithms\/co_clustering.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/co_clustering.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/matrix_factorization.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/optimize_baselines.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.c -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    copying surprise\/prediction_algorithms\/slope_one.pyx -&gt; build\/lib.linux-x86_64-2.7\/surprise\/prediction_algorithms\n    running build_ext\n    building 'surprise.similarities' extension\n    creating build\/temp.linux-x86_64-2.7\n    creating build\/temp.linux-x86_64-2.7\/surprise\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I\/usr\/local\/lib\/python2.7\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python2.7 -c surprise\/similarities.c -o build\/temp.linux-x86_64-2.7\/surprise\/similarities.o\n    unable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    ----------------------------------------\nERROR: Command \"\/usr\/bin\/python -u -c 'import setuptools, tokenize;__file__='\"'\"'\/tmp\/pip-install-VsuzGr\/scikit-surprise\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-rrsWf0\/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in \/tmp\/pip-install-VsuzGr\/scikit-surprise\/\nWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe command '\/bin\/sh -c pip install scikit-surprise' returned a non-zero code: 1\nThe push refers to repository [XXXXXXXX.dkr.ecr.ap-southeast-1.amazonaws.com\/products-recommender]\n89c1adca7d35: Layer already exists \nddcb6879486f: Layer already exists \n4a02efecad74: Layer already exists \n92d3f22d44f3: Layer already exists \n10e46f329a25: Layer already exists \n24ab7de5faec: Layer already exists \n1ea5a27b0484: Layer already exists \nlatest: digest: sha256:5ed35f1964d10f13bc8a05d379913c24195ea31ec848157016381fbd1bb12f28 size: 1782\n<\/code><\/pre>",
        "Challenge_closed_time":1579273412888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1579147150087,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user encountered challenges while following the scikit_bring_your_own example for product recommendations, including errors when attempting to install the scikit-surprise package in the dockerfile.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59762829",
        "Challenge_link_count":7,
        "Challenge_participation_count":1,
        "Challenge_readability":20.5,
        "Challenge_reading_time":212.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":223,
        "Challenge_solved_time":35.0730002778,
        "Challenge_title":"AWS Sagemaker scikit_bring_your_own example",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":341.0,
        "Challenge_word_count":1082,
        "Platform":"Stack Overflow",
        "Poster_created_time":1241005356852,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kuala Lumpur, Malaysia",
        "Poster_reputation_count":15794.0,
        "Poster_view_count":1032.0,
        "Solution_body":"<p>The 'x86_64-linux-gnu-gcc' binary can't be found in environment where you're building the container. Make sure that gcc is installed, and that you use the right name (gcc?).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":2.26,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2038.3938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Challenge_closed_time":1600718139000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593379921000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"Kedro Telemetry breaks packaged projects due to assuming that the `pyproject.toml` file exists, which is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases. This problem was introduced with a recent update and occurs when deploying Kedro projects with Kedro Telemetry installed. An exception is thrown when running the project in a folder where only the `conf\/` is present.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2038.3938888889,
        "Challenge_title":"Warning message appears when calling ``kedro mlflow init``",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":57,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1362580980910,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Akron, OH, USA",
        "Answerer_reputation_count":4013.0,
        "Answerer_view_count":72.0,
        "Challenge_adjusted_solved_time":61.5306563889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Challenge_closed_time":1621009841940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620788331577,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to mount an S3 container inside a docker image to be used as a ClearML agent. They have tried three solutions, including using prefabbed configuration in ClearML, mounting using s3fuse, and passing arguments to docker using docker_args and docker_bash_setup_script arguments to Task.create(). However, they have been unable to get any of these solutions to work.",
        "Challenge_last_edit_time":1621022250560,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67496760",
        "Challenge_link_count":5,
        "Challenge_participation_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":21.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":61.5306563889,
        "Challenge_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":770.0,
        "Challenge_word_count":202,
        "Platform":"Stack Overflow",
        "Poster_created_time":1362580980910,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Akron, OH, USA",
        "Poster_reputation_count":4013.0,
        "Poster_view_count":72.0,
        "Solution_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1621012052112,
        "Solution_link_count":3.0,
        "Solution_readability":12.2,
        "Solution_reading_time":29.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":277.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6838888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\nI am trying to launch an endpoint locally, to do couple inferences from my dev notebook (without having to wait for instanciation time of actual endpoint or batch training). I am running the following code:\n\n    # get trained model from s3\n    trained_S2S = SM.model.Model(\n        image=seq2seq,\n        model_data=('s3:\/\/XXXXXXXXXXXXX\/'\n            + 'output\/seq2seq-2018-07-30-16-55-12-521\/output\/model.tar.gz'),\n        role=role) \n    \n    S2S = trained_S2S.deploy(1, instance_type='local')    \n\nI get the following error (several hundreds of lines repeated):\n\n    WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa5c61eaac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': \/ping \n`RuntimeError: Giving up, endpoint: seq2seq-2018-08-01-14-18-06-555 didn't launch correctly`",
        "Challenge_closed_time":1533135863000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533133401000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"RuntimeError: Giving up, endpoint: didn't launch correctly\" error while trying to launch an endpoint locally to perform inferences from their dev notebook. The error message shows that the connection was broken and the endpoint failed to launch correctly.",
        "Challenge_last_edit_time":1668556657875,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU54PWM3V9QoybCiO5GvB03g\/sagemaker-local-deployment-runtimeerror-giving-up-endpoint-didn-t-launch-correctly",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":13.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.6838888889,
        "Challenge_title":"Sagemaker local deployment: \"RuntimeError: Giving up, endpoint: didn't launch correctly\"",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":431.0,
        "Challenge_word_count":102,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Currently, SageMaker local is supported only for SageMaker framework containers (MXNet, TensorFlow, PyTorch, Chainer and Spark) and not for the Builtin algorithms",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925544815,
        "Solution_link_count":0.0,
        "Solution_readability":15.4,
        "Solution_reading_time":2.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1522794798772,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":157.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":7.1225861111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>when I create a run using <code>mlflow.start_run()<\/code> ,even if my script is interrupted before executing <code>mlflow.end_run()<\/code>, the run gets tagged as finished instead of unfinished in Status?<\/p>",
        "Challenge_closed_time":1618223603527,
        "Challenge_comment_count":2,
        "Challenge_created_time":1618197962217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where an MLflow experiment run, created using \"mlflow.start_run()\", is getting tagged as finished even if the script is interrupted before executing \"mlflow.end_run()\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67052295",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.8,
        "Challenge_reading_time":3.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":7.1225861111,
        "Challenge_title":"MLflow unfinished experiment saved as finished",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":32,
        "Platform":"Stack Overflow",
        "Poster_created_time":1578750761196,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>When your notebook stops the run gets the status finished. However, if you want to continue logging metrics or artifacts to that run, you just need to use <code>mlflow.start_run(run_id=&quot;YourRunIDYouCanGetItFromUI&quot;)<\/code>. This is explained in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":5.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":38.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":13264.2386313889,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I have a notebook on SageMaker I would like to run every night. What's the best way to schedule this task. Is there a way to run a bash script and schedule Cron job from SageMaker?<\/p>",
        "Challenge_closed_time":1523213115336,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522449441927,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user wants to schedule a notebook on SageMaker to run every night and is seeking advice on the best way to do so, including the possibility of running a bash script and scheduling a Cron job from SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49582307",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":3.8,
        "Challenge_reading_time":2.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":14.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":212.1315025,
        "Challenge_title":"How to schedule tasks on SageMaker",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":17339.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Poster_created_time":1420001102892,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":173.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>Amazon SageMaker is a set of API that can help various machine learning and data science tasks. These API can be invoked from various sources, such as CLI, <a href=\"https:\/\/aws.amazon.com\/tools\/\" rel=\"noreferrer\">SDK<\/a> or specifically from schedule AWS Lambda functions (see here for documentation: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-scheduled-events.html<\/a> )<\/p>\n\n<p>The main parts of Amazon SageMaker are notebook instances, training and tuning jobs, and model hosting for real-time predictions. Each one has different types of schedules that you might want to have. The most popular are:<\/p>\n\n<ul>\n<li><strong>Stopping and Starting Notebook Instances<\/strong> - Since the notebook instances are used for interactive ML models development, you don't really need them running during the nights or weekends. You can schedule a Lambda function to call the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"noreferrer\">stop-notebook-instance<\/a> API at the end of the working day (8PM, for example), and the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StartNotebookInstance.html\" rel=\"noreferrer\">start-notebook-instance<\/a> API in the morning. Please note that you can also run crontab on the notebook instances (after opening the local terminal from the Jupyter interface).<\/li>\n<li><strong>Refreshing an ML Model<\/strong> - Automating the re-training of models, on new data that is flowing into the system all the time, is a common issue that with SageMaker is easier to solve. Calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTrainingJob.html\" rel=\"noreferrer\">create-training-job<\/a> API from a scheduled Lambda function (or even from a <a href=\"https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/events\/WhatIsCloudWatchEvents.html\" rel=\"noreferrer\">CloudWatch Event<\/a> that is monitoring the performance of the existing models), pointing to the S3 bucket where the old and new data resides, can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateModel.html\" rel=\"noreferrer\">create a refreshed model<\/a> that you can now deploy into an <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_UpdateEndpointWeightsAndCapacities.html\" rel=\"noreferrer\">A\/B testing environment<\/a> .<\/li>\n<\/ul>\n\n<p>----- UPDATE (thanks to @snat2100 comment) -----<\/p>\n\n<ul>\n<li><strong>Creating and Deleting Real-time Endpoints<\/strong> - If your realtime endpoints are not needed 24\/7 (for example, serving internal company users working during workdays and hours), you can also <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpoint.html\" rel=\"noreferrer\">create the endpoints<\/a> in the morning and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DeleteEndpoint.html\" rel=\"noreferrer\">delete them<\/a> at night. <\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1570200701000,
        "Solution_link_count":11.0,
        "Solution_readability":15.5,
        "Solution_reading_time":39.08,
        "Solution_score_count":19.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":307.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":15023.6469805556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Can someone explain or help me install <a href=\"https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/tree\/master\/python\" rel=\"nofollow noreferrer\">vowpalwabbit<\/a> (I'm interested in the python bindings) on an Amazon linux machine, either EC2 or SageMaker?\nfor some reason it is very hard and I can't find anything about it online...<\/p>\n\n<p>a <code>pip install vowpalwabbit<\/code> returns a <\/p>\n\n<pre><code>Using cached https:\/\/files.pythonhosted.org\/packages\/d1\/5a\/9fcd64fd52ad22e2d1821b2ef871e8783c324b37e2103e7ddefa776c2ed7\/vowpalwabbit-8.8.0.tar.gz\nBuilding wheels for collected packages: vowpalwabbit\n  Building wheel for vowpalwabbit (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0]= '\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-x0j85ac_ --python-tag cp36\n       cwd: \/tmp\/pip-install-tvp1174t\/vowpalwabbit\/\n<\/code><\/pre>\n\n<p>lower in the error I can also see a:<\/p>\n\n<pre><code>CMake Error at \/usr\/lib64\/python3.6\/dist-packages\/cmake\/data\/share\/cmake-3.13\/Modules\/FindBoost.cmake:2100 (message):\n    Unable to find the requested Boost libraries.\n\n    Boost version: 1.53.0\n\n    Boost include path: \/usr\/include\n\n    Could not find the following Boost libraries:\n\n            boost_python3\n\n    Some (but not all) of the required Boost libraries were found.  You may\n    need to install these additional Boost libraries.  Alternatively, set\n    BOOST_LIBRARYDIR to the directory containing Boost libraries or BOOST_ROOT\n    to the location of Boost.\n<\/code><\/pre>",
        "Challenge_closed_time":1634653018680,
        "Challenge_comment_count":1,
        "Challenge_created_time":1580567889550,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking help to install Vowpal Wabbit on an Amazon Linux machine, either EC2 or SageMaker, specifically the python bindings. The user has tried using \"pip install vowpalwabbit\" but encountered an error related to CMake and Boost libraries. The error message suggests that some of the required Boost libraries were not found and may need to be installed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60017893",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":15.4,
        "Challenge_reading_time":25.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":15023.6469805556,
        "Challenge_title":"How to install Vowpal Wabbit on Amazon EC2 or SageMaker? (amazon linux)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":367.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442180190107,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3203.0,
        "Poster_view_count":400.0,
        "Solution_body":"<p>Tested again 1.5 years later, and a <code>pip install vowpalwabbit<\/code> works fine on notebook instance. In training job, adding vowpalwabbit in a <code>requirements.txt<\/code> send to an AWS-managed Scikit learn container (<code>141502667606.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3<\/code>) also installs successfully. Both tested with vowpalwabbit-8.11.0<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":1.785035,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create my own custom Sagemaker Framework that runs a custom python script to train a ML model using the entry_point parameter.<\/p>\n\n<p>Following the Python SDK documentation (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>), I wrote the simplest code to run a training job just to see how it behaves and how Sagemaker Framework works.<\/p>\n\n<p>My problem is that I don't know how to properly build my Docker container in order to run the entry_point script.<\/p>\n\n<p>I added the <code>train.py<\/code> script into the container that only logs the folders and files paths as well as the variables in the containers environment.<\/p>\n\n<p>I was able to run the training job, but I couldn't find any reference of the entry_point script neither in environment variable nor the files in the container.<\/p>\n\n<p>Here is the code I used:<\/p>\n\n<ul>\n<li><strong>Custom Sagemaker Framework Class:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.estimator import Framework\n\nclass Doc2VecEstimator(Framework):\n    def create_model():\n        pass\n<\/code><\/pre>\n\n<ul>\n<li><strong>train.py:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nimport os\nfrom datetime import datetime\n\n\ndef log(*_args):\n    print('[log-{}]'.format(datetime.now().isoformat()), *_args)\n\n\ndef listdir_rec(path):\n    ls = os.listdir(path)\n    print(path, ls)\n\n    for ls_path in ls:\n        if os.path.isdir(os.path.join(path, ls_path)):\n            listdir_rec(os.path.join(path, ls_path))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--debug_size', type=int, default=None)\n\n    # # I commented the lines bellow since I haven't configured the environment variables in my container\n    #     # Sagemaker specific arguments. Defaults are set in the environment variables.\n    #     parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    #     parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    #     parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n    args, _ = parser.parse_known_args()\n\n    log('Received arguments {}'.format(args))\n\n    log(os.environ)\n\n    listdir_rec('.')\n\n<\/code><\/pre>\n\n<ul>\n<li><strong>Dockerfile:<\/strong><\/li>\n<\/ul>\n\n<pre class=\"lang-sh prettyprint-override\"><code>FROM ubuntu:18.04\n\nRUN apt-get -y update \\\n    &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n        wget \\\n        python3 \\\n        python3-pip \\\n        nginx \\\n        ca-certificates \\\n    &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN pip3 install --upgrade pip setuptools \\\n    &amp;&amp; \\\n    pip3 install \\\n        numpy \\\n        scipy \\\n        scikit-learn \\\n        pandas \\\n        flask \\\n        gevent \\\n        gunicorn \\\n        joblib \\\n        pyAthena \\\n        pandarallel \\\n        nltk \\\n        gensim \\\n    &amp;&amp; \\\n    rm -rf \/root\/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\n\nCOPY train.py \/train.py\n\nENTRYPOINT [\"python3\", \"-u\", \"train.py\"]\n<\/code><\/pre>\n\n<ul>\n<li><strong>Training Job Execution Script:<\/strong><\/li>\n<\/ul>\n\n<pre><code>framework = Doc2VecEstimator(\n    image_name=image,\n    entry_point='train_doc2vec_model.py',\n    output_path='s3:\/\/{bucket_prefix}'.format(bucket_prefix=bucket_prefix),\n\n    train_instance_count=1,\n    train_instance_type='ml.m5.xlarge',\n    train_volume_size=5,\n\n    role=role,\n    sagemaker_session=sagemaker_session,\n    base_job_name='gensim-doc2vec-train-100-epochs-test',\n\n    hyperparameters={\n        'epochs': '100',\n        'debug_size': '100',\n    },\n)\n\nframework.fit(s3_input_data_path, wait=True)\n<\/code><\/pre>\n\n<p>I haven't found a way to make the training job to run the <code>train_doc2vec_model.py<\/code>. So how do I create my own custom Framework class\/container?<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":1590435558776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590429132650,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a custom Sagemaker Framework that runs a custom Python script to train a machine learning model using the entry_point parameter. However, the user is unable to properly build the Docker container to run the entry_point script and cannot find any reference to the entry_point script in the environment variable or files in the container. The user has provided code for the custom Sagemaker Framework class, train.py, Dockerfile, and training job execution script.",
        "Challenge_last_edit_time":1590437261656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62007961",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.5,
        "Challenge_reading_time":49.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":1.785035,
        "Challenge_title":"Where does entry_point script is stored in custom Sagemaker Framework training job container?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":747.0,
        "Challenge_word_count":363,
        "Platform":"Stack Overflow",
        "Poster_created_time":1587648874872,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo, SP, Brasil",
        "Poster_reputation_count":23.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>SageMaker team created a <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">python package <code>sagemaker-training<\/code><\/a> to install in your docker so that your customer container will be able to handle external <code>entry_point<\/code> scripts.\nSee here for an example using Catboost that does what you want to do :)<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-byo-catboost-container-demo<\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.5,
        "Solution_reading_time":7.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":431.4666666667,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThe product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nDSVM, DB\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\neverything runs\r\n\r\n### Other Comments\r\nquestion to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?\r\n\r\n",
        "Challenge_closed_time":1555413510000,
        "Challenge_comment_count":12,
        "Challenge_created_time":1553860230000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user needs to remove the reference to Azure ML SDK preview private index from an operationalize notebook as it is now available through regular PyPi as a GA product and preview versions are unsupported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/695",
        "Challenge_link_count":0,
        "Challenge_participation_count":12,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":431.4666666667,
        "Challenge_title":"[BUG] Remove contrib from azureml",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"azureml_hyperdrive_wide_and_deep notebook does**n't** use any azureml contrib modules. papermill PR in progress is using azureml.contrib.notebook. If it's not desired in the yaml file, I can install this package only inside the notebook. Will this work? mmm, yeah that would be a workaround.  Not in the Hyperdrive notebooks. since @jingyanwangms is the only one using it, can you take care of this issue in your PR? Do we want to require people to install things at the beginning of notebooks, though?  azureml.contrib.notebook is required for submitting notebook through aml. But if we don't want azureml.contrib to install as default in base yaml files, @heatherbshapiro what would you recommend doing here? @miguelgfierro Sure. I can do it in my PR. Hey guys\r\nI am getting an error while trying to run this line \"from azureml.contrib.notebook import NotebookRunConfig, AzureMLNotebookHandler\".\r\n**The error is ModuleNotFoundError: No module named 'azureml.contrib'** although I have installed azureml-contrib-notebook from pip. What should I do?\r\n HI @Raman1121 , which file in the code is this line in? I am trying to experiment with jupyter notebook on Azure through API calls by following the code snippet given here - \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-notebook\/azureml.contrib.notebook.azuremlnotebookhandler?view=azure-ml-py Well, that code is not related to the Recommenders GitHub repo. Most likely you have not configured your conda or python settings appropriately.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.2,
        "Solution_reading_time":18.91,
        "Solution_score_count":null,
        "Solution_sentence_count":21.0,
        "Solution_word_count":210.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":590.8691666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nThis happens when i tried to configure my own metric functions. \r\n\r\n## Context\r\n\r\nI am trying to create a custom metric indicator, to be logged after each experimentation. When i run `kedro mlflow ui`, this is what I'm getting on the UI.\r\n![image](https:\/\/user-images.githubusercontent.com\/54475793\/184276876-57872dd2-3fb3-41c9-b3a3-edd6a4396aca.png)\r\n\r\n\r\n## Steps to Reproduce\r\n\r\nThis is my nodes.py\r\n```\r\ndef pnl_metrics(df:pd.DataFrame): \r\n    avg_pnl = {}\r\n    avg_pnl[f'{avg_metric}'] = {'trader1': df.pnl.mean()}\r\n    avg_pnl[f'{total_metric}'] = {'trader1': df.pnl.sum(), 'trader2': df.pnl.sum()}\r\n    return avg_pnl\r\n```\r\n\r\n\r\n## Expected Result\r\n\r\nHow do i get the metric to be displayed when i use the Mlflow ui? Are there specific keywords that mlflow is tracking to be logged as metric?\r\n\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): **0.10.0**\r\n* Python version used (`python -V`):  **3.9.0** \r\n* Operating system and version: Windows 10\r\n",
        "Challenge_closed_time":1662408460000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1660281331000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the mlflow experiment specified in mlflow.yml is not being used when setting up the mlflow configuration interactively. Instead, all runs are being stored in the \"Default\" experiment. The user has provided steps to reproduce the issue and suggests using the mlflow \"set_experiment\" method as a potential solution.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/346",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":8.4,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":590.8691666667,
        "Challenge_title":"MlflowMetricsDataSet logs invalid metric which breaks mlflow UI",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":142,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @xjlwi, sorry to see that you are facing issues with the plugins. There are two problems here:\r\n- kedro-mlflow logs an incorrect metric. We will solve the problem together. \r\n- mlflow does not complain when the incorrect metric is logged, but it breaks the database and hence the UI => we should open an issue in mlflow repo once we know what is going on. \r\n\r\nWould you mind give me some extra informations: \r\n- ``mlflow`` version\r\n- the catalog entry for ``avg_pnl`` (I guess it is a ``kedro_mlflow.io.metrics.MlflowMetricsDataSet``?) **If yes, check the documentation: [it should return something like ``{'trader1': {'step': 0, 'value': df.pnl.mean()}}``](https:\/\/kedro-mlflow.readthedocs.io\/en\/stable\/source\/04_experimentation_tracking\/05_version_metrics.html#how-to-return-metrics-from-a-node)**\r\n- the type of ``avg_metric`` and ``total_metric``: are they ``float`` instead of string?\r\n- can you check if ``df.pnl.mean()`` and ``df.pnl.sum()`` returns a float and not a single-row ``pandas.Series``?\r\n\r\nIf I can reproduce the bug, I will be able to give you a workaround. \r\n > Hi @xjlwi, sorry to see that you are facing issues with the plugins. There are two problems here:\r\n> \r\n> * kedro-mlflow logs an incorrect metric. We will solve the problem together.\r\n> * mlflow does not complain when the incorrect metric is logged, but it breaks the database and hence the UI => we should open an issue in mlflow repo once we know what is going on.\r\n> \r\n> Would you mind give me some extra informations:\r\n> \r\n> * `mlflow` version: <b> 1.26.1 <\/b>\r\n> * the catalog entry for `avg_pnl` (I guess it is a `kedro_mlflow.io.metrics.MlflowMetricsDataSet`?) **If yes, check the documentation: [it should return something like `{'trader1': {'step': 0, 'value': df.pnl.mean()}}`](https:\/\/kedro-mlflow.readthedocs.io\/en\/stable\/source\/04_experimentation_tracking\/05_version_metrics.html#how-to-return-metrics-from-a-node)** : \r\n\r\nYes it's a `kedro_mlflow.io.metrics.MlflowMetricsDataSet`. \r\n\r\ntype: kedro_mlflow.io.artifacts.MlflowArtifactDataSet \r\ndata_set:\r\n    type: pandas.CSVDataSet \r\n    filepath: \"${ml_model_output}PnL_summary_metrics_${current_date}_${model}.csv\" \r\n    save_args:\r\n      index: True\r\n\r\nMust the keywords for the output be specifically 'step'? This is my current node to return the output.\r\n`\r\ndef pnl_metrics(df:pd.DataFrame): \r\n    avg_pnl = {}\r\n    avg_pnl[f'{avg_metric}'] = {'trader1': df.pnl.mean()}\r\n    avg_pnl[f'{total_metric}'] = {'trader1': df.pnl.sum(), 'trader2': df.pnl.sum()}\r\n    return avg_pnl\r\n`\r\n\r\n> * the type of `avg_metric` and `total_metric`: are they `float` instead of string? Definitely float, because in my local mlruns folder, I am able to see them from the mlruns>metrics folder.\r\n\r\n1660874133345 [{'ml_model_13_logit_pnl_total': 0.0}, {'ml_model_13_logit_pnl_avg': nan}] ml_model_13_logit\r\n1660874133347 [{'ml_model_14_rf_pnl_total': 0.0}, {'ml_model_14_rf_pnl_avg': nan}] ml_model_14_rf\r\n1660874133349 [{'ml_model_15_naive_clf_pnl_total': 0.0}, {'ml_model_15_naive_clf_pnl_avg': nan}] ml_model_15_naive_clf\r\n1660874133352 [{'ml_model_16_svc_pnl_total': 0.0}, {'ml_model_16_svc_pnl_avg': nan}] ml_model_16_svc\r\n1660874133354 [{'ml_model_17_decisison_tree_pnl_total': 0.0}, {'ml_model_17_decisison_tree_pnl_avg': nan}] ml_model_17_decisison_tree\r\n1660874133356 [{'ml_model_18_grad_boost_pnl_total': 0.0}, {'ml_model_18_grad_boost_pnl_avg': nan}] ml_model_18_grad_boost\r\n\r\n> * can you check if `df.pnl.mean()` and `df.pnl.sum()` returns a float and not a single-row `pandas.Series`?\r\n\r\n> If I can reproduce the bug, I will be able to give you a workaround.\r\n\r\n > Must the keywords for the output be specifically 'step'? This is my current node to return the output.\r\n\r\nYes exactly. That's for consistency between loading and saving metrics.\r\n\r\nReplace each entry ``df.pnl.mean()`` by  a dict``{'step': 0, 'value': df.pnl.mean()}``and you will be fine. This adds an extra nested dict level and is not ideal. I let the issue opened to improve the API in the future. Hi, I close the issue but feel free to reopen if needed. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.5,
        "Solution_reading_time":50.7,
        "Solution_score_count":null,
        "Solution_sentence_count":47.0,
        "Solution_word_count":461.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":420.1906525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Please see the screenshots below. Once it said terminated but without reason:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/140829-screenshot-2021-10-13-221133.png?platform=QnA\" alt=\"140829-screenshot-2021-10-13-221133.png\" \/>    <br \/>\nThe other time there was nothing just stopped:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/140846-screenshot-2021-10-13-215523.png?platform=QnA\" alt=\"140846-screenshot-2021-10-13-215523.png\" \/>    <\/p>",
        "Challenge_closed_time":1635818701492,
        "Challenge_comment_count":2,
        "Challenge_created_time":1634306015143,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user's script is stopping without any explanation or error message, as shown in the attached screenshots. The script has terminated once without any reason and another time it just stopped abruptly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/592153\/my-script-stops-running-without-any-message-explai",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":16.6,
        "Challenge_reading_time":7.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":420.1906525,
        "Challenge_title":"my script stops running without any message explaining the reason",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello,  <\/p>\n<p>Hope you have solved this issue and we are sorry not seeing your response. Since this issue happened without any error details, support ticket would be the best way to debug that. Please let me know if you still need that. Thanks.  <\/p>\n<p>Regards,  <br \/>\nYutong<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.4,
        "Solution_reading_time":3.45,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.6766972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Started up a STANDARD_D11_V2 cluster to run some notebooks on.<\/p>\n<p>Wanted to use json_normalize from pandas: <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.json_normalize.html\">https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.json_normalize.html<\/a> and I get the below error:<\/p>\n<pre><code>AttributeError: module 'pandas' has no attribute 'json_normalize'\n<\/code><\/pre>\n<p>Pandas seems to be out of date. Checked the installed version of pandas:<\/p>\n<pre><code>$ python\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) [GCC 7.3.0] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import pandas\n\n&gt;&gt;&gt; pandas.__version__\n\n'0.23.4\n<\/code><\/pre>\n<p>Pandas is indeed out of date, the latest version is v1.1.1. Fired up a terminal to run:<\/p>\n<pre><code>conda update --all\n<\/code><\/pre>\n<p>On the azureml_py36 environment which I had selected to run the notebook on. It hangs on:<\/p>\n<pre><code>Solving environment: \/\n<\/code><\/pre>\n<p>Went to update conda to see if that would help:<\/p>\n<pre><code>conda update conda\n<\/code><\/pre>\n<p>But I get this error:<\/p>\n<pre><code>PackageNotInstalledError: Package is not installed in prefix.\n  prefix: \/anaconda\/envs\/azureml_py36\n  package name: conda\n<\/code><\/pre>\n<p>Which leads me to think this is not a typical installation of conda.<\/p>\n<p>Would like to run the most up to date packages on Azure Machine Learning to replicate the local environment I have setup. Does anyone know how to do this?<\/p>",
        "Challenge_closed_time":1598622188187,
        "Challenge_comment_count":0,
        "Challenge_created_time":1598608952077,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with updating out of date packages on an Azure Machine Learning Compute instance. They have identified that Pandas is out of date and attempted to update it using \"conda update --all\" command, but it hangs on \"Solving environment\". The user also tried to update conda but received an error message. They are seeking help to run the most up-to-date packages on Azure Machine Learning to replicate their local environment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/80212\/update-out-of-date-packages-on-azure-machine-learn",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":21.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":3.6766972222,
        "Challenge_title":"Update out of date packages on Azure Machine Learning Compute instance",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":200,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@philmariusnew-9791 I have tried the above steps and the installation completed successfully for conda. But when we upgrade pandas azureml package has a dependency  so it cannot use version v1.1.1     <\/p>\n<p>I have went ahead and upgrade the pandas version but there is a warning as seen below:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/21216-image.png?platform=QnA\" alt=\"21216-image.png\" \/>    <\/p>\n<p>We would recommend to use the package that is compatible with azureml so your environment setup works as expected.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.8,
        "Solution_reading_time":6.99,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":69.1720713889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy an R inference script to Azure ML Service Endpoint as an Azure Container Instance. I have made the following steps:<\/p>\n<ul>\n<li>   created a custom Docker image from scratch and pushed it to the Azure Container Registry (associated with AML Workspace)<\/li>\n<li>   registered a custom environment in AML Workspace, based on the image in ACR<\/li>\n<li>   deployed R entry script (just a simple hello world script with init() and run() functions defined)\n<ul>\n<li>   the inference configuration uses the custom AML environment<\/li>\n<li>   deployment is made with Azure ML R SDK<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>The container instance is created, but the endpoint startup runs into error. Here is the output from the container instance:<\/p>\n<pre><code>2020-10-16T12:56:21,639812796+00:00 - gunicorn\/run \n2020-10-16T12:56:21,639290594+00:00 - iot-server\/run \n2020-10-16T12:56:21,640405198+00:00 - rsyslog\/run \n2020-10-16T12:56:21,735291424+00:00 - nginx\/run \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2020-10-16T12:56:23,736657191+00:00 - iot-server\/finish 1 0\n2020-10-16T12:56:23,834747728+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 20.0.4\nListening at: http:\/\/127.0.0.1:31311 (11)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 38\n\/bin\/bash: \/root\/miniconda3\/lib\/libtinfo.so.6: no version information available (required by \/bin\/bash)\nSPARK_HOME not set. Skipping PySpark Initialization.\nException in worker process\nTraceback (most recent call last):\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 43, in &lt;module&gt;\n    from azureml.api.exceptions.ClientSideException import ClientSideException\nModuleNotFoundError: No module named 'azureml.api'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/lib\/python3\/dist-packages\/gunicorn\/util.py&quot;, line 383, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.8\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1014, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 671, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 783, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/var\/azureml-server\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/var\/azureml-server\/create_app.py&quot;, line 3, in &lt;module&gt;\n    from app import main\n  File &quot;\/var\/azureml-server\/app.py&quot;, line 45, in &lt;module&gt;\n    from azure.ml.api.exceptions.ClientSideException import ClientSideException\nModuleNotFoundError: No module named 'azure.ml'\nWorker exiting (pid: 38)\nShutting down: Master\nReason: Worker failed to boot.\n2020-10-16T12:56:39,434787859+00:00 - gunicorn\/finish 3 0\n2020-10-16T12:56:39,435715063+00:00 - Exit code 3 is not normal. Killing image.\n<\/code><\/pre>\n<p>How do I install the azureml.api dependency, which can not be found? It doesn't seem to be part of the Azure ML SDK. I have installed the following dependencies in my Dockerfile:<\/p>\n<pre><code>RUN apt-get -y install python3-flask python3-rpy2 python3-azure python3-applicationinsights\nRUN pip install azureml-core\n<\/code><\/pre>\n<p>I also have Miniconda installed. Pip refers to Miniconda's pip.<\/p>\n<p>Or, is this dependency available to install at all? Should I use some pre-defined AML environment as the base Docker image? (Note: I am currently using bare FROM: ubuntu). Suggestions how to find and use the base images are also welcome, since this is not documented very well.<\/p>",
        "Challenge_closed_time":1603104240927,
        "Challenge_comment_count":1,
        "Challenge_created_time":1602855221470,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while deploying an R inference script to Azure ML Service Endpoint as an Azure Container Instance. The container instance is created, but the endpoint startup runs into an error due to the missing 'azureml.api' module. The user has installed the required dependencies in the Dockerfile, but the issue persists. The user is seeking suggestions on how to find and use the base images and whether to use a pre-defined AML environment as the base Docker image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/129038\/r-model-deployment-with-custom-docker-image-module",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.6,
        "Challenge_reading_time":62.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":52,
        "Challenge_solved_time":69.1720713889,
        "Challenge_title":"R model deployment with custom Docker image: \"ModuleNotFoundError: No module named 'azureml.api'\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":500,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=c02d5983-5261-45cb-9bb0-3e5f19b42ae9\">@Lauri Lehman  <\/a> Thanks for the question. Here is the <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fmedium.com%2Fmicrosoftazure%2Fhow-to-create-custom-docker-base-images-for-azure-machine-learning-environments-86aa4c7bc7b9&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7C49414af813754671ec7908d83863d1b5%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637321350687268940&amp;sdata=f5ebitkOUNxY8v7cOS2vFy12mBfuP%2BJVVzLbhAKVyXE%3D&amp;reserved=0\">document<\/a>  to Create Custom Docker Base Images for Azure Machine Learning Environments for R people.    <br \/>\nWe have used the AzureML <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.r_script_step.rscriptstep?view=azure-ml-py\">RScriptStep<\/a>  pipeline feature which allows you to point to CRAN or Github or custom URLS, but this requires authoring the pipeline in python or YAML.. In R You can also  these arguments in the Azuremlsdk R estimator function:  <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html\">https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html<\/a>    <br \/>\nAnother option that are not available through conda install as part of the R script with install.packages(\u201cpath\/*.tar.gz\u201d, repos=NULL))    <\/p>\n<p>One of the challenges is that the build at runtime can take a while to prepare the environment.  R likes to compile packages on Linux environments and a large package could have lots of dependencies which would take a while.  This is an R on Linux\/PaaS thing, rather than specific to AzureML    <\/p>\n<p>To make start up fast we created a custom docker image where you can tightly control the image ahead of runtime.  If you want to go in this direction you can find an example Dockerfile to get you started here..    <br \/>\n<a href=\"https:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile\">https:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile<\/a>    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":15.4,
        "Solution_reading_time":27.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":191.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":98.9333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"I was able to create a deep learning VM from the marketplace and when I open up the VM instance in the Console I see a metadata tag called `proxy-url` which has a format like `https:\/\/[alphanumeric string]-dot-us-central1.notebooks.googleusercontent.com\/lab`\n\nClicking on that link takes me to a JupyterLab UI that is running on my VM. Amazing! Unfortunately, when I try opening that link on an incognito window, I'm asked to sign in. If I sign in, I get a 403 forbidden.\n\nMy question now is, how can I make that link available to someone else?",
        "Challenge_closed_time":1643637480000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643281320000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a deep learning VM from the marketplace and found a metadata tag called `proxy-url` that takes them to a JupyterLab UI running on their VM. However, when they try to open the link on an incognito window, they are asked to sign in and get a 403 forbidden error. The user wants to know how to make the link available to someone else.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Make-deep-learning-VM-JupyterLab-publicly-available\/m-p\/386576#M191",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.5,
        "Challenge_reading_time":7.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":98.9333333333,
        "Challenge_title":"Make deep learning VM JupyterLab publicly available?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":139.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi gopalv\n\nAs far as I understand, it sounds like your Jupyter Notebook isn't configured for remote access. since it doesn't work when trying to access it from the incognito window with a 403 error.\n\nYou can try looking here and here for details on how to set up a publicly accessible\/remote access notebook. There are additional troubleshooting steps in our documentation here as well.\n\nHope this helps!\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":73.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.83,
        "Challenge_answer_count":0,
        "Challenge_body":"logs: \r\n\r\n```\r\nRun papermill notebooks\/sklearn\/train-diabetes-mlproject.ipynb out.ipynb -k python\r\nInput Notebook:  notebooks\/sklearn\/train-diabetes-mlproject.ipynb\r\nOutput Notebook: out.ipynb\r\n\r\nExecuting:   0%|          | 0\/7 [00:00<?, ?cell\/s]Executing notebook with kernel: python\r\n\r\nExecuting:  14%|\u2588\u258d        | 1\/7 [00:01<00:07,  1.33s\/cell]\r\nExecuting:  29%|\u2588\u2588\u258a       | 2\/7 [00:02<00:07,  1.43s\/cell]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4\/7 [00:05<00:03,  1.32s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:07<00:01,  1.34s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:08<00:01,  1.40s\/cell]\r\nTraceback (most recent call last):\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/bin\/papermill\", line 8, in <module>\r\n    sys.exit(papermill())\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/cli.py\", line 240, in papermill\r\n    execute_notebook(\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 110, in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 222, in raise_for_execution_errors\r\n    raise error\r\npapermill.exceptions.PapermillExecutionError: \r\n---------------------------------------------------------------------------\r\nException encountered at \"In [5]\":\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-5-ef514d3992f5> in <module>\r\n----> 1 run = mlflow.projects.run(\r\n      2     uri=str(project_uri),\r\n      3     parameters=***\"alpha\": 0.3***,\r\n      4     backend=\"azureml\",\r\n      5     backend_config=backend_config,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in run(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\r\n    271     )\r\n    272 \r\n--> 273     submitted_run_obj = _run(\r\n    274         uri=uri,\r\n    275         experiment_id=experiment_id,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in _run(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, use_conda, storage_dir, synchronous)\r\n     98         backend = loader.load_backend(backend_name)\r\n     99         if backend:\r\n--> 100             submitted_run = backend.run(\r\n    101                 uri,\r\n    102                 entry_point,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/mlflow\/_internal\/projects.py in run(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id)\r\n    240         if compute and compute != _LOCAL and compute != _LOCAL.upper():\r\n    241             remote_environment = _load_remote_environment(mlproject)\r\n--> 242             remote_environment.register(workspace=workspace)\r\n    243             cpu_cluster = _load_compute_target(workspace, backend_config)\r\n    244             src.run_config.target = cpu_cluster.name\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/environment.py in register(self, workspace)\r\n    803         environment_client = EnvironmentClient(workspace.service_context)\r\n    804         environment_dict = Environment._serialize_to_dict(self)\r\n--> 805         response = environment_client._register_environment_definition(environment_dict)\r\n    806         env = Environment._deserialize_and_add_to_object(response)\r\n    807 \r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/_restclient\/environment_client.py in _register_environment_definition(self, environment_dict)\r\n     75             message = \"Error registering the environment definition. Code: ***\\n: ***\".format(response.status_code,\r\n     76                                                                                             response.text)\r\n---> 77             raise Exception(message)\r\n     78 \r\n     79     def _get_image_details(self, name, version=None):\r\n\r\nException: Error registering the environment definition. Code: 409\r\n: ***\r\n  \"error\": ***\r\n    \"code\": \"TransientError\",\r\n    \"severity\": null,\r\n    \"message\": \"Etag conflict on 0e149764-3720-4610-b0f3-3e3f974544ac\/8f54aa7d6c05b2722ba149d8ea3185c263ecf5310eb2d7271569d1918c736972 with etag .\",\r\n    \"messageFormat\": null,\r\n    \"messageParameters\": null,\r\n    \"referenceCode\": null,\r\n    \"detailsUri\": null,\r\n    \"target\": null,\r\n    \"details\": [],\r\n    \"innerError\": null,\r\n    \"debugInfo\": null\r\n  ***,\r\n  \"correlation\": ***\r\n    \"operation\": \"db22e6e6bfa07f499f1749f708b798c9\",\r\n    \"request\": \"f470e9430c5ed842\"\r\n  ***,\r\n  \"environment\": \"eastus\",\r\n  \"location\": \"eastus\",\r\n  \"time\": \"2020-10-01T20:17:52.8383774+00:00\",\r\n  \"componentName\": \"environment-management\"\r\n***\r\n\r\nError: Process completed with exit code 1.\r\n```",
        "Challenge_closed_time":1601658581000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601583593000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an MlflowException while running a double ensemble on Google Colab. They were able to solve some issues by cloning the repo and reinstalling numpy, but they are unsure how to solve this particular issue. The error message indicates an invalid value for metric 'IC' and a value error related to duplicate bin edges. The user followed instructions to download cn data and provided a yaml file with configuration details.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1170",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":20.3,
        "Challenge_reading_time":69.38,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":20.83,
        "Challenge_title":"mlflow.projects.run failing consistently with etag error ",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":338,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.4035702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Working on deployment of 170 ml models using ML studio and azure Kubernetes service which is referred on the  below doc  link &quot;https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-deploy-azure-kubernetes-service.md&quot;.     <\/p>\n<p>We are training the model using python script with the custom environment and we are registering the ml model on the  Azure ML services. Once we register the mode we are deploying it on the AKS by using the container images.     <\/p>\n<p>While deploying the ML model we are able to deploy up 10 to 11 models per pods for each Node in AKS. When we try to deploy the model on the same node we are getting deployment timeout error and we are getting the below error message.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/129300-image-2021-09-04t13-25-12-512z.png?platform=QnA\" alt=\"129300-image-2021-09-04t13-25-12-512z.png\" \/>    <\/p>",
        "Challenge_closed_time":1630758725400,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630746472547,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to deploy 170 ml models using ML studio and Azure Kubernetes service. They are able to deploy up to 10-11 models per pod for each node in AKS, but when they try to deploy more models on the same node, they encounter a deployment timeout error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/540001\/how-many-models-can-be-deployed-in-single-node-in",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.7,
        "Challenge_reading_time":12.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.4035702778,
        "Challenge_title":"how many models can be deployed in single node in azure kubernetes service?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a href=\"\/users\/na\/?userid=f023f08d-7d4a-4ac5-ba62-e9d37f7e7c70\">@suvedharan  <\/a>     <\/p>\n<p>The number of models to be deployed is limited to 1,000 models per deployment (per container).    <\/p>\n<p>Autoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\">more details<\/a>    <\/p>\n<p>If the Answer is helpful, please click <code>Accept Answer<\/code> and <strong>up-vote<\/strong>, so that it can help others in the community looking for help on similar topics.    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.2,
        "Solution_reading_time":9.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":86.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":386.3980555556,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n<img width=\"1430\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/5203025\/123860354-63399680-d958-11eb-9dc8-dc0a52d67cc2.png\">\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 109d9284-e234-5086-5da6-4155291361c8\r\n* Version Independent ID: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d\r\n* Content: [azureml.core.ScriptRunConfig class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.scriptrunconfig?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1626388206000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1624997173000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered an exception while importing azureml.core after installing azure ml using a conda environment yml. The error message indicates a failure while loading azureml_run_type_providers due to a cryptography version issue. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1534",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":25.6,
        "Challenge_reading_time":13.49,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":386.3980555556,
        "Challenge_title":"Broken link in AML doc to azureml.core.runconfig.MpiConfiguration",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":52,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for submitting the issue. I am fixing this broken link now.  The links should be fixed on next SDK release on Aug 3rd",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":1.47,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":168.8386111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nUsers get the error \"null is not an object\" when pop-ups are enabled in SWB (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620))\r\nThis error is illegible to the user and causes confusion. Can we make the error message more clear such as:\r\n\"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Diable pop-ups \r\n2. Connect to a workspace\r\n\r\n**Expected behavior**\r\nIf the workspace is unable to open, a more legible error message should be shown, such as \"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v4.3.1 and v5.0.0]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671209314000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1670601495000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where the idle Sagemaker Notebook instances are not stopping automatically after the specified time. The autostop.py script used by the `on-start` lifecycle rule of the instance CFN template is not working due to missing packages. The expected behavior is that the idle instances should stop automatically after the specified time. The user is using Release Version v5.0.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1081",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.6,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":168.8386111111,
        "Challenge_title":"[Bug] More descriptive error message for \"null is not an object\" while trying to connect to Sagemaker notebook. ",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @simranmakwana, thank you for creating this issue. There are currently no plans to enrich the error messages in the UI; the recommendation is for you to customize the error messages within your installation of SWB as you see fit. Please reply back if there are any concerns with this approach. Thank you! ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":3.75,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1334762714136,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Boston, MA",
        "Answerer_reputation_count":6557.0,
        "Answerer_view_count":2005.0,
        "Challenge_adjusted_solved_time":873.2213111111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Error installing component: azure_cli_ml_cliextension.windows \"The action failed catastrophically with <\/p>\n\n<p>Microsoft.MachineLearning.Installer.Engine.Actions.RegisteredActions.AzureCliException: Unable to get list of currently installed Azure CLI extensions<\/p>\n\n<p>at <\/p>\n\n<p>Microsoft.MachineLearning.Installer.Engine.Actions.RegisteredActions.InstallAzureCliExtensionAction.d__23.MoveNext() in C:\\swarm\\workspace\\Installer-1.2\\Installer.Engine\\Actions\\RegisteredActions\\InstallAzureCliExtensionAction.cs:line 97<\/p>\n\n<p>from there everything is stops....<\/p>",
        "Challenge_closed_time":1525629892870,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522486296150,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user encountered an error while installing Azure ML workbench on Windows 10 Enterprise. The error message indicates that the installation failed catastrophically due to an issue with the Azure CLI extension. The installation process stops after this error message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49586005",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":24.8,
        "Challenge_reading_time":8.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":873.2213111111,
        "Challenge_title":"Azure ML workbench failed installing on Windows 10 Enterprise",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":296.0,
        "Challenge_word_count":40,
        "Platform":"Stack Overflow",
        "Poster_created_time":1506435894640,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Southeast Asia",
        "Poster_reputation_count":1922.0,
        "Poster_view_count":404.0,
        "Solution_body":"<p>Workbench is a preview product and issues may occur. Please try and get a newer exe and try again. It also seems like you have azure powershell issues here which I would have expected to be taken care of by the installer, but perhaps you can try and install azure powershell first. <\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":3.48,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1589205020747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":76.5197022222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to upgrade the default Ubuntu version that comes with the Compute Instance in Azure ML.<\/p>\n\n<p>Anyone has any guide on safely upgrading to the latest LTS?<\/p>",
        "Challenge_closed_time":1591531320408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591255849480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to upgrade the default Ubuntu version that comes with the Compute Instance in Azure ML to the latest LTS and is seeking guidance on how to do it safely.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62189103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":3.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":76.5197022222,
        "Challenge_title":"Azure ML Compute Instance: How can I safely upgrade the default Azure Ubuntu 16.04 LTS to the latest LTS?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449542867716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1302.0,
        "Poster_view_count":157.0,
        "Solution_body":"<p>Any specific reason you want to do this?<\/p>\n\n<p>Since there are some heavy dependencies (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#contents\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#contents<\/a>), my guess is you have to try it yourself.<\/p>\n\n<p>Create a new one and run:<\/p>\n\n<pre><code>$ sudo apt update \n$ sudo apt upgrade\n$ sudo apt dist-upgrade\n<\/code><\/pre>\n\n<p>Let us know what happened.<\/p>\n\n<p>BTW: Are Compute Instance also Docker images? If so, the upgrade might be working, if not, there might be many drivers that need to be upgraded too. The ones from the GPU would be the easiest...<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.21,
        "Solution_score_count":2.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1515548304623,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vietnam",
        "Answerer_reputation_count":549.0,
        "Answerer_view_count":68.0,
        "Challenge_adjusted_solved_time":2.1902408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to do a neural network for regression analysis using optuna based on <a href=\"https:\/\/dreamer-uma.com\/pytorch-optuna-hyperparameter-tuning\/\" rel=\"nofollow noreferrer\">this site<\/a>.\nI would like to create a model with two 1D data as input and one 1D data as output in batch learning.<\/p>\n<p><code>x<\/code> is the training data and <code>y<\/code> is the teacher data.<\/p>\n<pre><code>class Model(nn.Module):\n    # \u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf(\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210\u6642\u306e\u521d\u671f\u5316)\n    def __init__(self,trial, mid_units1, mid_units2):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(2, mid_units1)\n        self.bn1 = nn.BatchNorm1d(mid_units1)\n        self.linear2 = nn.Linear(mid_units1, mid_units2)\n        self.bn2 = nn.BatchNorm1d(mid_units2)\n        self.linear3 = nn.Linear(mid_units2, 1)\n        self.activation = trial_activation(trial)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n\ndevice = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\nEPOCH = 100\nx = torch.from_numpy(a[0].astype(np.float32)).to(device)\ny = torch.from_numpy(a[1].astype(np.float32)).to(device)\n\ndef train_epoch(model, optimizer, criterion):\n    model.train()\n    optimizer.zero_grad()    # \u52fe\u914d\u60c5\u5831\u30920\u306b\u521d\u671f\u5316\n    y_pred = model(x)                                               # \u4e88\u6e2c\n    loss = criterion(y_pred.reshape(y.shape), y)          # \u640d\u5931\u3092\u8a08\u7b97(shape\u3092\u63c3\u3048\u308b)\n    loss.backward()                                                       # \u52fe\u914d\u306e\u8a08\u7b97\n    optimizer.step()                                                      # \u52fe\u914d\u306e\u66f4\u65b0\n    return loss.item()\n\ndef trial_activation(trial):\n    activation_names = ['ReLU','logsigmoid']\n    activation_name = trial.suggest_categorical('activation', activation_names)\n    if activation_name == activation_names[0]:\n        activation = F.relu\n    else:\n        activation = F.logsigmoid\n    return activation\n\ndef objective(trial):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # \u4e2d\u9593\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\u306e\u8a66\u884c\n    mid_units1 = int(trial.suggest_discrete_uniform(&quot;mid_units1&quot;, 1024*2,1024*4, 64*2))\n    mid_units2 = int(trial.suggest_discrete_uniform(&quot;mid_units2&quot;, 1024, 1024*2, 64*2))\n\n    net = Model(trial, mid_units1, mid_units2).to(device)\n\n    criterion = nn.MSELoss() \n    # \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a66\u884c\n    optimizer = trial_optimizer(trial, net)\n    train_loss = 0\n    for epoch in range(EPOCH):\n        train_loss = train_epoch(net, optimizer, criterion, device)\n    torch.save(net.state_dict(), str(trial.number) + &quot;new1.pth&quot;)\n    return train_loss\n\nstrage_name = &quot;a.sql&quot;\nstudy_name = 'a'\n\nstudy = optuna.create_study(\n    study_name = study_name,\n    storage='sqlite:\/\/\/'  + strage_name, \n    load_if_exists=True,\n    direction='minimize')\nTRIAL_SIZE = 100\n\nstudy.optimize(objective, n_trials=TRIAL_SIZE)\n<\/code><\/pre>\n<p>error message<\/p>\n<pre><code>---&gt; 28     loss = criterion(y_pred.reshape(y.shape), y)          # \u640d\u5931\u3092\u8a08\u7b97(shape\u3092\u63c3\u3048\u308b)\n     29     loss.backward()                                                       # \u52fe\u914d\u306e\u8a08\u7b97\n     30     optimizer.step()                                                      # \u52fe\u914d\u306e\u66f4\u65b0\n\nAttributeError: 'NoneType' object has no attribute 'reshape'\n<\/code><\/pre>\n<p>Because of the above error, I checked the value of <code>y_pred<\/code> and found it to be <code>None<\/code>.<\/p>\n<pre><code>    model.train()\n    optimizer.zero_grad()\n<\/code><\/pre>\n<p>I am thinking that these two lines may be wrong, but I don't know how to solve this problem.<\/p>",
        "Challenge_closed_time":1646568057230,
        "Challenge_comment_count":1,
        "Challenge_created_time":1646559560287,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create a PyTorch training model for regression analysis using Optuna, but encounters an error message stating that 'NoneType' object has no attribute 'reshape'. The user suspects that the issue may be with the lines of code related to model training and optimizer initialization, but is unsure how to resolve the problem.",
        "Challenge_last_edit_time":1646560172363,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71369132",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":39.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":2.3602619445,
        "Challenge_title":"The pytorch training model cannot be created successfully",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":267,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643702319420,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>With PyTorch, when you call <code>y_pred = model(x)<\/code> that will call the <code>forward<\/code> function which is defined in the <code>Model<\/code> class.<\/p>\n<p>So, <code>y_pred <\/code> will get the result of the <code>forward<\/code> function, in your case, it returns nothing, that's why you get a <code>None<\/code> value. You can change the <code>forward<\/code> function as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    def forward(self, x):\n        x = self.linear1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        return x\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":7.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":69.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":41.2755555556,
        "Challenge_answer_count":0,
        "Challenge_body":"I was getting an error when azuremllogonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. So, I installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below.\r\n\r\ngrep executes but now the error says \"Dataset with name 'mnist_opendataset' is not found\".\r\n\r\nAny help troubleshooting this error will be appreciated, I am trying to demo this to a customer. next week.\r\n\r\n**TEXT of the OUTPUT when error is encountered:**\r\n\r\n\r\nInstalling amlarc-compute K8s extension was successful.\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nLibrary configuration succeeded\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\n\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nfound compute target: ARC-ml\r\n\"\r\n Training model:\r\n                               \r\n            .....                                             .....\r\n         .........                                           .........\r\n        .........                 (((((((((##                 .........\r\n       .....                      (((((((####                      .....\r\n      ......                      #((########                      ......\r\n     ....... .............        ###########        ............. .......\r\n     ......................       ###########       ......................\r\n    .................*.....       ###########       ....,*.................\r\n    .........*******......       (((((((((((         ......*******.........\r\n         ............          (((((((((((     (.         ............\r\n                            .(((((((((((     (((((\/\r\n                          ((((((((((((     #(((((((##\r\n                        \/\/\/\/(((((((*     ##############\r\n                      \/\/\/\/\/\/(((((.         ,#############.\r\n                   ,**\/\/\/\/\/\/\/((               #############\/\r\n                    *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%##########\r\n                    \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######(\r\n                     \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%####\r\n                     .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#.\r\n\r\n\"\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nWARNING: Command group 'ml job' is experimental and under development. Reference and support levels: https:\/\/aka.ms\/CLI_refstatus\r\nUploading src:   0%|                                                                                                                                | 0.00\/3.08k [00:00<?, ?B\/s]\r\n\r\n**ERROR: Code: UserError**\r\n**Message: Dataset with name 'mnist_opendataset' is not found.**\r\n**You cannot call a method on a null-valued expression.**\r\n**At C:\\Temp\\AzureMLLogonScript.ps1:267 char:4**\r\n**+    $RunId = ($Job | grep '\\\"name\\\":').Split('\\\"')[3]**\r\n**+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**\r\n    **+ CategoryInfo          : InvalidOperation: (:) [], RuntimeException**\r\n    **+ FullyQualifiedErrorId : InvokeMethodOnNull**\r\n\r\n**RunId:**\r\n**Training model, hold tight...**\r\n**ERROR: argument --name\/-n: expected one argument**_****\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n",
        "Challenge_closed_time":1631711922000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1631563330000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has encountered broken links in the introduction section of the 11_exploring_hyperparameters_on_azureml notebook related to object detection. The links to two notebooks, 02_mask_rcnn.ipynb and 03_training_accuracy_vs_speed.ipynb, are not working. The user is working from the master branch of the repo and expects the notebooks to be present or the links to be removed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/758",
        "Challenge_link_count":5,
        "Challenge_participation_count":3,
        "Challenge_readability":10.4,
        "Challenge_reading_time":56.26,
        "Challenge_repo_contributor_count":62.0,
        "Challenge_repo_fork_count":369.0,
        "Challenge_repo_issue_count":1562.0,
        "Challenge_repo_star_count":527.0,
        "Challenge_repo_watch_count":26.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":41.2755555556,
        "Challenge_title":"error when installing AZURE ML training model piece",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":477,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @arturoqu77 - thanks for reaching out. We tried to repro this issue but couldn't.\r\n\r\nThis [line of code](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/AzureMLLogonScript.ps1#L266) leverages grep to parse the file name. `grep` should have been installed as part of the [bootstrap](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73). If  `grep` wasn't installed, this implies something must have interrupted the install before it got there.\r\n\r\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working. \r\n\r\nAre you seeing Postman installed - this happens [after `grep`](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73)? If not, this is probably what happened.\r\n\r\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above. Hello,\n\nThank you for your reply. I may have logged on before the bootstrap completed, I re-started the deployment to a new RG and seems to be working now.\n\nThanks for the help.\n\nRegards\n\n***@***.***\nArturo Quiroga\nSr. Cloud Solutions Architect (CSA)\nAzure Applications & Infrastructure\n***@***.******@***.***>\n[MSFT_logo_Gray DE sized SIG1.png]\n\n\nFrom: Raki ***@***.***>\nDate: Tuesday, September 14, 2021 at 6:22 PM\nTo: microsoft\/azure_arc ***@***.***>\nCc: Arturo Quiroga ***@***.***>, Mention ***@***.***>\nSubject: Re: [microsoft\/azure_arc] error when installing AZURE ML training model piece (#758)\n\nHi @arturoqu77<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Farturoqu77&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666347896%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=r1kAuKxYlYhONjoSTk83SERggUvNcbP1Hr4vmNh29io%3D&reserved=0> - thanks for reaching out. We tried to repro this issue but couldn't.\n\nThis line of code<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FAzureMLLogonScript.ps1%23L266&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=oAjL%2BfBBF4QXfnwN9gcM9UqEB4OA0ZZrzMuKilatz5A%3D&reserved=0> leverages grep to parse the file name. grep should have been installed as part of the bootstrap<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=V8dzJxj3W5a6IL8T%2BvB0mijBm5Ng4G46bb%2Fcdo2uvz4%3D&reserved=0>. If grep wasn't installed, this implies something must have interrupted the install before it got there.\n\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working.\n\nAre you seeing Postman installed - this happens after grep<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=XvpeFo2T7Kjr4qrIZYKO7eM0khlOddES9O3DGaw1yZ4%3D&reserved=0>? If not, this is probably what happened.\n\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fissues%2F758%23issuecomment-919554382&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ZBGNkrDGFcqvrdWHXy5iEGluQiq2Ph%2BZnfosqC3qTTU%3D&reserved=0>, or unsubscribe<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAHV4QUFA72NR7CEJ3UPS5NLUB7DLDANCNFSM5D6SSBHA&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ctEevpiqzC%2FQnTc6ho2hfr2PVGA%2FqwGJzj1pPUCEylY%3D&reserved=0>.\nTriage notifications on the go with GitHub Mobile for iOS<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=cMCZqYPB6q8c9n%2BgPTk9f3MCQr%2BlV4GsOW9iPFSZtgE%3D&reserved=0> or Android<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666387876%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=6B5T09q%2Bx2Q2rWftui6b32lD1VLrCRMPiLSrTUS7xnI%3D&reserved=0>.\n Great!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":11.0,
        "Solution_readability":21.6,
        "Solution_reading_time":96.07,
        "Solution_score_count":null,
        "Solution_sentence_count":39.0,
        "Solution_word_count":416.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1305851487736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Challenge_adjusted_solved_time":0.6922330556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Trying to understand <a href=\"https:\/\/dvc.org\/doc\/start\" rel=\"nofollow noreferrer\">dvc<\/a>, most tutorials mention generation of dvc.yaml by running <code>dvc run<\/code> command.<\/p>\n<p>But at the same time, dvc.yaml which defines the DAG is also <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files\" rel=\"nofollow noreferrer\">well documented<\/a>. Also the fact that it is a yaml format and human readable\/writable would point to the fact that it is meant to be a DSL for specifying your data pipeline.<\/p>\n<p>Can somebody clarify which is the better practice?\nWriting the dvc.yaml or let it be generated by <code>dvc run<\/code> command?\nOr is it left to user's choice and there is no technical difference?<\/p>",
        "Challenge_closed_time":1623859205940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623853195940,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to understand whether the dvc.yaml file, which defines the DAG in DVC, should be written manually or generated by the \"dvc run\" command. They are seeking clarification on which approach is better or if there is no technical difference between the two.",
        "Challenge_last_edit_time":1623873485368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68004538",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.6694444444,
        "Challenge_title":"Is dvc.yaml supposed to be written or generated by dvc run command?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1101.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1243446134992,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gothenburg, Sweden",
        "Poster_reputation_count":1547.0,
        "Poster_view_count":212.0,
        "Solution_body":"<p>I'd recommend manual editing as the main route! (I believe that's officially recommended since <a href=\"https:\/\/dvc.org\/blog\/dvc-2-0-release\" rel=\"nofollow noreferrer\">DVC 2.0<\/a>)<\/p>\n<p><code>dvc stage add<\/code> can still be very helpful for programmatic generation of pipelines files, but it doesn't support all the features of <code>dvc.yaml<\/code>, for example setting <code>vars<\/code> values or defining <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#foreach-stages\" rel=\"nofollow noreferrer\"><code>foreach<\/code> stages<\/a>.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1623875977407,
        "Solution_link_count":2.0,
        "Solution_readability":15.1,
        "Solution_reading_time":7.55,
        "Solution_score_count":4.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":54.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":15.5657958333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have done quite a few google searches but have not found a clear answer to the following use case. Basically, I would rather use cloud 9 (most of the time) as my IDE rather than Jupyter. What I am confused\/not sure about is, how I could executed long running jobs like (Bayesian) hyper parameter optimisation from there. Can I use Sagemaker capabilities? Should I use docker and deploy to ECR (looking for the cheapest-ish option)? Any pointers w.r.t. to this particular issue would be very much appreciated. Thanks.<\/p>",
        "Challenge_closed_time":1641735920212,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641644416603,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for guidance on how to execute long running jobs like hyper parameter optimization using Cloud 9 as their IDE instead of Jupyter. They are unsure if they can use Sagemaker capabilities or if they should use docker and deploy to ECR. They are seeking advice on the most cost-effective option.",
        "Challenge_last_edit_time":1641679883347,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70632239",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":25.4176691667,
        "Challenge_title":"cloud 9 and sagemaker - hyper parameter optimisation",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>You could use whatever IDE you choose (including your laptop).<br \/>\nSaegMaker tuning job (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex.html\" rel=\"nofollow noreferrer\">example<\/a>) is <strong>asynchronous<\/strong>, so you can safely close your IDE after launching it. You can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-monitor.html\" rel=\"nofollow noreferrer\">monitor the job the AWS web console,<\/a> or with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeHyperParameterTuningJob.html\" rel=\"nofollow noreferrer\">DescribeHyperParameterTuningJob API call<\/a>.<\/p>\n<p>You can launch TensorFlow, PyTorch, XGBoost, Scikit-learn, and other <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/frameworks.html\" rel=\"nofollow noreferrer\">popular ML frameworks<\/a>, using one of the built-in framework containers, avoiding the extra work of bringing your own container.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":20.2,
        "Solution_reading_time":13.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":81.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.0739286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Challenge_closed_time":1661603882123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661517215980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"BAD_REQUEST\" error while trying to fetch runs from search_runs in mlflow while training a machine learning model in an Azure environment. The error message indicates an issue with the input string. The mlflow version being used is 1.28.0 and the user has successfully created and run jobs in Azure studio.",
        "Challenge_last_edit_time":1661625379892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24.0739286111,
        "Challenge_title":"Getting Bad request while searching run in mlflow",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582101477803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":171.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":17.0,
        "Solution_reading_time":16.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1491467888608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":381.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":26.2894686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to use WanDB but when i use wandb.init() there is nothing.!<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EZfVt.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I am waiting a lot of time.<\/p>\n<p>However, there is nothing in window.<\/p>\n<p>This is working well in Kernel.<\/p>\n<p>please.. help me guys<\/p>",
        "Challenge_closed_time":1655111602567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1655016960480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with JupyterLab Wandb as there is no output when using wandb.init() and the process is taking a lot of time. The issue is not present in the Kernel and the user is seeking help to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72590067",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":5.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":26.2894686111,
        "Challenge_title":"jupyterLab Wandb does not iterative",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Poster_created_time":1609152494583,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"South Korea",
        "Poster_reputation_count":72.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>I work at Weights &amp; Biases. If you're in a notebook the quickest thing you can do to get going with <code>wandb<\/code> is simply:<\/p>\n<pre><code>wandb.init(project=MY_PROJECT, entity=MY_ENTITY)\n<\/code><\/pre>\n<p>No <code>!wandb login<\/code>, <code>wandb.login()<\/code> or <code>%%wandb<\/code> needed. If you're not already logged in then <code>wandb.init<\/code> will ask you for you API key.<\/p>\n<p>(curious where you found <code>%%wandb<\/code> by the way?)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":6.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":57.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1550902509267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2669.0,
        "Answerer_view_count":3292.0,
        "Challenge_adjusted_solved_time":1.5344702778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>How can I download a mlflow model artefact in a docker container from databricks workspace?<\/p>",
        "Challenge_closed_time":1645776779603,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645771255510,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in downloading a mlflow model artefact from a Databricks workspace in a docker container.",
        "Challenge_last_edit_time":1645795991323,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71262010",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":1.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1.5344702778,
        "Challenge_title":"Download model artefact from Databricks workspace",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":370.0,
        "Challenge_word_count":20,
        "Platform":"Stack Overflow",
        "Poster_created_time":1411361217027,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":569.0,
        "Poster_view_count":123.0,
        "Solution_body":"<p>To download a model from Databricks workspace you need to do two things:<\/p>\n<ol>\n<li><p>Set MLFlow tracking URI to databricks using python API<\/p>\n<\/li>\n<li><p>Setup databricks authentication. I prefer authenticating by setting the following environment variables, you can also use databricks CLI to authenticate:<\/p>\n<pre><code>DATABRICKS_HOST\n\nDATABRICKS_TOKEN\n<\/code><\/pre>\n<\/li>\n<li><p>Here's a basic code snippet to download a model from Databricks workspace model registry:<\/p>\n<pre><code>import os\nimport mlflow\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n\nmodel_name = &quot;example-model-name&quot;\nmodel_stage = &quot;Staging&quot;  # Should be either 'Staging' or 'Production'\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\n\nos.makedirs(&quot;model&quot;, exist_ok=True)\nlocal_path = ModelsArtifactRepository(\n    f'models:\/{model_name}\/{model_stage}').download_artifacts(&quot;&quot;, dst_path=&quot;model&quot;)\n\nprint(f'{model_stage} Model {model_name} is downloaded at {local_path}')\n<\/code><\/pre>\n<p>Running above python script will download an ML model in the model directory.<\/p>\n<p><strong>Containerizing MLFlow model serving with Docker<\/strong><\/p>\n<p>The next step is to package this downloaded model in a docker image and serve a model when you run the image.<\/p>\n<\/li>\n<\/ol>\n<p>Here's a basic Dockerfile to do the same:<\/p>\n<pre><code>FROM continuumio\/miniconda3\n\nENV MLFLOW_HOME \/opt\/mlflow\nENV MLFLOW_VERSION 1.12.1\nENV PORT 5000\n\nRUN conda install -c conda-forge mlflow=${MLFLOW_VERSION}\n\nCOPY model\/ ${MLFLOW_HOME}\/model\n\nWORKDIR ${MLFLOW_HOME}\n\nRUN mlflow models prepare-env -m ${MLFLOW_HOME}\/model\n\nRUN useradd -d ${MLFLOW_HOME} mlflow\nRUN chown mlflow: ${MLFLOW_HOME}\nUSER mlflow\n\nCMD mlflow models serve -m ${MLFLOW_HOME}\/model --host 0.0.0.0 --port ${PORT}\n<\/code><\/pre>\n<p>For more information you can follow this <a href=\"https:\/\/dev.to\/itachiredhair\/downloading-mlflow-model-from-databricks-and-serving-with-docker-38ip\" rel=\"nofollow noreferrer\">article<\/a> from Akshay Milmile<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1645795937627,
        "Solution_link_count":1.0,
        "Solution_readability":16.2,
        "Solution_reading_time":26.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":210.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1565528932887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"United Kingdom",
        "Answerer_reputation_count":1579.0,
        "Answerer_view_count":91.0,
        "Challenge_adjusted_solved_time":28.5942333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using VS Code to submit a Machine Learning experiment in Azure Portal. When running the experiment I'm obtaining the following error:<\/p>\n\n<p>Run failed: User program failed with ModuleNotFoundError: No module named 'amlrun'<\/p>\n\n<p>This is the code structure:<\/p>\n\n<p>.vscode (json configuration file)<\/p>\n\n<p>aml_config<\/p>\n\n<p>scripts<\/p>\n\n<p>----- amlrun.py (a script with some functions)<\/p>\n\n<p>----- model_training.py (a script creating and saving the model)<\/p>\n\n<p>This is the configuration file:<\/p>\n\n<pre><code>{\n    \"script\": \"model_training.py\",\n    \"framework\": \"Python\",\n    \"communicator\": \"None\",\n    \"target\": \"testazure\",\n    \"environment\": {\n        \"python\": {\n            \"userManagedDependencies\": false,\n            \"condaDependencies\": {\n                \"dependencies\": [\n                    \"python=3.6.2\",\n                    \"scikit-learn\",\n                    \"numpy\",\n                    \"pandas\",\n                    {\n                        \"pip\": [\n                            \"azureml-defaults\"\n                        ]\n                    }\n                ]\n            }\n        },\n        \"docker\": {\n            \"baseImage\": \"mcr.microsoft.com\/azureml\/base:0.2.4\",\n            \"enabled\": true,\n            \"baseImageRegistry\": {\n                \"address\": null,\n                \"username\": null,\n                \"password\": null\n            }\n        }\n    },\n    \"history\": {\n        \"outputCollection\": true,\n        \"snapshotProject\": false,\n        \"directoriesToWatch\": [\n            \"logs\"\n        ]\n    }\n}\n<\/code><\/pre>\n\n<p>Am I missing something?\nThanks<\/p>",
        "Challenge_closed_time":1571837118727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1571735433200,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running a Machine Learning experiment in Azure Portal using VS Code. The error message states that the user program failed with ModuleNotFoundError: No module named 'amlrun'. The user has provided the code structure and configuration file and is seeking help to identify the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58500807",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":28.2459797222,
        "Challenge_title":"Run failed: User program failed with ModuleNotFoundError: No module named 'amlrun' in Azure ML Experiment",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":901.0,
        "Challenge_word_count":125,
        "Platform":"Stack Overflow",
        "Poster_created_time":1547138031703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>When your training script is running in azure, it's not able to find all your local imports i.e. <code>amlrun.py<\/code> script. <\/p>\n\n<p>The submitted training job to azure builds a docker image with your files first and runs the experiment; but in this case the extension hasn't included <code>amlrun.py<\/code>. <\/p>\n\n<p>This is probably because when you have submit the training job with the extension, the visual studio code window opened is not pointing to be in <code>scripts<\/code> folder.<\/p>\n\n<p>Taken from one of the replies to a <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/24032\" rel=\"nofollow noreferrer\">previously raised github issue<\/a>:<\/p>\n\n<blockquote>\n  <p>The extension currently requires the script you are working on to be\n  in the folder that is open in VS Code and not in a sub-directory.<\/p>\n<\/blockquote>\n\n<hr>\n\n<p>To fix this you can do <strong>either<\/strong> of the following:<\/p>\n\n<ol>\n<li><p>You would need to re-open Visual Studio Code in <code>scripts<\/code> folder instead of parent directory.<\/p><\/li>\n<li><p>Move all files in <code>script<\/code> directory to be in it's parent directory.<\/p><\/li>\n<\/ol>\n\n<hr>\n\n<p>If you're looking for more flexible way to submit training jobs and managing aml - you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/intro?view=azure-ml-py\" rel=\"nofollow noreferrer\">azure machine learning sdk<\/a> for python.<\/p>\n\n<p>Some examples of using the SDK to manage expirements can be found in the links below:<\/p>\n\n<ol>\n<li><p><a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/service\/tutorial-train-models-with-aml.md\" rel=\"nofollow noreferrer\">Scikit Learn Model Training Docs<\/a> <\/p><\/li>\n<li><p><a href=\"https:\/\/github.com\/rithinch\/heartfulness-similar-content-service\" rel=\"nofollow noreferrer\">Basic Pytorch Model Training and Deployment Example Repo<\/a><\/p><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1571838372440,
        "Solution_link_count":4.0,
        "Solution_readability":13.0,
        "Solution_reading_time":24.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":226.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1318315569328,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":371.0,
        "Answerer_view_count":49.0,
        "Challenge_adjusted_solved_time":13.5453841667,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>When using the JupyterLab found within the azure ML compute instance, every now and then, I run into an issue where it will say that network connection is lost. <\/p>\n\n<p>I have confirmed that the computer is still running.\nthe notebook itself can be edited and saved, so the computer\/VM is definitely running\nOf course, the internet is fully functional<\/p>\n\n<p>On the top right corner <em>next to the now blank circle<\/em> it will say \"No Kernel!\"<\/p>",
        "Challenge_closed_time":1582323157643,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582274394260,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"No Kernel!\" error while using JupyterLab in Azure ML compute instance. Despite the computer running and the notebook being editable and savable, the user is experiencing network connection loss. The error message appears next to a blank circle in the top right corner.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60334889",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.1,
        "Challenge_reading_time":6.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":13.5453841667,
        "Challenge_title":"\"No Kernel!\" error Azure ML compute JupyterLab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1026.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442334437952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":2272.0,
        "Poster_view_count":516.0,
        "Solution_body":"<p>We can't repro the issue, can you help gives us more details? One possibility is that the kernel has bugs and hangs (could be due to extensions, widgets installed) or the resources on the machine are exhausted and kernel dies. What VM type are you using? If it's a small VM you may ran out of resources.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":3.73,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":46.5198344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to train <strong>Azure Machine Learning model<\/strong> on azure using <strong>Azure Machine Learning Service.<\/strong> But I want to use the <strong>custom Docker image<\/strong> for deploying the model on azure. I am not able to understand how to deploy Machine Learning models using Custom Docker Image.<\/p>\n\n<p>Please share me if there is any tutorial or blog about the deploy ml models using a custom image.<\/p>\n\n<p>Please check the below Docker file commands:-<\/p>\n\n<pre><code># Set locale\nRUN apt-get update\nRUN apt-get install locales\nRUN locale-gen en_US.UTF-8\nRUN update-locale LANG=en_US.UTF-8\n\n# Install MS SQL v13 driver for PyOdbc\nRUN apt-get install -y curl\nRUN apt-get install apt-transport-https\nRUN curl https:\/\/packages.microsoft.com\/keys\/microsoft.asc | apt-key add - \nRUN curl https:\/\/packages.microsoft.com\/config\/ubuntu\/16.04\/prod.list &gt; \/etc\/apt\/sources.list.d\/mssql-release.list\nRUN exit\nRUN apt-get update\n\nRUN ACCEPT_EULA=Y apt-get install -y msodbcsql\nRUN apt-get install -y unixodbc-dev\n<\/code><\/pre>\n\n<p>I want to use the <strong>Azure Container Registry<\/strong> for push the docker image and use the <strong>Custom Docker Image.<\/strong> Please let me know if there is any way.<\/p>\n\n<p><strong>Is there any way to Deploy Azure ML Models using Custom docker images?<\/strong><\/p>",
        "Challenge_closed_time":1569245799796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569236101160,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to deploy an Azure Machine Learning model on Azure using a custom Docker image but is unsure how to do so. They are seeking guidance on how to deploy Machine Learning models using a custom image and are interested in using Azure Container Registry to push the Docker image. The user has provided a Docker file with commands and is looking for any tutorials or blogs on the topic.",
        "Challenge_last_edit_time":1569244969488,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58060865",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":2.6940655556,
        "Challenge_title":"Deploy Azure Machine Learning models using Custom Docker Image on Azure Container Regisrty",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":864.0,
        "Challenge_word_count":184,
        "Platform":"Stack Overflow",
        "Poster_created_time":1554466050936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":219.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>You can do following:<\/p>\n\n<ol>\n<li>Create an [Environment][1] with the coordinates of your custom Docker image specified in Docker section.<\/li>\n<li>Create [InferenceConfig][2] with that Environment as argument, and use it when deploying the model.<\/li>\n<\/ol>\n\n<p>For example, assuming you have a model already and eliding other arguments:<\/p>\n\n<pre><code>from azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment(name=\"myenv\")\nenv.docker.base_image = \"mybaseimage\"\nenv.docker.base_image_registry.address = \"ip-address\"\nenv.docker.base_image_registry.username = \"my-username\"\nenv.docker.base_image_registry.password = \"my-password\"\n\nic = InferenceConfig(\u2026,environment = env)\nmodel.deploy(\u2026,inference_config = ic)\n<\/code><\/pre>\n\n<pre><code>\n  [1]: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py\n  [2]: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":1569412440892,
        "Solution_link_count":2.0,
        "Solution_readability":20.2,
        "Solution_reading_time":14.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.3654761111,
        "Challenge_answer_count":1,
        "Challenge_body":"I work in SM Studio, and I do not understand why CPU and memory usage do not appear in the notebook toolbar. These metrics should be there, at least given this description:\n\nhttps:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/notebooks-menu.html\n\nWhen I open a notebook in SM Studio, I see the same toolbar but without CPU and memory usage listed. Moreover, I see 'cluster' before the kernel's name in my toolbar.\n\nHas anyone experienced sth similar? I assume an alternative for me would be to use CloudWatch.",
        "Challenge_closed_time":1652798353412,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652797037698,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue in SM Studio where CPU and memory usage are missing from the notebook toolbar, despite being listed in the documentation. The user also notes that 'cluster' appears before the kernel's name in the toolbar. They are seeking advice from others who may have experienced a similar issue and suggest using CloudWatch as an alternative.",
        "Challenge_last_edit_time":1668563929559,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGQfGnTgqQcyNbWVb3U9V8Q\/cpu-memory-usage-missing-from-sm-studio-notebook-toolbar",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3654761111,
        "Challenge_title":"CPU + memory usage missing from SM Studio notebook toolbar",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":614.0,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, you should be able to see your CPU and Memory on the bottom toolbar, looks like `Kernel: Idle | Instance MEM`. You can click on that text to show the kernel and instance usage metrics.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1652798353412,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":2.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":35.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":6.0155075,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Challenge_closed_time":1625487892607,
        "Challenge_comment_count":0,
        "Challenge_created_time":1625466236780,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing inconvenience while using AWS Sagemaker notebook instance as it is designed to only persist data under ~\/Sagemaker, causing loss of settings and python packages every time the instance needs restarting. The user wonders why AWS made this decision while Google Cloud's AI platform does not have such limitations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68251533",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":6.0155075,
        "Challenge_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":266.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Poster_created_time":1408744280996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation_count":2758.0,
        "Poster_view_count":122.0,
        "Solution_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":6.27,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1526889513900,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":81.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":24.1450022222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to write a small program using the AzureML Python SDK (v1.0.85) to register an Environment in AMLS and use that definition to construct a local Conda environment when experiments are being run (for a pre-trained model). The code works fine for simple scenarios where all dependencies are loaded from Conda\/ public PyPI, but when I introduce a private dependency (e.g. a utils library) I am getting a InternalServerError with the message \"Error getting recipe specifications\".<\/p>\n\n<p>The code I am using to register the environment is (after having authenticated to Azure and connected to our workspace):<\/p>\n\n<pre><code>environment_name = config['environment']['name']\npy_version = \"3.7\"\nconda_packages = [\"pip\"]\npip_packages = [\"azureml-defaults\"]\nprivate_packages = [\".\/env-wheels\/utils-0.0.3-py3-none-any.whl\"]\n\nprint(f\"Creating environment with name {environment_name}\")\nenvironment = Environment(name=environment_name)\nconda_deps = CondaDependencies()\n\nprint(f\"Adding Python version: {py_version}\")\nconda_deps.set_python_version(py_version)\n\nfor conda_pkg in conda_packages:\n    print(f\"Adding Conda denpendency: {conda_pkg}\")\n    conda_deps.add_conda_package(conda_pkg)\n\nfor pip_pkg in pip_packages:\n    print(f\"Adding Pip dependency: {pip_pkg}\")\n    conda_deps.add_pip_package(pip_pkg)\n\nfor private_pkg in private_packages:\n    print(f\"Uploading private wheel from {private_pkg}\")\n    private_pkg_url = Environment.add_private_pip_wheel(workspace=ws, file_path=Path(private_pkg).absolute(), exist_ok=True)\n    print(f\"Adding private Pip dependency: {private_pkg_url}\")\n    conda_deps.add_pip_package(private_pkg_url)\n\nenvironment.python.conda_dependencies = conda_deps\nenvironment.register(workspace=ws)\n<\/code><\/pre>\n\n<p>And the code I am using to create the local Conda environment is:<\/p>\n\n<pre><code>amls_environment = Environment.get(ws, name=environment_name, version=environment_version)\n\nprint(f\"Building environment...\")\namls_environment.build_local(workspace=ws)\n<\/code><\/pre>\n\n<p>The exact error message being returned when <code>build_local(...)<\/code> is called is:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 814, in build_local\n    raise error\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\core\\environment.py\", line 807, in build_local\n    recipe = environment_client._get_recipe_for_build(name=self.name, version=self.version, **payload)\n  File \"C:\\Anaconda\\envs\\AMLSExperiment\\lib\\site-packages\\azureml\\_restclient\\environment_client.py\", line 171, in _get_recipe_for_build\n    raise Exception(message)\nException: Error getting recipe specifications. Code: 500\n: {\n  \"error\": {\n    \"code\": \"ServiceError\",\n    \"message\": \"InternalServerError\",\n    \"detailsUri\": null,\n    \"target\": null,\n    \"details\": [],\n    \"innerError\": null,\n    \"debugInfo\": null\n  },\n  \"correlation\": {\n    \"operation\": \"15043e1469e85a4c96a3c18c45a2af67\",\n    \"request\": \"19231be75a2b8192\"\n  },\n  \"environment\": \"westeurope\",\n  \"location\": \"westeurope\",\n  \"time\": \"2020-02-28T09:38:47.8900715+00:00\"\n}\n\nProcess finished with exit code 1\n<\/code><\/pre>\n\n<p>Has anyone seen this error before or able to provide some guidance around what the issue may be?<\/p>",
        "Challenge_closed_time":1583243758048,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583156836040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while building a local AMLS environment with a private wheel. The user is trying to register an environment in AMLS and use that definition to construct a local Conda environment when experiments are being run. The code works fine for simple scenarios where all dependencies are loaded from Conda\/public PyPI, but when the user introduces a private dependency, they are getting an InternalServerError with the message \"Error getting recipe specifications\". The user is seeking guidance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60490195",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":42.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":24.1450022222,
        "Challenge_title":"Unable to build local AMLS environment with private wheel",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":239.0,
        "Challenge_word_count":291,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526889513900,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>The issue was with out firewall blocking the required requests between AMLS and the storage container (I presume to get the environment definitions\/ private wheels).<\/p>\n\n<p>We resolved this by updating the firewall with appropriate ALLOW rules for the AMLS service to contact and read from the attached storage container.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":4.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1483370766803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation_count":15819.0,
        "Answerer_view_count":1395.0,
        "Challenge_adjusted_solved_time":0.1047952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was following a guide on mounting EFS in SageMaker studio, but when using the following as a notebook cell:<\/p>\n<pre><code>%%sh \n\nsudo mount -t nfs \\\n    -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\\n    172.31.5.227:\/ \\\n    ..\/efs\n\nsudo chmod go+rw ..\/efs\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>sh: 2: sudo: not found\nsh: 7: sudo: not found\n<\/code><\/pre>\n<p>Even in the terminal ('image terminal'), sudo is not found: <code># sudo \/bin\/sh: 1: sudo: not found<\/code><\/p>",
        "Challenge_closed_time":1596811524960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596811147697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to mount EFS in SageMaker studio. When using the given notebook cell, the user is getting an error message \"sudo: not found\" both in the notebook cell and the terminal.",
        "Challenge_last_edit_time":1596823164336,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63304005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1047952778,
        "Challenge_title":"sudo: not found on AWS Sagemaker Studio",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":980.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1483370766803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":15819.0,
        "Poster_view_count":1395.0,
        "Solution_body":"<p>I managed to get sudo working in the &quot;System Terminal&quot; instead. The image terminals don't seem to have access to sudo.<\/p>\n<p>Unrelated: But then when I tried to mount EFS onto the SageMaker studio app, it simply failed, saying mount target is not a directory. Looks like I'm not using Sagemaker Studio this year.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1596815281143,
        "Solution_link_count":0.0,
        "Solution_readability":8.6,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1361339272692,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"NYC",
        "Answerer_reputation_count":6281.0,
        "Answerer_view_count":958.0,
        "Challenge_adjusted_solved_time":3630.1468475,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>From <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Sagemaker python SDK<\/a> I have seen two API, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ScriptProcessor\" rel=\"nofollow noreferrer\">ScriptProcessor<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.Processor\" rel=\"nofollow noreferrer\">Processor<\/a>. It seems like we can achieve the same goals using either of them, the only difference I noticed ScriptProcessor support docker <code>command<\/code> parameter on the other hand Processor support docker <code>entrypoint<\/code> parameter. Is there any other difference amongst them? <\/p>",
        "Challenge_closed_time":1604879360048,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591810831397,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between ScriptProcessor and Processor in AWS Sagemaker SDK. Both APIs can achieve the same goals, but ScriptProcessor supports docker command parameter while Processor supports docker entrypoint parameter. The user is asking if there are any other differences between the two.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62309772",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":21.3,
        "Challenge_reading_time":11.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3630.1468475,
        "Challenge_title":"Difference between Processor and ScriptProcessor in AWS Sagemaker SDK",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":400.0,
        "Challenge_word_count":69,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370231111260,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Toronto, ON, Canada",
        "Poster_reputation_count":5295.0,
        "Poster_view_count":455.0,
        "Solution_body":"<p><code>sagemaker.processing.ScriptProcessor<\/code> subclasses <code>sagemaker.processing.Processor<\/code>. <code>ScriptProcessor<\/code> can be used to write a custom processing script. <code>Processor<\/code> can be subclassed to create a <code>CustomProcessor<\/code> class for a more complex use case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":17.5,
        "Solution_reading_time":4.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.6111111111,
        "Challenge_answer_count":0,
        "Challenge_body":"its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in sagemaker\r\nhow long is this going to even take man",
        "Challenge_closed_time":1667627477000,
        "Challenge_comment_count":4,
        "Challenge_created_time":1667438077000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to install libraries in SageMaker Studio Lab that require root privileges, but they are not able to access root user. They have tried running `whoami` and `sudo` commands, but the latter is not found. They have also tried to install `sudo` by following a link, but they are prompted for a password which they do not have. The user is seeking help to gain root access or install libraries that require root access.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/155",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":52.6111111111,
        "Challenge_title":"we are experiencing elevated fault rate in start runtime API. The SageMaker Studio Lab team is working to restore the service.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":51,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"up Ah finally btw, I can run the CPU but not GPU today\r\n @saleemmalik10835 Sorry for the long inconvenience of Studio Lab. As you know, the service was back and we confirmed that we can say it to you. I will close this issue because the mentioned problem is solved.\r\n\r\nBut as @aozorahime said, the GPU instance is sometime unavailable because of another instance allocation issue. Of course, we deal with this problem now. ",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":5.05,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":71.3516666667,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nI try to do multi-label classification with \"doc_classification_multilabel.py\". It worked at first. However when it came to `\"Train epoch 1\/1:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 17251\/26668 [10:19:41<4:04:28,  1.56s\/it]\"`, it stopped and report:\r\n\r\n```\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 672, in urlopen\r\n    chunked=chunked,\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 421, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 416, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 1331, in getresponse\r\n    response.begin()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 297, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 266, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n......\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\r\n```\r\n\r\n  I have checked that the Internet connection was ok. So I was confused why this error occured ?\r\n  \r\n\r\n**Error message**\r\nError that was thrown (if available)\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here, like type of downstream task, part of  etc.. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior\r\n\r\n**System:**\r\n - OS: \r\n - GPU\/CPU:\r\n - FARM version:\r\n",
        "Challenge_closed_time":1580393757000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1580136891000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a \"ModuleNotFoundError\" issue while running tests due to the absence of the \"mlflow\" module, which is required by the \"rikai.spark.sql.codegen.mlflow_logger\" module.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/deepset-ai\/FARM\/issues\/217",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.8,
        "Challenge_repo_contributor_count":36.0,
        "Challenge_repo_fork_count":231.0,
        "Challenge_repo_issue_count":844.0,
        "Challenge_repo_star_count":1598.0,
        "Challenge_repo_watch_count":56.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":71.3516666667,
        "Challenge_title":"MLFlowLogger: \"Connection aborted.\" - RemoteDisconnected Error",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":177,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @JiangYanting, \r\n\r\nAre you using our public mlflow server for logging (i.e. `ml_logger = MLFlowLogger(tracking_uri=\"https:\/\/public-mlflow.deepset.ai\/\")\r\n` in doc_classification_multilabel.py)? \r\n\r\nI would assume that your connection to that server was not available when the model tried to log the train_loss at step 17251. \r\n\r\nI see two solutions:\r\n- short term: you can log locally by setting `ml_logger = MLFlowLogger(tracking_uri=\"\")`\r\n- mid term: implementing a fix in FARM, so that we raise only a warning, if the logging doesn't succeed, but let the training continue. Let me know if you are interested in adding a PR for this. Otherwise, we can take care. It would be basically a try \/ catch block here: https:\/\/github.com\/deepset-ai\/FARM\/blob\/master\/farm\/utils.py#L126 @tholor By setting `ml_logger = MLFlowLogger(tracking_uri=\"\")` , it works. Thank you very much ! ^_^",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.4,
        "Solution_reading_time":10.9,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":117.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1447151270223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":141.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":215.0267602778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I created a docker image for training. In the dockerfile I have an entrypoint defined such that when <code>docker run<\/code> is executed, it will start running my python code.\nTo use this on aws sagemaker in my understanding I need to create a pytorch estimator in a jupyter notebook in sagemaker. I tried something like this:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nsagemaker_session = sagemaker.Session()\n\nrole = sagemaker.get_execution_role()\n\nestimator = PyTorch(entry_point='train.py',\n                    role=role,\n                    framework_version='1.3.1',\n                    image_name='xxx.ecr.eu-west-1.amazonaws.com\/xxx:latest',\n                    train_instance_count=1,\n                    train_instance_type='ml.p3.xlarge',\n                    hyperparameters={})\n\nestimator.fit({})\n\n<\/code><\/pre>\n\n<p>In the documentation I found that as image name I can specify the link the my docker image on aws ecr. When I try to execute this it keeps complaining<\/p>\n\n<pre><code>[Errno 2] No such file or directory: 'train.py'\n<\/code><\/pre>\n\n<p>It complains immidiatly, so surely I am doing something completely wrong. I would expect that first my docker image should run, and than it could find out that the entry point does not exist.<\/p>\n\n<p>But besides this, why do I need to specify an entry point, as in, should it not be clear that the entry to my training is simply <code>docker run<\/code>?<\/p>\n\n<p>For maybe better understanding. The entrypoint python file in my docker image looks like this:<\/p>\n\n<pre><code>if __name__=='__main__':\n    parser = argparse.ArgumentParser()\n\n    # Hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--batch_size', type=int, default=16)\n    parser.add_argument('--learning_rate', type=float, default=0.0001)\n\n    # Data and output directories\n    parser.add_argument('--output_data_dir', type=str, default=os.environ['OUTPUT_DATA_DIR'])\n    parser.add_argument('--train_data_path', type=str, default=os.environ['CHANNEL_TRAIN'])\n    parser.add_argument('--valid_data_path', type=str, default=os.environ['CHANNEL_VALID'])\n\n    # Start training\n    ...\n<\/code><\/pre>\n\n<p>Later I would like to specify the hyperparameters and data channels. But for now I simply do not understand what to put as entry point. In the documentation it says that the entrypoint is required and it should be a local\/global path to the entrypoint...<\/p>",
        "Challenge_closed_time":1579270087087,
        "Challenge_comment_count":3,
        "Challenge_created_time":1578494679740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user created a custom docker image for training and defined an entrypoint in the Dockerfile. They are trying to use this image on AWS Sagemaker by creating a PyTorch estimator in a Jupyter notebook. However, when executing the code, they receive an error message stating that the entry point does not exist. The user is confused about why they need to specify an entry point and what to put as the entry point.",
        "Challenge_last_edit_time":1578495990750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59648275",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":10.7,
        "Challenge_reading_time":31.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":215.3909297222,
        "Challenge_title":"What to define as entrypoint when initializing a pytorch estimator with a custom docker image for training on AWS Sagemaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":300,
        "Platform":"Stack Overflow",
        "Poster_created_time":1447151270223,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":141.0,
        "Poster_view_count":9.0,
        "Solution_body":"<p>If you really would like to use a complete separate by yourself build docker image, you should create an Amazon Sagemaker algorithm (which is one of the options in the Sagemaker menu). Here you have to specify a link to your docker image on amazon ECR as well as the input parameters and data channels etc. When choosing this options, you should <strong>not<\/strong> use the PyTorch estimater but the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/algorithm.html\" rel=\"nofollow noreferrer\">Algoritm estimater<\/a>. This way you indeed don't have to specify an entrypoint because it simple runs the docker when training and the default entrypoint can be defined in your docker file.<\/p>\n\n<p>The Pytorch estimator can be used when having you own model code, but you would like to run this code in an off-the-shelf Sagemaker PyTorch docker image. This is why you have to for example specify the PyTorch framework version. In this case the entrypoint file by default should be placed next to where your jupyter notebook is stored (just upload the file by clicking on the upload button). The PyTorch estimator inherits all options from the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"nofollow noreferrer\">framework estimator<\/a> where options can be found where to place the entrypoint and model, for example <em>source_dir<\/em>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.3,
        "Solution_reading_time":17.55,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":200.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.5986111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nI want to use `awswrangler` package in my Jupyter Notebook instance of SageMaker.\n\nI understand that we have to use **Lifecycle configuration**. I tried to do it using the following script:\n\n    #!\/bin\/bash\n    \n    pip install awswrangler==0.2.2\n\nBut when I import that package into my Notebook:\n\n    import boto3                                      # For executing native S3 APIs\n    import pandas as pd                               # For munging tabulara data\n    import numpy as np                                # For doing some calculation\n    import awswrangler as wr\n    import io\n    from io import StringIO\n\nI still get the following error:\n\n    ---------------------------------------------------------------------------\n    ModuleNotFoundError                       Traceback (most recent call last)\n    <ipython-input-1-f3d85c7dd0f6> in <module>()\n          2 import pandas as pd                               # For munging tabulara data\n          3 import numpy as np                                # For doing some calculation\n    ----> 4 import awswrangler as wr\n          5 import io\n          6 from io import StringIO\n    \n    ModuleNotFoundError: No module named 'awswrangler'\n\nAny documentation or reference on how to install certain package for Jupyter Notebook in SageMaker?",
        "Challenge_closed_time":1592832724000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592823369000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to install the `awswrangler` package in their Jupyter Notebook instance of SageMaker using a lifecycle configuration script, but is still getting a \"ModuleNotFoundError\" when trying to import the package. They are seeking documentation or reference on how to properly install a package for Jupyter Notebook in SageMaker.",
        "Challenge_last_edit_time":1668557190488,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU4pvReJNZS6eDLxhd4pK-tQ\/how-to-install-phyton-package-in-jupyter-notebook-instance-in-sagemaker",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2.5986111111,
        "Challenge_title":"How to install Phyton package in Jupyter Notebook instance in SageMaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1292.0,
        "Challenge_word_count":154,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\n\n example how to use lifecycle config to install python package in one environment : https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-single-environment\/on-start.sh\n\nand to all conda env - https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-all-environments\/on-start.sh",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925572268,
        "Solution_link_count":2.0,
        "Solution_readability":40.4,
        "Solution_reading_time":6.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.2510238889,
        "Challenge_answer_count":1,
        "Challenge_body":"I was working on a sagemaker studio for ML work, I attached Lifecycle Configuration with it, which was creating problem. Then I deleted the lifecycle configuration without detaching it, and this problem is happening. Can't start sagemaker studio notebook and this is shown.\n\n![Enter image description here](\/media\/postImages\/original\/IMg3hralubRIO8ITzuV3La8Q)\n\nAny suggestion to fix this ?",
        "Challenge_closed_time":1667785787444,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667756083758,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to open Sagemaker Studio after deleting a Lifecycle Configuration without detaching it, which is causing an error message to appear. They are seeking suggestions to fix the issue.",
        "Challenge_last_edit_time":1668480117644,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU91ywEwTsRRqmHKZJ1yVrrA\/sagemaker-studio-is-not-opening-after-deleting-lifecycle-configuration",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.2510238889,
        "Challenge_title":"Sagemaker Studio is not opening after deleting lifecycle configuration",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":169.0,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You can try detaching the LCC script using the CLI. You can use the [CloudShell](https:\/\/aws.amazon.com\/cloudshell\/) from console, since your console role is able to perform updates on the domain. \n\nUse the [update-domain](https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/update-domain.html) CLI call, and provide an empty configuration for the default user settings, something like- \n```\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'\n```",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1667797891160,
        "Solution_link_count":2.0,
        "Solution_readability":13.6,
        "Solution_reading_time":7.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1515518171123,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1108.0,
        "Answerer_view_count":183.0,
        "Challenge_adjusted_solved_time":142.1046519445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to execute R code as webservice, so i tried MLS and it works ok. \nThe problem is that the packages are too old, and i need functions that are not implemented on old packages. \nI asked microsoft support about it, and they have no data up upgrade it, and the new packages require a upgrade of it.<\/p>\n\n<p>How can i do that using other resources, like webapi instead of MLS?\nAll solutions i found requires R installed on machine, wich is a problem for create an azure webapp, function, or api.\nI need an endpoint for forecast on-demand.<\/p>",
        "Challenge_closed_time":1537264424307,
        "Challenge_comment_count":3,
        "Challenge_created_time":1536752847560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user needs to execute R code as a webservice but is facing challenges due to outdated packages in MLS. Microsoft support has no data to upgrade the packages and the new packages require an upgrade. The user is looking for alternative resources like webapi but all solutions require R installed on the machine, which is not possible for creating an Azure webapp, function, or API. The user needs an endpoint for on-demand forecasting.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52294404",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.7,
        "Challenge_reading_time":6.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":142.1046519445,
        "Challenge_title":"R Server on Azure",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":327.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1515518171123,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1108.0,
        "Poster_view_count":183.0,
        "Solution_body":"<p>I found one way to execute R on Azure functions.\nthe solutions is copy R-Portable in\n<a href=\"https:\/\/sourceforge.net\/projects\/rportable\/\" rel=\"nofollow noreferrer\">https:\/\/sourceforge.net\/projects\/rportable\/<\/a>\nunzip it using powershell and create a process on function code. In my case i used the code:<\/p>\n\n<pre><code>System.Diagnostics.Process process = new System.Diagnostics.Process();\n            process.StartInfo.WorkingDirectory = @\"D:\\home\\site\\tools\\R-Portable\\App\\R-Portable\\bin\\\";\n            process.StartInfo.FileName = @\"D:\\home\\site\\tools\\R-Portable\\App\\R-Portable\\bin\\Rscript.exe\";\n            process.StartInfo.Arguments = \"-e \\\"print('Hello world')\\\"\";\n            process.StartInfo.UseShellExecute = false;\n            process.StartInfo.RedirectStandardOutput = true;\n            process.StartInfo.RedirectStandardError = true;\n            process.Start();\n            string outputt = process.StandardOutput.ReadToEnd();\n            string err = process.StandardError.ReadToEnd();\n            process.WaitForExit();\n<\/code><\/pre>\n\n<p>On your script you can access csv files or write, and after on function read and return that file.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.1,
        "Solution_reading_time":13.94,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":84.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.6105555556,
        "Challenge_answer_count":0,
        "Challenge_body":"### Contact Details [Optional]\n\n_No response_\n\n### System Information\n\nZenml == 0.10.0\n\n### What happened?\n\nZenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### Reproduction steps\n\n1. Create a SageMaker pipeline.\r\n2. Create a s3 artifact store.\r\n3. Run the pipeline\r\n\n\n### Relevant log output\n\n```shell\nCreating run for pipeline: mnist_pipeline\r\nCache enabled for pipeline mnist_pipeline\r\nUsing stack sagemaker_stack to run pipeline mnist_pipeline...\r\nStep importer has started.\r\nUsing cached version of importer.\r\nStep importer has finished in 0.045s.\r\nStep trainer has started.\r\nINFO:botocore.credentials:Found credentials in shared credentials file: ~\/.aws\/credentials\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:752 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    749 \u2502   \u2502   \u2502   \u2502   \u2502   params[\"CreateBucketConfiguration\"] = {          \u2502\r\n\u2502    750 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \"LocationConstraint\": region_name            \u2502\r\n\u2502    751 \u2502   \u2502   \u2502   \u2502   \u2502   }                                                \u2502\r\n\u2502 >  752 \u2502   \u2502   \u2502   \u2502   await self._call_s3(\"create_bucket\", **params)       \u2502\r\n\u2502    753 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(\"\")                            \u2502\r\n\u2502    754 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(bucket)                        \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:302 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    299 \u2502   \u2502   \u2502   except Exception as e:                                   \u2502\r\n\u2502    300 \u2502   \u2502   \u2502   \u2502   err = e                                              \u2502\r\n\u2502    301 \u2502   \u2502   err = translate_boto_error(err)                              \u2502\r\n\u2502 >  302 \u2502   \u2502   raise err                                                    \u2502\r\n\u2502    303 \u2502                                                                    \u2502\r\n\u2502    304 \u2502   call_s3 = sync_wrapper(_call_s3)                                 \u2502\r\n\u2502    305                                                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:282 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    279 \u2502   \u2502   additional_kwargs = self._get_s3_method_kwargs(method, *akwa \u2502\r\n\u2502    280 \u2502   \u2502   for i in range(self.retries):                                \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   try:                                                     \u2502\r\n\u2502 >  282 \u2502   \u2502   \u2502   \u2502   out = await method(**additional_kwargs)              \u2502\r\n\u2502    283 \u2502   \u2502   \u2502   \u2502   return out                                           \u2502\r\n\u2502    284 \u2502   \u2502   \u2502   except S3_RETRYABLE_ERRORS as e:                         \u2502\r\n\u2502    285 \u2502   \u2502   \u2502   \u2502   logger.debug(\"Retryable error: %s\", e)               \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:198 in _make_api_call                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   195 \u2502   \u2502   \u2502   'has_streaming_input': operation_model.has_streaming_inpu \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   'auth_type': operation_model.auth_type,                   \u2502\r\n\u2502   197 \u2502   \u2502   }                                                             \u2502\r\n\u2502 > 198 \u2502   \u2502   request_dict = await self._convert_to_request_dict(           \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   api_params, operation_model, context=request_context)     \u2502\r\n\u2502   200 \u2502   \u2502   resolve_checksum_context(request_dict, operation_model, api_p \u2502\r\n\u2502   201                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:246 in _convert_to_request_dict                            \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   243 \u2502                                                                     \u2502\r\n\u2502   244 \u2502   async def _convert_to_request_dict(self, api_params, operation_mo \u2502\r\n\u2502   245 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      context=None):                 \u2502\r\n\u2502 > 246 \u2502   \u2502   api_params = await self._emit_api_params(                     \u2502\r\n\u2502   247 \u2502   \u2502   \u2502   api_params, operation_model, context)                     \u2502\r\n\u2502   248 \u2502   \u2502   request_dict = self._serializer.serialize_to_request(         \u2502\r\n\u2502   249 \u2502   \u2502   \u2502   api_params, operation_model)                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:275 in _emit_api_params                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502                                                                 \u2502\r\n\u2502   273 \u2502   \u2502   event_name = (                                                \u2502\r\n\u2502   274 \u2502   \u2502   \u2502   'before-parameter-build.{service_id}.{operation_name}')   \u2502\r\n\u2502 > 275 \u2502   \u2502   await self.meta.events.emit(                                  \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   event_name.format(                                        \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   service_id=service_id,                                \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   \u2502   operation_name=operation_name),                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\hooks.py:29 in _emit                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   26 \u2502   \u2502   \u2502   if asyncio.iscoroutinefunction(handler):                   \u2502\r\n\u2502   27 \u2502   \u2502   \u2502   \u2502   response = await handler(**kwargs)                     \u2502\r\n\u2502   28 \u2502   \u2502   \u2502   else:                                                      \u2502\r\n\u2502 > 29 \u2502   \u2502   \u2502   \u2502   response = handler(**kwargs)                           \u2502\r\n\u2502   30 \u2502   \u2502   \u2502                                                              \u2502\r\n\u2502   31 \u2502   \u2502   \u2502   responses.append((handler, response))                      \u2502\r\n\u2502   32 \u2502   \u2502   \u2502   if stop_on_response and response is not None:              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\botoc \u2502\r\n\u2502 ore\\handlers.py:243 in validate_bucket_name                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    240 \u2502   \u2502   \u2502   'Invalid bucket name \"%s\": Bucket name must match '      \u2502\r\n\u2502    241 \u2502   \u2502   \u2502   'the regex \"%s\" or be an ARN matching the regex \"%s\"' %  \u2502\r\n\u2502    242 \u2502   \u2502   \u2502   \u2502   bucket, VALID_BUCKET.pattern, VALID_S3_ARN.pattern)) \u2502\r\n\u2502 >  243 \u2502   \u2502   raise ParamValidationError(report=error_msg)                 \u2502\r\n\u2502    244                                                                      \u2502\r\n\u2502    245                                                                      \u2502\r\n\u2502    246 def sse_md5(params, **kwargs):                                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nParamValidationError: Parameter validation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\run-sagemaker.py:87 in       \u2502\r\n\u2502 <module>                                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   84 \u2502   \u2502   trainer=trainer(),                                             \u2502\r\n\u2502   85 \u2502   \u2502   evaluator=evaluator(),                                         \u2502\r\n\u2502   86 \u2502   )                                                                  \u2502\r\n\u2502 > 87 \u2502   pipeline.run()                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\pipelines\\base_pipeline.py:489 in run                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   486 \u2502   \u2502   self._reset_step_flags()                                      \u2502\r\n\u2502   487 \u2502   \u2502   self.validate_stack(stack)                                    \u2502\r\n\u2502   488 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 489 \u2502   \u2502   return stack.deploy_pipeline(                                 \u2502\r\n\u2502   490 \u2502   \u2502   \u2502   self, runtime_configuration=runtime_configuration         \u2502\r\n\u2502   491 \u2502   \u2502   )                                                             \u2502\r\n\u2502   492                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\stack\\stack.py:595 in deploy_pipeline                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   pipeline=pipeline, runtime_configuration=runtime_configur \u2502\r\n\u2502   593 \u2502   \u2502   )                                                             \u2502\r\n\u2502   594 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 595 \u2502   \u2502   return_value = self.orchestrator.run(                         \u2502\r\n\u2502   596 \u2502   \u2502   \u2502   pipeline, stack=self, runtime_configuration=runtime_confi \u2502\r\n\u2502   597 \u2502   \u2502   )                                                             \u2502\r\n\u2502   598                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:212 in run                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   pipeline=pipeline, pb2_pipeline=pb2_pipeline              \u2502\r\n\u2502   210 \u2502   \u2502   )                                                             \u2502\r\n\u2502   211 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 212 \u2502   \u2502   result = self.prepare_or_run_pipeline(                        \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   sorted_steps=sorted_steps,                                \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   pipeline=pipeline,                                        \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                                \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\local\\local_orchestrator.py:68 in prepare_or_run_pipeline    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   65 \u2502   \u2502                                                                  \u2502\r\n\u2502   66 \u2502   \u2502   # Run each step                                                \u2502\r\n\u2502   67 \u2502   \u2502   for step in sorted_steps:                                      \u2502\r\n\u2502 > 68 \u2502   \u2502   \u2502   self.run_step(                                             \u2502\r\n\u2502   69 \u2502   \u2502   \u2502   \u2502   step=step,                                             \u2502\r\n\u2502   70 \u2502   \u2502   \u2502   \u2502   run_name=runtime_configuration.run_name,               \u2502\r\n\u2502   71 \u2502   \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:316 in run_step                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   313 \u2502   \u2502   # This is where the step actually gets executed using the     \u2502\r\n\u2502   314 \u2502   \u2502   # component_launcher                                          \u2502\r\n\u2502   315 \u2502   \u2502   repo.active_stack.prepare_step_run()                          \u2502\r\n\u2502 > 316 \u2502   \u2502   execution_info = self._execute_step(component_launcher)       \u2502\r\n\u2502   317 \u2502   \u2502   repo.active_stack.cleanup_step_run()                          \u2502\r\n\u2502   318 \u2502   \u2502                                                                 \u2502\r\n\u2502   319 \u2502   \u2502   return execution_info                                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:340 in _execute_step                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   337 \u2502   \u2502   start_time = time.time()                                      \u2502\r\n\u2502   338 \u2502   \u2502   logger.info(f\"Step `{pipeline_step_name}` has started.\")      \u2502\r\n\u2502   339 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 > 340 \u2502   \u2502   \u2502   execution_info = tfx_launcher.launch()                    \u2502\r\n\u2502   341 \u2502   \u2502   \u2502   if execution_info and get_cache_status(execution_info):   \u2502\r\n\u2502   342 \u2502   \u2502   \u2502   \u2502   logger.info(f\"Using cached version of `{pipeline_step \u2502\r\n\u2502   343 \u2502   \u2502   except RuntimeError as e:                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:528 in launch                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   525 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self._pipeline_runtime_spe \u2502\r\n\u2502   526 \u2502                                                                     \u2502\r\n\u2502   527 \u2502   # Runs as a normal node.                                          \u2502\r\n\u2502 > 528 \u2502   execution_preparation_result = self._prepare_execution()          \u2502\r\n\u2502   529 \u2502   (execution_info, contexts,                                        \u2502\r\n\u2502   530 \u2502    is_execution_needed) = (execution_preparation_result.execution_i \u2502\r\n\u2502   531 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    execution_preparation_result.contexts,   \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:388 in _prepare_execution                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   385 \u2502   \u2502   \u2502     output_dict=output_artifacts,                           \u2502\r\n\u2502   386 \u2502   \u2502   \u2502     exec_properties=exec_properties,                        \u2502\r\n\u2502   387 \u2502   \u2502   \u2502     execution_output_uri=(                                  \u2502\r\n\u2502 > 388 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_executor_output_uri(execu \u2502\r\n\u2502   389 \u2502   \u2502   \u2502     stateful_working_dir=(                                  \u2502\r\n\u2502   390 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_stateful_working_director \u2502\r\n\u2502   391 \u2502   \u2502   \u2502     tmp_dir=self._output_resolver.make_tmp_dir(execution.id \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\outputs_utils.py:172 in get_executor_output_uri       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   169 \u2502   \"\"\"Generates executor output uri given execution_id.\"\"\"           \u2502\r\n\u2502   170 \u2502   execution_dir = os.path.join(self._node_dir, _SYSTEM, _EXECUTOR_E \u2502\r\n\u2502   171 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    str(execution_id))                   \u2502\r\n\u2502 > 172 \u2502   fileio.makedirs(execution_dir)                                    \u2502\r\n\u2502   173 \u2502   return os.path.join(execution_dir, _EXECUTOR_OUTPUT_FILE)         \u2502\r\n\u2502   174                                                                       \u2502\r\n\u2502   175   def get_driver_output_uri(self) -> str:                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\d \u2502\r\n\u2502 sl\\io\\fileio.py:80 in makedirs                                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    77                                                                       \u2502\r\n\u2502    78 def makedirs(path: PathType) -> None:                                 \u2502\r\n\u2502    79   \"\"\"Make a directory at the given path, recursively creating parents \u2502\r\n\u2502 >  80   _get_filesystem(path).makedirs(path)                                \u2502\r\n\u2502    81                                                                       \u2502\r\n\u2502    82                                                                       \u2502\r\n\u2502    83 def mkdir(path: PathType) -> None:                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\integrations\\s3\\artifact_stores\\s3_artifact_store.py:275 in makedirs       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502   Args:                                                         \u2502\r\n\u2502   273 \u2502   \u2502   \u2502   path: The path to create.                                 \u2502\r\n\u2502   274 \u2502   \u2502   \"\"\"                                                           \u2502\r\n\u2502 > 275 \u2502   \u2502   self.filesystem.makedirs(path=path, exist_ok=True)            \u2502\r\n\u2502   276 \u2502                                                                     \u2502\r\n\u2502   277 \u2502   def mkdir(self, path: PathType) -> None:                          \u2502\r\n\u2502   278 \u2502   \u2502   \"\"\"Create a directory at the given path.                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:85 in wrapper                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    82 \u2502   @functools.wraps(func)                                            \u2502\r\n\u2502    83 \u2502   def wrapper(*args, **kwargs):                                     \u2502\r\n\u2502    84 \u2502   \u2502   self = obj or args[0]                                         \u2502\r\n\u2502 >  85 \u2502   \u2502   return sync(self.loop, func, *args, **kwargs)                 \u2502\r\n\u2502    86 \u2502                                                                     \u2502\r\n\u2502    87 \u2502   return wrapper                                                    \u2502\r\n\u2502    88                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:65 in sync                                                        \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    62 \u2502   \u2502   # suppress asyncio.TimeoutError, raise FSTimeoutError         \u2502\r\n\u2502    63 \u2502   \u2502   raise FSTimeoutError from return_result                       \u2502\r\n\u2502    64 \u2502   elif isinstance(return_result, BaseException):                    \u2502\r\n\u2502 >  65 \u2502   \u2502   raise return_result                                           \u2502\r\n\u2502    66 \u2502   else:                                                             \u2502\r\n\u2502    67 \u2502   \u2502   return return_result                                          \u2502\r\n\u2502    68                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:25 in _runner                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    22 \u2502   if timeout is not None:                                           \u2502\r\n\u2502    23 \u2502   \u2502   coro = asyncio.wait_for(coro, timeout=timeout)                \u2502\r\n\u2502    24 \u2502   try:                                                              \u2502\r\n\u2502 >  25 \u2502   \u2502   result[0] = await coro                                        \u2502\r\n\u2502    26 \u2502   except Exception as ex:                                           \u2502\r\n\u2502    27 \u2502   \u2502   result[0] = ex                                                \u2502\r\n\u2502    28 \u2502   finally:                                                          \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:767 in _makedirs                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    764 \u2502                                                                    \u2502\r\n\u2502    765 \u2502   async def _makedirs(self, path, exist_ok=False):                 \u2502\r\n\u2502    766 \u2502   \u2502   try:                                                         \u2502\r\n\u2502 >  767 \u2502   \u2502   \u2502   await self._mkdir(path, create_parents=True)             \u2502\r\n\u2502    768 \u2502   \u2502   except FileExistsError:                                      \u2502\r\n\u2502    769 \u2502   \u2502   \u2502   if exist_ok:                                             \u2502\r\n\u2502    770 \u2502   \u2502   \u2502   \u2502   pass                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:758 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502    756 \u2502   \u2502   \u2502   \u2502   raise translate_boto_error(e)                        \u2502\r\n\u2502    757 \u2502   \u2502   \u2502   except ParamValidationError as e:                        \u2502\r\n\u2502 >  758 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"Bucket create failed %r: %s\" % (bu \u2502\r\n\u2502    759 \u2502   \u2502   else:                                                        \u2502\r\n\u2502    760 \u2502   \u2502   \u2502   # raises if bucket doesn't exist and doesn't get create  \u2502\r\n\u2502    761 \u2502   \u2502   \u2502   await self._ls(bucket)                                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nValueError: Bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': Parameter \r\nvalidation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1657782683000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1657726485000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug in SWB 5.2.6 version of SageMaker Jupyter Notebook workspace where a study fails to mount. The error message indicates that the FUSE package failed to install during on-start. The user can resolve the issue by running \"sudo yum install fuse\" and then running \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json to mount the study.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.1,
        "Challenge_reading_time":154.35,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":246.0,
        "Challenge_repo_issue_count":1160.0,
        "Challenge_repo_star_count":2570.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":15.6105555556,
        "Challenge_title":"[BUG]: SageMaker + S3 artifact store fails trying to create a new bucket",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":829,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi @danguitavinas,\r\n\r\nI'm guessing from the stack trace that you're running on windows with the local orchestrator? If that's the case, my guess is that this issue should be fixed by #735.\r\n\r\nIf you're interested in trying this, you could install ZenML from that branch using the command `pip install git+https:\/\/github.com\/zenml-io\/zenml.git@bugfix\/windows-source-utils` @schustmi Thank you so much, that worked! Im closing the issue!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.41,
        "Solution_score_count":null,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1746825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello community,     <br \/>\nI'm facing a problem, my ACR in my resource group was deleted and I couldn't create any instance. I created again and now I can create instances but i'm having problems to run the dataset profile. It's failing to pull the image docker.    <\/p>\n<p>This is the output    <\/p>\n<pre><code>AzureMLCompute job failed.  \nFailedPullingImage: Unable to pull docker image  \n\timageName: 19acd0cdf57549bcace363c924cf045b.azurecr.io\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f  \n\terror: Run docker command to pull public image failed with error: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.  \n.  \n\tReason: Error response from daemon: Get https:\/\/19acd0cdf57549bcace363c924cf045b.azurecr.io\/v2\/azureml\/azureml_e7e3dfebc6129c75c60868383ebc992f\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.  \n  \n\tInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.  \n<\/code><\/pre>\n<p>The ML Studio has the following permissions on the ACR permissions    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/132698-unbenannt.png?platform=QnA\" alt=\"132698-unbenannt.png\" \/>    <\/p>\n<p>The docker image appears in the repositories of the ACR    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/132781-unbenannt2.png?platform=QnA\" alt=\"132781-unbenannt2.png\" \/>    <\/p>\n<p>Any hint how can i solve this problem?    <\/p>\n<p>Thanks in advance    <\/p>",
        "Challenge_closed_time":1631803071767,
        "Challenge_comment_count":0,
        "Challenge_created_time":1631798842910,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to pull a docker image from the Container Registry after recreating it due to deletion. The error message indicates an authentication issue and the ML Studio has the necessary permissions on the ACR. The user is seeking help to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/555024\/not-able-to-pull-docker-image-from-container-regis",
        "Challenge_link_count":6,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":23.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1.1746825,
        "Challenge_title":"Not able to pull docker image from Container Registry",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":175,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=5565f700-af3b-41a9-b47f-9b8a6276d8fd\">@Moresi, Marco  <\/a> Does this container registry have the admin account enabled? A requirement while creating a workspace with an <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace-cli?tabs=bringexistingresources1%2Cvnetpleconfigurationsv1cli#create-a-workspace\">existing container registry<\/a> is to have the admin account enabled.     <\/p>\n<p>If you have already enabled it then a <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace-cli?tabs=bringexistingresources1%2Cvnetpleconfigurationsv1cli#sync-keys-for-dependent-resources\">re-sync of keys<\/a> might be required for your workspace.    <\/p>\n<pre><code>az ml workspace sync-keys -w &lt;workspace-name&gt; -g &lt;resource-group-name&gt;  \n<\/code><\/pre>\n<p>Deleting the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace?tabs=python#deleting-the-azure-container-registry\">default container registry<\/a> used by the workspace can also cause the workspace to break.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":18.3,
        "Solution_reading_time":14.97,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1517548787092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Challenge_adjusted_solved_time":12.2421333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am a newbie in this, and I am facing some problems with the Azure ML workspace. I ran a python code from the terminal, and then I opened another terminal to check the process. I got the following message in the terminal that checked the process:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<p>I appreciate any tips.<\/p>",
        "Challenge_closed_time":1651825622423,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651781550743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Azure ML workspace while running a python code from the terminal. They opened another terminal to check the process and received a message stating \"Reconnecting terminal.\" The user is unsure of the meaning of the message and is concerned about losing processing time. They are seeking tips to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72133111",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":6.3,
        "Challenge_reading_time":8.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":12.2421333334,
        "Challenge_title":"Azure ML: What means reconnecting terminal?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575137776887,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":21.0,
        "Solution_body":"<blockquote>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<\/blockquote>\n<ul>\n<li><code>Reconnecting terminal<\/code> message can appear for multiple reasons like intermittent connectivity issues, unused active terminal sessions, processing of different size\/format of data.<\/li>\n<li>Make sure you close any unused terminal sessions to preserve your compute instance's resources. Idle terminals may impact the performance of compute instances.<\/li>\n<\/ul>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#manage-terminal-sessions\" rel=\"nofollow noreferrer\">Access a compute instance terminal in your workspace<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\" rel=\"nofollow noreferrer\">Optimize data processing with Azure Machine Learning<\/a> and <a href=\"https:\/\/www.youtube.com\/watch?v=kiScfw9i4FM\" rel=\"nofollow noreferrer\">Azure ML: Speed up processing time<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":14.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":114.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":0.085435,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm training a network using custom docker image. First training with 50.000 steps everythig was ok, when I tried to increase to 80.000, I got error: \"ClientError: Artifact upload failed:Insufficient disk space\", I just increased the steps number.. this is weird to me. There are no errors in the cloudwatch log, my last entry is: <\/p>\n\n<blockquote>\n  <p>Successfully generated graphs: ['pipeline.config', 'tflite_graph.pb',\n  'frozen_inference_graph.pb', 'tflite_graph.pbtxt',\n  'tflite_quant_graph.tflite', 'saved_model', 'hyperparameters.json',\n  'label_map.pbtxt', 'model.ckpt.data-00000-of-00001',\n  'model.ckpt.meta', 'model.ckpt.index', 'checkpoint']<\/p>\n<\/blockquote>\n\n<p>Which basically means that those files have been created because is a simple:<\/p>\n\n<pre><code>    graph_files = os.listdir(model_path + '\/graph')\n<\/code><\/pre>\n\n<p>Which disk space is talking about? Also looking at the training job I see from the disk utilization chart that the rising curve peaks at 80%...\nI expect that after the successful creation of the aforementioned files, everything is uploaded to my s3 bucket, where no disk space issues are present. Why 50.000 steps is working and 80.000 is not working? \nIt is my understanding that the number of training steps don't influence the size of the model files..<\/p>",
        "Challenge_closed_time":1588846223956,
        "Challenge_comment_count":0,
        "Challenge_created_time":1588756994270,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while training a network using a custom docker image on AWS Sagemaker. The error message \"ClientError: Artifact upload failed:Insufficient disk space\" appeared when the user tried to increase the number of training steps from 50,000 to 80,000. The user is confused about which disk space the error is referring to since there are no errors in the cloudwatch log and all files have been successfully created. The user is also unsure why the error occurred since the number of training steps should not affect the size of the model files.",
        "Challenge_last_edit_time":1588845916390,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61631687",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":17.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":24.7860238889,
        "Challenge_title":"AWS Sagemaker failure after successful training \"ClientError: Artifact upload failed:Insufficient disk space\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1744.0,
        "Challenge_word_count":181,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>Adding volume size to the training job selecting \"additional storage volume per instance (gb)\" to 5GB on the creation seems to solve the problem. I still don't understand why, but problem seems solved.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":2.59,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4417.9619444444,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Challenge_closed_time":1599677796000,
        "Challenge_comment_count":15,
        "Challenge_created_time":1583773133000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to install AWS stepfunctions using pip install in SageMaker Studio Notebook. The error message shows that the metadata generation failed due to an AttributeError, and the user received a CryptographyDeprecationWarning. The user expected to be able to install AWS stepfunctions, but the installation failed. The environment used was AWS Step Functions Data Science Python SDK version 2.3.0 and Python version 3.7.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Challenge_link_count":0,
        "Challenge_participation_count":15,
        "Challenge_readability":18.6,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4417.9619444444,
        "Challenge_title":"unable to kick off the sagemaker job",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":193,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@charlesa101  Thanks for trying out. I am assuming you have replaced input, output buckets and role Arn. \r\n\r\nWould you please run the following command provide the output ?\r\n\r\n```\r\nkubectl  get trainingjobs xgboost-mnist\r\nkubectl describe trainingjob xgboost-mnist\r\n``` @gautamkmr, here you go thank you! yeah i have my own bucket and sagemaker executor role\r\n\r\n```kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nxgboost-mnist                               2020-03-09T16:51:08Z ```\r\n\r\n```kubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400``` @charlesa101  Thanks for providing the output. It appears that operator is not running successfully on your k8s cluster.  you can verify that \r\n\r\n```\r\n kubectl get pods -A | grep -i sagemaker\r\n```\r\n\r\nYou can follow steps from [here](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#setup-and-operator-deployment) to install the operator, let us know if you face any issue. yeah that's what i noticed as well now\r\n\r\n```kubectl get pods -n sagemaker-k8s-operator-system\r\nNAME                                                         READY   STATUS    RESTARTS   AGE\r\nsagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8   0\/2     Pending   0          24h``` ```kubectl describe pod  -n sagemaker-k8s-operator-system                                             \r\nName:               sagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8\r\nNamespace:          sagemaker-k8s-operator-system\r\nPriority:           0\r\nPriorityClassName:  <none>\r\nNode:               <none>\r\nLabels:             control-plane=controller-manager\r\n                    pod-template-hash=5858fd7b8d\r\nAnnotations:        kubernetes.io\/psp: eks.privileged\r\nStatus:             Pending\r\nIP:                 \r\nControlled By:      ReplicaSet\/sagemaker-k8s-operator-controller-manager-5858fd7b8d\r\nContainers:\r\n  kube-rbac-proxy:\r\n    Image:      gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Port:       8443\/TCP\r\n    Host Port:  0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    Environment:\r\n      AWS_ROLE_ARN:                 arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\n  manager:\r\n    Image:      957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:  \r\n      AWS_ROLE_ARN:                    arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\nConditions:\r\n  Type           Status\r\n  PodScheduled   False \r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  sagemaker-k8s-operator-default-token-rwdkn:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  sagemaker-k8s-operator-default-token-rwdkn\r\n    Optional:    false\r\nQoS Class:       Burstable\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type     Reason            Age                   From               Message\r\n  ----     ------            ----                  ----               -------\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n my eks\/ecr is on us-east2, but it seems all the crd artifacts are coming from us-east1 could that be the issue?\r\n EKS can pull the image from other region too. I think in your case it seems that you don't have any worker node associated to cluster?  At least thats what below message says.\r\n```\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n```\r\n\r\nCan you run ?  \r\n```\r\nkubectl get node\r\n``` @charlesa101  did you get chance to review it again? ``` kubectl get nodes\r\nNAME                                           STATUS   ROLES    AGE     VERSION\r\nip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n yeah i did, recreated the cluster again but still the same issue\r\n @charlesa101   In previous describe output of `pod` it appears that cluster did not have any worker nodes available `(no nodes available to schedule pods)`.\r\n\r\nBut based on recent output it appears that you have three worker nodes available. \r\n\r\n> NAME                                           STATUS   ROLES    AGE     VERSION\r\n> ip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n\r\n\r\nCould you please describe each of these nodes and operator pod ?\r\n\r\n```\r\n# Describe nodes , assuming the names of nodes are same as you mentioned in previous comment.\r\nkubectl describe node ip-172-16-116-51.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-121-255.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-137-197.us-east-2.compute.internal \r\n```\r\n\r\n\r\n```\r\n#Get the operator pod name \r\nkubectl get pods -A | grep -i sagemaker\r\nkubectl describe pod <put the pod name here>  -n sagemaker-k8s-operator-system\r\n```\r\n\r\n\r\nIf operator has been deployed successfully and if trainingjob is still not yet running please attach the out put of describe trainingjob as well ? \r\n```\r\nkubectl describe trainingjob xgboost-mnist\r\n\r\n```\r\n\r\n i tried to look checked the operator pod, here is  the log @gautamkmr \r\n\r\n```\r\nkubectl logs -f sagemaker-k8s-operator-controller-manager-5858fd7b8d-2dk5c  -n sagemaker-k8s-operator-system manager\r\n2020-03-15T18:09:13.864Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    setup   starting manager\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"trainingjob\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"model\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"batchtransformjob\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hostingdeployment\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"endpointconfig\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hyperparametertuningjob\"}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"trainingjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"model\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Job status is empty, setting to intermediate status     {\"trainingjob\": \"default\/xgboost-mnist\", \"status\": \"SynchronizingK8sJobWithSageMaker\"}\r\n2020-03-15T19:09:19.963Z        INFO    controllers.TrainingJob Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"new-status\": {\"trainingJobStatus\":\"SynchronizingK8sJobWithSageMaker\",\"lastCheckTime\":\"2020-03-15T19:09:19Z\"}}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Adding generated name to spec   {\"trainingjob\": \"default\/xgboost-mnist\", \"new-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}\r\n2020-03-15T19:09:19.982Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:20.916Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:09:20.916Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\",\"lastCheckTime\":\"2020-03-15T19:09:20Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:09:20.924Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:42.150Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:11:42.150Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\",\"lastCheckTime\":\"2020-03-15T19:11:42Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:11:42.159Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n```\r\n @charlesa101  Thanks for sharing the log. You are on right track. I think the issue now is operator pod is unable to retrieve credentials from IAM service to talk to sagemaker. \r\n\r\n`\"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n`\r\n\r\nCould you please check your [trust.json](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#create-an-iam-role) basically **trust policy have three places to update cluster region and OIDC ID and one place to add your AWS account number.** Hi @charlesa101\r\n\r\nClosing this issue since there has been no activity in 90 days. Please re-open if you still need help\r\n\r\nThanks Hi, I'm having the exact same issue except that my pod is running fine. I setup my k8s cluster using terraform with 1 master node and 1 worker node. When I submit the trainingjob, there is no status or job name or anything else. I tried all the commands above and it looks like the scheduler was able to assign the pods to the worker node. Any help would be appreciated! Please see outputs for commands below:\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get pods -A                                                                                                                                                                                                                                                    \r\nNAMESPACE        NAME                                                         READY   STATUS    RESTARTS   AGE                                                                                                                                                                                                                \r\nkube-system      aws-node-67tgx                                               1\/1     Running   0          2d18h\r\nkube-system      aws-node-k2q7z                                               1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-cwfvj                                     1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-x5ld9                                     1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-54vm5                                             1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-r8j7j                                             1\/1     Running   0          2d18h\r\nkube-system      metrics-server-64cf6869bd-6nppx                              1\/1     Running   0          2d18h\r\nsagemaker-jobs   sagemaker-k8s-operator-controller-manager-855f498957-fhkvv   2\/2     Running   0          2d18h\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe pod sagemaker-k8s-operator-controller-manager-855f498957-fhkvv -n sagemaker-jobs\r\nName:         sagemaker-k8s-operator-controller-manager-855f498957-fhkvv\r\nNamespace:    sagemaker-jobs\r\nPriority:     0\r\nNode:         ip-10-0-1-245.us-west-2.compute.internal\/10.0.1.245\r\nStart Time:   Fri, 24 Jun 2022 22:26:03 +0000\r\nLabels:       control-plane=controller-manager\r\n              pod-template-hash=855f498957\r\nAnnotations:  kubernetes.io\/psp: eks.privileged\r\nStatus:       Running\r\nIP:           10.0.1.144\r\nIPs:\r\n  IP:           10.0.1.144\r\nControlled By:  ReplicaSet\/sagemaker-k8s-operator-controller-manager-855f498957\r\nContainers:\r\n  manager:\r\n    Container ID:  docker:\/\/d8fc52b3e20a050999d3f24ab914f1d865a84a168a8b038f3fa81ce59cccbced\r\n    Image:         957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Image ID:      docker-pullable:\/\/957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s@sha256:94ffbba68954249b1724fdb43f1e8ab13547114555b4a217849687d566191e23\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n      --namespace=sagemaker-jobs\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:09 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:\r\n      AWS_DEFAULT_REGION:              us-west-2\r\n      AWS_REGION:                      us-west-2\r\n      AWS_ROLE_ARN:                    arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nkube-rbac-proxy:\r\n    Container ID:  docker:\/\/4ecdaa395fdc70d5cead609465dbf21f6e11771a80ad5db0a6125053ab08b9d3\r\n    Image:         gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Image ID:      docker-pullable:\/\/gcr.io\/kubebuilder\/kube-rbac-proxy@sha256:297896d96b827bbcb1abd696da1b2d81cab88359ac34cce0e8281f266b4e08de\r\n    Port:          8443\/TCP\r\n    Host Port:     0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:11 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:\r\n      AWS_DEFAULT_REGION:           us-west-2\r\n      AWS_REGION:                   us-west-2\r\n      AWS_ROLE_ARN:                 arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             True\r\n  ContainersReady   True\r\n  PodScheduled      True\r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  kube-api-access-6j8rt:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io\/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io\/unreachable:NoExecute op=Exists for 300s\r\nEvents:                      <none>\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl logs sagemaker-k8s-operator-controller-manager-855f498957-fhkvv manager -n sagemaker-jobs\r\nI0624 22:26:11.339445       1 request.go:621] Throttling request took 1.046981399s, request: GET:https:\/\/172.20.0.1:443\/apis\/extensions\/v1beta1?timeout=32s\r\n2022-06-24T22:26:12.443Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2022-06-24T22:26:12.443Z        INFO    Starting manager in the namespace:      sagemaker-jobs\r\n2022-06-24T22:26:12.443Z        INFO    setup   starting manager\r\n2022-06-24T22:26:12.444Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.665Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\"}\r\n2022-06-24T22:26:12.746Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\"}\r\n2022-06-24T22:26:12.747Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nosic-test-run                               2022-06-24T22:38:13Z  \r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe trainingjob osic-test-run                                                                                                                                                                                                                             \r\nName:         osic-test-run                                                                                                                                                                                                                                                                                                   \r\nNamespace:    default                                                                                                                                                                                                                                                                                                         \r\nLabels:       <none>                                                                                                                                                                                                                                                                                                          \r\nAnnotations:  <none>                                                                                                                                                                                                                                                                                                          \r\nAPI Version:  sagemaker.aws.amazon.com\/v1                                                                                                                                                                                                                                                                                     \r\nKind:         TrainingJob                                                                                                                                                                                                                                                                                                     \r\nMetadata:                                                                                                                                                                                                                                                                                                                     \r\n  Creation Timestamp:  2022-06-24T22:38:13Z                                                                                                                                                                                                                                                                                   \r\n  Generation:          1                                                                                                                                                                                                                                                                                                      \r\n  Managed Fields:\r\n    API Version:  sagemaker.aws.amazon.com\/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:kubectl.kubernetes.io\/last-applied-configuration:\r\n      f:spec:\r\n        .:\r\n        f:algorithmSpecification:\r\n          .:\r\n          f:trainingImage:\r\n          f:trainingInputMode:\r\n        f:inputDataConfig:\r\n        f:outputDataConfig:\r\n          .:\r\n          f:s3OutputPath:\r\n        f:region:\r\n        f:resourceConfig:\r\n          .:\r\n          f:instanceCount:\r\n          f:instanceType:\r\n          f:volumeSizeInGB:\r\n        f:roleArn:\r\n        f:stoppingCondition:\r\n          .:\r\n          f:maxRuntimeInSeconds:\r\n        f:trainingJobName:\r\n    Manager:         kubectl-client-side-apply\r\n    Operation:       Update\r\n    Time:            2022-06-24T22:38:13Z\r\n  Resource Version:  3182\r\n  UID:               0a0880c0-baf9-4f1a-8aa3-37480520c3e2\r\nSpec:\r\n  Algorithm Specification:\r\nTraining Image:       438029713005.dkr.ecr.us-west-2.amazonaws.com\/model-training:latest\r\n    Training Input Mode:  File\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Data Source:\r\n      s3DataSource:\r\n        s3DataDistributionType:  FullyReplicated\r\n        s3DataType:              S3Prefix\r\n        s3Uri:                   s3:\/\/osic-full-including-override\r\n  Output Data Config:\r\n    s3OutputPath:  s3:\/\/osic-full-including-override\/experiments\r\n  Region:          us-west-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.p3.2xlarge\r\n    Volume Size In GB:  500\r\n  Role Arn:             arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  900\r\n  Training Job Name:         osic-test-run\r\nEvents:                      <none>\r\n```\r\n\r\nplease let me know if you need to see anything else!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":18.8,
        "Solution_reading_time":403.18,
        "Solution_score_count":null,
        "Solution_sentence_count":227.0,
        "Solution_word_count":2188.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":191.5183333333,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nIdle Sagemaker Notebook instances do not stop after specified time.\r\n\r\nSWB runs autostop.py script to automatically stop Sagemaker Notebook instance. The script is used by `on-start` lifecycle rule of the instance CFN template. According to LifecycleConfigOnStart logs, some packages are missing and autostop script doesn\u2019t work.\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Make sure AutoStopIdleTimeInMinutes parameter in workspace type config is set to a required time (30 minutes in our case)\r\n2. Create a new workspace with Sagemaker notebook instance\r\n3. Leave the instance idle for the time specified (AutoStopIdleTimeInMinutes )\r\n4. After the specified time see that the instance is not stopped\r\n\r\n**Expected behavior**\r\nIdle Sagemaker Notebook instance automatically stops after specified time.\r\n\r\n**Screenshots**\r\n<img width=\"1308\" alt=\"Screen Shot 2022-12-07 at 10 43 09 am\" src=\"https:\/\/user-images.githubusercontent.com\/47466926\/206049662-5ff12457-8bd4-42bd-b12f-ce68fdfacaf6.png\">\r\n\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed v5.0.0\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671059684000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1670370218000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue where the study folders are not mounted when launching a Sagemaker notebook with an associated study. Upon investigation, the user found that there is no credentials file in the `~\/.aws` folder, which is generated by the `mount_s3.sh` script. The expected behavior is for the study folders to be mounted using the assumed roles in the AWS credentials file. This issue may or may not be associated with another bug the user noted with mounting s3 studies folders.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":10.0,
        "Challenge_reading_time":15.67,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":191.5183333333,
        "Challenge_title":"[Bug] Sagemaker instance does not stop automatically",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":156,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you. We are aware of this issue and have a backlog item to resolve this! See https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065 for more information. Hi, please check the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5) for the fix to this issue.  Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! Hi Marianna,\nThank you. I am going to migrate to v5.2.5 tomorrow. If everything goes\nwell, I'll close the ticket soon.\n\n\nOn Tue, Dec 13, 2022 at 7:55 AM Marianna Ghirardelli <\n***@***.***> wrote:\n\n> Hi! I also want to note that you may need to stop and start any affected\n> instances after upgrade and deploying SWB v5.2.5.\n>\n> If this fixes your issue, please go ahead and close this issue. I am going\n> to mark as closing-soon-if-no-response so we will close in about 7 days if\n> we do not hear that this did not resolve the issue.\n>\n> Thank you for the report!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076#issuecomment-1347314897>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ALKETLSEQCFIMAF5HX4CAT3WM6GN3ANCNFSM6AAAAAASWEAJP4>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n Closing this issue since the fix was merged, please feel free to reopen the issue if it persists on your end.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":6.9,
        "Solution_reading_time":21.01,
        "Solution_score_count":null,
        "Solution_sentence_count":20.0,
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1600461527240,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1051.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":7.3123536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am planning to start a Jupyter Notebook instance and execute each notebook file in AWS Sagemaker using AWS Step Functions. Can this be achieved?<\/p>",
        "Challenge_closed_time":1606148997440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606118086040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if it is possible to start and execute a Jupyter Notebook instance in AWS Sagemaker using AWS Step Functions.",
        "Challenge_last_edit_time":1606122672967,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64964435",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":2.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":8.5865,
        "Challenge_title":"Can you start and execute a Jupyter Notebook in Sagemaker using Step Functions?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":671.0,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Poster_created_time":1461162996723,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":882.0,
        "Poster_view_count":97.0,
        "Solution_body":"<p>The AWS Step Functions Data Science SDK is an open source library that allows data scientists to easily create workflows that process and publish machine learning models using SageMaker and Step Functions.<\/p>\n<p>The following <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-nbexamples.html\" rel=\"nofollow noreferrer\">Example notebooks<\/a>, which are available in Jupyter notebook instances in the <a href=\"https:\/\/console.aws.amazon.com\/sagemaker\/\" rel=\"nofollow noreferrer\">SageMaker console<\/a> and the related <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/step-functions-data-science-sdk\" rel=\"nofollow noreferrer\">GitHub project<\/a>:<\/p>\n<ul>\n<li>hello_world_workflow.ipynb<\/li>\n<li>machine_learning_workflow_abalone.ipynb<\/li>\n<li>training_pipeline_pytorch_mnist.ipynb<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":21.4,
        "Solution_reading_time":11.33,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1434.0155555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of the bug. -->\r\n\r\nWhen using `PyTorchLightningPruningCallback` to search best hyperparams, it reports `AttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'`\r\n\r\n### To Reproduce\r\n\r\n```python\r\nfrom typing import List, Optional\r\n\r\nimport optuna\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn as nn\r\nimport torchmetrics\r\nimport torchvision\r\nfrom optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback\r\nfrom torch.utils.data import random_split, DataLoader\r\n\r\n\r\nclass FashionDataModule(pl.LightningDataModule):\r\n    def __init__(self, data_dir: str, batch_size: int):\r\n        super().__init__()\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n        self.train_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=True, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.test_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=False, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.train_set, self.valid_set = random_split(self.train_set, [55000, 5000])\r\n\r\n    def train_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=4)\r\n\r\n    def val_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.valid_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n    def test_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super(SimpleNet, self).__init__()\r\n\r\n        hidden_layers = []\r\n        d_inp = 28 * 28\r\n        for d_hid in d_hids:\r\n            hidden_layers.append(nn.Linear(d_inp, d_hid))\r\n            hidden_layers.append(nn.ReLU())\r\n            hidden_layers.append(nn.Dropout(p_drop))\r\n            d_inp = d_hid\r\n        hidden_layers.append(nn.Linear(d_inp, 10))\r\n\r\n        self.layers = nn.Sequential(*hidden_layers)\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.layers(inputs)\r\n\r\n\r\nclass LitSimpleNet(pl.LightningModule):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super().__init__()\r\n        self.model = SimpleNet(d_hids, p_drop)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        self.accuracy = torchmetrics.Accuracy()\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.model(inputs.view(-1, 28 * 28))\r\n\r\n    def training_step(self, batch, batch_idx) -> torch.Tensor:\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        return self.criterion(outputs, targets)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        self.accuracy(outputs, targets)\r\n        self.log(\"valid_acc\", self.accuracy, on_step=False, on_epoch=True, prog_bar=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=3e-4, weight_decay=1e-5)\r\n\r\n\r\ndef objective(trial: optuna.trial.Trial) -> float:\r\n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\r\n    p_drop = trial.suggest_float(\"p_drop\", 0.1, 0.5)\r\n    d_hids = [trial.suggest_int(f\"d_hid_{i}\", 16, 128, log=True) for i in range(n_layers)]\r\n\r\n    datamodule = FashionDataModule(\".\", 128)\r\n    model = LitSimpleNet(d_hids, p_drop)\r\n    trainer = pl.Trainer(\r\n        max_epochs=20,\r\n        accelerator=\"gpu\",\r\n        devices=1,\r\n        enable_checkpointing=False,\r\n        logger=True,\r\n        default_root_dir=\".\",\r\n        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"valid_acc\")]\r\n    )\r\n\r\n    hparams = dict(n_layers=n_layers, d_hids=d_hids, p_drop=p_drop)\r\n    trainer.logger.log_hyperparams(hparams)\r\n    trainer.fit(model, datamodule=datamodule)\r\n    return trainer.callback_metrics[\"valid_acc\"].item()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    pruner = optuna.pruners.MedianPruner()\r\n    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\r\n    study.optimize(objective, n_trials=100, timeout=1000)\r\n\r\n    print(\"Number of Finished Trials:\", len(study.trials))\r\n\r\n    trial = study.best_trial\r\n    print(\"Best Trial:\")\r\n    print(\"\\tValue:\", trial.value)\r\n    print(\"\\tParams:\")\r\n    for key, value in trial.params.items():\r\n        print(f\"\\t\\t{key}: {value}\")\r\n\r\n```\r\n\r\n```bash\r\n[W 2022-09-08 20:14:45,294] Trial 0 failed because of the following error: AttributeError(\"'AcceleratorConnector' object has no attribute 'distributed_backend'\")\r\nTraceback (most recent call last):\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/study\/_optimize.py\", line 196, in _run_trial\r\n    value_or_values = func(trial)\r\n  File \"optuna_examples\/optuna_lightning_example.py\", line 89, in objective\r\n    trainer = pl.Trainer(\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/argparse.py\", line 345, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 497, in __init__\r\n    self._call_callback_hooks(\"on_init_start\")\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1585, in _call_callback_hooks\r\n    fn(self, *args, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/integration\/pytorch_lightning.py\", line 61, in on_init_start\r\n    trainer._accelerator_connector.distributed_backend is not None  # type: ignore\r\nAttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'\r\n```\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/github\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.ipynb\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n\r\n### Expected behavior\r\n\r\nShould not report any errors.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n\r\n```\r\n\r\n\r\n<details>\r\n  <summary>Details<\/summary>\r\n    Paste the output here and move this toggle outside of the comment block.\r\n<\/details>\r\n\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- Lightning Component:  Trainer\r\n- PyTorch Lightning Version:  1.7.5\r\n- PyTorch Version:  1.12.1\r\n- Python version: 3.8.13\r\n- OS: Linux (Ubuntu 20.04)\r\n- CUDA\/cuDNN version: 11.3.1\r\n- How you installed PyTorch: conda\r\n\r\n\n\ncc @akihironitta",
        "Challenge_closed_time":1667802669000,
        "Challenge_comment_count":9,
        "Challenge_created_time":1662640213000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has created a custom docker container to deploy a model on Vertex AI that uses LightGBM. While the user is able to get predictions, they encounter errors while trying to get explainable predictions from the model. The user has followed the Vertex AI guidelines to configure the model for explanations, but the issue persists. The error message suggests that the response field 'predictions' is missing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/14604",
        "Challenge_link_count":4,
        "Challenge_participation_count":9,
        "Challenge_readability":17.8,
        "Challenge_reading_time":88.49,
        "Challenge_repo_contributor_count":447.0,
        "Challenge_repo_fork_count":2788.0,
        "Challenge_repo_issue_count":14589.0,
        "Challenge_repo_star_count":22027.0,
        "Challenge_repo_watch_count":231.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":75,
        "Challenge_solved_time":1434.0155555556,
        "Challenge_title":"Optuna integration reports AttributeError",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":522,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey, @RegiusQuant. \r\n\r\nSide answer, you might be interested by Lightning HPO: https:\/\/github.com\/Lightning-AI\/lightning-hpo. This enables to run Optuna with PyTorch Lightning without friction and scalable in the cloud.\r\n\r\n Hey, @RegiusQuant - Thanks for the question. Can you please point me to the version of `optuna` that you are using?  For reference: https:\/\/github.com\/optuna\/optuna\/issues\/3978 @krshrimali Optuna version\uff1a3.0.0 I'm observing the same issue with Optuna 3.0.2 @hrzn Hi, I'm from the Optuna-dev team. Optuna's pytorch-lightning (PL) integration module doesn't support PL>=1.6 because it broke backwards-compatibility as investigated in https:\/\/github.com\/optuna\/optuna\/issues\/3418. Unfortunately, Optuna team doesn't have time to fix the module soon to support recent PL; we would like to wait for a PR from optuna and PL users.\r\n\r\n@tchaton I believe you can close this issue because the issue comes from Optuna... With Optuna==3.0.2 with lightning==1.5.10, I got \r\n`ValueError: optuna.integration.PyTorchLightningPruningCallback supports only optuna.storages.RDBStorage in DDP.`\r\nAfter downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.  @mikiotada Again, the error does not relate to PL. As the error message said, Optuna's integration does not support DDP without RDBStorage. \r\n\r\n> After downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.\r\n\r\nIn my understanding, Optuna 2.x didn't officially support DDP; it does not work as you expected, I'm afraid even though there was no error. Closing this issue as there seems nothing we can address from our side. Please refer to https:\/\/github.com\/optuna\/optuna\/issues\/3418.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":8.9,
        "Solution_reading_time":21.93,
        "Solution_score_count":null,
        "Solution_sentence_count":28.0,
        "Solution_word_count":232.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":1035.8951980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Challenge_closed_time":1594137982056,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590408759343,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has successfully created a training job on Sagemaker using the Tensorflow Object Detection API in a docker container. However, the user is facing challenges in monitoring the training job using Sagemaker and is unable to find any resources explaining how to do it. The user is considering saving the logs into an S3 bucket and pointing to a local tensorboard instance, but is unsure how to tell the Tensorflow Object Detection API where to save the logs. The user has tried using a script, but it failed because the training job did not have the required parameter. The user has found a solution that involves disabling network isolation in the training job configuration and providing credentials to the docker image to write to the S3 bucket. However, the user has",
        "Challenge_last_edit_time":1606323198012,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":27.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1035.8951980556,
        "Challenge_title":"Use tensorboard with object detection API in sagemaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":311.0,
        "Challenge_word_count":286,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416346350292,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation_count":2302.0,
        "Poster_view_count":227.0,
        "Solution_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.2,
        "Solution_reading_time":20.17,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":163.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1659144422092,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":166.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":1.1739147222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>l'am running a ipynb file on sagemaker, however the error of occurs.\nl have used 'pip install tqdm' in terminals to install the tqdm so l've no idea what's happening. Is it running in a different environment?\nThanks for any answer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bO6zS.png\" rel=\"nofollow noreferrer\">error report from my ipynb file <\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5tP1J.png\" rel=\"nofollow noreferrer\">what l've done in terminal<\/a><\/p>",
        "Challenge_closed_time":1659155782776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659151516027,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to import a certain module while running an ipynb file on Sagemaker. They have tried installing the module using 'pip install' in the terminal but are still encountering an error. They are unsure if the file is running in a different environment.",
        "Challenge_last_edit_time":1659151556683,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73172644",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":6.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.1852080556,
        "Challenge_title":"Running ipynb file, but can't import a certain module",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Poster_created_time":1658300099523,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>There is a possibility you may be executing &quot;pip&quot; in a different environment.<\/p>\n<p>Try executing &quot;!pip install tqdm&quot; or &quot;!pip3 install tqdm&quot; as a code cell in the Sagemaker document itself.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":2.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1526004205792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"China",
        "Answerer_reputation_count":28087.0,
        "Answerer_view_count":3298.0,
        "Challenge_adjusted_solved_time":64.41573,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an AKS cluster in a VNET\/Subnet. My AKS is linked to AzureML.<\/p>\n<p>I successfully deployed an Azure ML service to that AKS.<\/p>\n<p>However, I see that the azureml-fe service is responding to a public IP and not a private IP from my VNET\/Subnet.<\/p>\n<p>How can I make it so my AzureML inference service is exposed with a private IP?<\/p>",
        "Challenge_closed_time":1615790277368,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615558380740,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has an AKS cluster linked to AzureML and has successfully deployed an Azure ML service to it. However, the AzureML inference service is responding to a public IP instead of a private IP from the user's VNET\/Subnet. The user is seeking a solution to expose the AzureML inference service with a private IP.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66601526",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.1,
        "Challenge_reading_time":4.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":64.41573,
        "Challenge_title":"Azure ML Kubernetes - Private IP",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":232.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1274212839807,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":591.0,
        "Poster_view_count":75.0,
        "Solution_body":"<p>Maybe you need to use an internal load balancer, then it will use a private IP address. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-kubernetes?tabs=python#create-or-attach-an-aks-cluster-to-use-internal-load-balancer-with-private-ip\" rel=\"nofollow noreferrer\">Here<\/a> is the example code for Python:<\/p>\n<pre><code>from azureml.core.compute.aks import AksUpdateConfiguration\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# When you create an AKS cluster, you can specify Internal Load Balancer to be created with provisioning_config object\nprovisioning_config = AksCompute.provisioning_configuration(load_balancer_type = 'InternalLoadBalancer')\n\n# when you attach an AKS cluster, you can update the cluster to use internal load balancer after attach\naks_target = AksCompute(ws,&quot;myaks&quot;)\n\n# Change to the name of the subnet that contains AKS\nsubnet_name = &quot;default&quot;\n# Update AKS configuration to use an internal load balancer\nupdate_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)\naks_target.update(update_config)\n# Wait for the operation to complete\naks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":20.2,
        "Solution_reading_time":16.23,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":112.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1259808393296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":44706.0,
        "Answerer_view_count":4356.0,
        "Challenge_adjusted_solved_time":1835.4266388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using SageMaker for distributed TensorFlow model training and serving.  I am trying to get the shape of the pre-processed datasets from the ScriptProcessor so I can provide it to the TensorFlow Environment.<\/p>\n<pre><code>script_processor = ScriptProcessor(command=['python3'],\n                image_uri=preprocess_img_uri,\n                role=role,\n                instance_count=1,\n                sagemaker_session=sm_session,\n                instance_type=preprocess_instance_type)\n\nscript_processor.run(code=preprocess_script_uri,\n                inputs=[ProcessingInput(\n                        source=source_dir + username + '\/' + dataset_name,\n                        destination='\/opt\/ml\/processing\/input')],\n                outputs=[\n                        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n                        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;)\n                ],\n\n                arguments = ['--filepath', dataset_name, '--labels', 'labels', '--test_size', '0.2', '--shuffle', 'False', '--lookback', '5',])\n\npreprocessing_job_description = script_processor.jobs[-1].describe()\n\noutput_config = preprocessing_job_description[&quot;ProcessingOutputConfig&quot;]\nfor output in output_config[&quot;Outputs&quot;]:\n    if output[&quot;OutputName&quot;] == &quot;train_data&quot;:\n        preprocessed_training_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]\n    if output[&quot;OutputName&quot;] == &quot;test_data&quot;:\n        preprocessed_test_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]\n<\/code><\/pre>\n<p>I would like to get the following data:<\/p>\n<pre><code>pre_processed_train_data_shape = script_processor.train_data_shape?\n<\/code><\/pre>\n<p>I am just not sure how to get the value out of the docker container.  I have reviewed the documentation here:<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>",
        "Challenge_closed_time":1647621141128,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647581865580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using SageMaker for distributed TensorFlow model training and serving. They are trying to get the shape of the pre-processed datasets from the ScriptProcessor so they can provide it to the TensorFlow Environment, but they are not sure how to get the value out of the docker container. They have reviewed the documentation but are still unsure.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71522857",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":23.4,
        "Challenge_reading_time":25.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":10.9098744444,
        "Challenge_title":"Get Variable from SageMaker Script Processor",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":334.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1478923885800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>There are a few options:<\/p>\n<ol>\n<li><p>Write some data to a text file at <code>\/opt\/ml\/output\/message<\/code>, then call <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/describe-processing-job.html\" rel=\"nofollow noreferrer\">DescribeProcessingJob<\/a> (using Boto3 or the AWS CLI or API) and retrieve the <code>ExitMessage<\/code> value<\/p>\n<pre><code>aws sagemaker describe-processing-job \\\n  --processing-job-name foo \\\n  --output text \\\n  --query ExitMessage\n<\/code><\/pre>\n<\/li>\n<li><p>Add a new output to your processing job and send data there<\/p>\n<\/li>\n<li><p>If your <code>train_data<\/code> is in CSV, JSON, or Parquet then use an <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/s3api\/select-object-content.html\" rel=\"nofollow noreferrer\">S3 Select query<\/a> on <code>train_data<\/code> for it's # of rows\/columns<\/p>\n<pre><code>aws s3api select-object-content \\\n  --bucket foo \\\n  --key 'path\/to\/train_data.csv' \\\n  --expression &quot;SELECT count(*) FROM s3object&quot; \\\n  --expression-type 'SQL' \\\n  --input-serialization '{&quot;CSV&quot;: {}}' \\\n  --output-serialization '{&quot;CSV&quot;: {}}' \/dev\/stdout\n<\/code><\/pre>\n<\/li>\n<\/ol>\n<p>Set <code>expression<\/code> to <code>select * from s3object limit 1<\/code> to get the columns<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1654189401480,
        "Solution_link_count":2.0,
        "Solution_readability":21.7,
        "Solution_reading_time":16.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":116.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":63.7779516667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Challenge_closed_time":1547708377156,
        "Challenge_comment_count":1,
        "Challenge_created_time":1547478776530,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with AWS Sagemaker notebook where the package Scikit-Learn version is not updating to the desired version. The user tried using lifecycle configurations to update the package but it did not work. The correct version of the package is found in the terminal but not in the notebook. The user is seeking help to fix the notebook lifecycle configuration and wondering if there is any virtual environment on the SageMaker instances where the package should be installed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":63.7779516667,
        "Challenge_title":"AWS Sagemaker does not update the package",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1546.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527781503483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Metz, France",
        "Poster_reputation_count":352.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":2.01,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":25.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1405882600928,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":552.0,
        "Answerer_view_count":115.0,
        "Challenge_adjusted_solved_time":0.3938825,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an existing project, cloned with <code>git clone<\/code>.<\/p>\n\n<p>After I <code>pip install kedro<\/code> I can run <code>kedro info<\/code> fine but I dont seem to have access to the projects CLI for example if I try to run<code>kedro install<\/code> I get the following error:<\/p>\n\n<pre><code>Usage: kedro [OPTIONS] COMMAND [ARGS]...\nTry 'kedro -h' for help.\n\nError: No such command 'install'.\n<\/code><\/pre>\n\n<p>Any clues on what to do for existing projects are much appreciated.<\/p>\n\n<p>Not sure if this matters but I am working inside a conda environment which is inside a docker container.<\/p>",
        "Challenge_closed_time":1589565622150,
        "Challenge_comment_count":2,
        "Challenge_created_time":1589564204173,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has an existing project cloned with git and has installed Kedro using pip. While the user can run \"kedro info\" without any issues, they are unable to access the project's CLI and receive an error message when attempting to run \"kedro install\". The user is seeking guidance on how to access the CLI for existing projects. The user is working within a conda environment inside a docker container.",
        "Challenge_last_edit_time":1589721006283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61825202",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.5,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.3938825,
        "Challenge_title":"Accessing Kedro CLI from an existing project",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1175.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1387377887347,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Edinburgh, United Kingdom",
        "Poster_reputation_count":1240.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>Project CLIs are available if you run <code>kedro<\/code> at your Kedro project directory. <\/p>\n\n<ol>\n<li><p>Run <code>kedro new<\/code> to create a Kedro project<\/p><\/li>\n<li><p><code>cd &lt;your-kedro-project&gt;<\/code><\/p><\/li>\n<li><p><code>kedro<\/code> at the project directory<\/p><\/li>\n<\/ol>\n\n<p>And you should see the project level CLIs<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9NnAN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Also for your existing project, check if you have <code>kedro_cli.py<\/code> at your project directory.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":8.23,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3914.1044444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello, receiving the following error in an Azure Notebook VM while trying to import the ML library - \r\n\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\n# error here!!!\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b8d543bb7111> in <module>\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n----> 5 from azureml.train.automl import AutoMLConfig\r\n      6 from sklearn.externals import joblib\r\n      7 from azureml.core.model import Model\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/__init__.py in <module>\r\n     23     # Suppress the warnings at the import phase.\r\n     24     warnings.simplefilter(\"ignore\")\r\n---> 25     from ._automl import fit_pipeline\r\n     26     from .automlconfig import AutoMLConfig\r\n     27     from .automl_step import AutoMLStep, AutoMLStepRun\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_automl.py in <module>\r\n     17 from automl.client.core.runtime.cache_store import CacheStore\r\n     18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities\r\n---> 19 from azureml.automl.core import data_transformation, fit_pipeline as fit_pipeline_helper\r\n     20 from azureml.automl.core.automl_pipeline import AutoMLPipeline\r\n     21 from azureml.automl.core.data_context import RawDataContext, TransformedDataContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/fit_pipeline.py in <module>\r\n     18 from automl.client.core.common.limit_function_call_exceptions import TimeoutException\r\n     19 from automl.client.core.runtime.datasets import DatasetBase\r\n---> 20 from . import package_utilities, pipeline_run_helper, training_utilities\r\n     21 from .automl_base_settings import AutoMLBaseSettings\r\n     22 from .automl_pipeline import AutoMLPipeline\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/pipeline_run_helper.py in <module>\r\n     18 from automl.client.core.common.exceptions import ClientException\r\n     19 from automl.client.core.runtime import metrics\r\n---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module\r\n     21 from automl.client.core.runtime.datasets import DatasetBase\r\n     22 from automl.client.core.runtime.execution_context import ExecutionContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in <module>\r\n     21 \r\n     22 from automl.client.core.common import constants\r\n---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers\r\n     24 from automl.client.core.runtime.nimbus_wrappers import AveragedPerceptronBinaryClassifier, \\\r\n     25     AveragedPerceptronMulticlassClassifier, NimbusMlClassifierMixin, NimbusMlRegressorMixin\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in <module>\r\n     34 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n     35 if tf_found:\r\n---> 36     tf.logging.set_verbosity(tf.logging.ERROR)\r\n     37 \r\n     38     OPTIMIZERS = {\r\n \r\nAttributeError: module 'tensorflow' has no attribute 'logging'\r\n",
        "Challenge_closed_time":1587086020000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1572995244000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when attempting to import the \"ExplainationDashboard\" module from \"azureml.contrib.interpret\" in build 1.0.72, despite having installed and updated the SDK without any errors. The failing line is in a sample notebook for \"how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/644",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":45.44,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":3914.1044444444,
        "Challenge_title":"Error trying to load azureml.train.automl",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":272,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Do you know which version of tensorflow you are using? \r\n\r\nThis SO question may be applicable: https:\/\/stackoverflow.com\/questions\/55318626\/module-tensorflow-has-no-attribute-logging Hello, Not sure about tensorflow.  This is a \"stock\" Notebook VM that was created last week, so no changes were made to the libraries. Hello,\r\n\r\nSorry for the inconvenience. This issue has been fixed since v1.0.72 but, it's related to the fact that tf==2.0. is installed by default on the notebook instance. It broke other things too as TF2.0 has many changes in its API. Your two options are to upgrade to v1.0.72+ or use the following code to downgrade tensorflow.\r\n\r\npip install -U tensorflow-gpu==1.14.0 \r\ntensorflow==estimator==1.14.0 \r\n\r\nThat should fix it for you.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":5.2,
        "Solution_reading_time":9.24,
        "Solution_score_count":null,
        "Solution_sentence_count":14.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1544524371740,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cologne Germany",
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Challenge_adjusted_solved_time":3.6875811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running into a ModuleNotFoundError for pandas while using the following code to orchestrate my Azure Machine Learning Pipeline:<\/p>\n<pre><code># Loading run config\nprint(&quot;Loading run config&quot;)\ntask_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \n\ntask_1_script_run_config = ScriptRunConfig(\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    run_config=task_1_run_config    \n)\n\ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name=task_1_script_run_config.script,\n    source_directory=task_1_script_run_config.source_directory,\n    compute_target=compute_target\n)\n\npipeline_run_config = Pipeline(workspace=workspace, steps=[task_1_py_script_step])#, task_2])\n\npipeline_run = Experiment(workspace, 'Test_Run_New_Pipeline').submit(pipeline_run_config)\npipeline_run.wait_for_completion()\n<\/code><\/pre>\n<p>The environment.yml<\/p>\n<pre><code>name: phinmo_pipeline_env\ndependencies:\n- python=3.8\n- pip:\n  - pandas\n  - azureml-core==1.43.0\n  - azureml-sdk\n  - scipy\n  - scikit-learn\n  - numpy\n  - pyyaml==6.0\n  - datetime\n  - azure\nchannels:\n  - conda-forge\n<\/code><\/pre>\n<p>The loaded RunConfiguration in T01_Test_Task.yml looks like this:<\/p>\n<pre><code># The script to run.\nscript: T01_Test_Task.py\n# The arguments to the script file.\narguments: [\n  &quot;--test&quot;, False,\n  &quot;--date&quot;, &quot;2022-07-26&quot;\n]\n# The name of the compute target to use for this run.\ncompute_target: phinmo-compute-cluster\n# Framework to execute inside. Allowed values are &quot;Python&quot;, &quot;PySpark&quot;, &quot;CNTK&quot;, &quot;TensorFlow&quot;, and &quot;PyTorch&quot;.\nframework: Python\n# Maximum allowed duration for the run.\nmaxRunDurationSeconds: 6000\n# Number of nodes to use for running job.\nnodeCount: 1\n\n#Environment details.\nenvironment:\n  # Environment name\n  name: phinmo_pipeline_env\n  # Environment version\n  version:\n  # Environment variables set for the run.\n  #environmentVariables:\n  #  EXAMPLE_ENV_VAR: EXAMPLE_VALUE\n  # Python details\n  python:\n    # user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.\n    userManagedDependencies: false\n    # The python interpreter path\n    interpreterPath: python\n    # Path to the conda dependencies file to use for this run. If a project\n    # contains multiple programs with different sets of dependencies, it may be\n    # convenient to manage those environments with separate files.\n    condaDependenciesFile: environment.yml\n    # The base conda environment used for incremental environment creation.\n    baseCondaEnvironment: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n  # Docker details\n  \n# History details.\nhistory:\n  # Enable history tracking -- this allows status, logs, metrics, and outputs\n  # to be collected for a run.\n  outputCollection: true\n  # Whether to take snapshots for history.\n  snapshotProject: true\n  # Directories to sync with FileWatcher.\n  directoriesToWatch:\n  - logs\n# data reference configuration details\ndataReferences: {}\n# The configuration details for data.\ndata: {}\n# Project share datastore reference.\nsourceDirectoryDataStore:\n<\/code><\/pre>\n<p>I already tried a few things like overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml. But I am at a loss at what else to try. the T01_Test_Task.py runs without issues on its own. But putting it into a pipeline just does not seem to work.<\/p>",
        "Challenge_closed_time":1658937635368,
        "Challenge_comment_count":1,
        "Challenge_created_time":1658922798813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleNotFoundError for pandas while using Azure Machine Learning Pipeline with a yml file based RunConfiguration and environment.yml. The T01_Test_Task.py runs without issues on its own, but putting it into a pipeline is not working. The user has tried overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object and assigning a version number to pandas in the environment.yml, but the issue persists.",
        "Challenge_last_edit_time":1658924360076,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73137433",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":13.2,
        "Challenge_reading_time":47.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":4.1212652778,
        "Challenge_title":"ModuleNotFoundError while using AzureML pipeline with yml file based RunConfiguration and environment.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":366,
        "Platform":"Stack Overflow",
        "Poster_created_time":1544524371740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cologne Germany",
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Solution_body":"<p>Okay I found the issue.\nI am unnecessarily using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. I was able to see that only in the Task description in the Azure Machine Learning Studio UI.<\/p>\n<p>I was able to just remove that part and now it works:<\/p>\n<pre><code>task_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name='T01_Test_Task.py',\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    runconfig=task_1_run_config, \n    compute_target=compute_target\n)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.2,
        "Solution_reading_time":8.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":13.3298980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Usually when running an MLProject, I would use something similar to:<\/p>\n<pre><code>mlflow run . -P alpha=0.1 -P l1_ratio=0.9\n<\/code><\/pre>\n<p>Is it possible to pass a file containing the key\/value pairs instead ? so something like:<\/p>\n<pre><code>mlflow run . --file .\/parametrs\n<\/code><\/pre>\n<p>where .\/parameters contains the key\/value pairs (like an env file or something)<\/p>\n<p>One way I thought of is to make a seperate bash script that accept the file and extracts the key\/value pairs to be included in the run command, but I wonder if there's a way more native to mlflow.<\/p>",
        "Challenge_closed_time":1621931875960,
        "Challenge_comment_count":0,
        "Challenge_created_time":1621883888327,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to pass parameters in a file instead of key\/value pairs when running an MLProject using MLFlow. They are wondering if there is a more native way to do this within MLFlow, rather than creating a separate bash script to extract the key\/value pairs from the file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67677780",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":8.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":13.3298980556,
        "Challenge_title":"MLFlow run: Pass parameters in a file instead of key\/value pairs",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":222.0,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1540654775052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tunisia",
        "Poster_reputation_count":606.0,
        "Poster_view_count":69.0,
        "Solution_body":"<p>It's not supported functionality according to <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-run\" rel=\"nofollow noreferrer\">documentation<\/a>, and <a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/cli.py#L124\" rel=\"nofollow noreferrer\">source code<\/a>, so you'll need to add your own wrapper to read parameters from file &amp; pass them explicitly.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.7,
        "Solution_reading_time":5.04,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":33.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":890.5602777778,
        "Challenge_answer_count":0,
        "Challenge_body":"I am trying to deploy ml model using az ml model deploy command with additional files.\r\n\r\nEg:-\r\n\r\naz ml model deploy --ds  docker-additional-steps.txt \r\n```\r\ndocker-additional-steps.txt\r\n\r\nCOPY *.txt \/var\/azureml-app\/\r\n```\r\n\r\nbut it gives an error as below\r\n```\r\nFailed\r\nERROR: {'Azure-cli-ml Version': '1.29.0', 'Error': WebserviceException:\r\n\tMessage: Image creation polling reached non-successful terminal state, current state: Failed\r\nError response from server:\r\nStatusCode: 400\r\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Image creation polling reached non-successful terminal state, current state: Failed\\nError response from server:\\nStatusCode: 400\\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\"\r\n    }\r\n}}\r\n\r\n```",
        "Challenge_closed_time":1626798489000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1623592472000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a 'ClientRequestError' when trying to use Azure Computer Vision's OCR API in an Azure Machine Learning Notebook. The error occurs when trying to call the Computer Vision API from the notebook, but the same code works when run on a local machine. The error message suggests that there is a temporary failure in name resolution. The user has provided the code used and the Azure packages used, and suspects that the issue may be related to a previous issue (#1107).",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1509",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.9,
        "Challenge_reading_time":12.91,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":890.5602777778,
        "Challenge_title":"How to copy files into  docker image while deploying ml model using azure ml model deploy command",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":130,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Extra docker steps is no longer supported. Please create an environment instead where you can inject files as you wish and use that environment for deployment. Here is a sample.\r\n\r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb\r\n",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":4.22,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":6365.2066666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello, I can't open my project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? A screenshot is attached here to understand better. Thanks!\r\n<img width=\"1363\" alt=\"Screen Shot 2022-02-22 at 9 45 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Challenge_closed_time":1668499115000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1645584371000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a bug when attempting to clone a single notebook using the \"Open In in Sagemaker Studio Lab\" button. The error message \"Unable to copy notebook to project\" appears when the user selects \"Copy Notebook Only\" in the modal. Cloning the whole repository works without any issues. The expected behavior is for the notebook to open and appear as it would with cloning a directory. The user is using Windows 11 and Chrome version 97.0.4692.71.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6365.2066666667,
        "Challenge_title":"Can't open project on amazon sagemaker",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":91,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Not sure if your issue has been resolved or not.\r\nA quick fix is to delete your account and recreate. You will by pass the approval process if you use the same email that has already been approved. @bsaha205 do you still have the problem to open the project? Please let us know about your situation. I'll close the issue. If you have the trouble. please try @MicheleMonclova solution.  Hi @icoxfog417, yes the issue is resolved. Thanks. I am glad to hear that. Please enjoy your ML journey!",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":5.89,
        "Solution_score_count":null,
        "Solution_sentence_count":11.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5544444444,
        "Challenge_answer_count":1,
        "Challenge_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation.  I have some initialization to do at creation time (clone a github repo, etc.).  That all works fine.  The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a `Stopped` state.  A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\n```\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n```\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige.  I would have thought it would let me do the same in the lifecycle config.  Unfortunately, SageMaker still has the notebook in the `Pending` state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Challenge_closed_time":1539777191000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539775195000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to create a notebook instance using CloudFormation with SageMaker LifeCycleConfig, but there is no way to leave the newly-created instance in a `Stopped` state. The user tried using the AWS CLI to stop the instance from the lifecycle create script, but it failed. The user is looking for other hooks or creative options to solve this issue.",
        "Challenge_last_edit_time":1668613505334,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloudformation-with-sagemaker-lifecycleconfig-without-leaving-the-instance-running",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":17.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.5544444444,
        "Challenge_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":479.0,
        "Challenge_word_count":208,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"One solutions will be to create a [CFN custom resource](https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/template-custom-resources-lambda.html) backed by lambda.\nYou can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925548143,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":4.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1476806455803,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Holzkirchen, Deutschland",
        "Answerer_reputation_count":3068.0,
        "Answerer_view_count":386.0,
        "Challenge_adjusted_solved_time":1585.9678913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>An Azure Data Factory pipeline for updating a trained ML model returns this error:<\/p>\n\n<pre><code>HTTP 404. The resource you are looking for (or one of its dependencies) could have been removed, had its name changed, or is temporarily unavailable. Please review the following URL and make sure that it is spelled correctly.\nRequested URL: \/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update\n\nDiagnostic details: job ID xxxx. Endpoint https:\/\/services.azureml.net\/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update.\n<\/code><\/pre>\n\n<p>I don't even want to think about why it returned a HTML document...\nI am 100% sure that the endpoint exists and the key provided is correct.\nSo what is my mistake?<\/p>",
        "Challenge_closed_time":1508942544572,
        "Challenge_comment_count":0,
        "Challenge_created_time":1503048195263,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a 404 error while trying to update a trained ML model using an Azure Data Factory pipeline. The error message suggests that the resource may have been removed, had its name changed, or is temporarily unavailable. The user is certain that the endpoint exists and the key provided is correct.",
        "Challenge_last_edit_time":1503233060163,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45753090",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":9.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1637.3192525,
        "Challenge_title":"Azure ML endpoint 404 error",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":494.0,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1476806455803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Holzkirchen, Deutschland",
        "Poster_reputation_count":3068.0,
        "Poster_view_count":386.0,
        "Solution_body":"<p>Deleting and creating the endpoint again fixed it.<\/p>\n\n<p>Microsoft...<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426046529436,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":180.0,
        "Answerer_view_count":20.0,
        "Challenge_adjusted_solved_time":207.9795563889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use XGBoost on Sagemaker notebook.<\/p>\n\n<p>I am using <code>conda_python3<\/code> kernel, and the following packages are installed:<\/p>\n\n<ul>\n<li>py-xgboost-mutex<\/li>\n<li>libxgboost<\/li>\n<li>py-xgboost<\/li>\n<li>py-xgboost-gpu<\/li>\n<\/ul>\n\n<p>But once I am trying to import xgboost it fails on import:<\/p>\n\n<pre><code>ModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-5-5943d1bfe3f1&gt; in &lt;module&gt;()\n----&gt; 1 import xgboost as xgb\n\nModuleNotFoundError: No module named 'xgboost'\n<\/code><\/pre>",
        "Challenge_closed_time":1592571589163,
        "Challenge_comment_count":2,
        "Challenge_created_time":1591825043587,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to import xgboost on Sagemaker notebook using conda_python3 kernel. The user has installed all the required packages, but the import fails with a ModuleNotFoundError.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62313532",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":15.1,
        "Challenge_reading_time":7.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":207.3737711111,
        "Challenge_title":"xgboost on Sagemaker notebook import fails",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1532.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1391632571720,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Tel Aviv-Yafo, Israel",
        "Poster_reputation_count":1248.0,
        "Poster_view_count":137.0,
        "Solution_body":"<p>In Sagemaker notebooks  use the below steps <\/p>\n\n<h3>a) If in Notebook<\/h3>\n\n<p>i)  <code>!type python3<\/code><\/p>\n\n<p>ii) Say the above is \/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 for you <\/p>\n\n<p>iii) <code>!\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python3 -m pip install  xgboost<\/code><\/p>\n\n<p>iv)  <code>import xgboost<\/code><\/p>\n\n<hr>\n\n<h3>b) If using Terminal<\/h3>\n\n<p>i) <code>conda activate conda_python3<\/code><br>\nii) <code>pip install xgboost<\/code><\/p>\n\n<p>Disclaimer :  sometimes the installation would fail with gcc version ,in that case  update pip version before running install<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1592573769990,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":7.84,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.1017638889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>When users were creating a new compute in AML environment by default RStudio application was created.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238009-screenshot-2022-08-23-170502.png?platform=QnA\" alt=\"With RStudio\" \/>    <\/p>\n<p>However, from month of July, by default RStudio application is not getting created. Only JupyterLab, Jupyter, VS Code, Terminal, Notebook applications installed not a RStudio.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/237995-4.png?platform=QnA\" alt=\"without RStudio\" \/>    <\/p>\n<p>Is there any way to by default install RStudio application in azure ML compute instance?    <\/p>",
        "Challenge_closed_time":1662454114980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1662442948630,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where RStudio application is not installed by default in Azure ML compute since July, and is seeking a solution to install it by default.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/995175\/rstudio-application-not-installed-in-azure-ml-comp",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":13.4,
        "Challenge_reading_time":9.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.1017638889,
        "Challenge_title":"RStudio application not installed in Azure ML compute",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=8876fdcf-bcee-403a-827a-7cff9a68f8ee\">@SHAIKH, Alif Abdul  <\/a> Yes, this is a recent change in the setup of compute instance that happens during the creation of compute instance. This change does not setup the rstudio community edition by default but you can set it up while creation by adding it as a custom application from advanced settings. This change is done as part of RStudio requirements to allow users to setup their own license key or use open studio version during creation. Please refer this documentation <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=python#add-custom-applications-such-as-rstudio-preview\">page<\/a> for details to setup licensed and open version of the studio.    <\/p>\n<p>If you add a license key then please use RStudio Workbench (bring your own license) option from the advanced options and enter your own license key.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238075-image.png?platform=QnA\" alt=\"238075-image.png\" \/>    <\/p>\n<p>If you need to add the open version you need to select custom application and enter the docker image and mount point details to setup the rstudio during creation.    <\/p>\n<p>Target\/Published port <code>8787<\/code>    <br \/>\nDocker image set to <code>ghcr.io\/azure\/rocker-rstudio-ml-verse:latest<\/code>    <br \/>\n<code>\/home\/azureuser\/cloudfiles<\/code> for Host path    <br \/>\n<code>\/home\/azureuser\/cloudfiles<\/code> for Container path    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238135-image.png?platform=QnA\" alt=\"238135-image.png\" \/>    <\/p>\n<p>Once the creation and setup is complete the options to use Rstudio for open and licensed version will be visible as links on the compute instances page.     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/238142-image.png?platform=QnA\" alt=\"238142-image.png\" \/>    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":14.3,
        "Solution_reading_time":30.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":240.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":81.3314522222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an issue where vscode when connected to a VM on GCP cannot see packages installed in <code>\/opt\/conda\/lib\/python3.7\/site-packages.<\/code> I have created the VM using Vertex AI. When I open the jupyter notebook through the UI in a the browser I can see all the packages via <code>pip3 list<\/code>. But when I am connected to the VM through SSH in vscode I cannot see the packages installed such as nltk, spacy etc. and when I try to load it gives me <code>ModuleNotFoundError<\/code>. This error does not show up when I use the jupyter notebook from the Vertex AI UI. The site-packages folder is in my system path and the python that I am using is <code>\/opt\/conda\/bin\/python3<\/code>.<\/p>\n<p>Any help is appreciated. Please do let me know if my question is clear.<\/p>\n<p>EDIT: I figured out that my packages are running on a container in the VM. Is there a way for me to access those packages via jupyter notebook in vscode?<\/p>",
        "Challenge_closed_time":1638843453688,
        "Challenge_comment_count":3,
        "Challenge_created_time":1638473538253,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue where VSCode, when connected to a VM on GCP, cannot see packages installed in \/opt\/conda\/lib\/python3.7\/site-packages. The packages are running on a container in the VM, and the user is looking for a way to access those packages via Jupyter Notebook in VSCode. The error message received is ModuleNotFoundError.",
        "Challenge_last_edit_time":1638550660460,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70205432",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":6.4,
        "Challenge_reading_time":11.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":102.7542875,
        "Challenge_title":"VSCode cannot see packages on a GCP VM",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":230.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Poster_created_time":1580840045043,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":85.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Posting the answer as community wiki. As confirmed by @Abhishek, he was able to make it work by installing a docker extension on the VM then attach VS code to the container.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.4,
        "Solution_reading_time":2.19,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1327302732867,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"USA",
        "Answerer_reputation_count":19711.0,
        "Answerer_view_count":1030.0,
        "Challenge_adjusted_solved_time":0.1674261111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Challenge_closed_time":1597366468132,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597366468133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering errors when attempting to train an XGBoost model using the AWS hosted container in a local Jupyter notebook. The errors include a KeyError related to S3DistributionType and a runtime error when attempting to run the docker container. The issue does not occur when using the Amazon cloud SageMaker environment. It is suspected that there is an issue with how the LocalSession() works with the hosted docker container.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.3,
        "Challenge_reading_time":56.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1174.0,
        "Challenge_word_count":293,
        "Platform":"Stack Overflow",
        "Poster_created_time":1327302732867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"USA",
        "Poster_reputation_count":19711.0,
        "Poster_view_count":1030.0,
        "Solution_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1597367070867,
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":32.08,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":235.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1413222980680,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Richland, WA",
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":59.0,
        "Challenge_adjusted_solved_time":1.9509575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to build a Flask app with Gunicorn to serve concurrent requests. For what it's worth, the context is a bring-your-own-container Sagemaker application.<\/p>\n\n<p>The issue is that I need the application to periodically check for updates. So I thought to implement a thread for this. Here is a minimal example of some Flask code with an update thread. <\/p>\n\n<p>server.py<\/p>\n\n<pre><code>from flask import Flask\nimport time, threading\n\napp = Flask(__name__)\n\nmessage = True\n\ndef update():\n  while True:\n    message = not message\n    time.sleep(10)\n\n@app.route(\"\/\")\ndef hello():\n  global message\n  return message\n\nupdate_thread = threading.Thread(target=update)\n\nif __name__ == \"__main__\":\n  update_thread.start()\n  app.run()\n  update_thread.join()\n<\/code><\/pre>\n\n<p>I then launch with gunicorn:<\/p>\n\n<p><code>gunicorn -k gevent -b unix:\/tmp\/gunicorn.sock -w 4 server:app<\/code><\/p>\n\n<p>Perhaps unsurprisingly the update thread doesn't start since the <code>__main__<\/code> section is never executed. <\/p>\n\n<blockquote>\n  <p><strong>Question<\/strong>: <em>How can one use an update thread (or similar construct) in a Flask app with Gunicorn?<\/em><\/p>\n<\/blockquote>",
        "Challenge_closed_time":1565727707500,
        "Challenge_comment_count":0,
        "Challenge_created_time":1565720684053,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to build a Flask app with Gunicorn to serve concurrent requests and needs the application to periodically check for updates. They have implemented a thread for this, but the update thread doesn't start since the `__main__` section is never executed when launched with gunicorn. The user is seeking a solution to use an update thread in a Flask app with Gunicorn.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57483440",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":15.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1.9509575,
        "Challenge_title":"Flask and Gunicorn with Additional Threads",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":858.0,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Poster_created_time":1413222980680,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Richland, WA",
        "Poster_reputation_count":493.0,
        "Poster_view_count":59.0,
        "Solution_body":"<p>It looks like this can be accomplished using <code>Flask-APScheduler<\/code> as follows:<\/p>\n\n<p><code>pip install flask_apscheduler<\/code><\/p>\n\n<p>server.py<\/p>\n\n<pre><code>from flask import Flask\nfrom apscheduler.schedulers.background import BackgroundScheduler\nimport atexit\n\napp = Flask(__name__)\n\nmessage = True\n\ndef update():\n  global message\n  message = not message\n\nscheduler = BackgroundScheduler()\nscheduler.add_job(func=update,trigger=\"interval\",seconds=10)\nscheduler.start()\n# shut down the scheduler when exiting the app\natexit.register(scheduler.shutdown)\n\n@app.route(\"\/\")\ndef hello():\n  global message\n  return message\n\nif __name__ == \"__main__\":\n  app.run()\n<\/code><\/pre>\n\n<p>Then launching as usual with \n<code>gunicorn -k gevent -b unix:\/tmp\/gunicorn.sock -w 4 server:app<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.3,
        "Solution_reading_time":10.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.7837736111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Would like to upload Jupyter notebooks from different sources like GitHub into my workspace either directly or through my local machine (download locally first and then upload) but I would like to do it programmatically. Either with the AzureML SDK or azure cli  <\/p>",
        "Challenge_closed_time":1651104442392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651101620807,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to upload Jupyter notebooks from various sources like GitHub to their AzureML workspace programmatically using either AzureML SDK or Azure CLI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/829311\/how-could-i-upload-notebooks-to-my-azureml-workspa",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.7837736111,
        "Challenge_title":"How could I upload notebooks to my AzureML workspace programatically",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":53,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, thanks for reaching out. You can use compute instance <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal\">terminal<\/a> in AML notebooks to <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/samples-notebooks#get-samples-on-azure-machine-learning-compute-instance\">clone<\/a> the GitHub repo. There's currently no option to upload notebooks to your workspace programmatically using sdk or cli.<\/p>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":14.5,
        "Solution_reading_time":7.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":455.8612047222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,    <\/p>\n<p>I am doing the Challenge. <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/intro-to-azure-machine-learning-service\/\">https:\/\/learn.microsoft.com\/en-us\/learn\/modules\/intro-to-azure-machine-learning-service\/<\/a>    <\/p>\n<p>Please see what I have installed:    <\/p>\n<blockquote>\n<p>pip install azureml-sdk    <\/p>\n<\/blockquote>\n<p>I am getting the following messages at the end:    <\/p>\n<blockquote>\n<p>ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.    <\/p>\n<p>We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.    <\/p>\n<p>jupyterlab 2.2.9 requires jupyterlab-server&lt;2.0,&gt;=1.1.5, which is not installed.    <br \/>\nSuccessfully installed applicationinsights-0.11.9 azure-identity-1.4.1 azureml-automl-core-1.19.0 azureml-dataprep-2.6.3 azureml-dataprep-native-26.0.0 azureml-dataprep-rslex-1.4.0 azureml-dataset-runtime-1.19.0.post1 azureml-pipeline-1.19.0 azureml-pipeline-core-1.19.0 azureml-pipeline-steps-1.19.0 azureml-sdk-1.19.0 azureml-telemetry-1.19.0 azureml-train-1.19.0 azureml-train-automl-client-1.19.0 azureml-train-core-1.19.0 azureml-train-restclients-hyperdrive-1.19.0 distro-1.5.0 dotnetcore2-2.1.20 fusepy-3.0.1 msal-1.8.0 msal-extensions-0.2.2 numpy-1.19.3 portalocker-1.7.1 pyarrow-1.0.1 pywin32-227    <\/p>\n<\/blockquote>\n<p>Now I am trying to start up and type the following in .py file in Visual Studio Code    <\/p>\n<blockquote>\n<p>from azureml.core import Workspace    <\/p>\n<\/blockquote>\n<p>This is the error message I am getting:    <\/p>\n<blockquote>\n<p> File &quot;c:\/Users\/User\/OneDrive\/Desktop\/New folder\/Build AI Solution\/automl_python.py&quot;, line 1, in &lt;module&gt;    <br \/>\n    from azureml.core import Workspace    <br \/>\nModuleNotFoundError: No module named 'azureml'    <\/p>\n<\/blockquote>\n<p>Please could you help me?    <\/p>\n<p>thanks,    <\/p>\n<p>Naveen<\/p>",
        "Challenge_closed_time":1610753174427,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609112074090,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleNotFoundError while trying to import the 'azureml' module in a .py file in Visual Studio Code. The user has installed 'azureml-sdk' using pip, but is receiving an error message stating that the module is not found.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/211503\/modulenotfounderror-no-module-named-azureml",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":26.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":455.8612047222,
        "Challenge_title":"ModuleNotFoundError: No module named 'azureml'",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":190,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>This is now solved. Thanks!<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-1.9,
        "Solution_reading_time":0.44,
        "Solution_score_count":29.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":29.8171413889,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I hope I am not missing something obvious here. I am using the new azure ml studio designer. I am able to use to create datasets, train models and use them just fine.<\/p>\n\n<p>azure ml studio allows creation of Jupyter notebooks (also) and use them to do machine learning. I am able to do that too. <\/p>\n\n<p>So, now, I am wondering, can I build my ML pipeline\/experiment in ML studio designer, and once it is in good shape, export it as a python and jupyter notebook? then, use it in the same designer provided notebook option or may be use it locally?<\/p>",
        "Challenge_closed_time":1582139574710,
        "Challenge_comment_count":4,
        "Challenge_created_time":1582134046887,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is using Azure ML Studio Designer to create datasets, train models, and use Jupyter notebooks for machine learning. They are wondering if it is possible to export their ML pipeline\/experiment as a Python and Jupyter notebook to use locally or in the same designer provided notebook option.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60306240",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1.5355063889,
        "Challenge_title":"export azure ml studio designer project as jupyter notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":789.0,
        "Challenge_word_count":111,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442334437952,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":2272.0,
        "Poster_view_count":516.0,
        "Solution_body":"<p>This is not currently supported, but I am 80% sure it is in the roadmap.\nAn alternative would be to use the SDK to create the same pipeline using <code>ModuleStep<\/code> where  I <em>believe<\/em> you can reference a Designer Module by its name to use it like a <code>PythonScriptStep<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1582241388596,
        "Solution_link_count":0.0,
        "Solution_readability":11.5,
        "Solution_reading_time":3.66,
        "Solution_score_count":6.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":48.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1466260908296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":149.6007655556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the Azure Recommendation API sample there is a snippet like this:<\/p>\n\n<pre><code>     if (itemSets.RecommendedItemSetInfo != null)\n        {\n            ...\n        }\n        else\n        {\n            Console.WriteLine(\"No recommendations found.\");\n        }\n<\/code><\/pre>\n\n<p>So I assume that nullable recommended set means no recommendations. But what is the case with this set being not nullable but still empty ( as I am having it running the example)?<\/p>\n\n<p>I provided my own usages and catalog files. I have not too many entries there however for i2i recommendations I have results and for u2i there is an empty set.\nAllowColdItemPlacement doesn't change a think here.<\/p>",
        "Challenge_closed_time":1478186851716,
        "Challenge_comment_count":0,
        "Challenge_created_time":1477648288960,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on the difference between null and empty results in the Azure Recommendation API sample. They have provided their own usage and catalog files and are experiencing an empty set for u2i recommendations, despite the set being not nullable. They are unsure if an empty set means no recommendations.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40302499",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":149.6007655556,
        "Challenge_title":"Recommendation API: what is the difference between null results and empty results",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":130.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1354118434116,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Wroc\u0142aw, Poland",
        "Poster_reputation_count":393.0,
        "Poster_view_count":43.0,
        "Solution_body":"<p>We did not mean to convey a difference in meaning between null recommendations and empty recommendations. I will check why we may be sending two different types of results. Either way, don't treat those two cases as different cases. <\/p>\n\n<p>If you are not getting results for user-to-item recommendations, most likely there was no data for that user when the build was created or the items that the user interacted with do not have enough co-occurrences with other items in the usage.<\/p>\n\n<p>What to do when you get empty recommendations is up to you, you may decide to not show any recommendations, or back-fill with popular items you may want to promote.<\/p>\n\n<p>Thanks!<\/p>\n\n<p>Luis Cabrera\nProgram Manager - Recommendations API.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.8,
        "Solution_reading_time":9.08,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":119.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1572449042430,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":2082.0,
        "Answerer_view_count":238.0,
        "Challenge_adjusted_solved_time":2.8597922222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The entire error message after executing <code>terraform apply<\/code> within the terraform-folder of <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">this source code in my GitHub-repo<\/a> (inspired by <a href=\"https:\/\/www.linkedin.com\/pulse\/terraform-sagemaker-part-2a-creating-custom-notebook-instance-david\" rel=\"nofollow noreferrer\">this tutorial<\/a> and <a href=\"https:\/\/github.com\/dkhundley\/terraform-sagemaker-tutorial\" rel=\"nofollow noreferrer\">its related GitHub-repo<\/a>):<\/p>\n<pre><code>aws_sagemaker_notebook_instance.notebook_instance: Creating...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [10s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [20s elapsed]\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m21s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [15m31s elapsed]\n\u2577\n\u2502 Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)\n\u2502\n\u2502   with aws_sagemaker_notebook_instance.notebook_instance,\n\u2502   on notebook_instance.tf line 2, in resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot;:\n\u2502    2: resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n\u2502\n<\/code><\/pre>\n<p>Internet research seemed to provide the solution in <a href=\"https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\" rel=\"nofollow noreferrer\">this article<\/a>, which inspired be to increase the allowed <code>IDLE_TIME<\/code> in the <code>on-start.sh<\/code> - script to <code>IDLE_TIME=1800<\/code> (in seconds, which equals 30 minutes). This should've been sufficient for the deployment time of around 15 minutes; yet, it threw the same error again.<\/p>\n<p>Next, I found <a href=\"https:\/\/stackoverflow.com\/questions\/65884743\/resolving-broken-deleted-state-in-terraform\">this post on StackOverFlow<\/a> suggesting to<\/p>\n<blockquote>\n<p>run <code>terraform refresh<\/code>, which will cause Terraform to refresh its state\nfile against what actually exists with the cloud provider.<\/p>\n<\/blockquote>\n<p>Unfortunately, running <code>terraform apply<\/code> right after refreshing didn't resolve the issue either.\nI'm wondering why the aforementioned <code>IDLE_TIME=1800<\/code> - setting does not have any effect. This should be more than sufficient for a 15-minute apply-time.<\/p>\n<hr \/>\n<p><strong>EDIT: adding code specifics for enhanced understanding<\/strong><\/p>\n<p><strong>1. Creating the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance&quot; &quot;notebook_instance&quot; {\n  name                    = &quot;aws-sm-notebook-instance&quot;\n  role_arn                = aws_iam_role.notebook_iam_role.arn\n  instance_type           = &quot;ml.t2.medium&quot;\n  lifecycle_config_name   = aws_sagemaker_notebook_instance_lifecycle_configuration.notebook_config.name\n  default_code_repository = aws_sagemaker_code_repository.git_repo.code_repository_name\n}\n<\/code><\/pre>\n<p><strong>2. Defining the SageMaker notebook lifecycle configuration<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_notebook_instance_lifecycle_configuration&quot; &quot;notebook_config&quot; {\n  name      = &quot;dev-platform-al-sm-lifecycle-config&quot;\n  on_create = filebase64(&quot;..\/scripts\/on-create.sh&quot;)\n  on_start  = filebase64(&quot;..\/scripts\/on-start.sh&quot;)\n}\n<\/code><\/pre>\n<p><strong>3. Defining the Git repo to instantiate on the SageMaker notebook instance<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_code_repository&quot; &quot;git_repo&quot; {\n  code_repository_name = &quot;aws-sm-notebook-instance-repo&quot;\n\n  git_config {\n    repository_url = &quot;https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance.git&quot;\n  }\n}\n<\/code><\/pre>\n<p><strong>Contents of <code>on-start.sh<\/code> (including IDLE_TIME - parameter)<\/strong>\nNote that this script will be invoked by the <code>scripts\/autostop.py<\/code> - script, which you can find <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\/blob\/main\/scripts\/autostop.py\" rel=\"nofollow noreferrer\">here<\/a> in the associated <a href=\"https:\/\/github.com\/AndreasLuckert\/aws-sm-notebook-instance\" rel=\"nofollow noreferrer\">public repo containing the source code<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\n\n## IDLE AUTOSTOP STEPS\n## ----------------------------------------------------------------\n\n## Setting the timeout (in seconds) for how long the SageMaker notebook can run idly before being auto-stopped\n# -&gt; e.g. 1800 s = 30 min since first deployment can take between 15 and 20 minutes which could then fail like so:\n# &quot;Error: error waiting for sagemaker notebook instance (aws-sm-notebook-instance) to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(&lt;nil&gt;)&quot;\n# Hint for solution under following link: https:\/\/yuyasugano.medium.com\/machine-learning-infrastructure-terraforming-sagemaker-part-2-f2460a9a4663\nIDLE_TIME=1800\n\n# Getting the autostop.py script from GitHub\necho &quot;Fetching the autostop script...&quot;\nwget https:\/\/raw.githubusercontent.com\/andreasluckert\/aws-sm-notebook-instance\/main\/scripts\/autostop.py\n\n# Using crontab to autostop the notebook when idle time is breached\necho &quot;Starting the SageMaker autostop script in cron.&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/5 * * * * \/usr\/bin\/python $PWD\/autostop.py --time $IDLE_TIME --ignore-connections&quot;) | crontab -\n\n\n\n## CUSTOM CONDA KERNEL USAGE STEPS\n## ----------------------------------------------------------------\n\n# Setting the proper user credentials\nsudo -u ec2-user -i &lt;&lt;'EOF'\nunset SUDO_UID\n\n# Setting the source for the custom conda kernel\nWORKING_DIR=\/home\/ec2-user\/SageMaker\/custom-miniconda\nsource &quot;$WORKING_DIR\/miniconda\/bin\/activate&quot;\n\n# Loading all the custom kernels\nfor env in $WORKING_DIR\/miniconda\/envs\/*; do\n    BASENAME=$(basename &quot;$env&quot;)\n    source activate &quot;$BASENAME&quot;\n    python -m ipykernel install --user --name &quot;$BASENAME&quot; --display-name &quot;Custom ($BASENAME)&quot;\ndone\n<\/code><\/pre>",
        "Challenge_closed_time":1631187921612,
        "Challenge_comment_count":6,
        "Challenge_created_time":1631108848430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error while creating a SageMaker notebook instance using Terraform. The error message showed that the instance creation failed with an unexpected state 'Failed' instead of the target state 'InService'. The user tried increasing the allowed IDLE_TIME in the on-start.sh script but it did not resolve the issue. The user also tried refreshing the state file using 'terraform refresh' but it did not help either. The user is wondering why the IDLE_TIME setting did not have any effect. The post includes code specifics for creating the SageMaker notebook instance, defining the lifecycle configuration, and defining the Git repo to instantiate on the instance.",
        "Challenge_last_edit_time":1631177626360,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69104302",
        "Challenge_link_count":10,
        "Challenge_participation_count":7,
        "Challenge_readability":16.9,
        "Challenge_reading_time":85.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":21.9647727778,
        "Challenge_title":"Terraform Error: error waiting for sagemaker notebook instance to create: unexpected state 'Failed', wanted target 'InService'. last error: %!s(<nil>)",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":500,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572449042430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":2082.0,
        "Poster_view_count":238.0,
        "Solution_body":"<p>The solution to the problem was to check the CloudWatch Log events under <code>CloudWatch -&gt; Log groups -&gt; \/aws\/sagemaker\/NotebookInstances -&gt; aws-sm-notebook-instance\/LifecycleConfigOnCreate<\/code> to find the following error-message:<\/p>\n<pre><code>\/bin\/bash: \/tmp\/OnCreate_2021-09-08-12-24rw5al34g: \/bin\/bash^M: bad interpreter: No such file or directory\n<\/code><\/pre>\n<p>A bit of internet research brought me to <a href=\"https:\/\/askubuntu.com\/questions\/304999\/not-able-to-execute-a-sh-file-bin-bashm-bad-interpreter\/305001#305001\">this solution related to newline characters in shell-scripts<\/a>, which depend on whether you are on <code>Windows<\/code> or a <code>UNIX<\/code>-system.\nAs I'm working on Windows, the shell-scripts created in VS-Code comprised dos-specific <code>CRLF<\/code> newline-handling, which could be resolved via the button on the bottom-right in <code>VS-Code<\/code> to switch the <em>carriage return<\/em> (CRLF) character to the <em>line feed<\/em> (LF) character used by UNIX.<\/p>\n<p>As the compute instance employed by AWS Sagemaker is a Linux-system, it cannot handle the dos-style CRLF newline-characters in the shell-scripts and this &quot;adds&quot; a <code>^M<\/code> after <code>\/bin\/bash<\/code> which obviously leads to an error as such an interpreter does not exist.<\/p>\n<p>So, finally <code>terraform apply<\/code> worked out well:<\/p>\n<pre><code>$ terraform apply\n...\n...\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m30s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Still creating... [7m40s elapsed]\naws_sagemaker_notebook_instance.notebook_instance: Creation complete after 7m43s [id=aws-sm-notebook-instance]\n\nApply complete! Resources: 1 added, 1 changed, 1 destroyed.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":23.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":184.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":1.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2608333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Challenge_closed_time":1593108137000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593107198000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in changing instance types for SageMaker Studio as it seems to only support a limited number of types. They are seeking a way to change to other instance types, similar to how it can be done for SageMaker Notebook Instances.",
        "Challenge_last_edit_time":1668530553628,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sagemaker-studio",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":7.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.2608333333,
        "Challenge_title":"Notebook Instance Types for SageMaker Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":864.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The instance types you are seeing are Fast Launch Instances ( which are instance types designed to launch in under two minutes).\n\nIn order to see all the types of instances, click on the switch on top of the instance type list that says \"Fast Launch\", that should display the rest of available instances.\n\nHere is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html\n\nHope it helps!",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925564584,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":5.42,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1432655047272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":463.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":581.8209875,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have to do large scale feature engineering on some data. My current approach is to spin up an instance using <code>SKLearnProcessor<\/code> and then scale the job by choosing a larger instance size or increasing the number of instances. I require using some packages that are not installed on Sagemaker instances by default and so I want to install the packages using .whl files.<\/p>\n<p>Another hurdle is that the Sagemaker role does not have internet access.<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nsess = sagemaker.Session()\nsess.default_bucket()        \n\nregion = boto3.session.Session().region_name\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session = sess,\n                                     instance_type=&quot;ml.t3.medium&quot;,\n                                     instance_count=1)\n\nsklearn_processor.run(code='script.py')\n<\/code><\/pre>\n<p><strong>Attempted resolutions:<\/strong><\/p>\n<ol>\n<li>Upload the packages to a CodeCommit repository and clone the repo into the SKLearnProcessor instances. Failed with error <code>fatal: could not read Username for 'https:\/\/git-codecommit.eu-west-1.amazonaws.com': No such device or address<\/code>. I tried cloning the repo into a sagemaker notebook instance and it works, so its not a problem with my script.<\/li>\n<li>Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off <a href=\"https:\/\/medium.com\/@shadidc\/installing-custom-python-package-to-sagemaker-notebook-b7b897f4f655\" rel=\"nofollow noreferrer\">this post<\/a>. But the packages never get copied, and an error message is not thrown.<\/li>\n<li>Also looked into using the package <code>s3fs<\/code> but it didn't seem suitable to copy the wheel files.<\/li>\n<\/ol>\n<p><strong>Alternatives<\/strong><\/p>\n<p>My client is hesitant to spin up containers from custom docker images. Any alternatives?<\/p>",
        "Challenge_closed_time":1604006575952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601912020397,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to upload packages to an instance in a Processing step in Sagemaker for large scale feature engineering. However, the Sagemaker role does not have internet access and the required packages are not installed on Sagemaker instances by default. The user attempted to upload the packages to a CodeCommit repository and clone the repo into the SKLearnProcessor instances, but it failed. The user also tried using a bash script to copy the packages from S3 using the CLI, but the packages never got copied, and an error message was not thrown. The user is looking for alternatives to spinning up containers from custom docker images.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64211755",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":25.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":581.8209875,
        "Challenge_title":"How to upload packages to an instance in a Processing step in Sagemaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1071.0,
        "Challenge_word_count":242,
        "Platform":"Stack Overflow",
        "Poster_created_time":1535553190827,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Johannesburg",
        "Poster_reputation_count":438.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p><code>2. Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off this post. But the packages never get copied, and an error message is not thrown.<\/code><\/p>\n<p>This approach seems sound.<\/p>\n<p>You may be better off overriding the <code>command<\/code> field on the <code>SKLearnProcessor<\/code> to <code>\/bin\/bash<\/code>, run a bash script like <code>install_and_run_my_python_code.sh<\/code> that installs the wheel containing your python dependencies, then runs your main python entry point script.<\/p>\n<p>Additionally, instead of using AWS S3 calls to download your code in a script, you could use a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ProcessingInput\" rel=\"nofollow noreferrer\">ProcessingInput<\/a> to download your code rather than doing this with AWS CLI calls in a bash script, which is what the <code>SKLearnProcessor<\/code> does to download your entry point <code>script.py<\/code> code across all the instances.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.2,
        "Solution_reading_time":13.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":132.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":44.4506683334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am new to docker and environments. This could be basics but i have been trying to install packages in my pyproject.toml file in Dockerfile without success.   <\/p>\n<p>I have tried using poetry to export requirements.txt file  and using it with the   <br \/>\nEnvironment.from_pip_requirements('requirements.txt') function and a Dockerfile.   <\/p>\n<p>But could there be any elegant solution to use toml file directly for creating a custom environment ?  <\/p>",
        "Challenge_closed_time":1638981076963,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638821054557,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having trouble installing packages in their pyproject.toml file in a Dockerfile using poetry. They have tried exporting a requirements.txt file and using it with the Environment.from_pip_requirements function, but are looking for a more elegant solution to use the toml file directly for creating a custom environment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/653688\/custom-dockerfile-on-azure-environment-with-python",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":6.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":44.4506683334,
        "Challenge_title":"Custom Dockerfile on Azure Environment with python poetry",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":74,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for the response, <a href=\"\/users\/na\/?userid=1cea772e-bffd-0003-0000-000000000000\">@Ram R  <\/a>     <br \/>\nUsing  the Dockerfile :     <\/p>\n<pre><code>FROM python:3.8-slim-buster  \nENV PYTHONUNBUFFERED=1 \\  \n    PYTHONDONTWRITEBYTECODE=1 \\  \n    PIP_NO_CACHE_DIR=1 \\  \n    PIP_DISABLE_PIP_VERSION_CHECK=1 \\  \n    POETRY_VERSION=1.1.7 \\  \n    PYLINT_VERSION=2.9.4  \n  \nRUN pip install pylint==$PYLINT_VERSION \\  \n    &amp;&amp; pip install &quot;poetry==$POETRY_VERSION&quot;   \n  \nCOPY pyproject.toml .\/  \nRUN poetry config virtualenvs.create false   \n<\/code><\/pre>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":6.83,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.9663888889,
        "Challenge_answer_count":1,
        "Challenge_body":"What is value and use case for Deep Learning AMI (DLAMI)?\n\nIt seems that customers often pack ML dependencies at the docker level (themselves, or with DL containers or with SageMaker containers), instead of the AMI level. So what is the value and use-case of DL AMI ?",
        "Challenge_closed_time":1594216705000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594209626000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning the value and use case of Deep Learning AMI (DLAMI) as customers tend to pack ML dependencies at the docker level instead of the AMI level.",
        "Challenge_last_edit_time":1668530670747,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUQInSlgeCS6mIe4DJv3KwnQ\/what-is-value-and-use-case-for-deep-learning-ami-dlami",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":3.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1.9663888889,
        "Challenge_title":"What is value and use case for Deep Learning AMI (DLAMI)?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":186.0,
        "Challenge_word_count":57,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The value of the DLAMI (https:\/\/docs.aws.amazon.com\/dlami\/latest\/devguide\/what-is-dlami.html) is ease of use and saving time to get up to speed in a development environment.  If you are developing code for ML there is a huge variety of frameworks and software that you might need to install. The DLAMI includes the more popular ones, so you may quickly deploy a machine complete with common dependencies. This results in a reduction of the time needed for installing and configuring things. It speeds up experimentation and evaluation. If you want to try a new framework, it is already there.\n\nThe second reason is that AWS keeps the AMI up to date, so you may just deploy a new AMI periodically rather than having to patch.  Again, this saves you time and lets you concentrate on the underlying development and business activities.\n\nAll that said, for running in production and at volume you might want to use a different tool, I would imagine that for most cases creating docker images to your specific requirements would make a lot of sense. No need to go over the good and bad points of containers here.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1612481183580,
        "Solution_link_count":1.0,
        "Solution_readability":8.7,
        "Solution_reading_time":13.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1808333333,
        "Challenge_answer_count":1,
        "Challenge_body":"A customer wants to connect a Sagemaker notebook to Glue Catalog, but is not allowed to use developer endpoints because of security constraints.\n\nI can't seem to find documentation on the Glue Catalog API that would allow this, or examples of how this might be done.  Any links or pointers would be greatly appreciated.",
        "Challenge_closed_time":1594232347000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1594231696000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in connecting a Sagemaker notebook to Glue Catalog due to security constraints that prevent the use of developer endpoints. They are seeking documentation or examples on how to achieve this.",
        "Challenge_last_edit_time":1668594120152,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUoiI3L85FT6OmPewooCH4lQ\/how-to-connect-a-sagemaker-notebook-to-glue-catalog",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":4.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.1808333333,
        "Challenge_title":"How to connect a Sagemaker Notebook to Glue Catalog",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1061.0,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"So there is the catalog API which allows you to describe databases, tables, etc. Documentation regarding the calls and data structures can be found here:\n\n- https:\/\/docs.aws.amazon.com\/glue\/latest\/dg\/aws-glue-api-catalog-tables.html\n\nBoto3 for get_table\n- https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/glue.html#Glue.Client.get_table\n\nIf they have a restrictive security posture (as suggested by the avoidance of Dev Endpoints) you may also suggest a Glue VPC-E:  https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/vpce-interface.html\n\nI would ask what are they accessing the catalog for, as the Dev Endpoint isn't entirely about the Glue Catalog, but about the compute resources andSparkMagic.\n\nAlso, think about steering them towards AWS Data Wrangler for interacting with Glue Catalog if they are using Pandas. Helpful snippets can be found here: \n\n- https:\/\/github.com\/awslabs\/aws-data-wrangler\/blob\/master\/tutorials\/005%20-%20Glue%20Catalog.ipynb",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1598510602164,
        "Solution_link_count":4.0,
        "Solution_readability":15.0,
        "Solution_reading_time":12.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":105.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1579536819712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":18.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":4.6947408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I installed pyarrow using this command &quot;conda install pyarrow&quot;.\nI am running a sagemaker notebook and I am getting the error no module named pyarrow.\nI have python 3.8.3 installed on mac.<\/p>\n<p>I have numpy  1.18.5 , pandas 1.0.5 and pyarrow  0.15.1<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1604344726272,
        "Challenge_comment_count":1,
        "Challenge_created_time":1604343780117,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user installed pyarrow using \"conda install pyarrow\" but is encountering an error of \"no module named pyarrow\" while running a sagemaker notebook on a Mac with Python 3.8.3 installed. The user has numpy 1.18.5, pandas 1.0.5, and pyarrow 0.15.1 installed.",
        "Challenge_last_edit_time":1604344440696,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64651724",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":3.1,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.2628208333,
        "Challenge_title":"No Module named pyarrow",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":540.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1468599558956,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation_count":107.0,
        "Poster_view_count":26.0,
        "Solution_body":"<p>I have not yet used AWS Sagemaker notebooks, but they may be similar to GCP 'AI Platform notebooks', which I have used quite extensively. Additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks (whether from command line or from GUI)?<\/p>\n<p>In GCP, I defaulted to using <code>pip install<\/code> for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments.<\/p>\n<p>Assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the <code>CONDA<\/code> tab and selecting which notebook uses which conda environment.\n-Spencer<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1604361341763,
        "Solution_link_count":0.0,
        "Solution_readability":16.1,
        "Solution_reading_time":11.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":131.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1594906911350,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41528.0,
        "Answerer_view_count":6164.0,
        "Challenge_adjusted_solved_time":138.9138230556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are using AWS Sagemaker feature, bring your own docker, where we have inference model written in R. As I understood, batch transform job runs container in a following way:<\/p>\n<pre><code>docker run image serve\n<\/code><\/pre>\n<p>Also, on docker we have a logic to determine which function to invoke:<\/p>\n<pre><code>args &lt;- commandArgs()\nif (any(grepl('train', args))) {\n    train()}\nif (any(grepl('serve', args))) {\n    serve()}\n<\/code><\/pre>\n<p>Is there a way, to override default container invocation so we can pass some additional parameters?<\/p>",
        "Challenge_closed_time":1599691026140,
        "Challenge_comment_count":6,
        "Challenge_created_time":1599220820873,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is using AWS Sagemaker with a docker container running an inference model in R. They want to know if there is a way to override the default container invocation to pass additional parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63740792",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":16.6,
        "Challenge_reading_time":7.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":130.6125741667,
        "Challenge_title":"Provide additional input to docker container running inference model",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":702.0,
        "Challenge_word_count":84,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473837223712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Belgrade",
        "Poster_reputation_count":353.0,
        "Poster_view_count":66.0,
        "Solution_body":"<p>As you said, and is indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, Sagemaker will run your container with the following command:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>docker run image serve\n<\/code><\/pre>\n<p>By issuing this command Sagemaker will overwrite any <code>CMD<\/code> that you provide in your container Dockerfile, so you cannot use <code>CMD<\/code> to provide dynamic arguments to your program.<\/p>\n<p>We can think in use the Dockerfile <code>ENTRYPOINT<\/code> to consume some environment variables, but the documentation of AWS dictates that it is preferable use the <code>exec<\/code> form of the <code>ENTRYPOINT<\/code>. Somethink like:<\/p>\n<pre><code>ENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;]\n<\/code><\/pre>\n<p>I think that, for analogy with <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">model training<\/a>, they need this kind of container execution to enable the container to receive termination signals:<\/p>\n<blockquote>\n<p>The exec form of the <code>ENTRYPOINT<\/code> instruction starts the executable directly, not as a child of <code>\/bin\/sh<\/code>. This enables it to receive signals like <code>SIGTERM<\/code> and <code>SIGKILL<\/code> from SageMaker APIs.<\/p>\n<\/blockquote>\n<p>To allow variable expansion, we need to use the <code>ENTRYPOINT<\/code> <code>shell<\/code> form. Imagine:<\/p>\n<pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;, &quot;$ENV_VAR1&quot;]\n<\/code><\/pre>\n<p>If you try to do the same with the <code>exec<\/code> form the variables provided will be treated as a literal and will not be sustituited for their actual values.<\/p>\n<p>Please, see the approved answer of <a href=\"https:\/\/stackoverflow.com\/questions\/37904682\/how-do-i-use-docker-environment-variable-in-entrypoint-array\">this<\/a> stackoverflow question for a great explanation of this subject.<\/p>\n<p>But, one thing you can do is obtain the value of these variables in your R code, similar as when you process <code>commandArgs<\/code>:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>ENV_VAR1 &lt;- Sys.getenv(&quot;ENV_VAR1&quot;)\n<\/code><\/pre>\n<p>To pass environment variables to the container, as indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\"><code>CreateModel<\/code><\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\"><code>CreateTransformJob<\/code><\/a> requests on your container.<\/p>\n<p>You probably will need to include in your Dockerfile <code>ENV<\/code> definitions for every required environment variable on your container, and provide for these definitions default values with <code>ARG<\/code>:<\/p>\n<pre><code>ARG ENV_VAR1_DEFAULT_VALUE=VAL1\nENV_VAR1=$ENV_VAR1_DEFAULT_VALUE\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1599720910636,
        "Solution_link_count":6.0,
        "Solution_readability":17.2,
        "Solution_reading_time":43.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":313.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1377156004256,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":26.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1.8426047222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to get a a simple Azure ML pipeline with the dogs vs cats data set following the steps  - <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">documented here<\/a><\/p>\n\n<p>My notebook contains the following -<\/p>\n\n<pre><code>import azureml.core\nfrom azureml.core import Workspace, Datastore\nfrom azureml.core import Environment\nfrom azureml.core.environment import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\n\nws = Workspace.from_config()\n\nmyenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies()\nconda_dep.add_conda_package(\"keras\")\nconda_dep.add_conda_package(\"PIL\")\nmyenv.python.conda_dependencies=conda_dep\nmyenv.register(workspace=ws)\n<\/code><\/pre>\n\n<p>After setting up the data reference and the compute, here's how I am creating the pipeline -<\/p>\n\n<pre><code>trainStep = PythonScriptStep(\n    script_name=\"dogs_vs_cats.py\",\n    arguments=[\"--input\", blob_input_data, \"--output\", output_data1],\n    inputs=[blob_input_data],\n    outputs=[output_data1],\n    compute_target=compute_target,\n    source_directory=\"..\/dogs-vs-cats\"\n)\n\nSteps = [trainStep]\n\nfrom azureml.pipeline.core import Pipeline\npipeline1 = Pipeline(workspace=ws, steps=[Steps])\n\nfrom azureml.core import Experiment\n\npipeline_run1 = Experiment(ws, 'dogs_vs_cats_exp').submit(pipeline1)\npipeline_run1.wait_for_completion()\n<\/code><\/pre>\n\n<p>Once this steps is executed, the experiment fails and I get the following error after a bunch of information -<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"dogs_vs_cats.py\", line 30, in &lt;module&gt;\n    import keras\nModuleNotFoundError: No module named 'keras'\n<\/code><\/pre>\n\n<p>The terminal shows my conda environment set to azureml_py36 and Keras seems be listed in the output of <code>conda list<\/code>.<\/p>\n\n<p>Am I setting up the environment correctly? What is mising <\/p>",
        "Challenge_closed_time":1570089071900,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570082438523,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleNotFoundError while trying to execute a simple Azure ML pipeline with the dogs vs cats data set. The error occurs due to the absence of the 'keras' module, even though it is listed in the output of 'conda list'. The user is unsure if the environment is set up correctly and is seeking assistance to identify the missing element.",
        "Challenge_last_edit_time":1570094136667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58213125",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":25.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1.8426047222,
        "Challenge_title":"ModuleNotFoundError: No module named 'keras' in Azure ML Pipeline",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":697.0,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Poster_created_time":1570078371343,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":3.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>From the way you have specified your environment, it's hard to see if it's a proper RunConfiguration object. If it is, it should be a matter of adding it to you PythonScriptStep.<\/p>\n\n<pre><code>trainStep = PythonScriptStep(\n    script_name=\"dogs_vs_cats.py\",\n    arguments=[\"--input\", blob_input_data, \"--output\", output_data1],\n    inputs=[blob_input_data],\n    outputs=[output_data1],\n    compute_target=compute_target,\n    source_directory=\"..\/dogs-vs-cats\",\n    runconfig=myenv\n)\n<\/code><\/pre>\n\n<p>Right now you're defining the environment, but no using it anywhere it seems. If your trouble persists maybe try defining your RunConfiguration like they do under the \"Specify the environment to run the script\" step in this notebook:<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.7,
        "Solution_reading_time":14.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1608747030727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":105.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":15.1535161111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I'm a beginner in docker and containers and I've been getting this error for days now.\nI get this error when my lambda function runs a sagemaker processing job.\nMy core python file resides in an s3 bucket.\nMy docker image resides in ECR.\nBut I dont understand why I dont get a similar error when I run the same processing job with a python docker image.\nPFB the python docker file that didnt throw any errors.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM python:latest\n#installing dependencies\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\nRUN pip3 install matplotlib\n<\/code><\/pre>\n<p>I only get this error when i run this with a an ubunutu docker image with python3 installed.\nPFB the dockerfile which throws the error mentioned.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>FROM ubuntu:20.04\n\nRUN apt-get update -y\nRUN apt-get install -y python3\nRUN apt-get install -y python3-pip\n\nRUN pip3 install argparse\nRUN pip3 install boto3\nRUN pip3 install numpy==1.19.1\nRUN pip3 install scipy\nRUN pip3 install pandas\nRUN pip3 install scikit-learn\n\nENTRYPOINT [ &quot;python3&quot; ]\n<\/code><\/pre>\n<p>How do I fix this?<\/p>",
        "Challenge_closed_time":1622294378528,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622233871527,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error \"container_linux.go:367: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown\" while running a sagemaker processing job in a docker container. The error occurs when using an Ubuntu docker image with Python3 installed, but not with a Python docker image. The user is seeking a solution to fix this issue.",
        "Challenge_last_edit_time":1622239825870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67745141",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":17.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":16.8075002778,
        "Challenge_title":"Facing this error : container_linux.go:367: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1123.0,
        "Challenge_word_count":205,
        "Platform":"Stack Overflow",
        "Poster_created_time":1608747030727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>Fixed this error by changing the entry point to<\/p>\n<p><strong>ENTRYPOINT [ &quot;\/usr\/bin\/python3.8&quot;]<\/strong><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":1.65,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0780555556,
        "Challenge_answer_count":0,
        "Challenge_body":"This question was resolved and discussed on Polyaxon Slack. Posting for visibility if someone stumbles upon the same issue.\n\nGiven setup:\nDocker-registry provider: Amazon Elastic Container Registry (ECR)\nPolyaxon version: 1.7.3 CE\nDeployed with Kubernetes on AWS\nAnd credentials setup from Kaniko github: Pushing to Amazon ECR\nAnd Kaniko integration in polyaxon-config.yml:\nconnections:\n  - name: docker-registry\n    kind: registry\n    description: \"aws docker repository\"\n    schema:\n      url: https:\/\/ID.dkr.ecr.SOME-REGION.amazonaws.com\n    secret:\n      name: docker-conf\n      mountPath: \/kaniko\/.docker\nAnd polyaxonfile.yml from polyaxon example:\nversion: 1.1\nkind: operation\nname: build\nparams:\n  destination:\n    connection: docker-registry\n    value: polyaxon-examples:ml\nrunPatch:\n  init:\n  - dockerfile:\n      image: \"tensorflow\/tensorflow:2.0.1-py3\"\n      run:\n      - 'pip3 install --no-cache-dir -U polyaxon[\"polyboard\",\"polytune\"]'\n      langEnv: 'en_US.UTF-8'\nhubRef: kaniko\nThen warning was raised:\n\nand job would be stuck like this until manually stopped.\n\nTYPE     STATUS    REASON              MESSAGE                                                           LAST_UPDATE_TIME    LAST_TRANSITION_TIME\n-------  --------  ------------------  ----------------------------------------------------------------  ------------------  ----------------------\nwarning  True      ContainersNotReady  containers with unready status: [polyaxon-main polyaxon-sidecar]  a few seconds ago   a few seconds ago",
        "Challenge_closed_time":1619182673000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619182392000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with Kaniko integration in Polyaxon, where the image push is stuck on ContainersNotReady containers with unready status. The setup includes Amazon Elastic Container Registry (ECR) as the Docker-registry provider, Polyaxon version 1.7.3 CE, and Kubernetes on AWS. The warning message indicates that the job is stuck and cannot proceed until manually stopped.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1296",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":12.5,
        "Challenge_reading_time":19.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.0780555556,
        "Challenge_title":"Image push with Kaniko stuck on ContainersNotReady containers with unready status: [polyaxon-main polyaxon-sidecar]",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":150,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Resolution:\n\nThe issue was with aws-secret type.\n\nAt first we had the type of secret that expires every 12 hours.\nChanging it to one that does not expire allowed successful connection and push of built image.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.6,
        "Solution_reading_time":2.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1579718832727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":11055.1413697222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to install mlflow in R and im getting this error message saying <\/p>\n\n<blockquote>\n  <p>mlflow::install_mlflow()\n  Error in mlflow_conda_bin() :\n    Unable to find conda binary. Is Anaconda installed?\n    If you are not using conda, you can set the environment variable MLFLOW_PYTHON_BIN to the path of yourpython executable.<\/p>\n<\/blockquote>\n\n<p>I have tried the following<\/p>\n\n<pre><code>export MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\" \nsource ~\/.bashrc\necho $MLFLOW_PYTHON_BIN  -&gt; this prints the \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>or in R,<\/p>\n\n<pre><code>sys.setenv(MLFLOW_PYTHON_BIN=\"\/usr\/bin\/python\")\nsys.getenv() -&gt; prints MLFLOW_PYTHON_BIN is set to \/usr\/bin\/python.\n<\/code><\/pre>\n\n<p>however, it still does not work<\/p>\n\n<p>I do not want to use conda environment.<\/p>\n\n<p>how to I get past this error?<\/p>",
        "Challenge_closed_time":1584554585176,
        "Challenge_comment_count":1,
        "Challenge_created_time":1583947052940,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install mlflow in R, which says that the conda binary cannot be found. The user has tried setting the environment variable MLFLOW_PYTHON_BIN to the path of their python executable, but it still does not work. The user does not want to use a conda environment and is seeking a solution to this error.",
        "Challenge_last_edit_time":1584403666972,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60641337",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":168.7589544444,
        "Challenge_title":"mlflow R installation MLFLOW_PYTHON_BIN",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1141.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539211301843,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":117.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>The install_mlflow command only works with conda right now, sorry about the confusing message. You can either:<\/p>\n<ul>\n<li>install conda - this is the recommended way of installing and using mlflow<\/li>\n<\/ul>\n<p>or<\/p>\n<ul>\n<li>install mlflow python package yourself via pip<\/li>\n<\/ul>\n<p>To install mlflow yourself, pip install correct (matching the the R package) python version of mlflow and set the MLFLOW_PYTHON_BIN environment variable as well as MLFLOW_BIN evn variable: e.g.<\/p>\n<pre><code>library(mlflow)\nsystem(paste(&quot;pip install -U mlflow==&quot;, mlflow:::mlflow_version(), sep=&quot;&quot;))\nSys.setenv(MLFLOW_BIN=system(&quot;which mlflow&quot;))\nSys.setenv(MLFLOW_PYTHON_BIN=system(&quot;which python&quot;))\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1624202175903,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":9.75,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1564790214540,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":6.0,
        "Challenge_adjusted_solved_time":52.6982252778,
        "Challenge_answer_count":3,
        "Challenge_body":"<p><strong>SDK version<\/strong>: <code>1.0.43<\/code><\/p>\n\n<p>To minimize clicking and compare accuracy between <code>PipelineRun<\/code>s, I'd like to log a metric from inside a <code>PythonScriptStep<\/code> to the parent <code>PipelineRun<\/code>. I thought I could do this like:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Run\nrun = Run.get_context()\nfoo = 0.80\nrun.parent.log(\"accuracy\",foo)\n<\/code><\/pre>\n\n<p>however I get this error.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Traceback (most recent call last):\n  File \"get_metrics.py\", line 62, in &lt;module&gt;\n    run.parent.log(\"geo_mean\", top3_runs)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/run.py\", line 459, in parent\n    return None if parent_run_id is None else get_run(self.experiment, parent_run_id)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/run.py\", line 1713, in get_run\n    return next(runs)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/run.py\", line 297, in _rehydrate_runs\n    yield factory(experiment, run_dto)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/run.py\", line 325, in _from_dto\n    return PipelineRun(experiment=experiment, run_id=run_dto.run_id)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/run.py\", line 74, in __init__\n    service_endpoint=_service_endpoint)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/_graph_context.py\", line 46, in __init__\n    service_endpoint=service_endpoint)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/_aeva_provider.py\", line 118, in create_provider\n    service_endpoint=service_endpoint)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/_aeva_provider.py\", line 133, in create_service_caller\n    service_endpoint = _AevaWorkflowProvider.get_endpoint_url(workspace, experiment_name)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/pipeline\/core\/_aeva_provider.py\", line 153, in get_endpoint_url\n    workspace_name=workspace.name, workspace_id=workspace._workspace_id)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py\", line 749, in _workspace_id\n    self.get_details()\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py\", line 594, in get_details\n    self._subscription_id)\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/_project\/_commands.py\", line 507, in show_workspace\n    AzureMachineLearningWorkspaces, subscription_id).workspaces,\n  File \"\/azureml-envs\/azureml_ffecfef6fbfa1d89f72d5af22e52c081\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py\", line 112, in _get_service_client\n    all_subscription_list, tenant_id = self._get_all_subscription_ids()\nTypeError: 'NoneType' object is not iterable\n<\/code><\/pre>\n\n<h3>Update<\/h3>\n\n<p>On further investigation, I tried just printing the <code>parent<\/code> attribute of the run with the line below and got the same <a href=\"https:\/\/gist.github.com\/swanderz\/13a33babf8b09ff89791bdfa66e3148a\" rel=\"nofollow noreferrer\">Traceback<\/a><\/p>\n\n<p><code>print(\"print run parent attribute\", run.parent)<\/code><\/p>\n\n<p>The <code>get_properties()<\/code> method the below. I'm guessing that azureml just uses the <code>azureml.pipelinerunid<\/code> property for pipeline tree hierarchy, and that the <code>parent<\/code> attribute has been left for any user-defined hierarchies.<\/p>\n\n<pre><code>{\n    \"azureml.runsource\": \"azureml.StepRun\",\n    \"ContentSnapshotId\": \"45bdecd3-1c43-48da-af5c-c95823c407e0\",\n    \"StepType\": \"PythonScriptStep\",\n    \"ComputeTargetType\": \"AmlCompute\",\n    \"azureml.pipelinerunid\": \"e523d575-c373-46d2-a4bc-1717f5e34ec2\",\n    \"_azureml.ComputeTargetType\": \"batchai\",\n    \"AzureML.DerivedImageName\": \"azureml\/azureml_dfd7f4f952ace529f986fe919909c3ec\"\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1568685167387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1568330937317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to log a metric from inside a PythonScriptStep to the parent PipelineRun to minimize clicking and compare accuracy between PipelineRuns. However, they are encountering an error when trying to do so and even printing the parent attribute of the run results in the same error. The user suspects that AzureML uses the azureml.pipelinerunid property for pipeline tree hierarchy and that the parent attribute has been left for any user-defined hierarchies.",
        "Challenge_last_edit_time":1568495453776,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57915603",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":17.7,
        "Challenge_reading_time":59.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":98.3972416667,
        "Challenge_title":"log metric from inside PythonScriptStep to parent PipelineRun",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":363.0,
        "Challenge_word_count":265,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405457120427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Solution_body":"<p>Please upgrade your SDK to the latest version. Seems like this issue was fixed sometime after 1.0.43.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":1.35,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1448655975827,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1478.0,
        "Answerer_view_count":135.0,
        "Challenge_adjusted_solved_time":1.6806736111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to train a custom ML model with SageMaker. The model is written in Python and should be shipped to SageMaker in a Docker image. Here is a simplified version of my Dockerfile (the model sits in the train.py file):<\/p>\n\n<pre><code>FROM amazonlinux:latest\n\n# Install Python 3\nRUN yum -y update &amp;&amp; yum install -y python3-pip python3-devel gcc &amp;&amp; yum clean all\n\n# Install sagemaker-containers (the official SageMaker utils package)\nRUN pip3 install --target=\/usr\/local\/lib\/python3.7\/site-packages sagemaker-containers &amp;&amp; rm -rf \/root\/.cache\n\n# Bring the script with the model to the image \nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n\n<p>Now, if I initialize this image as a SageMaker estimator and then run the <code>fit<\/code> method on this estimator I get the following error:<\/p>\n\n<p>\"AlgorithmError: CannotStartContainerError. Please make sure the container can be run with 'docker run  train'.\"<\/p>\n\n<p>In other words: SageMaker is not able to get into the container and run the train.py file. But why? The way I am specifying the entrypoint with <code>ENV SAGEMAKER_PROGRAM train.py<\/code> is recommended in the <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\/blob\/master\/README.rst\" rel=\"nofollow noreferrer\">docs of the sagemaker-containers package<\/a> (see 'How a script is executed inside the container').<\/p>",
        "Challenge_closed_time":1585671760392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1585665709967,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"CannotStartContainerError\" while trying to train a custom ML model with SageMaker. The model is written in Python and is shipped to SageMaker in a Docker image. The user has specified the entrypoint with \"ENV SAGEMAKER_PROGRAM train.py\" as recommended in the sagemaker-containers package documentation. However, SageMaker is not able to get into the container and run the train.py file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60953289",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1.6806736111,
        "Challenge_title":"SageMaker gives CannotStartContainerError although I specified an entrypoint",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1133.0,
        "Challenge_word_count":189,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448655975827,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1478.0,
        "Poster_view_count":135.0,
        "Solution_body":"<p>I found a hint in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">the AWS docs<\/a> and came up with this solution:<\/p>\n\n<pre><code>ENTRYPOINT [\"python3.7\", \"\/opt\/ml\/code\/train.py\"]\n<\/code><\/pre>\n\n<p>With this the container <a href=\"https:\/\/docs.docker.com\/engine\/reference\/builder\/#entrypoint\" rel=\"nofollow noreferrer\">will run as an executable<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.3,
        "Solution_reading_time":5.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1221528724667,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"West Coast, North America",
        "Answerer_reputation_count":11340.0,
        "Answerer_view_count":737.0,
        "Challenge_adjusted_solved_time":1955.6086208333,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using custom algorithm running shipped with Docker image on p2 instance with AWS Sagemaker (a bit similar to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>)<\/p>\n\n<p>At the end of training process, I try to write down my model to output directory, that is mounted via Sagemaker (like in tutorial), like this:<\/p>\n\n<pre><code>model_path = \"\/opt\/ml\/model\"\nmodel.save(os.path.join(model_path, 'model.h5'))\n<\/code><\/pre>\n\n<p>Unluckily, apparently the model gets too big with time and I get the\nfollowing error:<\/p>\n\n<blockquote>\n  <p>RuntimeError: Problems closing file (file write failed: time = Thu Jul\n  26 00:24:48 2018<\/p>\n  \n  <p>00:24:49 , filename = 'model.h5', file descriptor = 22, errno = 28,\n  error message = 'No space left on device', buf = 0x1a41d7d0, total\n  write[...]<\/p>\n<\/blockquote>\n\n<p>So all my hours of GPU time are wasted. How can I prevent this from happening again? Does anyone know what is the size limit for model that I store on Sagemaker\/mounted directories?<\/p>",
        "Challenge_closed_time":1539631654852,
        "Challenge_comment_count":1,
        "Challenge_created_time":1532591463817,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while using a custom algorithm running on a Docker image on a p2 instance with AWS Sagemaker. At the end of the training process, the user tried to save the model to the output directory, which was mounted via Sagemaker, but the model got too big with time and the user received a \"No space left on device\" error. The user is seeking advice on how to prevent this from happening again and what the size limit is for models stored on Sagemaker\/mounted directories.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51533650",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.3,
        "Challenge_reading_time":16.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1955.6086208333,
        "Challenge_title":"No space left on device in Sagemaker model training",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2721.0,
        "Challenge_word_count":144,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366408339740,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":8057.0,
        "Poster_view_count":491.0,
        "Solution_body":"<p>When you train a model with <code>Estimators<\/code>, it <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/estimators.html\" rel=\"nofollow noreferrer\">defaults to 30 GB of storage<\/a>, which may not be enough. You can use the <code>train_volume_size<\/code> param on the constructor to increase this value. Try with a large-ish number (like 100GB) and see how big your model is. In subsequent jobs, you can tune down the value to something closer to what you actually need.<\/p>\n\n<p>Storage costs <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">$0.14 per GB-month of provisioned storage<\/a>. Partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":8.7,
        "Solution_reading_time":9.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":97.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.4081172222,
        "Challenge_answer_count":1,
        "Challenge_body":"The CloudWatch logs show following error\n```\n    Traceback (most recent call last):\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_scheduler\/handlers.py\", line 194, in post\n        job_id = await ensure_async(self.scheduler.create_job(CreateJob(**payload)))\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_server\/utils.py\", line 182, in ensure_async\n        result = await obj\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 109, in wrapper\n        raise excep\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 105, in wrapper\n        return await func(*args, **kwargs)\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 210, in create_job\n        s3_file_uploader = await self._prepare_job_artifacts(\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 168, in _prepare_job_artifacts\n        input_uri = S3URI(runtime_environment_parameters.s3_input)\n      File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/runtime_environment_parameters.py\", line 40, in s3_input\n        return self.parameters.get(RuntimeEnvironmentParameterName.S3_INPUT.value)\n    AttributeError: 'NoneType' object has no attribute 'get'\n[E 2023-03-16 15:36:01.070 SchedulerApp] 'NoneType' object has no attribute 'get' Traceback (most recent call last): File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_scheduler\/handlers.py\", line 194, in post job_id = await ensure_async(self.scheduler.create_job(CreateJob(**payload))) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/jupyter_server\/utils.py\", line 182, in ensure_async result = await obj File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 109, in wrapper raise excep File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/logging.py\", line 105, in wrapper return await func(*args, **kwargs) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 210, in create_job s3_file_uploader = await self._prepare_job_artifacts( File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/scheduler.py\", line 168, in _prepare_job_artifacts input_uri = S3URI(runtime_environment_parameters.s3_input) File \"\/opt\/conda\/envs\/studio\/lib\/python3.9\/site-packages\/sagemaker_scheduling\/runtime_environment_parameters.py\", line 40, in s3_input return self.parameters.get(RuntimeEnvironmentParameterName.S3_INPUT.value) AttributeError: 'NoneType' object has no attribute 'get'\n```\nAlso all the \"advanced options\" are missing in the \"create notebook job\" dialogue.\n\nThe setting is isolated VPC with permissions updated accordingly: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/scheduled-notebook-policies.html\n\nThe VPC endpoints for S3, SageMaker API and Runtime, SSM, STS, Metrics, Logs, ECR API and ECR DKR are deployed in the VPC. Notebooks are working fine.\n\nAny idea what could be wrong?",
        "Challenge_closed_time":1678987295300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678982226078,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create a notebook job in Sagemaker Studio in an isolated VPC. The CloudWatch logs show an AttributeError: 'NoneType' object has no attribute 'get'. Additionally, the \"advanced options\" are missing in the \"create notebook job\" dialogue. The VPC endpoints for S3, SageMaker API and Runtime, SSM, STS, Metrics, Logs, ECR API and ECR DKR are deployed in the VPC.",
        "Challenge_last_edit_time":1679330199392,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbLVvWJbeS42j0e1nymPYcg\/sagemaker-studio-in-vpc-only-fails-to-create-notebook-job-with-unexpected-error-occurred-during-creation-of-job",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.2,
        "Challenge_reading_time":42.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":1.4081172222,
        "Challenge_title":"Sagemaker Studio in VPC Only fails to create Notebook job with \"Unexpected error occurred during creation of job.\"",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":184.0,
        "Challenge_word_count":235,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for reporting the issue. \n\nCan you try also configuring following two vpc endpoints? \n1. Amazon EC2\n2. Amazon EventBridge\nAlso, if you have s3 vpc gateway endpoint policy with fine control of allowed s3 bucet, please allow s3 access for sagemakerheadlessexecution-prod-* like below. \nFYI - you can find more reference link from https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-notebook-auto-execution-advanced.html\n\n```\n{\n   \"Action\":[\n      \"s3:*\"\n   ],\n   \"Resource\":[\n      \"arn:aws:s3:::sagemakerheadlessexecution-prod-*\",\n      \"arn:aws:s3:::sagemakerheadlessexecution-prod-*\/*\"\n   ],\n   \"Effect\":\"Allow\",\n   \"Sid\":\"SCTASK14554266\"\n}\n```",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1678987295300,
        "Solution_link_count":1.0,
        "Solution_readability":15.5,
        "Solution_reading_time":8.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1636044845360,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":241.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":2833.1668325,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook.<\/p>\n<p>After having prepared the correct Dockerfile and the URI where to push the image that contains my train.py script, I try to push the image directly in a notebook cell.<\/p>\n<p>The exact command I try to execute is: <code>!docker build .\/ -t $IMAGE_URI<\/code>, where IMAGE_URI is the environmental variable previously defined. However I try to run this command I get the error: <code>\/bin\/bash: docker: command not found<\/code>. I also tried to execute it with the magic cell %%bash, importing the subprocess library and also execute the command stored in a .sh file.<\/p>\n<p>Unfortunately none of the above solutions work, they all return the same <strong>command not found<\/strong> error with code 127.<\/p>\n<p>If instead I run the command from a bash present in the Jupyterlab it works fine as expected.<\/p>\n<p>Is there any workaround to make the push execute inside the jupyter notebook? I was trying to keep the whole custom training process inside the same notebook.<\/p>",
        "Challenge_closed_time":1661350818332,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651142988007,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook. However, when trying to execute the command \"!docker build .\/ -t $IMAGE_URI\" in a notebook cell, the user gets the error \"\/bin\/bash: docker: command not found\". The user has tried various solutions, including using the magic cell %%bash and importing the subprocess library, but none of them work. The command works fine when executed from a bash present in the Jupyterlab. The user is looking for a workaround to make the push execute inside the jupyter notebook.",
        "Challenge_last_edit_time":1651151719750,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72042363",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":2835.5084236111,
        "Challenge_title":"Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":248.0,
        "Challenge_word_count":190,
        "Platform":"Stack Overflow",
        "Poster_created_time":1648140384823,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>If you follow this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/create-user-managed-notebooks-instance-console-quickstart\" rel=\"nofollow noreferrer\">guide<\/a> to create a user-managed notebook from Vertex AI workbench and select Python 3, then it comes with Docker available.<\/p>\n<p>So you will be able to use Docker commands such as <code>! docker build .<\/code> inside the user-managed notebook.<\/p>\n<p>Example:\n<a href=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" alt=\"Vertex AI Managed Notebook\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1661351120347,
        "Solution_link_count":3.0,
        "Solution_readability":12.3,
        "Solution_reading_time":8.08,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":57.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1433841188323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation_count":22467.0,
        "Answerer_view_count":2692.0,
        "Challenge_adjusted_solved_time":16.1130741667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Similar question as <a href=\"https:\/\/stackoverflow.com\/questions\/43176442\/install-r-packages-in-azure-ml\">here<\/a> but now on Python packages. Currently, the CVXPY is missing in Azure ML. I am also trying to get other solvers such as GLPK, CLP and COINMP working in Azure ML.<\/p>\n<p><strong>How can I install Python packages in Azure ML?<\/strong><\/p>\n<hr \/>\n<p><em>Update about trying to install the Python packages not found in Azure ML.<\/em><\/p>\n<blockquote>\n<p>I did as instructed by Peter Pan but I think the 32bits CVXPY files are wrong for the Anaconda 4 and Python 3.5 in Azure ML, logs and errors are <a href=\"https:\/\/pastebin.com\/zN5QrPtL\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<pre><code>[Information]         Running with Python 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)]\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rS0Us.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6qz3p.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9glSm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9glSm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/blockquote>\n<p><em>Update 2 with win_amd64 files (paste <a href=\"https:\/\/pastebin.com\/tisWuP5C\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code>[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         cvxopt-1.1.9-cp35-cp35m-win_amd64.whl          2017-06-07 01:03:34      1972074\n[Information]         __MACOSX\/                                      2017-06-07 01:26:28            0\n[Information]         __MACOSX\/._cvxopt-1.1.9-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:34          452\n[Information]         cvxpy-0.4.10-py3-none-any.whl                  2017-06-07 00:25:36       300880\n[Information]         __MACOSX\/._cvxpy-0.4.10-py3-none-any.whl       2017-06-07 00:25:36          444\n[Information]         ecos-2.0.4-cp35-cp35m-win_amd64.whl            2017-06-07 01:03:40        56522\n[Information]         __MACOSX\/._ecos-2.0.4-cp35-cp35m-win_amd64.whl 2017-06-07 01:03:40          450\n[Information]         numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl   2017-06-07 01:25:02    127909457\n[Information]         __MACOSX\/._numpy-1.13.0rc2+mkl-cp35-cp35m-win_amd64.whl 2017-06-07 01:25:02          459\n[Information]         scipy-0.19.0-cp35-cp35m-win_amd64.whl          2017-06-07 01:05:12     12178932\n[Information]         __MACOSX\/._scipy-0.19.0-cp35-cp35m-win_amd64.whl 2017-06-07 01:05:12          452\n[Information]         scs-1.2.6-cp35-cp35m-win_amd64.whl             2017-06-07 01:03:34        78653\n[Information]         __MACOSX\/._scs-1.2.6-cp35-cp35m-win_amd64.whl  2017-06-07 01:03:34          449\n[Information]         [ READING ] 0:00:00\n[Information]         Input pandas.DataFrame #1:\n[Information]         Empty DataFrame\n[Information]         Columns: [1]\n[Information]         Index: []\n[Information]         [ EXECUTING ] 0:00:00\n[Information]         [ WRITING ] 0:00:00\n<\/code><\/pre>\n<p>where <code>import cvxpy<\/code>, <code>import cvxpy-0.4.10-py3-none-any.whl<\/code> or <code>cvxpy-0.4.10-py3-none-any<\/code> do not work so<\/p>\n<p><strong>How can I use the following wheel files downloaded from <a href=\"http:\/\/www.lfd.uci.edu\/%7Egohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\">here<\/a> to use the external Python packages not found in Azure ML?<\/strong><\/p>\n<\/blockquote>\n<p><em>Update about permission problem about importing cvxpy (paste <a href=\"https:\/\/pastebin.com\/3kTKgLfc\" rel=\"nofollow noreferrer\">here<\/a>)<\/em><\/p>\n<blockquote>\n<pre><code> [Error]         ImportError: No module named 'canonInterface'\n<\/code><\/pre>\n<p>where the ZIP Bundle is organised a bit differently, the content of each wheel downloaded to a folder and the content having all zipped as a ZIP Bundle.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1496732346280,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496674339213,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user is trying to install Python packages such as CVXPY, GLPK, CLP, and COINMP in Azure ML but is encountering errors. They have tried installing 32-bit CVXPY files but it did not work. They have also downloaded win_amd64 files but are unsure how to use them. The user is seeking guidance on how to use the downloaded wheel files to install external Python packages not found in Azure ML. Additionally, they are facing a permission problem while importing cvxpy.",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44371692",
        "Challenge_link_count":11,
        "Challenge_participation_count":3,
        "Challenge_readability":10.2,
        "Challenge_reading_time":49.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":16.1130741667,
        "Challenge_title":"Install Python Packages in Azure ML?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":12625.0,
        "Challenge_word_count":346,
        "Platform":"Stack Overflow",
        "Poster_created_time":1251372839052,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":48616.0,
        "Poster_view_count":3348.0,
        "Solution_body":"<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts#limitations\" rel=\"nofollow noreferrer\"><code>Limitations<\/code><\/a> and <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of <code>Execute Python Script<\/code> tutorial, the only way to add custom Python modules is via the zip file mechanism to package the modules and all dependencies.<\/p>\n\n<p>For example to install <code>CVXPY<\/code>, as below.<\/p>\n\n<ol>\n<li>Download the wheel file of <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxpy\" rel=\"nofollow noreferrer\"><code>CVXPY<\/code><\/a> and its dependencies like <a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#cvxopt\" rel=\"nofollow noreferrer\"><code>CVXOPT<\/code><\/a>.<\/li>\n<li>Decompress these wheel files, and package these files in the path <code>cvxpy<\/code> and <code>cvxopt<\/code>, etc as a zipped file with your script.<\/li>\n<li>Upload the zip file as a dataset and use it as the script bundle.<\/li>\n<\/ol>\n\n<p>If you were using IPython, you also can try to install the Python Package via the code <code>!pip install cvxpy<\/code>.<\/p>\n\n<p>And there are some similar SO threads which may be helpful for you, as below.<\/p>\n\n<ol>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/44285641\/azure-ml-python-with-script-bundle-cannot-import-module\">Azure ML Python with Script Bundle cannot import module<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/8663046\/how-to-install-a-python-package-from-within-ipython\">How to install a Python package from within IPython?<\/a><\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>\n\n<hr>\n\n<p>Update:<\/p>\n\n<p>For IPython interface of Azure ML, you move to the <code>NOTEBOOKS<\/code> tab to create a notebook via <code>ADD TO PROJECT<\/code> button at the bottom of the page, as the figure below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/X2Asv.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Or you can directly login to the website <code>https:\/\/notebooks.azure.com<\/code> to use it.<\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1496760284030,
        "Solution_link_count":9.0,
        "Solution_readability":10.8,
        "Solution_reading_time":28.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":215.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1622117284230,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":316.0,
        "Answerer_view_count":40.0,
        "Challenge_adjusted_solved_time":21.7817686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running this notebook in my managed notebooks environment on Google Cloud and I'm getting the following error when trying to install the packages: &quot;WARNING: The script google-oauthlib-tool is installed in '\/home\/jupyter\/.local\/bin' which is not on PATH.\nConsider adding this directory to PATH.&quot;<\/p>\n<p>Here is the python code that I'm trying to run for reference. <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb<\/a><\/p>\n<p>Any suggestions on how I can update the package installation so it is on path and resolve the error? I'm currently working on GCP user-managed notebooks on a Mac.<\/p>\n<p>Thanks so much for any tips!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>",
        "Challenge_closed_time":1658596861407,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658518447040,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to install packages in their managed notebooks environment on Google Cloud. The error message suggests that the library is not installed on PATH and recommends adding the directory to PATH. The user is seeking suggestions on how to update the package installation to resolve the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73085293",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":12.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":21.7817686111,
        "Challenge_title":"Library is not installed on PATH - How can I install on path?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":52.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Poster_created_time":1621620820567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Open up your shell config file (likely .zshrc because the default shell on Mac is now zsh and that's the name of the zsh config file) located at your home directory in a text editor (TextEdit, etc) and add the path to the  executable.\nLike this:\nOpen the file:\n<code>open -e ~\/.zshrc<\/code>\nEdit the file:\nAdd this line at the top (may vary, check the documentation):\n<code>export PATH=&quot;\/home\/jupyter\/.local\/bin&quot;<\/code>\nThat may not work, try this:\n<code>export PATH=&quot;$PATH:\/home\/jupyter\/.local\/bin&quot;<\/code>\nYour best bet is to read the package documentation.<\/p>\n<p>After saving the config file, run <code>source ~\/.zshrc<\/code> and replace .zshrc with the config file name if it's different OR open a new terminal tab.<\/p>\n<p>What this does is tells the shell that the command exists and where to find it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":5.8,
        "Solution_reading_time":10.39,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":126.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.9591811111,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm following [this guide](https:\/\/aws.amazon.com\/blogs\/machine-learning\/migrate-your-work-to-amazon-sagemaker-notebook-instance-with-amazon-linux-2\/) for transitioning to Amazon Linux 2 provided by AWS \n\nI've set up the two needed lifecycle configurations and created a new S3 Bucket to store the backup. I've also ensured the IAM roles have the required S3 permissions and updated the notebook with the ebs-backup-bucket tag per the instructions.\n\nWhen I run the notebook with the new configuration I get the following error: \n\"Notebook Instance Lifecycle Config [LIFECYCLE ARN] for Notebook Instance [NOTEBOOK ARN] took longer than 5 minutes. Please check your CloudWatch logs for more details if your Notebook Instance has Internet access.\n\nLooking at the logs I get the error: \n`\/bin\/bash: \/tmp\/OnStart_2022-11-09-01-51ontlqcqt: \/bin\/bash^M: bad interpreter: No such file or directory`\n\nAny thoughts on how to resolve this issue? The code for the backup lifecycle configuration can be found [here](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/migrate-ebs-data-backup\/on-start.sh)",
        "Challenge_closed_time":1667972357492,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667961704440,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a directory error when running SageMaker backup-ebs lifecycle for Amazon Linux 2 transition. They have followed the guide provided by AWS, set up the required configurations, and ensured the IAM roles have the necessary permissions. However, when running the notebook with the new configuration, they receive an error message indicating that the Notebook Instance Lifecycle Config took longer than 5 minutes. Upon checking the logs, they found a bad interpreter error. The user is seeking advice on how to resolve this issue.",
        "Challenge_last_edit_time":1668335812740,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUPsaLux6EQcqvHW1HtNlkIw\/directory-error-when-running-sagemaker-backup-ebs-lifecycle-for-amazon-linux-2-transition",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.1,
        "Challenge_reading_time":15.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2.9591811111,
        "Challenge_title":"Directory Error when running SageMaker backup-ebs lifecycle for Amazon Linux 2 transition",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":146,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"The extra `^M` symbol (i.e. Ctrl-M) stopped the whole scrip from being interpreted properly. \n\nThis issue is normally seen in scripts prepared in MSDOS\/Windows based system but used in Linux system due to difference of line endings.\n\nIn Unix based OS, lines end with `\\n` but MSDOS\/Win based system ends with `\\r\\n`\n\nIn Linux based system, you could show your prepared scripts by running\n```\ncat -e some-script.sh \n``` \nThe results would be something similar to\n```\n#!\/bin\/bash^M$\n... ...^M$\n```\n`$` is normal Unix end-of-line symbol. Windows uses an extra one `^M` and this symbol is not recognized by Unix system. That's why, in SageMaker Notebook Lifecycle Configuration, which is running Linux, your script was interpreted as `\/bin\/bash^M`\n\n\nTo mitigate the issue, please convert the scripts to Unix based ending and update life cycle configuration. To achieve this, you could use `Notepad++` in Windows. You can go to the `Edit` menu, select the `EOL Conversion` submenu, and from the options that come up select `UNIX\/OSX Format`. The next time you save the file, its line endings will, all going well, be saved with UNIX-style line endings. \n\nAlternatively, you could put the script in a Linux environment, e.g. EC2 instance with Amazon Linux 2 and install `dos2unix` via `sudo yum install dos2unix`. After installation, you could convert your files via\n```\ndos2unix -n file.sh  output.sh \n```\nAfter the conversion, please update LCC with the new scripts. You could verify that `^M` has been removed via \n```\ncat -e your_script.sh\n```\nThe output will print all special characters directly without hiding.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1668093487360,
        "Solution_link_count":0.0,
        "Solution_readability":7.8,
        "Solution_reading_time":19.67,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":250.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1615216681047,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":43.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":0.174105,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to create a local environment for the ML Studio using the Python SDK, following\n<a href=\"https:\/\/azure.github.io\/azureml-cheatsheets\/docs\/cheatsheets\/python\/v1\/environment\/\" rel=\"nofollow noreferrer\">this official cheatsheet<\/a>. The result should be a conda-like environment that can be used for local testing. However, I am running into an error when importing the Numpy package with the <code>add_conda_package()<\/code> method of the <code>CondaDependencies()<\/code> class. Where I've tried not specifying, as well as specifying package versions, like:\n<code>add_conda_package('numpy')<\/code> or <code>add_conda_package('numpy=1.21.2')<\/code>, but it does not seem to make a difference.<\/p>\n<p>Numpy's error message is extensive, and I've tried many of the suggestions, without success nonetheless. I'm grateful for any tips on what might resolve my issues!<\/p>\n<hr \/>\n<h2>Full code<\/h2>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\ndef get_env() -&gt; Environment:\n    conda = CondaDependencies()\n\n    # add channels\n    conda.add_channel('defaults')\n    conda.add_channel('conda-forge')\n    conda.add_channel('pytorch')\n\n    # Python\n    conda.add_conda_package('python=3.8')\n\n    # Other conda packages\n    conda.add_conda_package('cudatoolkit=11.3')\n    conda.add_conda_package('pip')\n    conda.add_conda_package('python-dateutil')\n    conda.add_conda_package('python-dotenv')\n    conda.add_conda_package('pytorch=1.10')\n    conda.add_conda_package('torchaudio')\n    conda.add_conda_package('torchvision')\n    conda.add_conda_package('wheel')\n    conda.add_conda_package('numpy=1.21.2') # &lt;--- Error with this import \n\n    # create environment\n    env = Environment('test_env')\n    env.python.conda_dependencies = conda\n\n    return env\n<\/code><\/pre>\n<hr \/>\n<h2>Detailed error message:<\/h2>\n<p>User program failed with ImportError:<\/p>\n<p>IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!<\/p>\n<p>Importing the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.<\/p>\n<p>We have compiled some common reasons and troubleshooting tips at:<\/p>\n<pre><code>https:\/\/numpy.org\/devdocs\/user\/troubleshooting-importerror.html\n<\/code><\/pre>\n<p>Please note and check the following:<\/p>\n<ul>\n<li>The Python version is: Python3.8 from &quot;&lt;LOCAL_DIR&gt;.azureml\\envs\\azureml_&gt;\\python.exe&quot;<\/li>\n<li>The NumPy version is: &quot;1.19.1&quot;<\/li>\n<\/ul>\n<p>and make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.<\/p>\n<p>Original error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.<\/p>\n<hr \/>\n<h2>System specifications:<\/h2>\n<ul>\n<li>Local OS: Windows 10<\/li>\n<li>ML studio OS: Linux Ubuntu 18<\/li>\n<li>Python version: 3.8<\/li>\n<\/ul>",
        "Challenge_closed_time":1637743033888,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637742407110,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to create a local environment for the ML Studio using the Python SDK. They are facing an error when importing the Numpy package with the add_conda_package() method of the CondaDependencies() class. The error message suggests that the import of the Numpy C-extensions has failed, and the user has tried various solutions without success. The user has provided their system specifications, including the local OS, ML studio OS, and Python version.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70092793",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.7,
        "Challenge_reading_time":38.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":0.174105,
        "Challenge_title":"Azure ML Studio Local Environment \u2014 Numpy package import failure using the Azure ML Python SDK",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":135.0,
        "Challenge_word_count":302,
        "Platform":"Stack Overflow",
        "Poster_created_time":1615216681047,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":43.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>I was finally able to resolve the issue by using the pip method instead of the conda method:\n<code>add_pip_package('numpy')<\/code> instead of <code>add_conda_package('numpy')<\/code>\nI can imagine this being the reason for other packages as well.<\/p>\n<hr \/>\n<h2>Full solution<\/h2>\n<pre><code>from azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n\ndef get_env() -&gt; Environment:\n    conda = CondaDependencies()\n\n    # add channels\n    conda.add_channel('defaults')\n    conda.add_channel('conda-forge')\n    conda.add_channel('pytorch')\n\n    # Python\n    conda.add_conda_package('python=3.8')\n\n    # Other conda packages\n    conda.add_conda_package('cudatoolkit=11.3')\n    conda.add_conda_package('pip')\n    conda.add_conda_package('python-dateutil')\n    conda.add_conda_package('python-dotenv')\n    conda.add_conda_package('pytorch=1.10')\n    conda.add_conda_package('torchaudio')\n    conda.add_conda_package('torchvision')\n    conda.add_conda_package('wheel')\n    #conda.add_conda_package('numpy=1.21.2') # &lt;--- Error with this import \n\n    # Add pip packages\n    conda.add_pip_package('numpy') # &lt;--- Fixes import error\n\n    # create environment\n    env = Environment('test_env')\n    env.python.conda_dependencies = conda\n\n    return env\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":22.6,
        "Solution_reading_time":16.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":92.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1589293508567,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been trying to send a train job through azure ml python sdk with:<\/p>\n<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig \n\nif __name__ == &quot;__main__&quot;:\n    ws = Workspace.from_config()\n    experiment = Experiment(workspace=ws, name='ConstructionTopicsModel')\n\n    config = ScriptRunConfig(source_directory='.\/',\n                         script='src\/azureml\/train.py',\n                         arguments=None,\n                         compute_target='ComputeTargetName',\n                         )\n\n    env = ws.environments['test-env']\n    config.run_config.environment = env\n    run = experiment.submit(config)\n    \n    run.wait_for_completion(show_output=True)\n\n    aml_url = run.get_portal_url()\n    print(aml_url)\n<\/code><\/pre>\n<p>But I was getting the <code>ServiceError<\/code> message:<\/p>\n<pre><code>AzureMLCompute job failed. FailedLoginToImageRegistry: Unable to login to docker image repo\nReason: Failed to login to the docker registry\nerror: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nserviceURL: 7ac86b04d6564d36aa80ae2ad090582c.azurecr.io\nReason: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.\n<\/code><\/pre>\n<p>I also tried using the azure cli without success, same error message<\/p>",
        "Challenge_closed_time":1643645330912,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643645330913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue while trying to send a train job through Azure ML Python SDK. The job failed with a \"FailedLoginToImageRegistry\" error message, indicating that the user was unable to login to the docker image repo. The error message also suggested using \"--password-stdin\" instead of \"--password\" via the CLI. The user also tried using the Azure CLI but encountered the same error message.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70929123",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.2,
        "Challenge_reading_time":20.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureMLCompute job failed with `FailedLoginToImageRegistry`",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":202.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589293508567,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>The only way I've found so far to make this work, was to run it on a terminal of the compute-target itself. That's how the docker error goes away. Trying to run the experiment from a terminal of a different compute instance raises the exception.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":3.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1510046220943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":292.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":48.7532733334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a sagemaker instance up and running and I have a few libraries that I frequently use with it but each time I restart the instance they get wiped and I have to reinstall them. Is it possible to install my libraries to one of the anaconda environments and have the change remain?<\/p>",
        "Challenge_closed_time":1530555566567,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530380054783,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge with AWS Sagemaker instance where the libraries installed get wiped out every time the instance is restarted. The user is seeking a solution to install the libraries to one of the anaconda environments and make the change persist.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51117133",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":4.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":13.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":48.7532733334,
        "Challenge_title":"AWS Sagemaker - Install External Library and Make it Persist",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":10578.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Poster_created_time":1490229156263,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":401.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>The supported way to do this for Sagemaker notebook instances is with <strong>Lifecycle Configurations<\/strong>.<\/p>\n\n<p>You can create an <strong>onStart<\/strong> lifecycle hook that can install the required packages into the respective Conda environments each time your notebook instance starts.<\/p>\n\n<p>Please see the following blog post for more details<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":27.5,
        "Solution_reading_time":10.15,
        "Solution_score_count":12.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1317052342823,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Franklin, TN",
        "Answerer_reputation_count":8183.0,
        "Answerer_view_count":727.0,
        "Challenge_adjusted_solved_time":0.9585202778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When trying to upload a custom R module to Azure Machine Learning Studio what causes the following error.<\/p>\n\n<blockquote>\n  <p>[ModuleOutput]<\/p>\n<\/blockquote>\n\n<pre><code>\"ErrorId\":\"BuildCustomModuleFailed\",\"ErrorCode\":\"0114\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 0114: Custom module build failed with error(s): An item with the same key has already been added.\"}} [ModuleOutput] Error: Error 0114: Custom module build failed with error(s): An item with the same key has already been added. \n<\/code><\/pre>\n\n<p>I have tried renaming the module so a name that does not exists.<\/p>",
        "Challenge_closed_time":1496847821796,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496847821797,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error (0114) while trying to upload a custom R module to Azure Machine Learning Studio, which states that the custom module build has failed due to an item with the same key already being added. The user has attempted to rename the module to a non-existing name but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44416344",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":9.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio Custom Module Upload Error 0114 : An item with the same key has already been added",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":75.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Poster_created_time":1317052342823,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Franklin, TN",
        "Poster_reputation_count":8183.0,
        "Poster_view_count":727.0,
        "Solution_body":"<p>The duplicate key exception is a red herring. <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn962112.aspx\" rel=\"nofollow noreferrer\" title=\"MSDN Module Error Code 0114\">Build error 0114<\/a> is a general error that occurs if there is a system exception while building the custom module. The real issue my module was compressed using the built in compress folder option in the Mac Finder. To fix this compress the file using the command line interface for <code>zip<\/code> in Terminal in the following very specific manner.<\/p>\n\n<blockquote>\n  <p>The following example:<\/p>\n<\/blockquote>\n\n<pre><code>cd ScoredDatasetMetadata\/\nzip ScoredDatasetMetadata *\nmv ScoredDatasetMetadata.zip ..\/\n<\/code><\/pre>\n\n<p>Builds a zip file with the correct file structure.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1496851272470,
        "Solution_link_count":1.0,
        "Solution_readability":11.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1348171784712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":93.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":105.8507644444,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I use PyTorch estimator with SageMaker to train\/fine-tune my Graph Neural Net on multi-GPU machines.<\/p>\n<p>The <code>requirements.txt<\/code> that gets installed into the Estimator container, has lines like:<\/p>\n<pre><code>torch-scatter -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\ntorch-sparse -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\ntorch-cluster -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\ntorch-spline-conv -f https:\/\/data.pyg.org\/whl\/torch-1.10.0+cu113.html\n<\/code><\/pre>\n<p>When SageMaker installs these requirements in the Estimator on the endpoint, it takes ~<strong>2 hrs<\/strong> to build the wheel. It takes only seconds on a local Linux box.<\/p>\n<p>SageMaker Estimator:<\/p>\n<p>PyTorch v1.10\nCUDA 11.x\nPython 3.8\nInstance: ml.p3.16xlarge<\/p>\n<p>I have noticed the same issue with other wheel-based components that require CUDA.<\/p>\n<p>I have also tried building a Docker container on p3.16xlarge and running that on SageMaker, but it was unable to recognize the instance GPUs<\/p>\n<p>Anything I can do to cut down these build times?<\/p>",
        "Challenge_closed_time":1649126605252,
        "Challenge_comment_count":0,
        "Challenge_created_time":1648745542500,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing long build times for CUDA components when using Amazon SageMaker ScriptMode with PyTorch estimator. The requirements.txt file takes around 2 hours to build the wheel on the SageMaker Estimator, while it takes only seconds on a local Linux box. The user has tried building a Docker container on p3.16xlarge and running it on SageMaker, but it was unable to recognize the instance GPUs. The user is seeking advice on how to reduce the build times.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71696407",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":7.7,
        "Challenge_reading_time":14.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":105.8507644444,
        "Challenge_title":"Amazon SageMaker ScriptMode Long Python Wheel Build Times for CUDA Components",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":136.0,
        "Challenge_word_count":135,
        "Platform":"Stack Overflow",
        "Poster_created_time":1348171784712,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":93.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The solution is to augment the stock estimator image with the right components and then it can be run in the SageMaker script mode:<\/p>\n<pre><code>FROM    763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-training:1.10-gpu-py38\n\nCOPY requirements.txt \/tmp\/requirements.txt\nRUN pip install -r \/tmp\/requirements.tx\n<\/code><\/pre>\n<p>The key is to make sure <code>nvidia<\/code> runtime is used at build time, so <code>daemon.json<\/code> needs to be configured accordingly:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;default-runtime&quot;: &quot;nvidia&quot;,\n    &quot;runtimes&quot;: {\n        &quot;nvidia&quot;: {\n            &quot;path&quot;: &quot;nvidia-container-runtime&quot;,\n            &quot;runtimeArgs&quot;: []\n        }\n    }\n}\n<\/code><\/pre>\n<p>This is still not a complete solution, because viability of the build for SageMaker depends on the host where the build is performed.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.0,
        "Solution_reading_time":11.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.9680911111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello, I'd like to deploy SageMaker's built-in algorithm, BlazingText model on Fargate instead of Sagemaker endpoint. So, I tried to make an ECS task using BlazingText docker path. Here is my CDK code for it.\n\nconst loadBalancedFargateService = new ecsPatterns.ApplicationLoadBalancedFargateService(this, 'Service', {\n            memoryLimitMiB: 1024,\n            desiredCount: 1,\n            cpu: 512,\n            taskImageOptions: {\n              image: ecs.ContainerImage.fromRegistry(\"811284229777.dkr.ecr.us-east-1.amazonaws.com\/blazingtext:1\"),\n            },\n          });\n\nHowever, I got an error: \nCannotPullContainerError: inspect image has been retried 1 time(s): failed to resolve ref \"811284229777.dkr.ecr.us-east-1.amazonaws.com\/blazingtext:1\": pulling from host 811284229777.dkr.ecr.us-east-1.amazonaws.com failed with status code [manifests 1]...\n\nIs it impossible to pull docker container of sagemaker built-in algorithm from ECS?",
        "Challenge_closed_time":1651328900608,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651321815480,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is attempting to deploy a SageMaker built-in algorithm, BlazingText model on Fargate instead of Sagemaker endpoint by making an ECS task using BlazingText docker path. However, the user encountered an error \"CannotPullContainerError\" while trying to pull the docker container of the SageMaker built-in algorithm from ECS.",
        "Challenge_last_edit_time":1668180679166,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUCbp7XzQSSPSH200r45m4Uw\/ecs-load-container-image-from-sagemaker-built-in-algorithm-docker-path",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":12.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1.9680911111,
        "Challenge_title":"ECS load container image from sagemaker built-in algorithm docker path",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":99,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To my knowledge, no - it's not generally possible to pull the [built-in algorithm](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html) containers outside SageMaker: Your easiest route would probably just be to deploy the model on SageMaker and integrate your other containerized tasks to call the SageMaker endpoint.\n\nIt's maybe worth mentioning that the [framework containers](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/docker-containers-prebuilt.html) for custom\/script-mode modelling (e.g. the [AWS DLCs](https:\/\/github.com\/aws\/deep-learning-containers) for PyTorch\/HuggingFace\/etc) are not subject to this restriction (can check you should even be able to pull them locally): So if you were to use those to implement a customized text processing model I think you should be able to deploy it on ECS if needed. Of course this'd mean a more initial build and later maintenance effort though.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1651328900611,
        "Solution_link_count":3.0,
        "Solution_readability":13.2,
        "Solution_reading_time":11.62,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.4508038889,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi everyone, I am using wandb with Huggingface in a AWS Sagemaker notebook and I am refering to the tutorial here: <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/huggingface\" class=\"inline-onebox\">Hugging Face Transformers | Weights &amp; Biases Documentation<\/a>.<\/p>\n<p>I tried to set the <code>WANDB_PROJECT<\/code> environment variable before setting up the <code>huggingface_estimator<\/code>, which will call <code>train.py<\/code>.<\/p>\n<p><code>train.py<\/code> is where I initialize the <code>Trainer<\/code>. The above tutorial mentions to make sure to set the project name before initializing the <code>Trainer<\/code>, and I think I am doing this correctly here.<\/p>\n<p>Here are some useful snippets of my code.<\/p>\n<pre><code class=\"lang-auto\">import wandb\nwandb.login()\n\nWANDB_PROJECT=my_project_name\n\n...\n\nhuggingface_estimator = HuggingFace(\n  image_uri=image_uri,\n  entry_point='train.py',\n  source_dir='.\/scripts',\n  instance_type='ml.g4dn.xlarge',\n  instance_count=1,\n  role=role,\n  py_version='py39',\n  hyperparameters=hyperparameters,\n)\n<\/code><\/pre>\n<p>train.py<\/p>\n<pre><code class=\"lang-auto\">    training_args = TrainingArguments(\n        output_dir=args.output_dir,\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        num_train_epochs=args.epochs,\n        learning_rate=args.learning_rate,\n        save_strategy=\"epoch\",\n        logging_strategy='epoch',\n        report_to=\"wandb\",\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        data_collator=collate_fn,\n        tokenizer=image_processor,\n    )\n\n    trainer.train()\n<\/code><\/pre>\n<p>I would greatly appreciate any guidance or advice on how to resolve this issue. Thank you very much in advance for your help!<\/p>",
        "Challenge_closed_time":1678799383235,
        "Challenge_comment_count":0,
        "Challenge_created_time":1678765360341,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with setting the WANDB_PROJECT environment variable while using wandb with Huggingface in an AWS Sagemaker notebook. The user has followed the tutorial to set the project name before initializing the Trainer, but the project name is not being set properly. The user has provided code snippets for reference and is seeking guidance to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/set-the-wandb-project-environment-variable-cannot-name-the-project-properly\/4055",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":15.9,
        "Challenge_reading_time":23.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":9.4508038889,
        "Challenge_title":"Set the WANDB_PROJECT environment variable cannot name the project properly",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":253.0,
        "Challenge_word_count":156,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/oschan77\">@oschan77<\/a> ,<\/p>\n<p>You can set the project name in your script like so:<\/p>\n<pre><code class=\"lang-auto\">import os\nos.environ[\"WANDB_PROJECT\"] = \"sentiment-analysis\"\n<\/code><\/pre>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.3,
        "Solution_reading_time":3.03,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1393509037328,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":768.0,
        "Answerer_view_count":34.0,
        "Challenge_adjusted_solved_time":765.2169341667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/mlops\/mlops_first_steps\" rel=\"nofollow noreferrer\">ClearML<\/a>.<\/p>\n<p>The only line in my file is<\/p>\n<pre><code>from allegroai import Dataset, DatasetVersion\n<\/code><\/pre>\n<p>which yields<\/p>\n<pre><code>ModuleNotFoundError: No module named 'allegroai'\n<\/code><\/pre>\n<p>Looks like some pip package is missing, but I couldn't for the life of me find it in the docs.<\/p>\n<p>What should I pip install?<\/p>\n<p><strong>Not working:<\/strong><\/p>\n<ul>\n<li><code>pip install clearml-agent<\/code><\/li>\n<li><code>pip install clearml<\/code> and <code>clearml-init<\/code> as in <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/ds\/ds_first_steps\" rel=\"nofollow noreferrer\">here<\/a><\/li>\n<li><code>pip install allegroai<\/code><\/li>\n<\/ul>",
        "Challenge_closed_time":1657799581543,
        "Challenge_comment_count":2,
        "Challenge_created_time":1655044800580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a ModuleNotFoundError while trying to use ClearML and import the 'allegroai' module. They have tried installing various packages including clearml-agent, clearml, and allegroai, but none of them have resolved the issue. The user is seeking guidance on what package they should install to resolve the error.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72593187",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.8,
        "Challenge_reading_time":11.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":765.2169341667,
        "Challenge_title":"ModuleNotFoundError: No module named 'allegroai'",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1314313109232,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Technion, Israel",
        "Poster_reputation_count":18777.0,
        "Poster_view_count":2000.0,
        "Solution_body":"<p>Allegroai package should be taken from ClearML PyPi server.\nThis is only for paying customers (I think), and the way to retrieve it is by:<\/p>\n<ol>\n<li>Going to ClearML website (login with username\/company).<\/li>\n<li>Press the ? on the top right of the screen (next to your user icon) and choose the first one\n<a href=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" alt=\"1\" \/><\/a><\/li>\n<\/ol>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":5.83,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":60.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":1452696930640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":746.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":3.0575130556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How do I upgrade <code>azureml-sdk<\/code> such that the newest release of <code>azureml-core<\/code>, <code>1.1.5.5<\/code>, is installed? \nIf <code>azureml-sdk<\/code> is not installed, <code>pip install --upgrade azureml-sdk<\/code> will install <code>azureml-core==1.1.5.5<\/code>. If it is already installed, then it won't.<\/p>\n\n<pre><code>$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n$ pip install --upgrade azureml-sdk[interpret,notebooks]\n$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n<\/code><\/pre>",
        "Challenge_closed_time":1584735835707,
        "Challenge_comment_count":1,
        "Challenge_created_time":1584723850063,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in upgrading the azureml-sdk to the latest release of azureml-core, i.e., 1.1.5.5. The user has tried upgrading using pip install but it did not work. The user has shared the command used and the output received.",
        "Challenge_last_edit_time":1584724828660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60778546",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.2,
        "Challenge_reading_time":8.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.3293455556,
        "Challenge_title":"pip install azureml-sdk with latest patches to underlying libraries",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1142.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405457120427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Solution_body":"<p>You can use the eager strategy to force an upgrade of requirements:<\/p>\n\n<pre><code>pip install -U --upgrade-strategy eager azureml-sdk\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.2,
        "Solution_reading_time":1.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":1.1966977778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We developed a Jupyter Notebook in a local machine to train models with the Python (V3) libraries <code>sklearn<\/code> and <code>gensim<\/code>.\nAs we set the <code>random_state<\/code> variable to a fixed integer, the results were always the same.<\/p>\n\n<p>After this, we tried moving the notebook to a workspace in Azure Machine Learning Studio (classic), but the results differ even if we leave the <code>random_state<\/code> the same.<\/p>\n\n<p>As suggested in the following links, we installed the same libraries versions and checked the <code>MKL<\/code> version was the same and the <code>MKL_CBWR<\/code> variable was set to <code>AUTO<\/code>.<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/46766714\/t-sne-generates-different-results-on-different-machines\">t-SNE generates different results on different machines<\/a><\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/38228088\/same-python-code-same-data-different-results-on-different-machines\">Same Python code, same data, different results on different machines<\/a><\/p>\n\n<p>Still, we are not able to get the same results.<\/p>\n\n<p>What else should we check or why is this happening?<\/p>\n\n<p><strong>Update<\/strong><\/p>\n\n<p>If we generate a <code>pkl<\/code> file in the local machine and import it in AML, the results are the same (as the intention of the pkl file is).<\/p>\n\n<p>Still, we are looking to get the same results (if possible) without importing the pkl file.<\/p>\n\n<p><strong>Library versions<\/strong><\/p>\n\n<pre><code>gensim 3.8.3.\nsklearn 0.19.2.\nmatplotlib 2.2.3.\nnumpy 1.17.2.\nscipy 1.1.0.\n<\/code><\/pre>\n\n<p><strong>Code<\/strong><\/p>\n\n<p>Full code can be found <a href=\"https:\/\/t.ly\/YlCi\" rel=\"nofollow noreferrer\">here<\/a>, sample data link inside.<\/p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nfrom gensim.models import KeyedVectors\n%matplotlib inline\n\nimport time\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\nwordvectors_file_vec = '..\/libraries\/embeddings-new_large-general_3B_fasttext.vec'\nwordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)\n\nmath_quests = # some transformations using wordvectors\n\ndf_subset = pd.DataFrame()\n\npca = PCA(n_components=3, random_state = 42)\npca_result = pca.fit_transform(mat_quests)\ndf_subset['pca-one'] = pca_result[:,0]\ndf_subset['pca-two'] = pca_result[:,1] \n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state = 42)\ntsne_results = tsne.fit_transform(mat_quests)\n\ndf_subset['tsne-2d-one'] = tsne_results[:,0]\ndf_subset['tsne-2d-two'] = tsne_results[:,1]\n\npca_50 = PCA(n_components=50, random_state = 42)\npca_result_50 = pca_50.fit_transform(mat_quests)\nprint('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))\n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, random_state = 42)\ntsne_pca_results = tsne.fit_transform(pca_result_50)\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n<\/code><\/pre>",
        "Challenge_closed_time":1591493823768,
        "Challenge_comment_count":5,
        "Challenge_created_time":1591464199347,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user developed a Jupyter Notebook in a local machine to train models with the Python libraries sklearn and gensim. They set the random_state variable to a fixed integer, and the results were always the same. However, when they moved the notebook to a workspace in Azure Machine Learning Studio (classic), the results differ even if they leave the random_state the same. They installed the same libraries versions and checked the MKL version was the same and the MKL_CBWR variable was set to AUTO, but they are still not able to get the same results. If they generate a pkl file in the local machine and import it in AML, the results are the same.",
        "Challenge_last_edit_time":1591489515656,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62235365",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":12.2,
        "Challenge_reading_time":42.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":8.2290058333,
        "Challenge_title":"Models generate different results when moving to Azure Machine Learning Studio",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":201.0,
        "Challenge_word_count":320,
        "Platform":"Stack Overflow",
        "Poster_created_time":1585590244876,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>Definitely empathize with the issue you're having. Every data scientist has struggled with this at some point.<\/p>\n\n<p>The hard truth I have for you is that Azure ML Studio (classic) isn't really capable of  solving this \"works on my machine\" problem. However, the good news is that Azure ML Service is incredible at it. Studio classic doesn't let you define custom environments deterministically, only add and remove packages (and not so well even at that) <\/p>\n\n<p>Because ML Service's execution is built on top of <code>Docker<\/code> containers and <code>conda<\/code> environments, you can feel more confident in repeated results. I highly recommend you take the time to learn it (and I'm also happy to debug any issues that come up). Azure's <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\" rel=\"nofollow noreferrer\">MachineLearningNotebooks repo<\/a> has a lot of great tutorials for getting started.<\/p>\n\n<p>I spent two hours making <a href=\"https:\/\/github.com\/swanderz\/MachineLearningNotebooks\/blob\/SO_CPR\/how-to-use-azureml\/training\/train-on-amlcompute\/train-on-amlcompute.ipynb\" rel=\"nofollow noreferrer\">a proof of concept<\/a> that demonstrate how ML Service solves the problem you're having by synthesizing:<\/p>\n\n<ul>\n<li>your code sample (before you shared your notebook),<\/li>\n<li><a href=\"https:\/\/scikit-learn.org\/stable\/auto_examples\/manifold\/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py\" rel=\"nofollow noreferrer\">Jake Vanderplas's sklearn example<\/a>, and<\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-amlcompute\/train-on-amlcompute.ipynb\" rel=\"nofollow noreferrer\">this Azure ML tutorial<\/a> on remote training.<\/li>\n<\/ul>\n\n<p>I'm no T-SNE expert, but from the screenshot below, you can see that the t-sne outputs are the same when I run the script locally and remotely. This might be possible with Studio classic, but it would be hard to guarantee that it will always work.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mhlg6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mhlg6.png\" alt=\"Azure ML Experiment Results Page\"><\/a><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.5,
        "Solution_reading_time":28.5,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":242.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1359113510580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1076.0,
        "Answerer_view_count":81.0,
        "Challenge_adjusted_solved_time":0.3555730556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to create my own Kedro starter. I have tried to replicate the relevant portions of the pandas iris starter. I have a <code>cookiecutter.json<\/code> file with what I believe are appropriate mappings, and I have changed the repo and package directory names as well as any references to Kedro version such that they work with cookie cutter.<\/p>\n<p>I am able to generate a new project from my starter with <code>kedro new --starter=path\/to\/my\/starter<\/code>. <strong>However, the newly created project uses the default values for the project, package, and repo names, without prompting me for any input in the terminal<\/strong>.<\/p>\n<p>Have I misconfigured something? How can I create a starter that will prompt users to override the defaults when creating new projects?<\/p>\n<p>Here are the contents of <code>cookiecutter.json<\/code> in the top directory of my starter project:<\/p>\n<pre><code>{\n    &quot;project_name&quot;: &quot;default&quot;,\n    &quot;repo_name&quot;: &quot;{{ cookiecutter.project_name }}&quot;,\n    &quot;python_package&quot;: &quot;{{ cookiecutter.repo_name }}&quot;,\n    &quot;kedro_version&quot;: &quot;{{ cookiecutter.kedro_version }}&quot;\n}\n<\/code><\/pre>",
        "Challenge_closed_time":1641488156140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641486876077,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create their own Kedro starter by replicating the relevant portions of the pandas iris starter. They have a cookiecutter.json file with appropriate mappings and have changed the repo and package directory names. However, when they generate a new project from their starter, it uses default values for project, package, and repo names without prompting for any input in the terminal. The user is seeking help to create a starter that will prompt users to override the defaults when creating new projects.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70610418",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":15.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.3555730556,
        "Challenge_title":"Why doesn't my Kedro starter prompt for input?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416091573812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Texas, USA",
        "Poster_reputation_count":197.0,
        "Poster_view_count":23.0,
        "Solution_body":"<p>I think you may be missing <code>prompts.yml<\/code>\n<a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml<\/a><\/p>\n<p>Full instructions can be found here:\n<a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":38.0,
        "Solution_reading_time":7.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":21.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.1180555556,
        "Challenge_answer_count":0,
        "Challenge_body":"The DVC evaluation is crashing. After investigation, the bug was introduced by #348.\r\n\r\nThe bug:\r\n```\r\nTraceback (most recent call last):\r\n  File \"eval.py\", line 111, in <module>\r\n    main()\r\n  File \"eval.py\", line 107, in main\r\n    json.dump(all_metrics_dict, f)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/__init__.py\", line 179, in dump\r\n    for chunk in iterable:\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 431, in _iterencode\r\n    yield from _iterencode_dict(o, _current_indent_level)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type int64 is not JSON serializable\r\n```\r\n\r\nTo reproduce:\r\n\r\n```\r\n# For the bug introduced by #348, use 0bb500551b1b7c6f5bb9228335aa4df30a654e9c.\r\n# For the working code __before__ #348, use b9c886966ca4d893b41457a17262e198e3ba7f03.\r\nexport COMMIT=...\r\n\r\ngit clone https:\/\/github.com\/BlueBrain\/Search\r\ncd Search\/\r\n\r\n# Change <image> and <container>.\r\ndocker build -f data_and_models\/pipelines\/ner\/Dockerfile --build-arg BBS_REVISION=$COMMIT -t <image> .\r\ndocker run -it --rm -v \/raid:\/raid --name <container> <image>\r\n\r\ngit checkout $COMMIT\r\ngit checkout -- data_and_models\/pipelines\/ner\/dvc.lock\r\n\r\ncd data_and_models\/pipelines\/ner\/\r\ndvc pull --with-deps evaluation@organism\r\ndvc repro -fs evaluation@organism\r\n```\r\n\r\n_Originally posted by @pafonta in https:\/\/github.com\/BlueBrain\/Search\/issues\/335#issuecomment-833506692_",
        "Challenge_closed_time":1620393602000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620385977000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where using only \"zn.Method\" without \"zn.params\" in a Node does not add parameters to the \"dvc.yaml\" file, causing it to not depend on the \"params.yaml\" file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/361",
        "Challenge_link_count":2,
        "Challenge_participation_count":0,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":644.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":2.1180555556,
        "Challenge_title":"DVC eval crashes \"int64 not JSON serializable\"",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":166,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":1483548930012,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1875.0,
        "Answerer_view_count":146.0,
        "Challenge_adjusted_solved_time":0.4867144444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>[UPDATED] We are currently working on creating a Multi-Arm Bandit model for sign up optimization using the Build Your Own workflow that can be found here (basically substituting the model for our own):<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/p>\n<p>Our project directory is set up as:\n<a href=\"https:\/\/i.stack.imgur.com\/QwaIQ.png\" rel=\"nofollow noreferrer\">Project Directory<\/a><\/p>\n<p>The issue is that I added some code including the dataclasses library that is only available since Python 3.7, and our project seems to keep using 3.6, causing a failure when running the Cloud Formation set up. The error in our Cloudwatch Logs is:<\/p>\n<pre><code>2021-03-31T11:04:11.077-05:00 Copy\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import predictor as myapp\n  File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt;\n    from model_contents.model import MultiArmBandit, BanditParameters\n  File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt;\n    from dataclasses import dataclass, field, asdict\nTraceback (most recent call last): File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker worker.init_process() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process self.load_wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi self.wsgi = self.app.wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi self.callable = self.load() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load return self.load_wsgiapp() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp return util.import_app(self.app_uri) File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app mod = importlib.import_module(module) File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt; import predictor as myapp File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt; from model_contents.model import MultiArmBandit, BanditParameters File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt; from dataclasses import dataclass, field, asdict\n\n    2021-03-31T11:04:11.077-05:00\n\nCopy\nModuleNotFoundError: No module named 'dataclasses'\nModuleNotFoundError: No module named 'dataclasses'\n<\/code><\/pre>\n<p>Our updated Dockerfile is:<\/p>\n<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:18.04\n\n# Retrieves information about what packages can be installed\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python3-pip \\\n         python3.8 \\\n         python3-setuptools \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Set python 3.8 as default\nRUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n\n# Here we get all python packages.\nRUN pip --no-cache-dir install numpy boto3 flask gunicorn\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\nENV PYTHONPATH \/model_contents\n\n# Set up the program in the image\nCOPY bandit\/ \/opt\/program\/\nWORKDIR \/opt\/program\/\n\nRUN chmod +x \/opt\/program\/serve &amp;&amp; chmod +x \/opt\/program\/train\nLABEL git_tag=$GIT_TAG\n<\/code><\/pre>\n<p>I'm not sure if the nginx.conf file defaults to Py 3.6 so I want to make sure that it's not a big hassle to upgrade to Py 3.7 or 3.8 without many changes.<\/p>",
        "Challenge_closed_time":1617311610732,
        "Challenge_comment_count":1,
        "Challenge_created_time":1617309858560,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while creating a Multi-Arm Bandit model for sign-up optimization using the Build Your Own workflow in Sagemaker Endpoint. The issue is caused by the use of the dataclasses library, which is only available in Python 3.7 and above, while the project is using Python 3.6, resulting in a failure when running the Cloud Formation set up. The user has updated the Dockerfile to include Python 3.8 and wants to ensure that upgrading to Python 3.7 or 3.8 will not cause any issues with the nginx.conf file.",
        "Challenge_last_edit_time":1617383727407,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66911321",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":13.5,
        "Challenge_reading_time":88.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":77,
        "Challenge_solved_time":0.4867144444,
        "Challenge_title":"Upgrading Python version for running and creating custom container for Sagemaker Endpoint",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1398.0,
        "Challenge_word_count":615,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526288719836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation_count":33.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>You can update the Dockerfile after it install Python3.8 using <code>apt-get<\/code> with the following <code>RUN<\/code> commands<\/p>\n<pre><code>RUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n<\/code><\/pre>\n<p>The first <code>RUN<\/code> command will link <code>\/usr\/bin\/python<\/code> to <code>\/usr\/bin\/python3.8<\/code> and the second one will link <code>\/usr\/bin\/python3<\/code> to <code>\/usr\/bin\/python3.8<\/code><\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1617312029390,
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":7.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1545360696800,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Earth",
        "Answerer_reputation_count":1011.0,
        "Answerer_view_count":93.0,
        "Challenge_adjusted_solved_time":6002.4414652778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I started with SageMaker recently, and I'm loving it. However, I've been installing the same libraries over and over again to one of the in-built conda environments, and I want to create a life cycle configuration to do that automatically on startup. based on <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">the bottom of this<\/a>:<\/p>\n<blockquote>\n<p>notebook instance lifecycle configurations are available when you create a new notebook instance.<\/p>\n<\/blockquote>\n<p>the trouble is, I already have a notebook I've been working in for a while. Is there any way to apply a life cycle configuration on startup to an already existing notebook?<\/p>",
        "Challenge_closed_time":1597108074440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597107545867,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to create a life cycle configuration in SageMaker to automatically install libraries on startup, but they have already been working on an existing notebook. They are wondering if it is possible to apply a life cycle configuration to an already existing notebook.",
        "Challenge_last_edit_time":1597107925232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63350039",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.1468258333,
        "Challenge_title":"Add lifecycle configuration to existing notebook in SageMaker?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":395.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545360696800,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Earth",
        "Poster_reputation_count":1011.0,
        "Poster_view_count":93.0,
        "Solution_body":"<p>You need to shut the instance down, then you can edit it. Then, if you use your eyes (which I neglected to do) you can see the &quot;Additional Configurations&quot; section contains lifecycle configurations<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1618716714507,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1552934828727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":254.0,
        "Answerer_view_count":62.0,
        "Challenge_adjusted_solved_time":0.5847786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Challenge_closed_time":1618410091900,
        "Challenge_comment_count":1,
        "Challenge_created_time":1618407986697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" issue while trying to run an object detection code in AWS SageMaker. The error message indicates that the \"cv2\" module is missing, even though it is listed in the requirement file. The user has tried installing \"imgaug\" and \"opencv-python headless\" but the issue persists. The error occurs during the training job and causes it to fail.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":30.4,
        "Challenge_reading_time":154.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":56,
        "Challenge_solved_time":0.5847786111,
        "Challenge_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1218.0,
        "Challenge_word_count":510,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558009697176,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Cergy-Pontoise, Cergy, France",
        "Poster_reputation_count":53.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":1.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.8833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\u00a0\n\nI'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.\u00a0 I have enabled\u00a0aiplatform.googleapis.com.\n\nCommand:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifier\n\nUsing endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.\n\nPlease suggest what am I missing.",
        "Challenge_closed_time":1661575080000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661561100000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to create a Vertex AI endpoint from a workbench notebook. The error message states that the project is not active, even though the user has enabled aiplatform.googleapis.com. The user is seeking suggestions on what they might be missing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project\/m-p\/460565#M543",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":7.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":3.8833333333,
        "Challenge_title":"Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":322.0,
        "Challenge_word_count":56,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.3,
        "Solution_reading_time":3.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":51.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":5.3284266667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to run the notebook in studio, but can I export the studio as notebook and import it in another place?<\/p>",
        "Challenge_closed_time":1667268149163,
        "Challenge_comment_count":1,
        "Challenge_created_time":1667248966827,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to know if they can build their ML pipeline\/experiment in ML studio designer and export it as a Python and Jupyter notebook to be used in another place. They are able to run the notebook in studio but want to know if it can be exported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1069926\/can-i-build-my-ml-pipeline-experiment-in-ml-studio",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":5.3284266667,
        "Challenge_title":"can I build my ML pipeline\/experiment in ML studio designer, and export it as a python and jupyter notebook?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":41,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=a7efa3c2-0ff5-4cc8-8b14-63a031eb0713\">@usui gina  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A. Sorry, this is not support at this moment. But Azure Machine Learning Studio already has Notebook function just for your reference.     <\/p>\n<p>I will forward your feedback to product team and at the same time, I would highly recommend you provide your feedback in Azure Machine Learning portal to raise more visability - top right side as below screenshot    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/255817-image.png?platform=QnA\" alt=\"255817-image.png\" \/>    <\/p>\n<p>I hope this helps!    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.0,
        "Solution_reading_time":9.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":100.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":43.3186111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**SSL Connection to remote Neptune not working**\r\nI am unable to figure out how can I specify the correct certificate SFSRootCAG2.pem when running queries against SSL-enabled Neptune.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. I set up SSH tunnel via bastion to the Neptune cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -N -L 8182:yourneptuneendpoint:8182_'\r\n2. I start graph-notebook as '_jupyter notebook notebook\/destination_neptune_'. This gives me the output _Jupyter Notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_\r\n3. I open my notebook and run the following magic commands \r\n_'%%graph_notebook_config\r\n{\r\n  \"host\": \"localhost\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"iam_credentials_provider_type\": \"ROLE\",\r\n  \"load_from_s3_arn\": \"\",\r\n  \"aws_region\": <myregion>,\r\n  **\"ssl\": true**\r\n}'_\r\n4. I run the command \r\n_%%sparql        \r\nSELECT * WHERE {?s ?p ?o} LIMIT 1_\r\n\r\n5. It gives me the error\r\n**{'error': SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'localhost\\', port=8182): Max retries exceeded with url: \/sparql (Caused by SSLError(SSLCertVerificationError(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............**\r\n\r\n**Expected behavior**\r\nI expect to be able to connect to a remote neptune that has ssl enabled.\r\n\r\n**Screenshots**\r\nNone\r\n\r\n**Desktop (please complete the following information):**\r\n - macOS 10.15.7 Catalina\r\n - Browser Chrome\r\n - Version 86.0.4240.198 (Official Build) (x86_64)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.None",
        "Challenge_closed_time":1607104425000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1606948478000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered a broken notebook due to missing permission and is unsure if there are more issues. They are creating an issue to keep track of it and the severity is medium.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/40",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":20.85,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":43.3186111111,
        "Challenge_title":"[BUG] No documentation on how to connect local notebook to remote Neptune SSL",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":192,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Looks like we have a missing piece in our walkthrough for connecting to Neptune:\r\n\r\nhttps:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune\r\n\r\nCould you set your hostname from localhost to your Neptune endpoint:\r\n\r\n```\r\n%graph_notebook_host <your endpoint here>\r\n```\r\n\r\nAnd give it a try? I  updated \/etc\/hosts on my Mac and added an alias for localhost as\r\n\r\n> _27.0.0.1       localhost    yourneptuneendpoint_\r\n\r\nFlushed DNS cache.\r\nSet the hostname in Jupyter notebook graph_notebook_config command to **yourneptuneendpoint**.\r\nRan sparql query and it successfully completed.\r\n\r\n\r\n Glad it worked! I have filed an issue for us to expand our documentation to cover the steps you had to take. Closing this but feel free to open or submit a new issue if you need further assistance",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.9,
        "Solution_reading_time":9.58,
        "Solution_score_count":null,
        "Solution_sentence_count":9.0,
        "Solution_word_count":110.0,
        "Tool":"Neptune"
    },
    {
        "Answerer_created_time":1536318818623,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"\u0130zmit, Kocaeli, T\u00fcrkiye",
        "Answerer_reputation_count":1033.0,
        "Answerer_view_count":55.0,
        "Challenge_adjusted_solved_time":1138.2136686111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute the Azure ml sdk from the local system using the Jupyter notebook. When I run the below code i am getting an error.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\n\nModuleNotFoundError: No module named 'ruamel' \n<\/code><\/pre>",
        "Challenge_closed_time":1626743937907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1622646368700,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when trying to execute the Azure ml sdk from their local system using Jupyter notebook. The error message specifically states that there is no module named 'ruamel'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/67807756",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.2,
        "Challenge_reading_time":4.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1138.2136686111,
        "Challenge_title":"ModuleNotFoundError: No module named 'ruamel' when excuting from azureml.core",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Poster_created_time":1599816833352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"New Delhi, Delhi, India",
        "Poster_reputation_count":329.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>You have to add pip 20.1.1<\/p>\n<p>Conda ruamel needs higher version of pip<\/p>\n<pre><code>conda install pip=20.1.1\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":115.1236055556,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Dear community,<\/p>\n<p>I want to use WandB locally in my VSCode project, but my Ipython kernel keeps dying. After restarting the kernel it always prints out the errore message: \u201cFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\u201d and \u201cwandb: Currently logged in as: XXX. Use <code>wandb login --relogin<\/code> to force relogin\u201d<\/p>\n<p>I already tried to import os, as well as setting the environment variabele to my local notebook, but this didnt change a thing. I am using python 3.9.12<\/p>\n<p>I hope you can help me in this matter<\/p>",
        "Challenge_closed_time":1675104696691,
        "Challenge_comment_count":0,
        "Challenge_created_time":1674690251711,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues while trying to use WandB locally in their VSCode project. Their Ipython kernel keeps dying and they are receiving error messages related to failed detection of notebook name and WandB login. They have tried importing os and setting environment variables but the issue persists. They are using Python 3.9.12 and seeking help to resolve the matter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/using-wandb-in-visual-studio-code\/3752",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.1,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":115.1236055556,
        "Challenge_title":"Using WandB in Visual Studio Code",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":127.0,
        "Challenge_word_count":107,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/martin-woschitz\">@martin-woschitz<\/a> thank you for reporting this issue. This first message is just a warning so it shouldn\u2019t cause any issues running your code, also the second message  is an informatio output about the user account that you\u2019ve logged in. When does the Ipython kernel stops working, is it when you\u2019re running a python script? would it be possible to make a new virtual environment and install <code>wandb<\/code> only there to test if that\u2019s what\u2019s causing the issue for you?<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.3,
        "Solution_reading_time":6.6,
        "Solution_score_count":null,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1510046220943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":292.0,
        "Answerer_view_count":43.0,
        "Challenge_adjusted_solved_time":8.58242,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I want to import a custom module in my jupyter notebook in Sagemaker. Trying the import from Untitled1.ipynb I have tried two different structures. The first one is:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tJb5l.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tJb5l.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Inside \"package folder\" there were the files \"cross_validation.py\" and \"<strong>init<\/strong>.py\". The followings commands have been tried:<\/p>\n\n<pre><code>from package import cross_validation\nimport package.cross_validation\n<\/code><\/pre>\n\n<p>The second one is<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/eplmc.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eplmc.png\" alt=\"emak\"><\/a><\/p>\n\n<p>and I have coded  <code>import cross_validation<\/code><\/p>\n\n<p>In both cases I get no error at all when importing, but I can't use the class inside the module because I receive the error name <code>Class_X is not defined<\/code><\/p>\n\n<p>I also have restarted the notebook, just in case and it still not working. How could I make it?<\/p>",
        "Challenge_closed_time":1558571220172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558540323460,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges while trying to import a custom module in their Jupyter notebook in Amazon Sagemaker. They have tried two different structures to import the module, but they are unable to use the class inside the module as they receive an error stating that the class is not defined. The user has also restarted the notebook, but the issue persists.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56260720",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":9.6,
        "Challenge_reading_time":14.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":8.58242,
        "Challenge_title":"Import custom modules in Amazon Sagemaker Jupyter notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":6033.0,
        "Challenge_word_count":130,
        "Platform":"Stack Overflow",
        "Poster_created_time":1523298968403,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Solution_body":"<p>You can add a <code>__init__.py<\/code> file to your <code>package<\/code> directory to make it a Python package. Then you will be import the modules from the package inside your Jupyter notebook<\/p>\n\n<pre><code>\/home\/ec2-user\/SageMaker\n    -- Notebook.ipynb \n    -- mypackage\n        -- __init__.py\n        -- mymodule.py\n<\/code><\/pre>\n\n<p>Contents of Notebook.ipynb<\/p>\n\n<pre><code>from mypackage.mymodule import SomeClass, SomeOtherClass\n<\/code><\/pre>\n\n<p>For more details, see <a href=\"https:\/\/docs.python.org\/3\/tutorial\/modules.html#packages\" rel=\"nofollow noreferrer\">https:\/\/docs.python.org\/3\/tutorial\/modules.html#packages<\/a><\/p>\n\n<p>Thanks for using Amazon SageMaker!<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.7,
        "Solution_reading_time":8.68,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":58.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":320.9530555556,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi, I am trying to install some libraries in Studio Lab which requires root privileges. \r\n\r\nBelow I have run `whoami` to check if I am root user. (I am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nBelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\nI followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\nOn running `su -` , It asks for the password, but we don't have any password for Studio Lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\nCan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nPlease let me know if my query is not clear. ",
        "Challenge_closed_time":1655933766000,
        "Challenge_comment_count":5,
        "Challenge_created_time":1654778335000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to open a database file and is encountering an unexpected error while saving a file. They have deleted some unwanted notebooks from the studio lab's files and are now unable to install libraries with pip, create new files, or start the kernel.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/118",
        "Challenge_link_count":4,
        "Challenge_participation_count":5,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.06,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":320.9530555556,
        "Challenge_title":"How to get root access in SageMaker Studio Lab",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":122,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thank you for trying Studio Lab. Now Studio Lab does not allow `sudo` (similar issue: https:\/\/github.com\/aws\/studio-lab-examples\/issues\/40#issuecomment-1005305538). We can use `pip` and `conda` instead. Please refer the following issue.\r\n\r\nWhat software do you try to install? Some libraries will be available in `conda-forge` . Here is the sample of search.\r\n\r\nhttps:\/\/anaconda.org\/search?q=gym Thanks for the reply, I will try to explain the issue.\r\n I am trying to run bipedal robot from Open-ai gym. \r\n```\r\n!pip install gym\r\n!apt-get update\r\n!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> \/dev\/null\r\n!apt-get install xvfb\r\n!pip install pyvirtualdisplay \r\n!pip -q install pyglet\r\n!pip -q install pyopengl\r\n!apt-get install swig\r\n!pip install box2d box2d-kengz\r\n!pip install pybullet\r\n```\r\nThese are the libraries which I need to install for the code to work. \r\nIt works fine on google-colab: (screenshot below)\r\n![colab_gym_ss](https:\/\/user-images.githubusercontent.com\/91401599\/173034039-1973ee00-6c0b-4f49-adad-9bb324c59b8c.png)\r\n\r\nBut it throws error when I run it on SageMaker Studio Lab: (screenshot below)\r\n![sagemaker1](https:\/\/user-images.githubusercontent.com\/91401599\/173034679-f49340dc-de46-42bb-be1b-7c7e3616dac4.png)\r\n\r\nError Log (I have made gym_install.sh file which installs everything described above, I am running it below.): \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ sh gym_install.sh\r\nRequirement already satisfied: gym in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (0.24.1)\r\nRequirement already satisfied: gym-notices>=0.0.4 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (0.0.7)\r\nRequirement already satisfied: numpy>=1.18.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (1.22.4)\r\nRequirement already satisfied: cloudpickle>=1.2.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (2.1.0)\r\nReading package lists... Done\r\nE: List directory \/var\/lib\/apt\/lists\/partial is missing. - Acquire (13: Permission denied)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nRequirement already satisfied: pyvirtualdisplay in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.0)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nCollecting box2d\r\n  Using cached Box2D-2.3.2.tar.gz (427 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting box2d-kengz\r\n  Using cached Box2D-kengz-2.3.3.tar.gz (425 kB)\r\n  Preparing metadata (setup.py) ... done\r\nBuilding wheels for collected packages: box2d, box2d-kengz\r\n  Building wheel for box2d (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d\r\n  Running setup.py clean for box2d\r\n  Building wheel for box2d-kengz (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d-kengz\r\n  Running setup.py clean for box2d-kengz\r\nFailed to build box2d box2d-kengz\r\nInstalling collected packages: box2d-kengz, box2d\r\n  Running setup.py install for box2d-kengz ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Running setup.py install for box2d-kengz did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [18 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running install\r\n      \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n        warnings.warn(\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: legacy-install-failure\r\n\r\n\u00d7 Encountered error while trying to install package.\r\n\u2570\u2500> box2d-kengz\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for output from the failure.\r\nRequirement already satisfied: pybullet in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.2.5)\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\n### To be specific I am getting error in this line:\r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ apt-get install xvfb\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\n```\r\nSo as mentioned by you I tried to find xvfb in conda-forge, but couldn't find it. \r\nThere are some wrapper xvfb on conda-forge but installing them didn't help with the error. \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ python check.py\r\nTraceback (most recent call last):\r\n  File \"\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/GM_\/ARS_src\/check.py\", line 9, in <module>\r\n    display = Display(visible=0, size=(1024, 768))\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/display.py\", line 54, in __init__\r\n    self._obj = cls(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/xvfb.py\", line 44, in __init__\r\n    AbstractDisplay.__init__(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/abstractdisplay.py\", line 85, in __init__\r\n    helptext = get_helptext(program)\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/util.py\", line 13, in get_helptext\r\n    p = subprocess.Popen(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 966, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 1842, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\nTo reproduce above error run below code (check.py) :->\r\n```\r\nimport os\r\nimport numpy as np\r\nimport gym\r\nfrom gym import wrappers\r\nimport pyvirtualdisplay\r\nfrom pyvirtualdisplay import Display\r\n\r\nif __name__ == \"__main__\":\r\n    display = Display(visible=0, size=(1024, 768))\r\n```\r\n Thank you for sharing the error message. Studio Lab does not allow `apt install` so that the `gym_install.sh` does not work straightly. We have to prepare the environment by `conda` and `pip`.\r\n\r\nAt first we can install `swig` from `conda`. We can not install `xvfb` from `conda`, is this necessary to run the code?  Basically I have to capture the video output using `Display` of `pyvirtualdisplay` library. Everything else works. \r\nAs you can see `display = Display(visible=0, size=(1024, 768))` this line throws error  `FileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'` , so I am trying to install `Xvfb`.\r\n\r\nThanks, @ar8372 Sorry for the late reply. I raised the #124 to work OpenAI Gym in Studio Lab. Please comment to #124 if you have additional information. We need your insight to solve the issue. If you do not mind, please close this issue to suppress the duplication of issues.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":11.2,
        "Solution_reading_time":134.31,
        "Solution_score_count":null,
        "Solution_sentence_count":129.0,
        "Solution_word_count":1049.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1517578984080,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2090.0,
        "Answerer_view_count":163.0,
        "Challenge_adjusted_solved_time":22.2842275,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have finally arrived in the cloud to put my NLP work to the next level, but I am a bit overwhelmed with all the possibilities I have. So I am coming to you for advice.<\/p>\n\n<p>Currently I see three possibilities:<\/p>\n\n<ul>\n<li><strong>SageMaker<\/strong>\n\n<ul>\n<li>Jupyter Notebooks are great<\/li>\n<li>It's quick and simple<\/li>\n<li>saves a lot of time spent on managing everything, you can very easily get the model into production<\/li>\n<li>costs more<\/li>\n<li>no version control<\/li>\n<\/ul><\/li>\n<li><strong>Cloud9<\/strong><\/li>\n<li><strong>EC2(-AMI)<\/strong><\/li>\n<\/ul>\n\n<p>Well, that's where I am for now. I really like SageMaker, although I don't like the lack of version control (at least I haven't found anything for now).<\/p>\n\n<p>Cloud9 seems just to be an IDE to an EC2 instance.. I haven't found any comparisons of Cloud9 vs SageMaker for Machine Learning. Maybe because Cloud9 is not advertised as an ML solution. But it seems to be an option.<\/p>\n\n<p>What is your take on that question? What have I missed? What would you advise me to go for? What is your workflow and why? <\/p>",
        "Challenge_closed_time":1536933417932,
        "Challenge_comment_count":3,
        "Challenge_created_time":1536853194713,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking advice on which AWS service to use for their NLP work. They are considering SageMaker, which is quick and simple but lacks version control and is more expensive, Cloud9, which seems to be an IDE to an EC2 instance, and EC2-AMI. The user is leaning towards SageMaker but is concerned about the lack of version control. They are seeking advice on which service to use and what workflow to follow.",
        "Challenge_last_edit_time":1540233603016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52317237",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":6.3,
        "Challenge_reading_time":14.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":22.2842275,
        "Challenge_title":"Machine Learning (NLP) on AWS. Cloud9? SageMaker? EC2-AMI?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1442.0,
        "Challenge_word_count":180,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473262595243,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Netherlands",
        "Poster_reputation_count":23.0,
        "Poster_view_count":6.0,
        "Solution_body":"<blockquote>\n  <p>I am looking for an easy work environment where I can quickly test my models, exactly. And it won't be only me working on it, it's a team effort. <\/p>\n<\/blockquote>\n\n<p>Since you are working as a team I would recommend to use sagemaker with custom docker images. That way you have complete freedom over your algorithm. The docker images are stored in ecr. Here you can upload many versions of the same image and tag them to keep control of the different versions(which you build from a git repo).<\/p>\n\n<p>Sagemaker also gives the execution role to inside the docker image. So you still have full access to other aws resources (if the execution role has the right permissions)<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>\nIn my opinion this is a good example to start because it shows how sagemaker is interacting with your image.<\/p>\n\n<p><strong>Some notes on other solutions:<\/strong><\/p>\n\n<p>The problem of every other solution you posted is you want to build and execute on the same machine. Sure you can do this but keep in mind, that gpu instances are expensive and therefore you might only switch to the cloud when the code is ready to run.<\/p>\n\n<p><strong>Some other notes<\/strong><\/p>\n\n<ul>\n<li><p>Jupyter Notebooks in general are not made for collaborative programming. I think they want to change this with jupyter lab but this is still in development and sagemaker only use the notebook at the moment.<\/p><\/li>\n<li><p>EC2 is cheaper as sagemaker but you have  to do more work. Especially if you want to run your model as docker images. Also with sagemaker you can easily  build an endpoint for model inference which would be even more complex to realize with ec2.<\/p><\/li>\n<li><p>Cloud 9 I never used this service and but on first glance it seems good to develop on, but the question remains if you want to do this on a gpu machine. Because you're using ec2 as instance you have the same advantage\/disadvantage.<\/p><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":27.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":326.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":46.9908527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created 4 instances in my AWS sagemaker NOTEBOOKS tab.<\/p>\n<p>I want to create a life cycle configuration where the instance should stop every day at 9:00 PM.<\/p>\n<p>I have seen some examples but it is with IDLE TIME but not with the specific time<\/p>\n<pre><code>#!\/bin\/bash\nset -e\n\n# PARAMETERS\nIDLE_TIME=3600\n\necho &quot;Fetching the autostop script&quot;\nwget -O autostop.py https:\/\/raw.githubusercontent.com\/mariokostelac\/sagemaker-setup\/master\/scripts\/auto-stop-idle\/autostop.py\n\necho &quot;Starting the SageMaker autostop script in cron&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/5 * * * * \/bin\/bash -c '\/usr\/bin\/python3 $DIR\/autostop.py --time ${IDLE_TIME} | tee -a \/home\/ec2-user\/SageMaker\/auto-stop-idle.log'&quot;) | crontab -\n\necho &quot;Changing cloudwatch configuration&quot;\ncurl https:\/\/raw.githubusercontent.com\/mariokostelac\/sagemaker-setup\/master\/scripts\/publish-logs-to-cloudwatch\/on-start.sh | sudo bash -s auto-stop-idle \/home\/ec2-user\/SageMaker\/auto-stop-idle.log\n<\/code><\/pre>\n<p>Can anyone help me out on this one?<\/p>",
        "Challenge_closed_time":1645343727107,
        "Challenge_comment_count":0,
        "Challenge_created_time":1645181373237,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created 4 instances in AWS Sagemaker Notebooks and wants to create a life cycle configuration to stop the instance every day at 9:00 PM. The user has found examples with idle time but not with a specific time. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":1645181463240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71172207",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":45.0982972222,
        "Challenge_title":"AWS Sagemaker Life Cycle Configuration - AutoStop",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":357.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Poster_created_time":1608154572552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Islamabad, Pakistan",
        "Poster_reputation_count":75.0,
        "Poster_view_count":33.0,
        "Solution_body":"<p>Change the crontab syntax to <code>0 21 * * * shutdown.py<\/code><\/p>\n<p>Then create a shutdown.py which is reduced version of the <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py\" rel=\"nofollow noreferrer\">autostop.py<\/a> and contains mainly:<\/p>\n<pre><code>...\nprint('Closing notebook')\nclient = boto3.client('sagemaker')\nclient.stop_notebook_instance(NotebookInstanceName=get_notebook_name())\n<\/code><\/pre>\n<p>BTW: triggering <code>shutdown now<\/code> directly from the crontab command didn't work for me, therefore calling the SageMaker API instead.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1645350630310,
        "Solution_link_count":1.0,
        "Solution_readability":15.6,
        "Solution_reading_time":8.86,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1407449881432,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":228.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":2.7172083334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Iam new with SageMaker and I try to use my own sickit-learn algorithm . For this I use Docker.\nI try to do the same task as described here in this github account : <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a><\/p>\n\n<p>My question is should I create manually the repository <strong><code>\/opt\/ml<\/code><\/strong>  (I work with windows OS) ?<\/p>\n\n<p>Can you explain me please?<\/p>\n\n<p>thank you<\/p>",
        "Challenge_closed_time":1533928824743,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533919042793,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to AWS SageMaker and is trying to use their own sickit-learn algorithm using Docker. They are following a guide on GitHub and are unsure if they need to manually create the repository \"\/opt\/ml\" since they are using Windows OS. They are seeking clarification on this matter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51790720",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.8,
        "Challenge_reading_time":9.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":2.7172083334,
        "Challenge_title":"Brewing up custom ML models on AWS SageMaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1518617852856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":495.0,
        "Poster_view_count":81.0,
        "Solution_body":"<p>You don't need to create <code>\/opt\/ml<\/code>, SageMaker will do it for you when it launches your training job.<\/p>\n\n<p>The contents of the <code>\/opt\/ml<\/code> directory are determined by the parameters you pass to the CreateTrainingJob API call. The scikit example notebook you linked to describes this (look at the <strong>Running your container<\/strong> sections). You can find more info about this in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">Create a Training Job<\/a> section of the main SageMaker documentation.<\/p>\n\n<hr>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":12.7,
        "Solution_reading_time":7.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505166133223,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":146.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":43.793195,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to pull the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pre-built-docker-containers-frameworks.html\" rel=\"noreferrer\">pre-built docker images<\/a> for SageMaker. I am able to successfully <code>docker login<\/code> to ECR (my AWS credentials). When I try to pull the image I get the standard <code>no basic auth credentials<\/code>.<\/p>\n\n<p>Maybe I'm misunderstanding... I assumed those ECR URLs were public.<\/p>\n\n<pre><code>$(aws ecr get-login --region us-west-2 --no-include-email)\n\ndocker pull 246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn\n<\/code><\/pre>",
        "Challenge_closed_time":1557174090812,
        "Challenge_comment_count":0,
        "Challenge_created_time":1557016435310,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is having trouble pulling pre-built docker images for SageMaker despite successfully logging in to ECR with their AWS credentials. They receive a \"no basic auth credentials\" error when attempting to pull the image and are unsure if the ECR URLs are public.",
        "Challenge_last_edit_time":1557175542127,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55987935",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":8.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":43.793195,
        "Challenge_title":"How do I pull the pre-built docker images for SageMaker?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2738.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1343237570416,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"California",
        "Poster_reputation_count":1325.0,
        "Poster_view_count":131.0,
        "Solution_body":"<p>Could you show your ECR login command and pull command in the question?<\/p>\n\n<p>For SageMaker pre-built image 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/p>\n\n<p>What I do is:<\/p>\n\n<ol>\n<li>Log in ECR<\/li>\n<\/ol>\n\n<p><code>$(aws ecr get-login --no-include-email --registry-ids 520713654638 --region us-west-2)<\/code><\/p>\n\n<ol start=\"2\">\n<li>Pull the image<\/li>\n<\/ol>\n\n<p><code>docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.3.0-cpu-py3<\/code><\/p>\n\n<p>These images are public readable so you can pull them from any AWS account. I guess the reason you failed is that you did not specify --registry-ids in your login. But it's better if you can provide your scripts for others to identify what's wrong.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":9.83,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1589205020747,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":163.0,
        "Answerer_view_count":16.0,
        "Challenge_adjusted_solved_time":479.4038066667,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I constantly run into problems when working on Azure Compute Instances and trying to connect from the Jupyter Lab to the workspace.<\/p>\n<p>With InteractiveLoginAuthentication I get the following message:<\/p>\n<pre><code>AuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<p>With a Service Principal this one (SP is owner in the ML Workspace):<\/p>\n<pre><code>WorkspaceException: WorkspaceException:\n    Message: No workspaces found with name=xxx in all the subscriptions that you have access to.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;No workspaces found with name=xxx in all the subscriptions that you have access to.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I had another workspace in a different subscription where I could resolve it by giving the tennant as an extra input to the InteractiveLoginAuthentication. This time, no chance.<\/p>\n<p>The funny thing is, though, that I can login to the workspace via InteractiveLoginAuthentication when doing it from my local computer.<\/p>\n<p>I supsected that some old tokens are cached somewhere so I tried to use the &quot;Private browsing&quot; function of my browser. Furthermore, I deleted <code>\/home\/azureuser\/.azure\/accessTokens.json<\/code> but no effect.<\/p>\n<p>Maybe some of you had this problem before and have an idea?<\/p>\n<p>For reference some sites I checked:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/4618\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/4618<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/6147\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/6147<\/a><\/li>\n<\/ul>\n<h1>Update<\/h1>\n<p>When I run this code:<\/p>\n<pre><code>from azureml.core.authentication import InteractiveLoginAuthentication\ninteractive_auth = InteractiveLoginAuthentication(tenant_id='xxx')\n\nws = Workspace.get(name='xxx',\n                   subscription_id='xxx',\n                   resource_group='xxx',\n                   auth=interactive_auth)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>---------------------------------------------------------------------------\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    288                     module_logger.debug(&quot;{} acquired lock in {} s.&quot;.format(type(self).__name__, duration))\n--&gt; 289                 return test_function(self, *args, **kwargs)\n    290             except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n\nDuring handling of the above exception, another exception occurred:\n\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n&lt;ipython-input-2-fd1276999d15&gt; in &lt;module&gt;\n      5                    subscription_id='00c983e5-d766-480b-be75-abf95d1a46c3',\n      6                    resource_group='BusinessIntelligence',\n----&gt; 7                    auth=interactive_auth)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in get(name, auth, subscription_id, resource_group)\n    547 \n    548         result_dict = Workspace.list(\n--&gt; 549             subscription_id, auth=auth, resource_group=resource_group)\n    550         result_dict = {k.lower(): v for k, v in result_dict.items()}\n    551         name = name.lower()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in list(subscription_id, auth, resource_group)\n    637         elif subscription_id and resource_group:\n    638             workspaces_list = Workspace._list_legacy(\n--&gt; 639                 auth, subscription_id=subscription_id, resource_group_name=resource_group)\n    640 \n    641             Workspace._process_autorest_workspace_list(\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1373                 return None\n   1374             else:\n-&gt; 1375                 raise e\n   1376 \n   1377     @staticmethod\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1367             # azureml._base_sdk_common.workspace.models.workspace.Workspace\n   1368             workspace_autorest_list = _commands.list_workspace(\n-&gt; 1369                 auth, subscription_id=subscription_id, resource_group_name=resource_group_name)\n   1370             return workspace_autorest_list\n   1371         except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_project\/_commands.py in list_workspace(auth, subscription_id, resource_group_name)\n    386         if resource_group_name:\n    387             list_object = WorkspacesOperations.list_by_resource_group(\n--&gt; 388                 auth._get_service_client(AzureMachineLearningWorkspaces, subscription_id).workspaces,\n    389                 resource_group_name)\n    390             workspace_list = list_object.value\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_service_client(self, client_class, subscription_id, subscription_bound, base_url)\n    155         # in the multi-tenant case, which causes confusion.\n    156         if subscription_id:\n--&gt; 157             all_subscription_list, tenant_id = self._get_all_subscription_ids()\n    158             self._check_if_subscription_exists(subscription_id, all_subscription_list, tenant_id)\n    159 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_all_subscription_ids(self)\n    497         :rtype: list, str\n    498         &quot;&quot;&quot;\n--&gt; 499         arm_token = self._get_arm_token()\n    500         return self._get_all_subscription_ids_internal(arm_token)\n    501 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    293                     InteractiveLoginAuthentication(force=True, tenant_id=self._tenant_id)\n    294                     # Try one more time\n--&gt; 295                     return test_function(self, *args, **kwargs)\n    296                 else:\n    297                     raise e\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    473             return self._ambient_auth._get_arm_token()\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n    477     @_login_on_failure_decorator(_interactive_auth_lock)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    588         profile_object = Profile(async_persist=False, cloud_type=cloud_type)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n    592         if self._tenant_id and fetch_tenantid_from_aad_token(arm_token) != self._tenant_id:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    324             while True:\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n    328                     if &quot;Connection aborted.&quot; in str(e) and attempt &lt;= retries:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1828         if not token_about_to_expire:\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n   1832     try:\n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<ul>\n<li><code>azureml-sdk<\/code> is on version 1.9.0<\/li>\n<li>I can connect an authenticate from my local machine. Problems only occur when I want to work on a compute instance.<\/li>\n<\/ul>",
        "Challenge_closed_time":1596034370720,
        "Challenge_comment_count":0,
        "Challenge_created_time":1591616299803,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"the user is encountering challenges when trying to connect from the jupyter lab to the workspace, with authentication exceptions due to more than one token matching the criteria.",
        "Challenge_last_edit_time":1594308517016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62261222",
        "Challenge_link_count":8,
        "Challenge_participation_count":3,
        "Challenge_readability":18.4,
        "Challenge_reading_time":220.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":139,
        "Challenge_solved_time":1227.2419213889,
        "Challenge_title":"Workspace Authentication: More than one token matches the criteria",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2737.0,
        "Challenge_word_count":1188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589205020747,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":163.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Okay, here is the answer:<\/p>\n<ul>\n<li>You work for company A which is on Azure.<\/li>\n<li>You get access to company B's subscription.<\/li>\n<li>Problem is: You are associated to A's AAD in ML-Studio.<\/li>\n<li>You need to specify the tenant ID in the <code>InteractiveLoginAuthentication<\/code> like so:<\/li>\n<\/ul>\n<pre><code>interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)\n\nworkspace = Workspace.get(name=workspace_name,\n                          subscription_id=subscription_id,\n                          resource_group=resource_group,\n                          auth=interactive_auth)\n<\/code><\/pre>\n<ul>\n<li>Now the <strong>important<\/strong> part: You need to use company B's <code>tenant_id<\/code> (I used company A's all the time since I thought that was my authentication point)<\/li>\n<li>Of course, this is obvious while you read it...as it is to me now :)<\/li>\n<\/ul>\n<p>Hope this helps you. Took me some time but learned a lot ;)<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":11.38,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":15.915975,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I tried to import causalImpact library from github using \"devtools\" in AzureML studio for one of my projects.\ncode used was:<\/p>\n\n<pre><code>library(devtools)\ndevtools::install_github(\"google\/CausalImpact\")\n<\/code><\/pre>\n\n<p>Unfortunately, Azure doesn't support this.So tried importing it following the procedure in this <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow\">blog<\/a>.It is giving multiple errors on the name of dependent packages of casualImpact(i.e. BOOM, BH etc.). Can anyone help me out in importing this package on Azure?<\/p>\n\n<p>This is the R-script I used following the link given above:<\/p>\n\n<pre><code>library(assertthat)\nlibrary(dplyr)\nlibrary(hflights)\nlibrary(Lahman)\nlibrary(magrittr)\nlibrary(LGPL)\ninstall.packages(\"src\/BH_1.55.0-3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BH \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\nlibrary(BH)\ninstall.packages(\"src\/Boom_0.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"Boom \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/BoomSpikeSlab.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BoomSpikeSlab\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/bsts_0.5.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"bsts\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\nlibrary(zoo)\nlibrary(xts)\ninstall.packages(\"src\/CausalImpact.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"CausalImpact\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n<\/code><\/pre>",
        "Challenge_closed_time":1459314427723,
        "Challenge_comment_count":0,
        "Challenge_created_time":1459257130213,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to import the \"causalImpact\" library from GitHub using \"devtools\" in AzureML studio, but Azure does not support it. The user tried to import it following a blog post, but it gave multiple errors on the name of dependent packages of casualImpact (i.e. BOOM, BH, etc.). The user is seeking help in importing this package on Azure.",
        "Challenge_last_edit_time":1459402444996,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36285329",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":23.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":15.915975,
        "Challenge_title":"How to import a third party library \"causalImpact\" using R script in AzureML studio?",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":326.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1423214734843,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bangalore",
        "Poster_reputation_count":55.0,
        "Poster_view_count":34.0,
        "Solution_body":"<p>You will have to upload all dependent packages of casualImpact as a zip file - see sample <a href=\"http:\/\/gallery.azureml.net\/Details\/7507f907deb845d9b9b193b455a8615d\" rel=\"nofollow\">here<\/a> which shows uploading two packages required for xgboost<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.9,
        "Solution_reading_time":3.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":27.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4348.4297222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Challenge_closed_time":1586730089000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1571075742000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while using PyTorch MNIST Notebook with SageMaker Studio. They had to add two commands to the notebook, but even after that, SageMaker local mode still showed some errors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":16.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":91.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4348.4297222222,
        "Challenge_title":"Can not compile SageMaker examples",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":19,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This is reported by user and the problem is kubeflow pipeline has some breaking changes on parameters but we always install latest KFP pipeline which is not compatible. \r\n\r\nShort term. use lower kfp version\r\n```\r\n!pip install https:\/\/storage.googleapis.com\/ml-pipeline\/release\/0.1.29\/kfp.tar.gz --upgrade\r\n```\r\n\r\nLong term, update examples and make sure it leverages latest features of KFP.  Will check on the [SageMaker example](https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/01438d181f502504056eac89bfc0eb091733e9a8\/notebooks\/05_Kubeflow_Pipeline\/05_04_Pipeline_SageMaker.ipynb) and file a PR to make it leverage the latest features of KFP. And the master example of [SageMaker Kubeflow Pipeline](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/samples\/contrib\/aws-samples\/mnist-kmeans-sagemaker), will try to use [master yaml file](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/aws\/sagemaker). After so, will try to use latest version 2.05 of kfp to make it compatible. Potential SageMaker example issues with users: [1st](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1401) and [2nd](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1642). But the issue description is not that informative. Will talk with users if necessary. Let's not put time on this one. I will ask SM team to fix Op issue and we can concentrate on others. Since the updated SageMaker example has been merged, let's close this issue.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":10.4,
        "Solution_reading_time":18.52,
        "Solution_score_count":null,
        "Solution_sentence_count":17.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.2302777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Does SageMaker pipe mode serve as a cost saving measure? Or is is just faster than file mode but generally not much cheaper? The cost savings of it might be 1. no need to copy data to training instances and 2. training instances need less space. Are these savings generally significant for customers?",
        "Challenge_closed_time":1590165887000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1590161458000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether SageMaker pipe mode is a cost-saving measure or just faster than file mode. They are considering the potential cost savings of not needing to copy data to training instances and requiring less space on training instances, but are unsure if these savings are significant for customers.",
        "Challenge_last_edit_time":1668521878124,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QURbsBp9m5TsqKWWDdP8VJyw\/sagemaker-pipe-mode",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.2,
        "Challenge_reading_time":3.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1.2302777778,
        "Challenge_title":"SageMaker Pipe Mode",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":55,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"To the best of my understanding, pipe mode decreases startup times, but frequently increases the bill.\n\nThe SageMaker billing starts after the data has been copied onto the container in File mode and control is transferred to the user script. \n\nReading the data in pipe mode starts after control is transferred, so the data transfer happens during the billable time. \n\nFurther the data is, to the best of my knowledge, not hitting the disk (EBS). This is fast, but also means that if you pass over your data multiple times, you have to re-read it again, on your dime (S3 requests and container wait times).\n\nPipe mode is still a good idea. For example if you have only few passes over the data and the data is rather large, so that it would not fit on an EBS volume.\n\nAlso, in PyTorch for example, data loading can happen in parallel. So while the GPU is chucking away on one batch, the CPUs load and prepare the data for the next batch.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925574687,
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":11.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1306085731476,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Cleveland, TN",
        "Answerer_reputation_count":7737.0,
        "Answerer_view_count":454.0,
        "Challenge_adjusted_solved_time":3.9352741667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to follow a Python tutorial and I have been able to execute almost everything, until the point of Deploying an endpoint to Azure with python.<\/p>\n\n<p>In order to give some context I have uploaded the scripts to my git account:\n<a href=\"https:\/\/github.com\/levalencia\/MLTutorial\" rel=\"nofollow noreferrer\">https:\/\/github.com\/levalencia\/MLTutorial<\/a><\/p>\n\n<p>File 1 and 2 Work perfectly fine<\/p>\n\n<p>However the following section in File 3 fails:<\/p>\n\n<pre><code>%%time\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\n\ninference_config = InferenceConfig(runtime= \"python\", \n                                   entry_script=\"score.py\",\n                                   conda_file=\"myenv.yml\")\n\nservice = Model.deploy(workspace=ws, \n                       name='keras-mnist-svc2', \n                       models=[amlModel], \n                       inference_config=inference_config, \n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>with below error:<\/p>\n\n<pre><code>ERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n\nERROR - Service deployment polling reached non-successful terminal state, current service state: Transitioning\nOperation ID: 8353cad2-4218-450a-a03b-df418725acb1\nMore information can be found here: https:\/\/machinelearnin1143382465.blob.core.windows.net\/azureml\/ImageLogs\/8353cad2-4218-450a-a03b-df418725acb1\/build.log?sv=2018-03-28&amp;sr=b&amp;sig=UKzefxIrm3l7OsXxj%2FT4RsvUfAuhuaBwaz2P4mJu7vY%3D&amp;st=2020-03-11T12%3A23%3A33Z&amp;se=2020-03-11T20%3A28%3A33Z&amp;sp=r\nError:\n{\n  \"code\": \"EnvironmentBuildFailed\",\n  \"statusCode\": 400,\n  \"message\": \"Failed Building the Environment.\"\n}\n<\/code><\/pre>\n\n<p>When I download the logs, I got this:<\/p>\n\n<pre><code>wheel-0.34.2         | 24 KB     |            |   0% [0m[91m\nwheel-0.34.2         | 24 KB     | ########## | 100% [0m\nDownloading and Extracting Packages\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... failed\n[91m\nERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'.\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\nAttempting to roll back.\n\n[0mRolling back transaction: ...working... done\n[91m\nFileNotFoundError(2, \"No such file or directory: '\/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56\/bin\/python3.6'\")\n\n\n[0mThe command '\/bin\/sh -c ldconfig \/usr\/local\/cuda\/lib64\/stubs &amp;&amp; conda env create -p \/azureml-envs\/azureml_6abde325a12ccdba9b5ba76900b99b56 -f azureml-environment-setup\/mutated_conda_dependencies.yml &amp;&amp; rm -rf \"$HOME\/.cache\/pip\" &amp;&amp; conda clean -aqy &amp;&amp; CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; rm -rf \"$CONDA_ROOT_DIR\/pkgs\" &amp;&amp; find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + &amp;&amp; ldconfig' returned a non-zero code: 1\n2020\/03\/11 12:28:11 Container failed during run: acb_step_0. No retries remaining.\nfailed to run step ID: acb_step_0: exit status 1\n\nRun ID: cb3 failed after 2m21s. Error: failed during run, err: exit status 1\n<\/code><\/pre>\n\n<p>Update 1:<\/p>\n\n<p>I tried to run:\nconda list    --name base  conda<\/p>\n\n<p>inside the notebook and I got this:<\/p>\n\n<pre><code> # packages in environment at \/anaconda:\n    #\n    # Name                    Version                   Build  Channel\n    _anaconda_depends         2019.03                  py37_0  \n    anaconda                  custom                   py37_1  \n    anaconda-client           1.7.2                    py37_0  \n    anaconda-navigator        1.9.6                    py37_0  \n    anaconda-project          0.8.4                      py_0  \n    conda                     4.8.2                    py37_0  \n    conda-build               3.17.6                   py37_0  \n    conda-env                 2.6.0                         1  \n    conda-package-handling    1.6.0            py37h7b6447c_0  \n    conda-verify              3.1.1                    py37_0  \n\n    Note: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n\n<p>However in the deployment log I got this:<\/p>\n\n<pre><code>Solving environment: ...working... \ndone\n[91m\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.5.11\n  latest version: 4.8.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n<\/code><\/pre>",
        "Challenge_closed_time":1584015256823,
        "Challenge_comment_count":6,
        "Challenge_created_time":1583931341127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy an endpoint to Azure with Python. The error message indicates that the environment build has failed due to an error in installing the package 'conda-forge::astor-0.7.1-py_0'. The user has tried to run 'conda list --name base conda' inside the notebook and found that the current version of conda is 4.8.2, while the deployment log suggests updating conda to the latest version.",
        "Challenge_last_edit_time":1584001089836,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60636558",
        "Challenge_link_count":4,
        "Challenge_participation_count":7,
        "Challenge_readability":12.9,
        "Challenge_reading_time":59.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":23.3099155556,
        "Challenge_title":"ERROR conda.core.link:_execute(502): An error occurred while installing package 'conda-forge::astor-0.7.1-py_0'",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":5656.0,
        "Challenge_word_count":421,
        "Platform":"Stack Overflow",
        "Poster_created_time":1302030303092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Solution_body":"<p>Unfortunately there seems to an issue with this version of Conda (4.5.11). To complete this task in the tutorial, you can just update the dependency for Tensorflow and Keras to be from <code>pip<\/code> and not <code>conda<\/code>. There are reasons why this is less than ideal for a production environment. The Azure ML <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py\" rel=\"nofollow noreferrer\">documentation states<\/a>:<\/p>\n\n<blockquote>\n  <p>\"If your dependency is available through both Conda and pip (from\n  PyPi), use the Conda version, as Conda packages typically come with\n  pre-built binaries that make installation more reliable.\"<\/p>\n<\/blockquote>\n\n<p>In this case though, if you update the following code block:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_conda_package(\"tensorflow\")\nmyenv.add_conda_package(\"keras\")\n\nwith open(\"myenv.yml\",\"w\") as f:\n    f.write(myenv.serialize_to_string())\n\n# Review environment file\nwith open(\"myenv.yml\",\"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>To be the following:<\/p>\n\n<pre><code>from azureml.core.conda_dependencies import CondaDependencies \n\nmyenv = CondaDependencies()\nmyenv.add_pip_package(\"tensorflow==2.0.0\")\nmyenv.add_pip_package(\"azureml-defaults\")\nmyenv.add_pip_package(\"keras\")\n\nwith open(\"myenv.yml\", \"w\") as f:\n    f.write(myenv.serialize_to_string())\n\nwith open(\"myenv.yml\", \"r\") as f:\n    print(f.read())\n<\/code><\/pre>\n\n<p>The tutorial should be able to be completed.  Let me know if any of this does not work for you once this update has been made.<\/p>\n\n<p>I have also reported this issue to Microsoft (in regards to the Conda version).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.7,
        "Solution_reading_time":22.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":187.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1395235213383,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Norway",
        "Answerer_reputation_count":46807.0,
        "Answerer_view_count":3021.0,
        "Challenge_adjusted_solved_time":6.7343452778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I recently (and sceptically) started messing around with <strong>Azure Machine Learning Studio<\/strong>. When I stumbled accross the menu option for a machine learning work-flow <strong>Open in a new Notebook<\/strong> (For Python 3, 2 or R) I thought it was too good to be true:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AYDbo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AYDbo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And it most likely is, since this option is seemingly only available for the first step of the process. The option still exists in the <strong>right-click menu<\/strong> elsewhere, but it's greyed out:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cy6MG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cy6MG.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does anyone know why it is like this? Do I have to activate something in the menus, or buy some sort of a premium license? Is the functionality only available for <em>some<\/em> of the machine learning algorithms? Or is it just not supposed to be an available option in the menus?<\/p>\n\n<p>By the way, if you click <kbd>Python 3<\/kbd> 3 in the first step, you get a corresponding Python 3 code snippet in a Jupyter Notebook where you immediately can start messing around with the dataset:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/gTHGF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gTHGF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I realize that making this functionality available for each step in each and every model that anyone chooses to design would be an extremely difficult and maybe even impossible thing to do. But again, why is the option still in the menu?<\/p>",
        "Challenge_closed_time":1529068177343,
        "Challenge_comment_count":1,
        "Challenge_created_time":1529043933700,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is unable to open a new Python or R notebook in Azure Machine Learning Studio, as the option is greyed out in the right-click menu. The user is unsure if this is due to a premium license requirement or if the functionality is only available for certain machine learning algorithms. The option is available for the first step of the process, but not for subsequent steps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50870166",
        "Challenge_link_count":6,
        "Challenge_participation_count":2,
        "Challenge_readability":9.6,
        "Challenge_reading_time":22.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":6.7343452778,
        "Challenge_title":"Can not open new Python or R notebook in Azure Machine Learning Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1386.0,
        "Challenge_word_count":243,
        "Platform":"Stack Overflow",
        "Poster_created_time":1395235213383,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":46807.0,
        "Poster_view_count":3021.0,
        "Solution_body":"<p>Following the suggestion from @Jon in the comment section as well as a suggestion on <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/windowsdesktop\/en-US\/84a33ecc-1db2-4d11-83d2-3e96f0bcfaa7\/why-open-in-a-new-notebook-is-invalid?forum=MachineLearning\" rel=\"nofollow noreferrer\">microsoft.com<\/a>, I added a <strong>Convert to CSV Module<\/strong> at the end. After running the experiment, <strong>Open in a new Notebook<\/strong> is available when you right clik the <strong>Convert to CSV Module<\/strong>:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Bz7nJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Bz7nJ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What you get by clicking <kbd>Python 3<\/kbd> is this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ywbHz.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ywbHz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The functionality is certainly not as magnificent as I was hoping, but it's still pretty cool. If anyone knows <em>anything<\/em> about other possibilites or plans for future development, please don't hesitate to contribute with an answer!<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":5.0,
        "Solution_readability":15.2,
        "Solution_reading_time":15.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":112.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":147.5366666667,
        "Challenge_answer_count":0,
        "Challenge_body":"The problem described in this issue es very similar to #77 .\r\n\r\nCurrently, the \"delete\" action just removes the base image file. This is not correct for some reasons:\r\n\r\n- The base images are under control by DVC. The right way to remove a file that has been previously added to DVC is using its remove command, which removes the file pointer.\r\n- The deletion of the base image is not needed because it is not actually in the repository: it is pushed to the DVC remote storage during the base image generation and does not persist after this finishes. In case that the file were in the working tree because it was pulled at the beginning of some workflow execution, we can remove it just for good practices, but it would be removed at the end of the execution anyhow.\r\n\r\nTo summarize: the right way to do the deletion would be using DVC _remove_ command, which is already available in the wrapper, and is how it must be implemented in the action.\r\n",
        "Challenge_closed_time":1643645434000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643114302000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue with the missing \"params\" field for the evaluate stage in their dvc.yaml file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Nautilus-Cyberneering\/nautilus-librarian\/issues\/79",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.7,
        "Challenge_reading_time":11.96,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":112.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":147.5366666667,
        "Challenge_title":"Use DVC remove instead of just removing the base image file",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":180,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.1158333333,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nWhen I schedule a tensorboard from the UI does it always schedule on the polyaxon \"core nodes\", or does it schedule on any cpu machine that is available?\n\nWe have an issue sometimes with cpu nodes not scaling down, and I am wondering if it is because tensorboards are running on them?",
        "Challenge_closed_time":1649335625000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649335208000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing an issue where CPU nodes are not scaling down and suspects that Tensorboard services may be the cause. They are unsure if Tensorboard always schedules on the core nodes or any available CPU machine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1482",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":9.0,
        "Challenge_reading_time":4.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.1158333333,
        "Challenge_title":"Are Tensorboad services preventing CPU nodes to scale down?",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":62,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"By default, Tensorboard service does not use any node scheduling, so it probably uses the default nodes.\nIf you need to use a different node selectors or attach presets. When you click start tensorboard or run downstreamOp, the modal shows an editor which is basically prefilled a polyaxonfile:\n\nyou can just assign a preset if you have one:\n\npresets: [tensorboard-cpu-preset]\n\nOr patch the run with a node selector:\n\nrunPatch:\n  environment:\n    nodeSelector:\n      polyaxon_pool_type: polyaxon-tensorboard\n\nNote that for Tensorboad, by default it loads the component from here: Tensorboard Component\nYou can create your own Tensorboad component by following this tutorial\nIn that case you will use:\n\nhubRef: ORG\/tensorboard:VERSION\n\nFinally you can add this preset if you think that you might forget about a services running forever: https:\/\/polyaxon.com\/docs\/core\/scheduling-presets\/services-timeout\/\n\nOr adding it before clicking start:\n\ntermination:\n  timeout: 86400 # 24 hours",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":17.7,
        "Solution_reading_time":12.09,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":136.0,
        "Tool":"Polyaxon"
    },
    {
        "Answerer_created_time":1394547235287,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation_count":352.0,
        "Answerer_view_count":22.0,
        "Challenge_adjusted_solved_time":1.4516513889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm struggling with the DVC experiment management. Suppose the following scenario:<\/p>\n<p>I have <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 66\n  q: 5\n<\/code><\/pre>\n<p>I run the experiment with <code>dvc exp run -n exp_66<\/code>, and then I do <code>dvc exp push origin exp_66<\/code>. After this, I modify <code>params.yaml<\/code> file:<\/p>\n<pre><code>recommendations:\n  k: 99\n  q: 5\n<\/code><\/pre>\n<p>and then run another experiment <code>dvc exp run -n exp_99<\/code>, after which I commit with <code>dvc exp push origin exp_99<\/code>.<\/p>\n<p>Now, when I pull the corresponding branch with Git, I try to pull <code>exp_66<\/code> from dvc by running <code>dvc exp pull origin exp_66<\/code>. This does the pull (no error messages), but the content of the <code>params.yaml<\/code> file is with <code>k: 99<\/code> (and I would expect <code>k: 66<\/code>). What am I doing wrong? Does <code>git push<\/code> have to be executed after <code>dvc push<\/code>? Apart from that, I also found <code>dvc exp apply exp_66<\/code>, but I'm not sure what it does (it is suggested that after <code>apply<\/code> one should execute <code>git add .<\/code>, then <code>git commit<\/code>?<\/p>\n<p>I would really appreciate if you could write down the workflow with committing different experiments, pushing, pulling, applying, etc.<\/p>",
        "Challenge_closed_time":1646319959072,
        "Challenge_comment_count":1,
        "Challenge_created_time":1646314733127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with DVC experiment management workflow. They have modified the params.yaml file and run two experiments, exp_66 and exp_99, and pushed them to origin. However, when they try to pull exp_66 from DVC, the content of the params.yaml file is incorrect. The user is seeking guidance on the correct workflow for committing different experiments, pushing, pulling, and applying.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71338160",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":17.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1.4516513889,
        "Challenge_title":"DVC Experiment management workflow",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":152.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Poster_created_time":1643373769632,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":67.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>You did everything alright. In the end, after pulling, you can see that when using <code>dvc exp show<\/code> your experiments will be there. To restore the experiment available from your experiment list into your workspace, you simply need to run <code>dvc exp apply exp_66<\/code>. DVC will make sure that the changes corresponding to this experiment will be checked out.<\/p>\n<p>Your workflow seems correct so far. One addition: once you make sure one of the experiments is what you want to &quot;keep&quot; in git history, you can use <code>dvc exp branch {exp_id} {branch_name}<\/code> to create a separate branch for this experiment. Then you can use <code>git<\/code> commands to save the changes.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":8.77,
        "Solution_score_count":3.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":110.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":3.9835497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>According to the adoption plan, we need to rebuild everything, there is no quick way to push <a href=\"https:\/\/github.com\/Azure\/Azure-Machine-Learning-Adoption-Framework\">https:\/\/github.com\/Azure\/Azure-Machine-Learning-Adoption-Framework<\/a>    <\/p>\n<p>Am I correct?     <\/p>\n<p>It\u2019s not user friendly if I am not wrong.<\/p>",
        "Challenge_closed_time":1656630131776,
        "Challenge_comment_count":0,
        "Challenge_created_time":1656615790997,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering challenges with the Azure Machine Learning Adoption Framework, as they need to rebuild everything according to the adoption plan and there is no quick way to push changes. The user also finds the framework not user-friendly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/909961\/azure-machine-learning-adoption-framework",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.9835497222,
        "Challenge_title":"Azure-Machine-Learning-Adoption-Framework",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":33,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ce84de18-8973-44f3-8101-f191f9216b1f\">@Alexandre  <\/a>    <\/p>\n<p>Thanks for reaching out to us, I have answered this question as well in your other post. I think you are talking about move from Studio classc to Designer, please refer to below document:    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview<\/a>    <\/p>\n<p>Basically yes for your other thread, you need to rebuild the whole pipeline since we can not copy - paste your orignal structure to Designer.    <\/p>\n<p>I am sorry for the inconveniences since the new studio has a disfferent structure to make this migration not that easy. Please let me know if you have any question during this process, we will provide help.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.3,
        "Solution_reading_time":12.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":126.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.4182138889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to retrieve notebooks that were hosted on notebooks.azure.com? If so, how? The service is now discontinued but I would like to retrieve files that were hosted on the service.<\/p>",
        "Challenge_closed_time":1615975649020,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615959743450,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on how to retrieve notebooks that were previously hosted on notebooks.azure.com, which is now discontinued.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/317875\/retrieve-notebooks-azure-files",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":5.5,
        "Challenge_reading_time":2.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.4182138889,
        "Challenge_title":"Retrieve Notebooks Azure Files",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":null,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=e2b81f66-f91b-46e3-964b-29275f3216c7\">@Sean  <\/a> I am afraid that the option to retrieve this data is not possible. Please refer this <a href=\"https:\/\/github.com\/microsoft\/AzureNotebooks\/issues\/838\">thread<\/a> for information and the options that were available before the last day to migrate them. Thanks!!    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":4.48,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":38.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1553882107003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":294.0,
        "Answerer_view_count":28.0,
        "Challenge_adjusted_solved_time":715.1033155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Somewhere this spring the behaviour of the sagemaker docker image changed and I cannot find the way I need to construct it now.<\/p>\n<p><strong>Directory structure<\/strong><\/p>\n<pre><code>\/src\/some\/package\n\/project1\n    \/some_entrypoint.py\n    \/some_notebook.ipynb\n\/project2\n    \/another_entrypoint.py\n    \/another_notebook.ipynb\nsetup.py\n<\/code><\/pre>\n<p><strong>Docker file<\/strong><\/p>\n<p>Note that I want to shift tensorflow version, so I changed the <code>FROM<\/code> to the latest version. This was the\nbreaking change.<\/p>\n<pre><code># Core\nFROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com\/tensorflow-training:2.3.0-cpu-py37-ubuntu18.04\n\nCOPY . \/opt\/ml\/code\/all\/\nRUN pip install \/opt\/ml\/code\/all\/\n\nWORKDIR &quot;\/opt\/ml\/code&quot;\n<\/code><\/pre>\n<p><strong>Python code<\/strong><\/p>\n<p>This code should start the entrypoints, for example here we have the code of some_notebook.ipynb. I tried all possible combinations of working directory + source_dir (None, '.', or '..'), entry_point (with or without \/), dependencies ('src')...<\/p>\n<ul>\n<li>if setup is present it tries to call my project as a module (python -m some_entrypoint)<\/li>\n<li>if not, it often is not able to find my entrypoint. Which I don't understand because the TensorFlow is supposed to add it to the container, isn't it?<\/li>\n<\/ul>\n<pre><code>estimator = TensorFlow(\n   entry_point='some_entrypoint.py', \n   image_name='ECR.dkr.ecr.eu-west-1.amazonaws.com\/overall-project\/sagemaker-training:latest',\n   source_dir='.',\n#    dependencies=['..\/src\/'],\n   script_mode=True,\n\n   train_instance_type='ml.m5.4xlarge',\n   train_instance_count=1,\n   train_max_run=60*60,  # seconds * minutes\n   train_max_wait=60*60,  # seconds * minutes. Must be &gt;= train_max_run\n   hyperparameters=hyperparameters,\n   metric_definitions=metrics,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/some-data\/&quot;}\n#   , wait=False\n)\n<\/code><\/pre>\n<p>Ideally I would want to understand the logic within: what is called given what settings?<\/p>",
        "Challenge_closed_time":1600272249596,
        "Challenge_comment_count":0,
        "Challenge_created_time":1597694559337,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges with the behavior of the sagemaker docker image, specifically with the directory structure, docker file, and python code. The user is trying to start the entrypoints but is unable to find them, even though TensorFlow is supposed to add them to the container. The user is seeking to understand the logic within and what is called given the settings.",
        "Challenge_last_edit_time":1597697877660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63457857",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.4,
        "Challenge_reading_time":26.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":716.0250719444,
        "Challenge_title":"What is called within a sagemaker custom (training) container?",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":203,
        "Platform":"Stack Overflow",
        "Poster_created_time":1484838464572,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation_count":3937.0,
        "Poster_view_count":387.0,
        "Solution_body":"<p>when the training container runs, your entry_point script will be executed.<\/p>\n<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir<\/code> should just be &quot;.&quot;<\/p>\n<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":5.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.6287383334,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello. I created a Notebook Job Definition, scheduled to run every hour. According to the status it is \"Active\". Whenever I manually trigger the job with the \"Run Job\" button, it creates a new Notebook Job and runs successfully. However, the notebook never runs on the schedule. It should execute every hour, but instead only executes when manually triggered.\n\nIs there anything I need to do for the notebook to obey the schedule? Thanks,",
        "Challenge_closed_time":1675405386807,
        "Challenge_comment_count":0,
        "Challenge_created_time":1675370723349,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a Notebook Job Definition scheduled to run every hour, but the notebook never runs on the schedule and only executes when manually triggered. The user is seeking advice on how to make the notebook obey the schedule.",
        "Challenge_last_edit_time":1675718147630,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUjxD8Cf8lTc2Tknbt4MAzFQ\/sagemaker-notebook-doesn-t-run-on-job-schedule",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":5.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":9.6287383334,
        "Challenge_title":"Sagemaker notebook doesn't run on Job schedule",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":90.0,
        "Challenge_word_count":80,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Are there any problems with IAM role settings, etc.?\n\nPlease check this document for reference only.\nhttps:\/\/aws.amazon.com\/jp\/blogs\/machine-learning\/operationalize-your-amazon-sagemaker-studio-notebooks-as-scheduled-notebook-jobs\/",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1675405386807,
        "Solution_link_count":1.0,
        "Solution_readability":21.9,
        "Solution_reading_time":3.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":63.2731055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm running a SageMaker Training Job with a custom algorithm and the input data from s3. The SagaMaker AIM role ARN has a Read\/Put policy on the specified S3 bucket folder, but while creating the job I get a client error:<\/p>\n\n<p><code>ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.<\/code><\/p>\n\n<p>Unfortunately no more error info is provided in the SageMaker dashboard to investigate further. <\/p>",
        "Challenge_closed_time":1541230379527,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541002596347,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a client error while running a SageMaker Training Job with a custom algorithm and input data from S3. The error message states \"Data download failed:NoSuchKey (404): The specified key does not exist.\" The user has checked that the SageMaker AIM role ARN has a Read\/Put policy on the specified S3 bucket folder, but no further error information is provided in the SageMaker dashboard to investigate the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53087851",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":6.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":63.2731055556,
        "Challenge_title":"Amazon SageMaker: ClientError: Data download failed:NoSuchKey (404): The specified key does not exist",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":1219.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1305708350447,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bologna, Italy",
        "Poster_reputation_count":14823.0,
        "Poster_view_count":1847.0,
        "Solution_body":"<p>SageMaker team member here.<\/p>\n\n<p>The problem here is that the training job in question was setup with S3DataType=ManifestFile. In this case SageMaker expects to be able to download a single manifest file from the location specified by the S3Uri, if the file does not exist in S3 we get a 404 which is what we're sending back as the error here.<\/p>\n\n<p>See here for documentation on S3DataType\/S3Uri and manifests: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html#SageMaker-Type-S3DataSource-S3DataType\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html#SageMaker-Type-S3DataSource-S3DataType<\/a><\/p>\n\n<p>We will work to make this error message a bit more user-friendly, thanks for calling this out!<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.9,
        "Solution_reading_time":10.17,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":90.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.5563888889,
        "Challenge_answer_count":1,
        "Challenge_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Challenge_closed_time":1606709124000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1606707121000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information about the advantages of using SageMaker Jupyter instance instead of running it locally and whether there is any special integration with SageMaker that is lost if not using SageMaker Jupyter instance.",
        "Challenge_last_edit_time":1668525106056,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":2.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.5563888889,
        "Challenge_title":"Sagemaker Jupyter notebook",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Some useful points:\n\n- The **typical arguments of cloud vs local** will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\n- SageMaker notebooks already run in an explicit **IAM context** (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run `sagemaker.get_execution_role()`\n- **Pre-built environments** for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and **easy wiping\/reset** of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\n- Linux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\n- If you started using **SageMaker Studio**, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925565128,
        "Solution_link_count":0.0,
        "Solution_readability":11.1,
        "Solution_reading_time":14.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":179.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1548188011640,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bellevue, WA, USA",
        "Answerer_reputation_count":131.0,
        "Answerer_view_count":15.0,
        "Challenge_adjusted_solved_time":102.9447575,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using Azure Machine Learning Service with the azureml-sdk python library.<\/p>\n\n<p>I'm using azureml.core version 1.0.8<\/p>\n\n<p>I'm following this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline<\/a> tutorial.<\/p>\n\n<p>I've got it working when I use Azure Compute resources. But I would like to run it locally.<\/p>\n\n<p>I get the following error<\/p>\n\n<pre><code>raise ErrorResponseException(self._deserialize, response)\nazureml.pipeline.core._restclients.aeva.models.error_response.ErrorResponseException: (BadRequest) Response status code does not indicate success: 400 (Bad Request).\nTrace id: [uuid], message: Can't build command text for [train.py], moduleId [uuid] executionId [id]: Assignment for parameter Target is not specified\n<\/code><\/pre>\n\n<p>My code looks like:<\/p>\n\n<pre><code>run_config = RunConfiguration()\ncompute_target = LocalTarget()\nrun_config.target = LocalTarget()    \nrun_config.environment.python.conda_dependencies = CondaDependencies(conda_dependencies_file_path='environment.yml')\nrun_config.environment.python.interpreter_path = 'C:\/Projects\/aml_test\/.conda\/envs\/aml_test_env\/python.exe'\nrun_config.environment.python.user_managed_dependencies = True\nrun_config.environment.docker.enabled = False\n\ntrainStep = PythonScriptStep(\n    script_name=\"train.py\",\n    compute_target=compute_target,\n    source_directory='.',\n    allow_reuse=False,\n    runconfig=run_config\n)\n\nsteps = [trainStep]\n\n# Build the pipeline\npipeline = Pipeline(workspace=ws, steps=[steps])\npipeline.validate()\n\nexperiment = Experiment(ws, 'Test')\n\n# Fails, locally, works on Azure Compute\nrun = experiment.submit(pipeline)\n\n\n# Works both locally and on Azure Compute\nsrc = ScriptRunConfig(source_directory='.', script='train.py', run_config=run_config)\nrun = experiment.submit(src)\n<\/code><\/pre>\n\n<p>The <code>train.py<\/code> is a very simple self contained script only dependent on numpy that approximates pi.<\/p>",
        "Challenge_closed_time":1548188011640,
        "Challenge_comment_count":0,
        "Challenge_created_time":1547817410513,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to run an Azure Machine Learning Service pipeline locally using the azureml-sdk python library. The error message indicates that the target parameter is not specified in the train.py script. The user's code includes a run configuration and a PythonScriptStep, and the train.py script is a simple self-contained script that approximates pi. The code works when using Azure Compute resources but fails when running locally.",
        "Challenge_last_edit_time":1554703788007,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54254830",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.2,
        "Challenge_reading_time":28.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":102.9447575,
        "Challenge_title":"Running Azure Machine Learning Service pipeline locally",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":2476.0,
        "Challenge_word_count":164,
        "Platform":"Stack Overflow",
        "Poster_created_time":1313998995867,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3148.0,
        "Poster_view_count":347.0,
        "Solution_body":"<p>Local compute cannot be used with ML Pipelines. Please see this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-set-up-training-targets#supported-compute-targets\" rel=\"noreferrer\">article<\/a>.<\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1548188395572,
        "Solution_link_count":1.0,
        "Solution_readability":23.7,
        "Solution_reading_time":3.2,
        "Solution_score_count":7.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":14.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":16.5961252778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting consistent error \"Your session has expired\" (screenshot below), after logging in to machine learning studio. <\/p>\n\n<p>I have tried chrome incognito and guest windows, but no difference. <\/p>\n\n<p>I am using a new account and have signed up for Free workspace. Any suggestion to get past this or delete workspace, to start again?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fsqtw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fsqtw.png\" alt=\"Error screenshot\"><\/a><\/p>",
        "Challenge_closed_time":1557300039456,
        "Challenge_comment_count":1,
        "Challenge_created_time":1557237613340,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a consistent \"Session has expired\" error message when logging into Machine Learning Studio, despite trying different browsers. They are using a new account and have signed up for a free workspace. The user is seeking suggestions on how to resolve the issue or delete the workspace to start again.",
        "Challenge_last_edit_time":1557240710416,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56024354",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":17.3405877778,
        "Challenge_title":"\"Session has expired\" message with Machine Learning Studio",
        "Challenge_topic":"Package Management",
        "Challenge_topic_macro":"Environment Management",
        "Challenge_view_count":482.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":1255194026492,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bedford, MA, USA",
        "Poster_reputation_count":300.0,
        "Poster_view_count":63.0,
        "Solution_body":"<p>I can reproduce your issue, I sign out and log in <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a> again, it solved my problem. Or you can try to clear the browsing data or change a browser. Anyway, the issue should be caused by the browser, not azure. Even if your account is not the owner of the workspace, when you click <code>Sign In<\/code> in <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a> , it will create a free workspace(with a different workspace id) for you automatically.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/MXDJC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MXDJC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you want to delete the workspace, you need to let the owner of the workspace delete it, navigate to the <code>SETTINGS<\/code> on the left of the studio -> <code>NAME<\/code> -> <code>DELETE WORKSPACE<\/code>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/E6aUl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6aUl.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1557300456467,
        "Solution_link_count":8.0,
        "Solution_readability":8.0,
        "Solution_reading_time":14.76,
        "Solution_score_count":2.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":128.0,
        "Tool":"Azure Machine Learning"
    }
]