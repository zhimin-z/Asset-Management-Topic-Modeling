[
    {
        "Answerer_created_time":1478712810887,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation_count":11153.0,
        "Answerer_view_count":888.0,
        "Challenge_adjusted_solved_time":5.928925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've run a simple two-class neural network where I got this result in the end (eval):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I think I am going to be happy with the <code>True Positive<\/code> and <code>False Negative<\/code> results. But what does <code>False Positive<\/code> mean? <code>False Positive<\/code> means it did not correctly classify 2002 elements and missed them?<\/p>\n\n<p>The <code>Accuracy<\/code> is 66%, that's really bad right? Whats the difference between that and <code>AUC<\/code>?<\/p>\n\n<p><code>Precision<\/code> suffers because Accuracy also is bad (I hoping for a 80%+)?<\/p>\n\n<p>And how do I flip <code>Positive Label<\/code> and <code>Negative Label<\/code>? I really want to predict the classification where the target is to find <code>CANDIDATE<\/code><\/p>",
        "Challenge_closed_time":1488062187216,
        "Challenge_comment_count":0,
        "Challenge_created_time":1488042320197,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has run a two-class neural network and is trying to understand the classification results. They are unsure about the meaning of False Positive and the difference between Accuracy and AUC. They are also concerned about the low accuracy and how it affects Precision. The user wants to know how to flip Positive and Negative Labels to predict the classification for finding CANDIDATE.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42458982",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5.5186163889,
        "Challenge_title":"Understanding classification results",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":113,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267709938320,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Norway",
        "Poster_reputation_count":13032.0,
        "Poster_view_count":1004.0,
        "Solution_body":"<p>Basically, for the false\/true positives and false\/true negatives :\nYou have detected almost all the CANDIDATE samples in your dataset, 3420 of them were correctly predicted as TRUE and 31 of them were predicted as FALSE. This information is captured in the Recall ratio : 3420\/(3420+31) = 99.1%. It is very high, so very good. <\/p>\n\n<p>However, you have predicted <strong>too many<\/strong> CANDIDATE. Indeed, in all the TRUE values predicted by the model, 3420 were actually TRUE and 2002 were actually FALSE. This makes the Precision ratio bad : 3420\/(3420+2002)=63.1%. Which is not that good. <\/p>\n\n<p>F1 is a combinaison between Precision and Recall, it summarizes them into one value, some kind of weighted average. The formula is 2*(P*R)\/(P+R). So if one of Precision or Recall is bad : the F1score will capture it. <\/p>\n\n<p>You can see that you have a total of 5999 examples in your data set. Out of those, 3451 are really TRUE and 2548 are really FALSE. So you have 57% of your data that is TRUE. If you make a really stupid classifier that classifies everything as TRUE whatever the features are, then you will get 57% accuracy. Given that, 66.1% accuracy is not really good. \nIf you look at the second column of that table, you only predict 577 FALSE out of the 5999 samples. Your classifier is heavily biased towards TRUE predictions. <\/p>\n\n<p>For the AUC, it stands for Area Under the Curve. You can read <a href=\"http:\/\/fastml.com\/what-you-wanted-to-know-about-auc\/\" rel=\"nofollow noreferrer\">more detailed info about it here<\/a>. To summarize : when you predic a value, you don't really get True or False directly. You get a real number between 0 (False) and 1 (True). The way to classify a predicted value, say 0.2, is to use a Threshold. The threshold is by default set to 0.5. So if you predict 0.2, your model will predict to classify it as a False because 0.2&lt;0.5. But you could make that treshold move between 0 and 1. If the classifier is really good, if it discriminates really well the Falses and Trues predictions, then the AUC will be close to 1. If it's really bad, it will be close to 0.5. Refer to the link if you need more information. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1488063664327,
        "Solution_link_count":1.0,
        "Solution_readability":6.6,
        "Solution_reading_time":26.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":368.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1250347954880,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":5575.0,
        "Answerer_view_count":358.0,
        "Challenge_adjusted_solved_time":0.4832136111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to classify ~1m+ documents and have a Version Control System for in- and Output of the corresponding model. <\/p>\n\n<p>The data changes over time:<\/p>\n\n<ul>\n<li>sample size increases over time<\/li>\n<li>new Features might appear<\/li>\n<li>anonymization procedure might Change over time<\/li>\n<\/ul>\n\n<p>So basically \"everything\" might change: amount of observations, Features and the values.\nWe are interested in making the ml model Building reproducible without using 10\/100+ GB \nof disk volume, because we save all updated versions of Input data. Currently the volume size of the data is ~700mb.<\/p>\n\n<p>The most promising tool i found is: <a href=\"https:\/\/github.com\/iterative\/dvc\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc<\/a>. Currently the data\nis stored in a database in loaded in R\/Python from there.<\/p>\n\n<p><strong>Question:<\/strong><\/p>\n\n<p>How much disk volume can be (very approx.) saved by using dvc? <\/p>\n\n<p>If one can roughly estimate that. I tried to find out if only the \"diffs\" of the data are saved. I didnt find much info by reading through: <a href=\"https:\/\/github.com\/iterative\/dvc#how-dvc-works\" rel=\"noreferrer\">https:\/\/github.com\/iterative\/dvc#how-dvc-works<\/a> or other documentation. <\/p>\n\n<p><strong>I am aware that this is a very vague question. And it will highly depend on the dataset. However, i would still be interested in getting a very approximate idea.<\/strong><\/p>",
        "Challenge_closed_time":1582487867856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1582482701247,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary_original":"The user wants to classify over 1 million documents and have a version control system for input and output of the corresponding model. They are interested in making the ML model building reproducible without using a large amount of disk volume. The user is seeking an estimate of how much disk volume can be saved by using dvc, a tool they found promising, but they are aware that the answer will depend on the dataset.",
        "Challenge_last_edit_time":1582486128287,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60365473",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.4351691667,
        "Challenge_title":"By how much can i approx. reduce disk volume by using dvc?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":689.0,
        "Challenge_word_count":204,
        "Platform":"Stack Overflow",
        "Poster_created_time":1504097190907,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1365.0,
        "Poster_view_count":193.0,
        "Solution_body":"<p>Let me try to summarize how does DVC store data and I hope you'll be able to figure our from this how much space will be saved\/consumed in your specific scenario.<\/p>\n\n<p><strong>DVC is storing and deduplicating data on the individual <em>file level<\/em>.<\/strong> So, what does it usually mean from a practical perspective.<\/p>\n\n<p>I will use <code>dvc add<\/code> as an example, but the same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<h2>Scenario 1: Modifying file<\/h2>\n\n<p>Let's imagine I have a single 1GB XML file. I start tracking it with DVC:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ dvc add data.xml\n<\/code><\/pre>\n\n<p>On the modern file system (or if <code>hardlinks<\/code>, <code>symlinks<\/code> are enabled, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noreferrer\">this<\/a> for more details) after this command we still consume 1GB (even though file is moved into DVC cache and is still present in the workspace).<\/p>\n\n<p>Now, let's change it a bit and save it again:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ echo \"&lt;test\/&gt;\" &gt;&gt; data.xml\n$ dvc add data.xml\n<\/code><\/pre>\n\n<p>In this case we will have 2GB consumed. <strong>DVC does not do diff between two versions of the same file<\/strong>, neither it splits files into chunks or blocks to understand that only small portion of data has changed.<\/p>\n\n<blockquote>\n  <p>To be precise, it calculates <code>md5<\/code> of each file and save it in the content addressable key-value storage. <code>md5<\/code> of the files serves as a key (path of the file in cache) and value is the file itself:<\/p>\n  \n  <pre class=\"lang-sh prettyprint-override\"><code>(.env) [ivan@ivan ~\/Projects\/test]$ md5 data.xml\n0c12dce03223117e423606e92650192c\n\n(.env) [ivan@ivan ~\/Projects\/test]$ tree .dvc\/cache\n.dvc\/cache\n\u2514\u2500\u2500 0c\n   \u2514\u2500\u2500 12dce03223117e423606e92650192c\n\n1 directory, 1 file\n\n(.env) [ivan@ivan ~\/Projects\/test]$ ls -lh data.xml\ndata.xml ----&gt; .dvc\/cache\/0c\/12dce03223117e423606e92650192c (some type of link)\n<\/code><\/pre>\n<\/blockquote>\n\n<h2>Scenario 2: Modifying directory<\/h2>\n\n<p>Let's now imagine we have a single large 1GB directory <code>images<\/code> with a lot of files:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>$ du -hs images\n1GB\n\n$ ls -l images | wc -l\n1001\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>At this point we still consume 1GB. Nothing has changed. But if we modify the directory by adding more files (or removing some of them):<\/p>\n\n<pre><code>$ cp \/tmp\/new-image.png images\n\n$ ls -l images | wc -l\n1002\n\n$ dvc add images\n<\/code><\/pre>\n\n<p>In this case, after saving the new version we <strong>still close to 1GB<\/strong> consumption. <strong>DVC calculates diff on the directory level.<\/strong> It won't be saving all the files that were existing before in the directory.<\/p>\n\n<p>The same logic applies to all commands that save data files or directories into DVC cache - <code>dvc add<\/code>, <code>dvc run<\/code>, etc.<\/p>\n\n<p>Please, let me know if it's clear or we need to add more details, clarifications.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":8.5,
        "Solution_reading_time":39.4,
        "Solution_score_count":12.0,
        "Solution_sentence_count":26.0,
        "Solution_word_count":428.0,
        "Tool":"DVC"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":2.6086344444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey experts, I am looking for a document of how features in SDK V1 mapping to SDK v2 to show my team and plan how we should move to SDK v2. I cannot find a summary for that. Can you please help with this <\/p>",
        "Challenge_closed_time":1682725999300,
        "Challenge_comment_count":0,
        "Challenge_created_time":1682716608216,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a document that maps the features of SDK V1 to SDK V2 to help their team plan the transition to the newer version, but they are unable to find a summary for it. They are seeking assistance in locating such a document.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1266094\/sdk-features-mapping",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.6086344444,
        "Challenge_title":"SDK features mapping",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"https:\/\/learn.microsoft.com\/users\/na\/?userid=9603a4b0-3119-4f80-93b6-9637337c7a94\">@otto atler<\/a> <\/p>\n<p>Thanks for reaching out to us again, please see below list: <\/p>\n<p>For workspace - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace\">Method\/API in SDK v1 (use links to ref docs)<\/a>    <\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.workspace\">Method\/API in SDK v2 (use links to ref docs)<\/a><\/p>\n<p>For compute - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.compute.amlcompute(class)\">Method\/API in SDK v1 (use links to ref docs)<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.amlcompute\">Method\/API in SDK v2 (use links to ref docs)<\/a><\/p>\n<p>For datastore -<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_storage_datastore.azureblobdatastore?view=azure-ml-py&amp;preserve-view=true\">azureml_blob_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen1datastore\">azureml_blob_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_data_lake_datastore.azuredatalakedatastore?view=azure-ml-py&amp;preserve-view=true\">azureml_data_lake_gen1_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen1datastore\">azureml_data_lake_gen1_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_data_lake_datastore.azuredatalakegen2datastore?view=azure-ml-py&amp;preserve-view=true\">azureml_data_lake_gen2_datastore<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities.azuredatalakegen2datastore\">azureml_data_lake_gen2_datastore<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_sql_database_datastore.azuresqldatabasedatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_sql_database_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_my_sql_datastore.azuremysqldatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_my_sql_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.azure_postgre_sql_datastore.azurepostgresqldatastore?view=azure-ml-py&amp;preserve-view=true\">azuremlml_postgre_sql_datastore<\/a><\/p>\n<p>V2 Will be supported via import &amp; export functionalities|<\/p>\n<p>For data assets - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data\">Method\/API in SDK v1<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.entities\">Method\/API in SDK v2<\/a><\/p>\n<p>For model assets - <\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model(class)#azureml-core-model-register\">Model.register<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update\">ml_client.models.create_or_update<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run#azureml-core-run-run-register-model\">run.register_model<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-create-or-update\">ml_client.models.create_or_update<\/a><\/p>\n<p>V1 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model(class)#azureml-core-model-deploy\">Model.deploy<\/a><\/p>\n<p>V2 <a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azure-ai-ml\/azure.ai.ml.mlclient#azure-ai-ml-mlclient-begin-create-or-update\">ml_client.begin_create_or_update(blue_deployment)<\/a><\/p>\n<p>I hope this helps, please let me know if you have any questions.<\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer and vote 'Yes' if you feel helpful to support he community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":22.0,
        "Solution_readability":30.1,
        "Solution_reading_time":61.37,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":197.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1546969667040,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation_count":1689.0,
        "Answerer_view_count":170.0,
        "Challenge_adjusted_solved_time":1.9175436111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know there is a good tutorial on how to create jupyter notebooks on AWS sagemaker \"the easy way\".<\/p>\n\n<p>Do you know if it is possible to allow 10 students to create jupyter-notebooks who do not have an AWS accounts, and also allow them to edit jupyter-notebooks?<\/p>",
        "Challenge_closed_time":1554143903380,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554137000223,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking information on whether it is possible to allow 10 students to create and edit Jupyter notebooks on AWS Sagemaker without requiring them to have AWS accounts.",
        "Challenge_last_edit_time":1554180914787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55459903",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":3.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.9175436111,
        "Challenge_title":"multiuser public jupyter notebook on AWS sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1259.0,
        "Challenge_word_count":53,
        "Platform":"Stack Overflow",
        "Poster_created_time":1394050586807,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":568.0,
        "Poster_view_count":67.0,
        "Solution_body":"<p>Enabling multiple users to leverage the same notebook (in this case, without authentication) will involve managing your Security Groups to enable open access. You can filter, allowing access for a known IP address range, if your students are accessing it from a classroom or campus, for example.<\/p>\n\n<p>Tips for this are available in <a href=\"https:\/\/stackoverflow.com\/questions\/42617692\/is-it-possible-to-grant-multiple-users-to-jupyter-notebook\">this answer<\/a> and this page from the documentation, diving into <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/\" rel=\"nofollow noreferrer\">network configurations for SageMaker hosted notebook instances<\/a>.<\/p>\n\n<p>As for enabling students to spin up their own notebooks, I'm not sure if it's possible to enable completely unauthenticated AWS-level resource provisioning -- however once you've spun up a single managed notebook instance yourself, students can create their own notebooks directly from the browser in Jupyter, once they've navigated to the publicly available IP. You may need to attach a new SageMaker IAM role that enables notebook creation (amongst other things, depending on the workload requirements). Depending on the computational needs (number, duration, and types of concurrent workloads), there will be different optimal setups of number of managed instances and instance type to prevent computational bottlenecking.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1554169700776,
        "Solution_link_count":2.0,
        "Solution_readability":16.8,
        "Solution_reading_time":19.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":180.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.1985233334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I\u2019m trying to train a model, but I keep receiving an error that tells me \u201ctrain.py: error: unrecognized arguments: --save_period 1.\u201d<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png\" data-download-href=\"\/uploads\/short-url\/8oz1v5B3P0W95o6FWMDht4yfVLD.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png\" alt=\"image\" data-base62-sha1=\"8oz1v5B3P0W95o6FWMDht4yfVLD\" width=\"690\" height=\"337\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_1035x505.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1326\u00d7648 66 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nWhat issue do I have here?  Thanks in advance.<\/p>",
        "Challenge_closed_time":1636785650863,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636684136179,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to train a model, specifically with the \"--save_period 1\" argument. They are seeking assistance in resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/save-period-not-working\/1264",
        "Challenge_link_count":6,
        "Challenge_participation_count":3,
        "Challenge_readability":28.8,
        "Challenge_reading_time":22.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":28.1985233334,
        "Challenge_title":"Save_period Not Working",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":264.0,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I think you have  a typo. According to the usage info, the argument name is <code>--save-period<\/code>  , not <code>--save_period<\/code><\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":1.81,
        "Solution_score_count":null,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.4171983334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking at Azure's training modules and it states I can learn no-code models with Azure, but it also tells me I should know python. I'm a little confused at where I should spend time training in most efficient pathway. My goal is to just do predictive modeling within Azure. I have technical\/IT literacy however coding is at a basic level.   <\/p>\n<p>Ideally id like some sort of Certification, if possible from just &quot;Create no-code predictive models with Azure Machine Learning&quot;  <\/p>\n<p>Is &quot;Microsoft Certified: Azure Data Scientist Associate&quot; going to require a lot of pre work on python\/torch\/tensor? I'd ideally like Azure to be my entry. <\/p>",
        "Challenge_closed_time":1592924808467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1592869306553,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is confused about the most efficient pathway to learn code-free predictive modeling in Azure, as they have basic coding skills and want to obtain a certification. They are unsure if the \"Microsoft Certified: Azure Data Scientist Associate\" certification will require pre-work on Python\/Torch\/Tensor and would prefer Azure to be their entry point.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/38841\/pathway-for-code-free-predictive-modeling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.6,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.4171983334,
        "Challenge_title":"Pathway for code free predictive modeling",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":114,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Thanks for reaching out. Azure machine learning has a drag and drop interface (Designer) that supports code free predictive modeling. <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/paths\/create-no-code-predictive-models-azure-machine-learning\/\">Create no-code predictive models with Azure Machine Learning<\/a> training modules is a great starting point and provides a pathway for <a href=\"https:\/\/learn.microsoft.com\/en-us\/learn\/certifications\/azure-data-scientist\">Azure Data Scientist Associate certification<\/a>. However, you also need programming experience and familiarity with various data science processes\/principles to be successful on the certification exam.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.9,
        "Solution_reading_time":8.98,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":68.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1384343462316,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pune, Maharashtra, India",
        "Answerer_reputation_count":478.0,
        "Answerer_view_count":118.0,
        "Challenge_adjusted_solved_time":48.2131977778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Challenge_closed_time":1562240543612,
        "Challenge_comment_count":0,
        "Challenge_created_time":1562066976100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an MLproject with zero parameters using mlflow, but is encountering an error message indicating that 'NoneType' object has no attribute 'items'. The user is unsure if they are missing something or if mlflow does not allow projects with zero parameters.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":30.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":48.2131977778,
        "Challenge_title":"How do I specify mlflow MLproject with zero parameters?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":353.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1384343462316,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":478.0,
        "Poster_view_count":118.0,
        "Solution_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":3.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":6.1298294445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to deploy a xgboost model trained locally using amazon sagemaker? I only saw tutorial talking about both training and deploying model with amazon sagemaker.\nThanks.<\/p>",
        "Challenge_closed_time":1531778753543,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531756686157,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to deploy a locally trained xgboost model on Amazon SageMaker, as they have only found tutorials that cover both training and deployment on SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51365850",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":2.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":6.1298294445,
        "Challenge_title":"how to deploy a xgboost model on amazon sagemaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1641.0,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530193663836,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":53.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>This <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/d5681a07611ae29567355b60b2f22500b561218b\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\" rel=\"nofollow noreferrer\">example notebook<\/a> is good starting point showing how to use a pre-existing scikit-learn xgboost model with the Amazon SageMaker to create a hosted endpoint for that model.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.1,
        "Solution_reading_time":5.55,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":347.7586111111,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nWhen activating the Comet contrib, most of Ludwig log message disappears.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nLaunch: `ludwig experiment --data_csv reuters-allcats.csv --model_definition_file model_definition.yaml -l info --comet`\r\n\r\nYou won't see the following output:\r\n```\r\n _         _        _      \r\n| |_  _ __| |_ __ _(_)__ _ \r\n| | || \/ _` \\ V  V \/ \/ _` |\r\n|_|\\_,_\\__,_|\\_\/\\_\/|_\\__, |\r\n                     |___\/ \r\nludwig v0.1.2 - Experiment\r\n\r\nExperiment name: experiment\r\nModel name: run\r\nOutput path: results\/experiment_run_43\r\n\r\n\r\nludwig_version: '0.1.2'\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe log messages should be displayed when the Comet contrib is activated.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Fedora\r\n - Version 28\r\n- Python version: 3.6.8\r\n- Ludwig version: 0.1.2\r\n\r\n**Additional context**\r\n\r\nI think the issue is that ludwig is using the root-level logger configured through `logging.basicConfig`. The comet contrib integration contains some logging calls, for example, https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/contribs\/comet.py#L56.\r\n\r\nThose calls happen before any `basicConfig` call https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/experiment.py#L461.\r\n\r\nThe issue with calling the root-level `logging.info`, `logging.error` and so on is that they will call `logging.basicConfig` on their own if the root logger is not configured yet https:\/\/github.com\/python\/cpython\/blob\/master\/Lib\/logging\/__init__.py#L2065. The direct effect is that the first call to `logging.info` will configure the root logger with no configuration which will create a StreamHandler pointing to `\/dev\/stderr`.\r\n\r\nThe unfortunate side-effect is that calling `basicConfig` will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device.\r\n\r\nI would recommend moving from using the root logger and configure the logger through `basicConfig` to using a `ludwig` logger and configure it manually, it's not that more complex. I can help if wanted.\r\n\r\nOne last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. That includes requests and is polluting the output. Using a separate logger would also solve this issue.\r\n",
        "Challenge_closed_time":1559077203000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1557825272000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel, according to the user. No error messages, stack traces, or logs were provided. The user did not provide any steps to reproduce the issue or any additional context.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/340",
        "Challenge_link_count":3,
        "Challenge_participation_count":8,
        "Challenge_readability":11.0,
        "Challenge_reading_time":29.59,
        "Challenge_repo_contributor_count":123.0,
        "Challenge_repo_fork_count":1021.0,
        "Challenge_repo_issue_count":2798.0,
        "Challenge_repo_star_count":8658.0,
        "Challenge_repo_watch_count":181.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":347.7586111111,
        "Challenge_title":"Logging issue when activating Comet contrib",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":315,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"@Lothiraldan thanks for reporting this. It is indeed a bug to fix. Also notifying @dsblank as he is the main contributor of the comet integration.\r\nWould gladly accept your help offer on this. I'd like to avoid having a logger object that is passed around the whole codebase, but apart from that I'm open to suggestions.\r\n @w4nderlust Yes, and @Lothiraldan and I both are on the Comet team, so we have already been discussing this. @Lothiraldan has much expertise in loggers, so I look forward to his suggestions as well. Hi @w4nderlust, thank you for your prompt answer. I forgot to said that I'm working with @dsblank in the Comet team.\r\n\r\nHaving a non-global logger is ideal but not always feasible.\r\n\r\nThe approach I'm using in my projects is the following, use a logger per module: `LOGGER = logging.getLogger(__name__)`. As the __name__ often contains your project name, you get will get loggers like `dulwich`, `dulwich.experiment`, `dulwich.contribs.comet`. I think I have taken this idea from Django https:\/\/docs.djangoproject.com\/en\/2.1\/topics\/logging\/#using-logging.\r\n\r\nThis way you can configure the top-level logger for your project and every other loggers will just propagate the log messages to it and uses the configured handlers. This unlock having different log level on a module basis or even different handlers if needed.\r\n\r\nI would highly recommend having a central function where the top-level logger is configured, something like https:\/\/github.com\/Lothiraldan\/balto\/blob\/master\/balto\/_logging.py#L6, I found it that it really helps for maintaining a coherent logging configuration.\r\n\r\nApart from that, the Ludwig project seems to be only using a StreamHandler right now so there is no much expertise I can give you on the handlers subjects.\r\n\r\nDon't hesitate if you have some questions. Thanks for the detailed explanation @Lothiraldan . @msaisumanth Is on top of it. It looks pretty straightforward: have a single global logger setup function, add a logger in every module, use that logger instead of logging. I expect this to be solved pretty quickly.\r\nThank you again! @Lothiraldan please take a look at https:\/\/github.com\/uber\/ludwig\/pull\/352\r\nI was able to verify that the output is getting printed as expected. @w4nderlust if this makes sense, I'll modify the other modules as well.  @Lothiraldan it would be great if you could take a look at the PR, it should solve the issue, but wanted to doublecheck with you before merging it. We merged the PR as we believe it works fine, @Lothiraldan if you could take a look at it to confirm it's fine for you too, that owuld be great. I made some comments, sorry for the delay, I was busy with some other stuff.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":7.6,
        "Solution_reading_time":32.79,
        "Solution_score_count":null,
        "Solution_sentence_count":30.0,
        "Solution_word_count":426.0,
        "Tool":"Comet"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":23.4807313889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>my dataset is 3 folders (train, validation and test) of images. each folder has two subfolders (cat1 and cat2). I am using AWS sage maker to preprocess my data and train my model. we all know that we have to upload the training data to S3 bucket before starting the &quot;.fit&quot; process.\nI want to know how to upload my data set to S3<\/p>\n<pre><code># general prefix\nprefix='chest-xray'\n#unique train\/test prefixes\ntrain_prefix   = '{}\/{}'.format(prefix, 'train')\nval_prefix   = '{}\/{}'.format(prefix, 'validation')\ntest_prefix    = '{}\/{}'.format(prefix, 'test')\n\n# uploading data to S3, and saving locations\ntrain_path  = sagemaker_session.upload_data(train_data, bucket=bucket, key_prefix=train_prefix)\n<\/code><\/pre>\n<p>what the train_data parameters should look like<\/p>",
        "Challenge_closed_time":1611678605710,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611594075077,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know how to upload their image dataset, which consists of three folders (train, validation, and test) with two subfolders each (cat1 and cat2), to S3 in AWS SageMaker for preprocessing and model training. They have provided a code snippet for uploading data to S3 but are unsure about the format of the \"train_data\" parameter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65889143",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":23.4807313889,
        "Challenge_title":"upload image dataset to S3 sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":444.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1477757915556,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":79.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>According to the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session.upload_data\" rel=\"nofollow noreferrer\">documentation<\/a> <code>train_data<\/code> is the local path of the file to upload to S3, so you need this file locally where you are launching the training job. If you are using a notebook this is not the way to do. You have instead to manually upload your dataset in a S3 bucket. I suggest to preprocess your dataset in a single file (tfrecord for example if you are using TF) and upload that file to S3. You can do it using the AWS web console or using the AWS-CLI with the <code>aws s3 cp yourfile s3:\/\/your-bucket <\/code>command.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":8.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":102.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":25.8006916667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Challenge_closed_time":1539965456440,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539872573950,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a Sagemaker object detection model and wants to test it on a dataset of 2000 pictures saved in an S3 bucket. They are currently using code to load a single picture from the bucket and test the model on it, but they want to know how to apply the model to all 2000 pictures at once and save the results.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":18.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":25.8006916667,
        "Challenge_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":112.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1489873508190,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":509.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":8.31,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":99.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":64.0951252778,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm using SageMaker Studio, and I have my data files as well as a requirements.txt organized under my home directory. All works fine when I run notebook kernels interactively: they can access my files just fine. However, when I create a \"notebook job\", it doesn't seem to have access to any of my files. Is there a way to give my notebook job access to the same file system as my interactive notebooks?\n\nAfter I run a job, I see that a folder for the job was created within the input S3 bucket, and within that folder there's a \"input\/\" subfolder. But I don't know how to predict the name of the temp folder created for the job, so it doesn't seem like I could myself drop additional inputs in there, even if I wanted to. And if I could, how would I find them, at run-time?\n\nCould sure use guidance on how my notebook jobs can access input files.\n\nThanks,\n\nChris",
        "Challenge_closed_time":1671466765571,
        "Challenge_comment_count":1,
        "Challenge_created_time":1671236023120,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in accessing data files and requirements.txt organized under their home directory in SageMaker Studio when running a \"notebook job\". While interactive notebook kernels can access the files, the notebook job does not seem to have access to them. The user is seeking guidance on how to give the notebook job access to the same file system as the interactive notebooks.",
        "Challenge_last_edit_time":1671582880756,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUqh6oq1d6QbKzNSszEwPm0g\/can-sagemaker-notebook-jobs-access-studio-storage",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":7.5,
        "Challenge_reading_time":10.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":64.0951252778,
        "Challenge_title":"Can SageMaker notebook jobs access studio storage?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":68.0,
        "Challenge_word_count":168,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi Chris, the option to use input files is to directly use the S3 URIs in the notebook itself, i.e., instead of reading from an `inputs` folder in your local EFS storage (which doesn't get copied over to `inputs` folder for the training job), read the inputs directly from the S3 URI. If the inputs will be dynamic for your notebook jobs, use parameterized executions (reference - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-auto-run-troubleshoot-override.html)",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1671466784964,
        "Solution_link_count":1.0,
        "Solution_readability":13.2,
        "Solution_reading_time":5.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":142.8473586111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a pipeline in Azure Machine Learning that includes a <strong>Math Operation<\/strong> (natural logarithm of a column named <em>charges<\/em>). The next pill to the <strong>Math Operatio<\/strong>n is <strong>Select Column in Dataset<\/strong>. Since the pipeline has not ben submitted and run I cannot access the column <em>ln(charges)<\/em> in the pill <strong>Select Column in Dataset<\/strong>.\nMy problem is that if I submit it I am able to run it and see the results in the pipeline once completed, but I have found no way of accessing those results (and thus the <em>ln(charges)<\/em> column in Designer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DOddA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DOddA.png\" alt=\"Pipeline Job after submitting and running\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Hp6Dc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Hp6Dc.png\" alt=\"Pipeline in designer after submitting and running the job\" \/><\/a><\/p>\n<p><strong>UPDATE:<\/strong><\/p>\n<p><strong>I have found a workaround. Still in designer the column ln(charges) is not selectable but if I manually enter Ln(charges) in the select column fields it works.<\/strong><\/p>",
        "Challenge_closed_time":1663747263323,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663174586247,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in selecting a column product of a math operation in Azure Machine Learning Designer. They have created a pipeline that includes a math operation, but cannot access the column in the Select Column in Dataset pill until the pipeline is submitted and run. The user has found a workaround by manually entering the column name in the select column fields.",
        "Challenge_last_edit_time":1663233012832,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73720626",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":16.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":159.0769655556,
        "Challenge_title":"How can I select a column product of a math operation in Azure Machine Learning Designer?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Poster_created_time":1526397625168,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Spain",
        "Poster_reputation_count":47.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>The following is the procedure of the math operation in Azure ML designer to select the column to be implemented. The following procedure will help to give the column name as well as we can also give the index number of the column. This answer contains both the procedures.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9YV8f.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9YV8f.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We can click on edit column.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAnfq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAnfq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/cZFfF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cZFfF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Based on the dataset which the experiment was running, both are options are mentioned in the above screen. We can choose either of the options.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PblH7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PblH7.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To access the data, right click and go to access data and click on result_dataset<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8jHz5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8jHz5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The following page will open and click on any file mentioned in the box<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4jWxT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4jWxT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on download and open in the editor according to your wish.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wfGPV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wfGPV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>It looks like the above result screen.\nThe below screens are the designer created.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bTWGR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bTWGR.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6kUAw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6kUAw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/I2Ej1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I2Ej1.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To check the final model result. Go to evaluate model and get the results in visualization manner.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":20.0,
        "Solution_readability":11.2,
        "Solution_reading_time":34.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":252.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":75.75,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\n\u00a0\nIs there a way to download my Google AutoML Transation model and use it offline once it's trained?\n\u00a0\nAnd in what format can the model be exported?\u00a0\n\u00a0\nThank you",
        "Challenge_closed_time":1664553420000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664280720000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to download and use their Google AutoML Translation model offline after it has been trained. They are also inquiring about the format in which the model can be exported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/exporting-a-google-autoML-translate-model\/m-p\/471646#M607",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.7,
        "Challenge_reading_time":2.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":75.75,
        "Challenge_title":"exporting a google autoML translate model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":142.0,
        "Challenge_word_count":35,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"1.- No.\n\n2.- You can create a Feature Request at\u00a0Issue Tracker\u00a0and\u00a0add a description about the feature you want(Export Translation Models), and the engineer team will look at it. You can see here how it is more likely that the team prioritize the work of the Feature Request\/Issues.\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":3.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1405457120427,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":3.9360030556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a simply model and then registered in azure. How can I make a prediction?<\/p>\n<pre><code>from sklearn import svm\nimport joblib\nimport numpy as np\n\n# customer ages\nX_train = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nX_train = X_train.reshape(-1, 1)\n# churn y\/n\ny_train = [&quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;]\n\nclf = svm.SVC(gamma=0.001, C=100.)\nclf.fit(X_train, y_train)\n\njoblib.dump(value=clf, filename=&quot;churn-model.pkl&quot;)\n<\/code><\/pre>\n<p>Registration:<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.get(name=&quot;myworkspace&quot;, subscription_id='My_subscription_id', resource_group='ML_Lingaro')\n\nfrom azureml.core.model import Model\nmodel = Model.register(workspace=ws, model_path=&quot;churn-model.pkl&quot;, model_name=&quot;churn-model-test&quot;)\n<\/code><\/pre>\n<p>Prediction:<\/p>\n<pre><code>from azureml.core.model import Model\nimport os\n\nmodel = Model(workspace=ws, name=&quot;churn-model-test&quot;)\nX_test = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nmodel.predict(X_test) ???? \n<\/code><\/pre>\n<p>Error: <code>AttributeError: 'Model' object has no attribute 'predict'<\/code><\/p>",
        "Challenge_closed_time":1609737626627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1609722575690,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created and registered a model in Azure, but is facing an error while trying to make a prediction using the registered model. The error message states that the 'Model' object has no attribute 'predict'.",
        "Challenge_last_edit_time":1609723457016,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65556574",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":17.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":4.1808158334,
        "Challenge_title":"How to make prediction after model registration in azure?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":540.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1605834001336,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":73.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>great question -- I also had the same misconception starting out. The missing piece is that there's a difference between model 'registration' and model 'deployment'. Registration is simply for tracking and for easy downloading at a later place and time. Deployment is what you're after, making a model available to be scored against.<\/p>\n<p>There's a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">whole section in the docs about deployment<\/a>. My suggestion would be to deploy it locally first for testing.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":8.06,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":75.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":23.4833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"I have set up an Image classification (Single-label).\n\nThe model trained for 18 min 25 sec before I recieved the following error:\n\nDue to one or more errors, this training job was canceled on Jan 11, 2022 at 07:34AM Batch prediction job GAF-prediction-test encountered the following errors: No valid preprocessed examples.\n\nThere is no documentation that I could find that explains this error type. Anyone with any ideas what this means?",
        "Challenge_closed_time":1641993840000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1641909300000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user encountered an error while using AutoML Vision for image classification, where the training job was cancelled due to \"No valid preprocessed examples\" error. The user is seeking help to understand the meaning of this error as there is no documentation available.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Vision-Error-type-No-valid-preprocessed-examples\/m-p\/183067#M170",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.4,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":23.4833333333,
        "Challenge_title":"AutoML Vision - Error type - No valid preprocessed examples",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":97.0,
        "Challenge_word_count":78,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, Thank you for reporting this behavior.\u00a0 \u00a0\n\nPlease note that\u00a0Groups is reserved for general product discussions. If you require further technical support it is recommended to post your detailed question on Stack Overflow which i can see that you have correctly did. [1].\n\n\u00a0\n\n[1]:https:\/\/stackoverflow.com\/questions\/70673890\/automl-vision-error-no-valid-preprocessed-examples\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":50.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2574272222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I repeat the example from <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-sql-edge\/deploy-onnx\">https:\/\/learn.microsoft.com\/en-us\/azure\/azure-sql-edge\/deploy-onnx<\/a> &quot;Deploy and make predictions with an ONNX model and SQL machine learning&quot; In this quickstart, you'll learn how to train a model, convert it to ONNX, deploy it to Azure SQL Edge, and then run native PREDICT on data using the uploaded ONNX model.    <\/p>\n<p>Successfully create a model using Python, convert to onnx format, I test the model using Python, save the model to the database, load the necessary data and try to execute the SQL query    <br \/>\nUSE onnx    <br \/>\nDECLARE <a href=\"\/users\/na\/?userid=0f55de7e-bffd-0003-0000-000000000000\">@\u9ed8  <\/a> VARBINARY(max) = (    <br \/>\n    SELECT DATA  <br \/>\n    FROM dbo.models  <br \/>\n    WHERE id = 1  <br \/>\n    );  <br \/>\nWITH predict_input    <br \/>\nAS (    <br \/>\n    SELECT TOP (1000) [id]  <br \/>\n    , CRIM  <br \/>\n    , ZN  <br \/>\n    , INDUS  <br \/>\n    , CHAS  <br \/>\n    , NOX  <br \/>\n    , RM  <br \/>\n    , AGE  <br \/>\n    , DIS  <br \/>\n    , RAD  <br \/>\n    , TAX  <br \/>\n    , PTRATIO  <br \/>\n    , B  <br \/>\n    , LSTAT  <br \/>\nFROM [dbo].[features]    <br \/>\n)    <br \/>\nSELECT predict_input.id    <br \/>\n, p.variable1 AS MEDV    <br \/>\nFROM PREDICT(MODEL = <a href=\"\/users\/na\/?userid=0f55de7e-bffd-0003-0000-000000000000\">@\u9ed8  <\/a>, DATA = predict_input, RUNTIME=ONNX) WITH (variable1 FLOAT) AS p;    <\/p>\n<p>As a result I get an error Msg 102, Level 16, State 5, Line 27 Incorrect syntax near 'RUNTIME'.    <\/p>\n<p>I can't figure out what's wrong. The documentation clearly says &quot;The RUNTIME = ONNX argument is only available in Azure SQL Edge, Azure Synapse Analytics, and is in Preview in Azure SQL Managed Instance.&quot;<\/p>",
        "Challenge_closed_time":1650461393248,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650460466510,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to execute a SQL query using an ONNX model in Azure SQL Managed Instance. The error message states \"Incorrect syntax near 'RUNTIME'\". The user is unsure of what is causing the error and mentions that the \"RUNTIME = ONNX\" argument is only available in Azure SQL Edge, Azure Synapse Analytics, and is in Preview in Azure SQL Managed Instance.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/819485\/azure-sql-managed-instance-predict-with-an-onnx-mo",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.7,
        "Challenge_reading_time":21.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":0.2574272222,
        "Challenge_title":"Azure SQL Managed Instance PREDICT with an ONNX model",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":228,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I think there is a misunderstanding here. The quickstart article named &quot;<strong><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-sql-edge\/deploy-onnx\">Deploy and make predictions with an ONNX model and SQL machine learning<\/a><\/strong>&quot; can be successfully implemented only with Azure SQL Edge and cannot be implemented with Azure SQL Managed Instance.    <\/p>\n<p>You cannot have an ONNX model and make predictions with it on Azure SQL Managed Instance. Please deploy Azure SQL Edge on an IoT device using <strong><a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/azure-sql-edge\/deploy-portal\">this<\/a><\/strong> documentation.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.3,
        "Solution_reading_time":8.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":73.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":795.3458333333,
        "Challenge_answer_count":0,
        "Challenge_body":"I got this error with Azure Machine Learning. \r\n\r\nConfigException: ConfigException:\r\n\tMessage: blacklisted and whitelisted models are exactly the same. Found: {'XGBoostClassifier'}.Please remove models from the blacklist or add models to the whitelist.\r\n\r\nThe settings are as follow. 'XGBoostClassifier' is in the whitelist; and backlist is None. Would you please help with the error?\r\n\r\nautoml_settings = {\r\n    \"iteration_timeout_minutes\": 2,\r\n    \"experiment_timeout_minutes\": 20,\r\n    \"enable_early_stopping\": True,\r\n    \"primary_metric\": 'accuracy',\r\n    \"featurization\": 'auto',\r\n    \"verbosity\": logging.INFO,\r\n    \"n_cross_validations\": 5\r\n}\r\n\r\nfrom azureml.train.automl import AutoMLConfig\r\n\r\nautoml_config = AutoMLConfig(task='classification',\r\n                             enable_tf = True,\r\n                             debug_log='automated_ml_errors.log',\r\n                             X=x_train.values,\r\n                             y=y_train.values.flatten(),\r\n                             blacklist_models = None,\r\n                             whitelist_models = ['XGBoostClassifier'],\r\n                             **automl_settings)\r\n\r\n(Note: XGBoostClassifier was installed in the notebook)",
        "Challenge_closed_time":1583863460000,
        "Challenge_comment_count":8,
        "Challenge_created_time":1581000215000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an ImportError while trying to import 'AutoMLStep' from 'azureml.train.automl'.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_participation_count":8,
        "Challenge_readability":16.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":795.3458333333,
        "Challenge_title":"Azure Machine Learning error: Can not use 'XGBoostClassifier'",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":98,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hello,\r\n\r\nWe're so sorry you've encountered this issue. I've gone ahead and filed a work item to investigate and fix the issue around whitelisting XGBoost. We will reach out again here once a fix is in.\r\n\r\nThank you,\r\nSabina It could be possible that XGBoostClassifier was blacklisted by the system. We can double check if you can share your runId. In the meanwhile, we will improve the error msg for this scenario. Thanks! @waltz2u Can you please run the following line of code in your jupyter notebook and let me know what it says? \r\n\r\n`import xgboost`\r\n\r\nThanks,\r\nSabina Hi @waltz2u, I was able to reproduce and overcome this issue by double checking that import xgboost was installed correctly by trying `import xgboost`.\r\n\r\n\r\n`pip install \"py-xgboost<=0.80\"` fixed it on my end. Can you please try that and let us know if it solved the issue?  Hi @cartacioS and @jialiu103, sorry for the late reply. Yes it works now for me. Thank you very much.\r\n\r\nCD\r\n Will now proceed to close this thread. Thanks. @cartacioS I'm facing the same error, except that I'm kicking off the AutoML run from my local machine, using a remote compute as my aml compute target. Using this issue above, and this [one](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/313), it seems that I would still need to add xgboost to my env (locally) although technically I won't be using that package in my AutoML exercise? @jadhosn If you do not require XGBoost for your training, you can simply ignore this warning. But if you want XGBoost to be a potential recommended model, then yes you will need to add XGBoost to your local environment regardless of local\/remote compute.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":6.3,
        "Solution_reading_time":19.93,
        "Solution_score_count":null,
        "Solution_sentence_count":20.0,
        "Solution_word_count":275.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2222222222,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\nHow can we know that checkpoint works before launching a sagemaker spot training job?\nIs there a way to force a regular checkpoint to s3 instead of waiting for the SIGTERM?\n\ncheers",
        "Challenge_closed_time":1584346840000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584346040000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to verify if checkpoints work for SageMaker Spot Training before launching a job. They are also looking for a way to force a regular checkpoint to S3 instead of waiting for the SIGTERM.",
        "Challenge_last_edit_time":1667926338452,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUbvA_lGXgQ3CdPuEoImWQVw\/how-to-verify-that-checkpoints-work-for-sagemaker-spot-training",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":3.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2222222222,
        "Challenge_title":"How to verify that checkpoints work for SageMaker Spot Training?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":101.0,
        "Challenge_word_count":42,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":1.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":1.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi olivier, \nIf you enable Sagemaker checkpointing , it periodically saves a copy of the artifacts into S3. I have used this in pytorch and it works by checkpointing periodically and the  blog on [Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs][1] also mentions the same \n\n> To avoid restarting a training job from scratch should it be interrupted, we strongly recommend that you implement checkpointing, a technique that saves the model in training at periodic intervals\n\n\n  [1]: https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925558612,
        "Solution_link_count":1.0,
        "Solution_readability":21.6,
        "Solution_reading_time":7.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":82.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1611181716003,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":5.0,
        "Challenge_adjusted_solved_time":169.1649333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a Pipeline in Azure ML which makes calls to Azure Cognitive Services Text Analytics using its Python API. When I run the code I have written locally, it executes without error, but when run it in the pipeline it fails to perform the Sentiment Analysis and Key Phrase Extraction calls with a strange error message:<\/p>\n<blockquote>\n<p>Got exception when invoking script at line 243 in function azureml_main: 'ServiceRequestError: &lt;urllib3.connection.HTTPSConnection object at 0x7ff4dc727588&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'.<\/p>\n<\/blockquote>\n<p>Upon further testing, it appears that it is able to open the Text Analytics Client correctly (Or at least without throwing an error), but when it gets to the line that actually makes the call out using the Python API it throws the above error.<\/p>\n<p>I wondered if it was an Open SSL issue, but when I checked the version it had access to TLS 1.2: <code>OpenSSL 1.1.1k  25 Mar 2021<\/code><\/p>\n<p>It does not appear to be a temporary issue; I started seeing the issue last week, and I have seen it over a number of environments and with different input datasets.<\/p>\n<p>Has anyone seen a similar issue before? Any ideas on how it could be resolved?<\/p>",
        "Challenge_closed_time":1631580778540,
        "Challenge_comment_count":2,
        "Challenge_created_time":1630894745580,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error when executing calls to Azure Cognitive Services Text Analytics using its Python API in an Azure ML pipeline. The error message indicates a failure to establish a new connection, and the issue persists across different environments and input datasets. The user is seeking advice on how to resolve the issue.",
        "Challenge_last_edit_time":1630971784780,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69068520",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.6,
        "Challenge_reading_time":16.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":190.5647111111,
        "Challenge_title":"Azure Machine Learning Python Module failing to Execute Calls to Cognitive Services",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":299.0,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Poster_created_time":1611181716003,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":5.0,
        "Solution_body":"<p>After speaking with Microsoft Support, it turns out this error was a platform error introduced in a recent update of Azure ML. Their product team are currently investigating a solution.<\/p>\n<p>As a temporary fix, if you see this issue, you can try switching between using your personal endpoint and the generic regional endpoint; In this case, the error was only introduced for using personal endpoints. The endpoints in question have the following formats:<\/p>\n<ul>\n<li>Personal: <code>https:\/\/&lt;COGNITIVE-SERVICES-INSTANCE&gt;.cognitiveservices.azure.com\/<\/code><\/li>\n<li>Regional: <code>https:\/\/&lt;REGION&gt;.api.cognitive.microsoft.com\/<\/code><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.0,
        "Solution_reading_time":8.61,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":79.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1525393797416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Philadelphia, USA",
        "Answerer_reputation_count":130.0,
        "Answerer_view_count":10.0,
        "Challenge_adjusted_solved_time":0.4764786111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Challenge_closed_time":1611840603100,
        "Challenge_comment_count":0,
        "Challenge_created_time":1611838887777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to serve an mlflow model locally and is encountering a \"FileNotFoundError\" when trying to run the model on a local port. The error message suggests that the file or directory specified in the model URI does not exist.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65937623",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":23.6,
        "Challenge_reading_time":47.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":0.4764786111,
        "Challenge_title":"Unable to serve an mlflow model locally",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1277.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1573739890560,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Solution_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":15.2,
        "Solution_reading_time":11.53,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1374169767267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation_count":548.0,
        "Answerer_view_count":70.0,
        "Challenge_adjusted_solved_time":7.0366380556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to call my SageMaker model endpoint both from Postman and the AWS CLI. The endpoint's status is &quot;in service&quot; but whenever I try to call it it gives me an error. When I try to use the predict function in the SageMaker notebook and provide it a numpy array (ex. <code>np.array([1,2,3,4])<\/code>), it successfully gives me an output. I'm unsure what I'm doing wrong.<\/p>\n<pre><code>$ aws2 sagemaker-runtime invoke-endpoint \\\n$ --endpoint-name=pytorch-model \\\n$ --body=1,2 \\\n$ --content-type=text\/csv \\\n$ --cli-binary-format=raw-in-base64-out \\\n$ output.json\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message &quot;tensors used as indices must be long, byte or bool tensors\nTraceback (most recent call last):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 125, in transform\n    result = self._transform_fn(self._model, input_data, content_type, accept)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py&quot;, line 215, in _default_transform_fn\n    prediction = self._predict_fn(data, model)\n  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>",
        "Challenge_closed_time":1596555829307,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596530497410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to call their SageMaker model endpoint using Postman and AWS CLI. The endpoint's status is \"in service,\" but the error message indicates that tensors used as indices must be long, byte, or bool tensors. However, when the user tries to use the predict function in the SageMaker notebook and provides a numpy array, it successfully gives an output.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63243154",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.5,
        "Challenge_reading_time":18.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":7.0366380556,
        "Challenge_title":"Invoking SageMaker Endpoint for PyTorch Model",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":394.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>The clue is in the final few lines of your stacktrace:<\/p>\n<pre><code>  File &quot;\/opt\/ml\/model\/code\/pytorch-model-reco.py&quot;, line 268, in predict_fn\n    return torch.argsort(- final_matrix[input_data, :], dim = 1)\nIndexError: tensors used as indices must be long, byte or bool tensors\n<\/code><\/pre>\n<p>In your <code>predict_fn<\/code> in <code>pytorch-model-reco.py<\/code> on line 268, you're trying to use <code>input_data<\/code> as indices for <code>final_matrix<\/code>, but <code>input_data<\/code> is the wrong type.<\/p>\n<p>I would guess there is some type casting that your <code>predict_fn<\/code> should be doing when the input type is <code>text\/csv<\/code>. This type casting is happening outside of the <code>predict_fn<\/code> when your input type is numpy data. Taking a look at the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/tree\/master\/src\/sagemaker_inference\" rel=\"nofollow noreferrer\"><code>sagemaker_inference<\/code><\/a> source code might reveal more.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.3,
        "Solution_reading_time":12.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":109.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":18.5230555556,
        "Challenge_answer_count":0,
        "Challenge_body":"This is more like a suggestion than a bug. The `config` parameter to the WandBLogger is supposed to be of type `args.namespace`. Therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. This might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a YAML file). Wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nThanks :)",
        "Challenge_closed_time":1635948747000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1635882064000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing a bug where the wandb value is not updating, except for the learning rate (lr). The issue seems to be caused by mistakenly indexing a continuously updating list with list[0].",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/ContinualAI\/avalanche\/issues\/797",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":208.0,
        "Challenge_repo_issue_count":1067.0,
        "Challenge_repo_star_count":1173.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":18.5230555556,
        "Challenge_title":"Config type in WandBLogger",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":87,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"I agree, we can easily add support for plain dictionary. @digantamisra98 are you still working on the logger right? Can you take care of this? I've made a simple fix to it by removing the conversion inside WandBLogger. It works with plain dictionaries now. I also made a PR just in case.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":3.47,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":28.9666666667,
        "Challenge_answer_count":2,
        "Challenge_body":"In our project we use 21 custom trained models to translate from EN to target_language.\n\nLast week 30% of all our requests finishes with timeout! What is the problem?!\n\nHow can you\/we fix it?",
        "Challenge_closed_time":1680180000000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680075720000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is experiencing a problem where 30% of their requests for translation using 21 custom trained models are timing out. They are seeking assistance in identifying and resolving the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-Translation-30-of-all-our-requests-finishes-with-timeout\/m-p\/538365#M1523",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":4.3,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":28.9666666667,
        "Challenge_title":"AutoML Translation: 30% of all our requests finishes with timeout",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":43,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi\u00a0@ochkarik\u00a0\n\nWelcome back to Google Cloud Community.\n\nSetting request timeout (services)\nFor Cloud Run services, the request timeout setting specifies the time within which a response must be returned by services deployed to Cloud Run. If a response isn't returned within the time specified, the request ends and error 504 is returned.\n\nThe timeout is set by default to 5 minutes and can be extended up to 60 minutes.\n\nHere are some articles that might help you:\nhttps:\/\/cloud.google.com\/run\/docs\/configuring\/request-timeout\n\nhttps:\/\/cloud.google.com\/python\/docs\/reference\/storage\/1.39.0\/retry_timeout?_ga=2.21056062.-48059091...\n\nhttps:\/\/cloud.google.com\/translate\/docs\/reference\/rpc\/google.longrunning?_ga=2.25258296.-480590913.1...\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":12.4,
        "Solution_reading_time":9.92,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":86.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1459917054448,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Frankfurt, Germany",
        "Answerer_reputation_count":9168.0,
        "Answerer_view_count":675.0,
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created a time series predictor with Keras and  Dockerized the model with with Flash and Gunicorn as per AWS docs. I am loading the serialized model with this code.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n    return cls.model\n<\/code><\/pre>\n\n<p>Then I used the predict method to produce the results , the dockerized container is working perfectly in the local environment , but when I try to host the model in sagemaker it produces this error.<\/p>\n\n<pre><code>ValueError: Tensor Tensor(\"dense_1\/BiasAdd:0\", shape=(?, 1), dtype=float32) is not an element of this graph.\n<\/code><\/pre>\n\n<p>So how can I resolve this issue ?<\/p>",
        "Challenge_closed_time":1541480239328,
        "Challenge_comment_count":0,
        "Challenge_created_time":1541480239330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a ValueError while hosting a time series predictor model in Sagemaker with Gunicorn and Flask and Keras. The error message states that the Tensor is not an element of this graph. The model works fine in the local environment but produces an error when hosted in Sagemaker. The user is seeking a solution to resolve this issue.",
        "Challenge_last_edit_time":1541480860787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/53165953",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"ValueError: Tensor is not an element of this graph, when hosting a model in Sagemaker with Gunicorn and Flask and Keras",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":223.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Poster_created_time":1459917054448,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Frankfurt, Germany",
        "Poster_reputation_count":9168.0,
        "Poster_view_count":675.0,
        "Solution_body":"<p>The issue was resolved by calling _make_predict_function() method in the model load phase.<\/p>\n\n<pre><code>@classmethod\ndef get_model(cls):\n    if cls.model == None:\n        cls.model = load_model('\/opt\/ml\/bitcoin_model.h5')\n        cls.model._make_predict_function()\n    return cls.model\n<\/code><\/pre>\n\n<p>Bug Reference : <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/6462\" rel=\"nofollow noreferrer\">https:\/\/github.com\/keras-team\/keras\/issues\/6462<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":18.3,
        "Solution_reading_time":5.96,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":31.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1606724007903,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Challenge_adjusted_solved_time":69.0259405556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a AutoML Object Detection model in Vertex AI (a service under AI Platform in GCP). I am trying to access model evaluation metrics for each label (precision, recall, accuracy etc.) for varying Confidence Score Threshold and IoU Threshold.<\/p>\n<p>However, I am stuck at step one, even to get model's aggerate performance metric much less to the performance metric at granular levels. I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/evaluating-automl-models?authuser=1#aggregate\" rel=\"nofollow noreferrer\">this instruction<\/a> But I cannot seem to figure out what is <code>evaluation_id<\/code> (also see the official sample code snippet <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/blob\/main\/samples\/snippets\/model_service\/get_model_evaluation_image_object_detection_sample.py\" rel=\"nofollow noreferrer\">here<\/a>), which is:<\/p>\n<pre><code>def get_model_evaluation_image_object_detection_sample(\n    project: str,\n    model_id: str,\n    evaluation_id: str,\n    location: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n):\n    # The AI Platform services require regional API endpoints.\n    client_options = {&quot;api_endpoint&quot;: api_endpoint}\n    # Initialize client that will be used to create and send requests.\n    # This client only needs to be created once, and can be reused for multiple requests.\n    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n    name = client.model_evaluation_path(\n        project=project, location=location, model=model_id, evaluation=evaluation_id\n    )\n    response = client.get_model_evaluation(name=name)\n    print(&quot;response:&quot;, response)\n<\/code><\/pre>\n<p>After sometime I have figured out that for model trained in EU,  and <code>api_endpoint<\/code> shall be passed as:<\/p>\n<pre><code>location: str = &quot;europe-west4&quot;\napi_endpoint: str = &quot;europe-west4-aiplatform.googleapis.com&quot;\n<\/code><\/pre>\n<p>But whatever I try for <code>evaluation_id<\/code> leads to the following errors:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: name; Message: Invalid ModelEvaluation resource name.\n<\/code><\/pre>\n<p>There in the documentation it says (which is seems it contains what I need):<\/p>\n<blockquote>\n<p>For the bounding box metric, Vertex AI returns an array of metric\nvalues at different IoU threshold values (between 0 and 1) and\nconfidence threshold values (between 0 and 1). For example, you can\nnarrow in on evaluation metrics at an IoU threshold of 0.85 and a\nconfidence threshold of 0.8228. By viewing these different threshold\nvalues, you can see how they affect other metrics such as precision\nand recall.<\/p>\n<\/blockquote>\n<p>Without knowing that is contained in the output array, how would that work for each class? Basically I need for each class the model metrics for varying IoU threshold values and confidence threshold.<\/p>\n<p>Also I have tried to query from AutoML API instead, like:<\/p>\n<pre><code>client_options = {'api_endpoint': 'eu-automl.googleapis.com:443'}\n\nclient = automl.AutoMlClient(client_options=client_options)\n# Get the full path of the model.\nmodel_full_id = client.model_path(project_id, &quot;europe-west4&quot;, model_id)\n\nprint(&quot;List of model evaluations:&quot;)\nfor evaluation in client.list_model_evaluations(parent=model_full_id, filter=&quot;&quot;):\n    print(&quot;Model evaluation name: {}&quot;.format(evaluation.name))\n    print(&quot;Model annotation spec id: {}&quot;.format(evaluation.annotation_spec_id))\n    print(&quot;Create Time: {}&quot;.format(evaluation.create_time))\n    print(&quot;Evaluation example count: {}&quot;.format(evaluation.evaluated_example_count))\n    print(\n        &quot;Classification model evaluation metrics: {}&quot;.format(\n            evaluation.classification_evaluation_metrics\n        )\n    )\n<\/code><\/pre>\n<p>No surprise, also this doesn't work, and leads to:<\/p>\n<pre><code>InvalidArgument: 400 List of found errors:  1.Field: parent; Message: The provided location ID doesn't match the endpoint. For automl.googleapis.com, the valid location ID is `us-central1`. For eu-automl.googleapis.com, the valid location ID is `eu`.\n<\/code><\/pre>",
        "Challenge_closed_time":1635129616532,
        "Challenge_comment_count":0,
        "Challenge_created_time":1634902641630,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained an AutoML Object Detection model in Vertex AI and is trying to access model evaluation metrics for each label for varying Confidence Score Threshold and IoU Threshold. However, they are unable to get the model's aggregate performance metric, much less the performance metric at granular levels. They are stuck at step one and cannot figure out what the evaluation_id is. They have tried passing the correct api_endpoint for a model trained in EU, but whatever they try for evaluation_id leads to errors. They need the model metrics for varying IoU threshold values and confidence threshold for each class. They have also tried querying from AutoML API, but it doesn't work either.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69676225",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":54.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":63.0485838889,
        "Challenge_title":"GCP AI Platform API - Object Detection Metrics at Class Level (Python)",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":298.0,
        "Challenge_word_count":441,
        "Platform":"Stack Overflow",
        "Poster_created_time":1490792741112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":423.0,
        "Poster_view_count":84.0,
        "Solution_body":"<p>I was able to get the response of the model evaluation using <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">aiplatform_v1<\/a> which is well documented and this is the reference linked from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/start\/client-libraries?authuser=1#client_libraries\" rel=\"nofollow noreferrer\">Vertex AI reference page<\/a>.<\/p>\n<p>On this script I ran <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluations\" rel=\"nofollow noreferrer\">list_model_evaluations()<\/a> to get the evaluation name and used it as input for <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation\" rel=\"nofollow noreferrer\">get_model_evaluation()<\/a> which will return the evaluation details for Confidence Score Threshold, IoU Threshold, etc.<\/p>\n<p>NOTE: I don't have a trained model in <code>europe-west4<\/code> so I used <code>us-central1<\/code> instead. But if you have trained in <code>europe-west4<\/code> you should use <code>https:\/\/europe-west4-aiplatform.googleapis.com<\/code> as <code>api_endpoint<\/code> as per <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/locations#specifying_the_location_using_the\" rel=\"nofollow noreferrer\">location document<\/a>.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nget_eval_request = aiplatform.types.GetModelEvaluationRequest(name=eval_name)\nget_eval = client_model.get_model_evaluation(request=get_eval_request)\nprint(get_eval)\n<\/code><\/pre>\n<p>See response snippet:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999999\/evaluations\/1234567890&quot;\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.20201288\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.15670435\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.09326923\n                          }\n                        }\n                        fields {\n                          key: &quot;recall&quot;\n                          value {\n                            number_value: 0.48989898\n                          }\n                        }\n                      }\n                    }\n                    values {\n                      struct_value {\n....\n<\/code><\/pre>\n<p><strong>EDIT 1: Get response per class<\/strong><\/p>\n<p>To get metrics per class, you can use <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.list_model_evaluation_slices\" rel=\"nofollow noreferrer\">list_model_evaluation_slices()<\/a> to get the name for each class, then use the name to <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform_v1\/model_service.html#google.cloud.aiplatform_v1.services.model_service.ModelServiceClient.get_model_evaluation_slice\" rel=\"nofollow noreferrer\">get_model_evaluation_slice()<\/a>. In this code I pushed the names to a list since I have multiple classes. Then just use the values stored in the array to get the metric per class.<\/p>\n<p>In my code I used <code>label[0]<\/code> to get a single response from this class.<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n\napi_endpoint = 'us-central1-aiplatform.googleapis.com'\nclient_options = {&quot;api_endpoint&quot;: api_endpoint}\nclient_model = aiplatform.services.model_service.ModelServiceClient(client_options=client_options)\nproject_id = 'your-project-id'\nlocation = 'us-central1'\nmodel_id = '999999999999'\n\nmodel_name = f'projects\/{project_id}\/locations\/{location}\/models\/{model_id}'\nlist_eval_request = aiplatform.types.ListModelEvaluationsRequest(parent=model_name)\nlist_eval = client_model.list_model_evaluations(request=list_eval_request)\neval_name=''\nfor val in list_eval:\n    eval_name = val.name\n\nlabel=[]\nslice_eval_request = aiplatform.types.ListModelEvaluationSlicesRequest(parent=eval_name)\nslice_eval = client_model.list_model_evaluation_slices(request=slice_eval_request)\nfor data in slice_eval:\n    label.append(data.name)\n\nget_eval_slice_request = aiplatform.types.GetModelEvaluationSliceRequest(name=label[0])\nget_eval_slice = client_model.get_model_evaluation_slice(request=get_eval_slice_request)\nprint(get_eval_slice)\n<\/code><\/pre>\n<p>Print all classes:\n<a href=\"https:\/\/i.stack.imgur.com\/pinWU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pinWU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Classes in UI:\n<a href=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXlIW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Response snippet for a class:<\/p>\n<pre><code>name: &quot;projects\/xxxxxxxxx\/locations\/us-central1\/models\/999999999\/evaluations\/0000000000\/slices\/777777777&quot;\nslice_ {\n  dimension: &quot;annotationSpec&quot;\n  value: &quot;Cheese&quot;\n}\nmetrics_schema_uri: &quot;gs:\/\/google-cloud-aiplatform\/schema\/modelevaluation\/image_object_detection_metrics_1.0.0.yaml&quot;\nmetrics {\n  struct_value {\n    fields {\n      key: &quot;boundingBoxMeanAveragePrecision&quot;\n      value {\n        number_value: 0.14256561\n      }\n    }\n    fields {\n      key: &quot;boundingBoxMetrics&quot;\n      value {\n        list_value {\n          values {\n            struct_value {\n              fields {\n                key: &quot;confidenceMetrics&quot;\n                value {\n                  list_value {\n                    values {\n                      struct_value {\n                        fields {\n                          key: &quot;confidenceThreshold&quot;\n                          value {\n                            number_value: 0.06579724\n                          }\n                        }\n                        fields {\n                          key: &quot;f1Score&quot;\n                          value {\n                            number_value: 0.10344828\n                          }\n                        }\n                        fields {\n                          key: &quot;precision&quot;\n                          value {\n                            number_value: 0.06198347\n                          }\n                        }\n....\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1635151135016,
        "Solution_link_count":12.0,
        "Solution_readability":25.3,
        "Solution_reading_time":87.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":51.0,
        "Solution_word_count":407.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1436771091480,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Brno, \u010cesko",
        "Answerer_reputation_count":51.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":1411.4133319445,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using AutoML called via Custom Python Script module in AzureML designer.\nFor that, I need to install automl packages:<\/p>\n\n<pre><code>os.system(f\"pip install azureml-sdk[automl]==1.0.85 --upgrade\")\n<\/code><\/pre>\n\n<p>It worked correctly, but now when I call automl training I received this error:<\/p>\n\n<pre><code>pkg_resources.ContextualVersionConflict: (azureml-dataprep 1.3.2 (\/azureml-envs\/azureml_8d08fe76aaa5abe0ec642fd2de335a04\/lib\/python3.6\/site-packages), Requirement.parse('azureml-dataprep&lt;1.2.0a,&gt;=1.1.37a'), {'azureml-automl-core'})\n<\/code><\/pre>\n\n<p>Looks like there was an update in azureml-dataprep to version 1.3.2 which is not compatible with azureml-sdk[automl]==1.0.85.<\/p>\n\n<ol>\n<li>Would it be possible to add AutoML packages as default package in AzureML designer?<\/li>\n<li>Would it be possible to update azureml-sdk version in AzureML designer?<\/li>\n<li>Is there any workaround right now?<\/li>\n<\/ol>",
        "Challenge_closed_time":1589523303107,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584439428327,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered a ContextualVersionConflict issue with azureml-automl-core in AzureML designer. The error occurred after installing the automl packages and calling automl training. The issue is caused by an update in azureml-dataprep to version 1.3.2, which is not compatible with azureml-sdk[automl]==1.0.85. The user is seeking possible solutions such as adding AutoML packages as default package in AzureML designer, updating azureml-sdk version, or finding a workaround.",
        "Challenge_last_edit_time":1584442215112,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60720060",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":13.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1412.1874388889,
        "Challenge_title":"ContextualVersionConflict issue with azureml-automl-core in AzureML designer",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":210.0,
        "Challenge_word_count":103,
        "Platform":"Stack Overflow",
        "Poster_created_time":1436771091480,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Brno, \u010cesko",
        "Poster_reputation_count":51.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>Fixed after release new version of AzureML SDK in designer and update script to:<\/p>\n\n<pre><code>os.system(f\"pip install azureml-sdk[automl]==1.4.0 --upgrade\")\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":2.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":19.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1476711295896,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Malabon, Metro Manila, Philippines",
        "Answerer_reputation_count":3520.0,
        "Answerer_view_count":962.0,
        "Challenge_adjusted_solved_time":21.1495677778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am unclear abut Vertex AI pricing for model predictions. In the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing\" rel=\"nofollow noreferrer\">documentation<\/a>, under the heading <strong>More about automatic scaling of prediction nodes<\/strong> one of the points mentioned is:<\/p>\n<blockquote>\n<p>&quot;If you choose automatic scaling, the number of nodes scales\nautomatically, and can scale down to zero for no-traffic durations&quot;<\/p>\n<\/blockquote>\n<p>The example provided in the documentation later also seems to suggest that during a period with no traffic, zero nodes are in use. However, when I create an Endpoint in Vertex AI, under the <strong>Autoscaling<\/strong> heading it says:<\/p>\n<blockquote>\n<p><em>&quot;Autoscaling: If you set a minimum and maximum, compute nodes will scale to meet traffic demand within those boundaries&quot;<\/em><\/p>\n<\/blockquote>\n<p>The  value of 0 under <em>&quot;Minimum number of compute nodes&quot;<\/em> is not allowed so you have to enter 1 or greater, and it is mentioned that:<\/p>\n<blockquote>\n<p>Default is 1. If set to 1 or more, then compute resources will\ncontinuously run even without traffic demand. This can increase cost\nbut avoid dropped requests due to node initialization.<\/p>\n<\/blockquote>\n<p>My question is, what happens when I select autoscaling by setting Minimum to 1 and Maximum to, say, 10. Does 1 node always run continuously? Or does it scale down to 0 nodes in no traffic condition as the documentation suggests.<\/p>\n<p>To test I deployed an Endpoint with Autoscaling (min and max set to 1) and then when I sent a prediction request the response was almost immediate, suggesting the node was already up. I did that again after about an hour and again the response was immediate suggesting that the node never shut down probably. Also, for high latency requirements, is having autoscale to 0 nodes, if that is indeed possible, even practical, i.e., what latency can we expect for starting up from 0 nodes?<\/p>",
        "Challenge_closed_time":1636603298292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1636487376953,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is confused about the pricing and functionality of Vertex AI's autoscaling feature for model predictions. The documentation suggests that the number of nodes can scale down to zero during no-traffic durations, but the Autoscaling feature in the Endpoint creation process requires a minimum of one node to be continuously running, which can increase costs. The user is unsure if selecting a minimum of one node will always keep a node running or if it will scale down to zero during no-traffic periods. The user also questions the practicality of scaling down to zero nodes for high latency requirements.",
        "Challenge_last_edit_time":1636603578603,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69904211",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":25.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":32.2003719444,
        "Challenge_title":"Vertex AI prediction - Autoscaling cannot set minimum node to 0",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":894.0,
        "Challenge_word_count":307,
        "Platform":"Stack Overflow",
        "Poster_created_time":1471292986790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":700.0,
        "Poster_view_count":90.0,
        "Solution_body":"<p>Are you using an N1 or a non-N1 machine type? If you want to autoscale to zero, you must use non-N1 machines. See <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/overview#node_allocation_for_online_prediction\" rel=\"nofollow noreferrer\">second note<\/a> from node allocation:<\/p>\n<blockquote>\n<p>Note: Versions that use a Compute Engine (N1) machine type cannot scale down to zero nodes. They can scale down to 1 node, at minimum.<\/p>\n<\/blockquote>\n<p><em>Update<\/em>: AI Platform supports scaling to zero, while Vertex AI currently does not. From the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#scaling\" rel=\"nofollow noreferrer\">scaling<\/a> documentation, nodes can scale but there is no mention that it can scale down to zero. Here's a public <a href=\"https:\/\/issuetracker.google.com\/206042974\" rel=\"nofollow noreferrer\">feature request<\/a> for people who wants to track this issue.<\/p>\n<p>With regards to latency requirements, the actual output will vary. However, one thing to note according to the documentation is that the service may not be able to bring nodes online fast enough to keep up with large spikes of request traffic. If your traffic regularly has steep spikes, and if reliably low latency is important to your application, you may want to consider manual scaling.<\/p>\n<p>Additional Reference: <a href=\"https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/machine-types-online-prediction#automatic_scaling<\/a><\/p>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1636679717047,
        "Solution_link_count":5.0,
        "Solution_readability":12.7,
        "Solution_reading_time":21.04,
        "Solution_score_count":2.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":181.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":40.1701361111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to share a model registry completely between Dev and Prod environment? So my idea is to create 10000 models in dev and maybe select 2000 from there to work in prod. I am planning to use AWS model registry. So if I do the training and testing and hyperparameter tuning in my AWS dev environment, is it possible to then share the registry in prod? The obvious reason is that it does not make sense to use the prod to do the training and testing again.<\/p>\n<p>Please advise!<\/p>\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1638378803110,
        "Challenge_comment_count":0,
        "Challenge_created_time":1638234190620,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of sharing a model registry between their development and production environments using AWS model registry. They want to create 10,000 models in dev and select 2,000 for use in prod without having to repeat the training and testing process in prod. They are seeking advice on how to accomplish this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70163094",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":6.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":40.1701361111,
        "Challenge_title":"SageMaker Model Registry Sharing",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1557333597230,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":99.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>It depends how you define Dev and Prod.<\/p>\n<ul>\n<li><p>If by Dev and Prod you mean different AWS account (which is a good practice - see <a href=\"https:\/\/docs.aws.amazon.com\/whitepapers\/latest\/organizing-your-aws-environment\/benefits-of-using-multiple-aws-accounts.html\" rel=\"nofollow noreferrer\">doc<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/devops\/aws-building-a-secure-cross-account-continuous-delivery-pipeline\/\" rel=\"nofollow noreferrer\">blog<\/a>), you cannot share fractions of a model registry from a given account to another account, but you can create triggers to export models from one model registry to another, as documented in this blog post <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/patterns-for-multi-account-hub-and-spoke-amazon-sagemaker-model-registry\/<\/a><\/p>\n<\/li>\n<li><p>If your Dev and Prod live in the same AWS account and you are just looking for ways to differentiate them, you can use:<\/p>\n<ul>\n<li>Model Registry Status information<\/li>\n<li>Tags<\/li>\n<\/ul>\n<\/li>\n<\/ul>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":19.7,
        "Solution_reading_time":15.51,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":107.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.6102777778,
        "Challenge_answer_count":0,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nHi,\r\n\r\nI am trying to integrate pycaret with mlflow using your parameter `log_experiment` in `setup()`. When I set it to true, everything is stores as planned in my local MlFlow server, but not the metrics.\r\n\r\nIn the documentation is says the `log_experiment=True` should control everything. So I am not sure if I do something wrong here of if it is a bug from your side.\r\n\r\nWould be glad if you could help!\n\n### Reproducible Example\n\n```python\nfrom pycaret.datasets import get_data\r\nfrom pycaret.regression import *\r\ndf = get_data('bike')\r\nexp = RegressionExperiment()\r\nexp.setup(data=df, log_experiment=True)\r\nmodel = exp.create_model(\"lr\")\r\npred = exp.predict_model(estimator=model)\r\nexp.finalize_model(estimator=model)\n```\n\n\n### Expected Behavior\n\nshould log metrics\n\n### Actual Results\n\n```python-traceback\nNo metrics logged.\n```\n\n\n### Installed Versions\n\n<details>\r\nPyCaret 3.0.0rc3\r\n<\/details>\r\n",
        "Challenge_closed_time":1660653306000,
        "Challenge_comment_count":2,
        "Challenge_created_time":1660651109000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to save plots through MLFLOW. The error message states that \"plot_model() got an unexpected keyword argument 'system'\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2856",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":16.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.6102777778,
        "Challenge_title":"MlFlow not logging metrics",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":161,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Update:\r\n\r\nWhen I write `exp.get_logs()` I can see some metrics there. But some runs still have the status \"RUNNING\", unsure why.\r\n\r\nAlso, all the runs that exist when I start the server using `!mlflow ui` are missing metrics. Edit: found issue, not on you! Sorry :)",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":2.7,
        "Solution_reading_time":3.16,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1.1990072222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are planing for next gen of product. Will V2 provide way more changes than V1? <\/p>",
        "Challenge_closed_time":1661981560816,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661977244390,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is planning for the next generation of their product and is wondering if there will be significant changes between SDK v1 and v2.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/989368\/sdk-v1-or-v2",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":1.2,
        "Challenge_reading_time":1.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.1990072222,
        "Challenge_title":"SDK v1 or V2",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":20,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=7f2ff54e-2fc4-4d74-b946-fc6ec46d4863\">@nam  <\/a>    <\/p>\n<p>Thanks for using Microsoft Q&amp;A. I will recommend you keeping in V1 at this moment.     <\/p>\n<p>SDK v2 is currently in public preview. The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see Supplemental Terms of Use for Microsoft Azure Previews.    <\/p>\n<p><a href=\"https:\/\/azure.microsoft.com\/support\/legal\/preview-supplemental-terms\/\">https:\/\/azure.microsoft.com\/support\/legal\/preview-supplemental-terms\/<\/a>    <\/p>\n<p>I hope this helps.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.6,
        "Solution_reading_time":10.77,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":97.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4035222223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I was trying AutoML to do some easy NLP model. Data set is small. For NER training, .csv is constantly failing. Am I missing something?<\/p>",
        "Challenge_closed_time":1675098209790,
        "Challenge_comment_count":1,
        "Challenge_created_time":1675096757110,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing issues with AutoML NER training using a small dataset in a .csv file, as it is constantly failing. They are seeking assistance to identify if they are missing any crucial steps.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1165491\/automl-ner-failed-with-csv-file",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":2.9,
        "Challenge_reading_time":2.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.4035222223,
        "Challenge_title":"Automl NER failed with .csv file",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":30,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=0e37a0a7-a326-4fb6-9c52-7c2fff3df543\">Nick<\/a><\/p>\n<p>Thanks for reaching out to us. I am sorry .csv actually not works for NER. <\/p>\n<p>Unlike multi-class or multi-label, which takes <code>.csv<\/code> format datasets, <strong>named entity recognition requires CoNLL format.<\/strong> The file must contain exactly two columns and in each row, the token and the label is separated by a single space.<\/p>\n<p>For example,<\/p>\n<p><code>Hudson B-loc<\/code><\/p>\n<p><code>Square I-loc<\/code><\/p>\n<p><code>is O<\/code><\/p>\n<p><code>a O<\/code><\/p>\n<p><code>famous O<\/code><\/p>\n<p><code>place O<\/code><\/p>\n<p><code>in O<\/code><\/p>\n<p><code>New B-loc<\/code><\/p>\n<p><code>York I-loc<\/code><\/p>\n<p><code>City I-loc<\/code><\/p>\n<p><code>Stephen B-per<\/code><\/p>\n<p><code>Curry I-per<\/code><\/p>\n<p><code>got O<\/code><\/p>\n<p><code>three O<\/code><\/p>\n<p><code>championship O<\/code><\/p>\n<p><code>rings O<\/code><\/p>\n<p>More information please refer to here - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli#named-entity-recognition-ner\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-nlp-models?tabs=cli#named-entity-recognition-ner<\/a><\/p>\n<p>I hope this helps! Let me know if you have more questions. <\/p>\n<p>Regards,<\/p>\n<p>Yutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":19.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":129.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1619163566860,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1730.0,
        "Answerer_view_count":555.0,
        "Challenge_adjusted_solved_time":6.7927280555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm thinking of deploying a TensorFlow model using Vertex AI in GCP. I am almost sure that the cost will be directly related to the number of queries per second (QPS) because I am going to use automatic scaling. I also know that the type of machine (with GPU, TPU, etc.) will have an impact on the cost.<\/p>\n<ul>\n<li>Do you have any estimation about the cost versus the number of queries per second?<\/li>\n<li>How does the type of virtual machine changes this cost?<\/li>\n<\/ul>\n<p>The type of model is for object detection.<\/p>",
        "Challenge_closed_time":1657283230408,
        "Challenge_comment_count":0,
        "Challenge_created_time":1657258379287,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is planning to deploy a TensorFlow model using Vertex AI in GCP and is concerned about the cost, which they believe will be directly related to the number of queries per second and the type of machine used. They are seeking an estimation of the cost based on the number of queries per second and how the cost is affected by the type of virtual machine used. The model is for object detection.",
        "Challenge_last_edit_time":1657258776587,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72907038",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":6.9030891667,
        "Challenge_title":"Cost of deploying a TensorFlow model in GCP?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":74.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Poster_created_time":1569457921527,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Luis Potos\u00ed, S.L.P., M\u00e9xico",
        "Poster_reputation_count":41.0,
        "Poster_view_count":7.0,
        "Solution_body":"<p>Autoscaling depends on the CPU and GPU utilization which directly correlates to the QPS, as you have said. To estimate the cost based on the QPS, you can deploy a custom prediction container to a Compute Engine instance directly, then benchmark the instance by making prediction calls until the VM hits 90+ percent CPU utilization (consider GPU utilization if configured). Do this multiple times for different machine types, and determine the &quot;QPS per cost per hour&quot; of different machine types. You can re-run these experiments while benchmarking latency to find the <strong>ideal cost per QPS per your latency targets<\/strong> for your specific custom prediction container. For more information about choosing the ideal machine for your workload, refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#finding_the_ideal_machine_type\" rel=\"nofollow noreferrer\">documentation<\/a>.<\/p>\n<p>For your second question, as per the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/pricing#custom-trained_models:%7E:text=a%20specific%20job.-,Prediction%20and%20explanation,-This%20table%20provides\" rel=\"nofollow noreferrer\">Vertex AI pricing documentation<\/a> (for model deployment), cost estimation is done based on the node hours. A node hour represents the time a virtual machine spends running your prediction job or waiting in a ready state to handle prediction or explanation requests. Each type of VM offered has a specific pricing per node hour depending on the number of cores and the amount of memory. Using a VM with more resources will cost more per node hour and vice versa. To choose an ideal VM for your deployment, please follow the steps given in the first paragraph which will help you find a good trade off between cost and performance.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.4,
        "Solution_reading_time":22.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":243.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1446631384107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Helsinki, Finland",
        "Answerer_reputation_count":4255.0,
        "Answerer_view_count":877.0,
        "Challenge_adjusted_solved_time":0.5821208334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am running the <code>pipeline.submit()<\/code> in AzureML, which has a <code>PythonScriptStep<\/code>.\nInside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip<\/code>, and finally, I would like to register it in the Azure ML.\nBut as inside the script I do not have a workspace, <code>Model.register()<\/code> is not the case.\nSo I am trying to use <code>Run.register_model()<\/code> method as below:<\/p>\n\n<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), \n           os.path.join('.', 'outputs', archive_name + '.zip'))\n\nprint(os.listdir('.\/outputs'))\nprint('========================')\n\nrun_context = Run.get_context()\nfinetuning_model = run_context.register_model(model_name='finetuning_similarity_model',\n                                              model_path=os.path.join(archive_name+'.zip'),\n                                              tags={},\n                                              description=\"Finetuning Similarity model\")\n<\/code><\/pre>\n\n<p>But then I have got an error:<\/p>\n\n<blockquote>\n  <p>ErrorResponse \n  {\n      \"error\": {\n          \"message\": \"Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:<\/p>\n<\/blockquote>\n\n<p>despite I have the retrained <code>.zip<\/code> in the <code>.\/outputs<\/code> dir as we can see from the log:<\/p>\n\n<pre><code>['retrained.zip']\n========================\n<\/code><\/pre>\n\n<p>I guess that I am doing something wrong?<\/p>",
        "Challenge_closed_time":1578745983587,
        "Challenge_comment_count":0,
        "Challenge_created_time":1574164584153,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to register a model in Azure ML from a PythonScriptStep in a pipeline. They download a model from tensorflow-hub, retrain it, save it as a .zip, and attempt to register it using Run.register_model() method. However, they receive an error stating that the provided model_path cannot be located in the set of files uploaded to the run, despite having the .zip in the .\/outputs directory. The user is seeking assistance in resolving this issue.",
        "Challenge_last_edit_time":1578744224352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58933565",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":17.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":1272.6109538889,
        "Challenge_title":"How to register model from the Azure ML Pipeline Script step",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3429.0,
        "Challenge_word_count":150,
        "Platform":"Stack Overflow",
        "Poster_created_time":1574162655727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>I was able to fix the same issue (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py\" rel=\"noreferrer\"><code>ModelPathNotFoundException<\/code><\/a>) by explicitly uploading the model into the run history record before trying to register the model:<\/p>\n\n<pre><code>run.upload_file(\"outputs\/my_model.pickle\", \"outputs\/my_model.pickle\")\n<\/code><\/pre>\n\n<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()<\/code> <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-\" rel=\"noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Runs automatically capture file in the specified output directory, which defaults to \".\/outputs\" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.<\/p>\n<\/blockquote>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1578746319987,
        "Solution_link_count":2.0,
        "Solution_readability":19.6,
        "Solution_reading_time":13.4,
        "Solution_score_count":14.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":88.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":4.8427816667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I registered a model in my AML workspace, and I can see it in the Model List:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rtL5Q.png\" rel=\"nofollow noreferrer\">Model List view<\/a><\/p>\n<p>But I cannot see it in Designer (preview), which prevents me from using the new model there.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WpvIb.png\" rel=\"nofollow noreferrer\">Designer view<\/a><\/p>\n<p>Looks like a bug to me. Datasets work fine.<\/p>",
        "Challenge_closed_time":1596073257467,
        "Challenge_comment_count":3,
        "Challenge_created_time":1596056234913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has registered a model in their AML workspace, but it is not showing up in Designer (preview), which is preventing them from using the new model. The user suspects it is a bug as datasets are working fine.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63162310",
        "Challenge_link_count":2,
        "Challenge_participation_count":5,
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":4.7284872222,
        "Challenge_title":"Models registered in workspace do not show up in Designer (preview)",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":81.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531852372996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>This is known issue as the models registered in workspace cannot be consumed in Designer without the new custom module capability (in private preview) available.<\/p>\n<p>The models showing up in Designer today are these generated from Designer training -&gt; inference pipeline conversion and can only be used in Designer (not registered in the workspace).\nWe have an effort ongoing to reduce the confusion.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1596073668927,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":5.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1424453610300,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1237.0,
        "Answerer_view_count":116.0,
        "Challenge_adjusted_solved_time":40.1738408334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I plan to try different regression methods provided by Azure ML Studio to predict numeric values. I wonder if it is possible to get the predictions together with corresponding confidence intervals. In other words, I would like the regression function to tell me not only the expected value (prediction) but also how confident it (the model) is about this value. Does Azure regression support this functionality?<\/p>\n\n<p><strong>ADDED<\/strong><\/p>\n\n<p>A related question. Can build in \"regressors\" estimate probability density functions? For example for a given case (a row in a data table) I would like to have not only a single number as a prediction (expected value) but also probabilities of all possible values.<\/p>",
        "Challenge_closed_time":1467268176063,
        "Challenge_comment_count":0,
        "Challenge_created_time":1467121249227,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring if Azure ML Studio's regression methods can provide confidence intervals for predicted numeric values and if it can estimate probability density functions for all possible values.",
        "Challenge_last_edit_time":1467123550236,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38077884",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.4,
        "Challenge_reading_time":9.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":40.81301,
        "Challenge_title":"Can Azure calculate confidence interval for regressions?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":332.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Poster_created_time":1262870449816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":116085.0,
        "Poster_view_count":4661.0,
        "Solution_body":"<p>Currently, you will have to use R or python within Azure ML for confidence interval <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.12,
        "Solution_score_count":2.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":212.0517972223,
        "Challenge_answer_count":1,
        "Challenge_body":"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor-data-capture-endpoint.html\n\nI have followed the steps mentioned in this and it appears I cannot change the encoding for EndpointOutput in datacapture file. It's coming BASE64 for xgboost model. I am using latest version 1.2.3.\n\nFor monitor scheduler it required both EndpointOutput and EndpointInput to have the same encoding. My EndpointInput  is CSV but EndpointOutput is coming to be BASE64 and nothing can change it.\n\nThis is causing issue while run of analyzer. After baseline is generated and data is captured, when monitoring schedule runs the analyzer it throws error of Encoding mismatch. For it to run EndpointOutput and EndpointInput should have same encoding.\n\nI saw we cannot do anything to change the encoding of output. I used LightGBM, CatBoost algorithms also and found for these EndpointOuput encoding is JSON, which is readable but still not solving the purpose.\n\nIs there a way we can change EndpointOutput Encoding for DataCapture.",
        "Challenge_closed_time":1675066186694,
        "Challenge_comment_count":1,
        "Challenge_created_time":1673956972508,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with the EndpointOutput encoding in data capture files while using Amazon SageMaker. The EndpointOutput is coming as BASE64, which cannot be changed, causing an encoding mismatch error while running the analyzer. The user has tried using different algorithms, but the EndpointOutput encoding remains the same. The user is seeking a solution to change the EndpointOutput encoding for DataCapture.",
        "Challenge_last_edit_time":1674302800224,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUGSFVfrFJS_KsdrOMeepDPg\/model-monitor-capture-data-endpointoutput-encoding-is-base64",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":13.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":308.1150516667,
        "Challenge_title":"Model Monitor Capture data - EndpointOutput Encoding is BASE64",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":77.0,
        "Challenge_word_count":156,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Output encoding can be configured by using the [CaptureContentTypeHeader \nin EndpointConfig.DataCaptureConfig](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DataCaptureConfig.html#sagemaker-Type-DataCaptureConfig-CaptureContentTypeHeader). I believe since this is not being set, default encoding i.e. base64 is being used. \n\nPlease try once with this attribute set as below:\n```\n\"CaptureContentTypeHeader\": { \n         \"CsvContentTypes\": [ \"text\/csv\" ]\n      }\n```\n> Assuming that content_type\/accept is \"text_csv\" for the concerned model.",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1675066186694,
        "Solution_link_count":1.0,
        "Solution_readability":22.1,
        "Solution_reading_time":7.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":47.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":220.4830555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987466000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1619193727000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to load a previously saved KedroPipelineModel from mlflow due to a \"cannot pickle context artifacts\" error caused by a non-deepcopyable dataset (in this case, a keras tokenizer). The issue occurs with kedro and kedro-mlflow versions 0.16.5 and 0.4.0, and the bug also happens with the latest version on develop. A potential solution involves modifying a line of code in the kedro_mlflow package.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":220.4830555556,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":273,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":7.8,
        "Solution_reading_time":66.71,
        "Solution_score_count":null,
        "Solution_sentence_count":48.0,
        "Solution_word_count":849.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1437986390372,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation_count":361.0,
        "Answerer_view_count":149.0,
        "Challenge_adjusted_solved_time":0.0983611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have an sklearn k-means model. I am training the model and saving it in a pickle file so I can deploy it later using azure ml library. The model that I am training uses a custom Feature Encoder called <strong>MultiColumnLabelEncoder<\/strong>.\nThe pipeline model is defined as follow :<\/p>\n\n<pre><code># Pipeline\nkmeans = KMeans(n_clusters=3, random_state=0)\npipe = Pipeline([\n(\"encoder\", MultiColumnLabelEncoder()),\n('k-means', kmeans),\n])\n#Training the pipeline\nmodel = pipe.fit(visitors_df)\nprediction = model.predict(visitors_df)\n#save the model in pickle\/joblib format\nfilename = 'k_means_model.pkl'\njoblib.dump(model, filename)\n<\/code><\/pre>\n\n<p>The model saving works fine. The Deployment steps are the same as the steps in this link : <\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb<\/a><\/p>\n\n<p>However the deployment always fails with this error :<\/p>\n\n<pre><code>  File \"\/var\/azureml-server\/create_app.py\", line 3, in &lt;module&gt;\n    from app import main\n  File \"\/var\/azureml-server\/app.py\", line 27, in &lt;module&gt;\n    import main as user_main\n  File \"\/var\/azureml-app\/main.py\", line 19, in &lt;module&gt;\n    driver_module_spec.loader.exec_module(driver_module)\n  File \"\/structure\/azureml-app\/score.py\", line 22, in &lt;module&gt;\n    importlib.import_module(\"multilabelencoder\")\n  File \"\/azureml-envs\/azureml_b707e8c15a41fd316cf6c660941cf3d5\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named 'multilabelencoder'\n<\/code><\/pre>\n\n<p>I understand that pickle\/joblib has some problems unpickling the custom function MultiLabelEncoder. That's why I defined this class in a separate python script (which I executed also). I called this custom function in the training python script, in the deployment script and in the scoring python file (score.py). The importing in the score.py file is not successful. \nSo my question is how can I import custom python module to azure ml deployment environment ?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>EDIT: \nThis is my .yml file<\/p>\n\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  - multilabelencoder==1.0.4\n  - scikit-learn\n  - azureml-defaults==1.0.74.*\n  - pandas\nchannels:\n- conda-forge\n<\/code><\/pre>",
        "Challenge_closed_time":1575635052340,
        "Challenge_comment_count":1,
        "Challenge_created_time":1575463048677,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has trained an sklearn k-means model using a custom Feature Encoder called MultiColumnLabelEncoder and saved it in a pickle file for deployment using Azure ML library. However, the deployment fails with an error due to the inability to import the custom module MultiLabelEncoder. The user has defined the class in a separate python script and called it in the training, deployment, and scoring python files. The user is seeking a solution to import the custom python module to the Azure ML deployment environment.",
        "Challenge_last_edit_time":1575634698240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59176241",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":35.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":47.7787952778,
        "Challenge_title":"import custom python module in azure ml deployment environment",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2611.0,
        "Challenge_word_count":276,
        "Platform":"Stack Overflow",
        "Poster_created_time":1437986390372,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation_count":361.0,
        "Poster_view_count":149.0,
        "Solution_body":"<p>In fact, the solution was to import my customized class <strong>MultiColumnLabelEncoder<\/strong> as a pip package (You can find it through pip install multilllabelencoder==1.0.5).\nThen I passed the pip package to the .yml file or in the InferenceConfig of the azure ml environment.\nIn the score.py file, I imported the class as follows :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder\ndef init():\n    global model\n\n    # Call the custom encoder to be used dfor unpickling the model\n    encoder = multilabelencoder.MultiColumnLabelEncoder() \n    # Get the path where the deployed model can be found.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'k_means_model_45.pkl')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>Then the deployment was successful. \nOne more important thing is I had to use the same pip package (multilabelencoder) in the training pipeline as here :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder \npipe = Pipeline([\n    (\"encoder\", multilabelencoder.MultiColumnLabelEncoder(columns)),\n    ('k-means', kmeans),\n])\n#Training the pipeline\ntrainedModel = pipe.fit(df)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":13.2,
        "Solution_reading_time":14.38,
        "Solution_score_count":4.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":132.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1436184843608,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":305.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":2.9136647222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>So I have been playing around with Azure ML lately, and I got one dataset where I have multiple values I want to predict. All of them uses different algorithms and when I try to train multiple models within one experiment; it says the \u201ctrain model can only predict one value\u201d, and there are not enough input ports on the train-model to take in multiple values even if I was to use the same algorithm for each measure. I tried launching the column selector and making rules, but I get the same error as mentioned. How do I predict multiple values and later put the predicted columns together for the web service output so I don\u2019t have to have multiple API\u2019s?<\/p>",
        "Challenge_closed_time":1465904564903,
        "Challenge_comment_count":0,
        "Challenge_created_time":1465894075710,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge while trying to train multiple models with different algorithms to predict multiple values using Azure ML. The \"train model can only predict one value\" error is encountered when attempting to train multiple models within one experiment. The user has tried using the column selector and making rules, but the same error persists. The user is seeking a solution to predict multiple values and combine the predicted columns for the web service output to avoid having multiple APIs.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37807158",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.9136647222,
        "Challenge_title":"Train multiple models with various measures and accumulate predictions",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1763.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1463041289043,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":4.0,
        "Solution_body":"<p>What you would want to do is to train each model and save them as already trained models.\nSo create a new experiment, train your models and save them by right clicking on each model and they will show up in the left nav bar in the Studio. Now you are able to drag your models into the canvas and have them score predictions where you eventually make them end up in the same output as I have done in my example through the \u201cAdd columns\u201d module. I made this example for Ronaldo (Real Madrid CF player) on how he will perform in match after training day. You can see my demo on <a href=\"http:\/\/ronaldoinform.azurewebsites.net\" rel=\"nofollow noreferrer\">http:\/\/ronaldoinform.azurewebsites.net<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZwzUy.png\" alt=\"Ronaldo InForm\"><\/a><\/p>\n\n<p>For more detailed explanation on how to save the models and train multiple values; you can check out Raymond Langaeian (MSFT) answer in the comment section on this link:\n<a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/\" rel=\"nofollow noreferrer\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-convert-training-experiment-to-scoring-experiment\/<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":13.0,
        "Solution_reading_time":17.07,
        "Solution_score_count":2.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":157.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1499772840847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":4.0,
        "Challenge_adjusted_solved_time":39.9339302778,
        "Challenge_answer_count":1,
        "Challenge_body":"<h1>Context<\/h1>\n<p>Hi!<\/p>\n<p>In <code>wandb<\/code> I can download a model based on a tag (<code>prod<\/code> for example), but I would like to also get all metrics associated to that run by using tags.<\/p>\n<p>The problem is that I don't know how to a get specific run ID based a tag.<\/p>\n<h1>Example<\/h1>\n<p>Using the code bellow we can extract a run summary metrics, but setting run IDs is setting me back.<\/p>\n<p>So if I can get run IDs based on tag or just explicitly download metrics  with another API call, like with a special sintax in <code>api.run<\/code>, that would be great! In the code example bellow I would like to use the <code>what_i_want_to_use<\/code> string to call the API instead of <code>what_i_use<\/code>.<\/p>\n<pre><code>import wandb\nfrom ast import literal_eval\napi = wandb.Api()\n\nwhat_i_use = &quot;team_name\/project_name\/runID_h3h3h4h4h4h4&quot;\n# what_i_want_to_use = &quot;team_name\/project_name\/artifact_name\/prod_tag&quot;\n\n# run is specified by &lt;entity&gt;\/&lt;project&gt;\/&lt;run_id&gt;\nrun = api.run(what_i_use)\n\n\n# save the metrics for the run to a csv file\nmetrics_dataframe = run.summary\nprint(metrics_dataframe['a_summary_metric'])\n\n<\/code><\/pre>\n<p>By running through the docs I didn't find any solution so far. Any ideias?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mWl3I.png\" rel=\"nofollow noreferrer\">wandb public api run details<\/a><\/p>\n<p>Thanks for reading!<\/p>",
        "Challenge_closed_time":1650458488356,
        "Challenge_comment_count":0,
        "Challenge_created_time":1650314726207,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing a challenge in getting a specific run ID based on a tag in WANDB. They are able to download a model based on a tag but are unable to get all metrics associated with that run by using tags. The user is seeking a solution to get run IDs based on tags or explicitly download metrics with another API call.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71916901",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":18.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":39.9339302778,
        "Challenge_title":"WANDB Getting a run id based on tag",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":645.0,
        "Challenge_word_count":184,
        "Platform":"Stack Overflow",
        "Poster_created_time":1444675970627,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":5.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>It is possible to filter runs by tags as well. You can read more about it <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/api#runs\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>You can filter by config.*, summary_metrics.*, tags, state, entity, createdAt, etc.\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.1,
        "Solution_reading_time":3.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":32.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":110.8842475,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to open a pickled XGBoost model I created in AWS Sagemaker to look at feature importances in the model. I'm trying to follow the answers in <a href=\"https:\/\/stackoverflow.com\/questions\/55621967\/feature-importance-for-xgboost-in-sagemaker\">this post<\/a>. However, I get an the error shown below. When I try to call <code>Booster.save_model<\/code>, I get an error saying <code>'Estimator' object has no attribute 'save_model'<\/code>. How can I resolve this? <\/p>\n\n<pre><code># Build initial model\nsess = sagemaker.Session()\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train\/'.format(bucket, prefix), content_type='csv')\nxgb_cont = get_image_uri(region, 'xgboost', repo_version='0.90-1')\nxgb = sagemaker.estimator.Estimator(xgb_cont, role, train_instance_count=1, train_instance_type='ml.m4.4xlarge',\n                                    output_path='s3:\/\/{}\/{}'.format(bucket, prefix), sagemaker_session=sess)\nxgb.set_hyperparameters(eval_metric='rmse', objective='reg:squarederror', num_round=100)\nts = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nxgb_name = 'xgb-initial-' + ts\nxgb.set_hyperparameters(eta=0.1, alpha=0.5, max_depth=10)\nxgb.fit({'train': s3_input_train}, job_name=xgb_name)\n\n# Load model to get feature importances\nmodel_path = 's3:\/\/{}\/{}\/\/output\/model.tar.gz'.format(bucket, prefix, xgb_name)\nfs = s3fs.S3FileSystem()\nwith fs.open(model_path, 'rb') as f:\n    with tarfile.open(fileobj=f, mode='r') as tar_f:\n        with tar_f.extractfile('xgboost-model') as extracted_f:\n            model = pickle.load(extracted_f)\n\nXGBoostError: [19:16:42] \/workspace\/src\/learner.cc:682: Check failed: header == serialisation_header_: \n\n  If you are loading a serialized model (like pickle in Python) generated by older\n  XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version.  There's a simple script for helping\n  the process. See:\n\n    https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\n\n  for reference to the script, and more details about differences between saving model and\n  serializing.\n<\/code><\/pre>",
        "Challenge_closed_time":1584357657183,
        "Challenge_comment_count":2,
        "Challenge_created_time":1583954281893,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while trying to open a pickled XGBoost model created in AWS Sagemaker to look at feature importances in the model. The user is following the answers in a post but is getting an error saying \"'Estimator' object has no attribute 'save_model'\" when trying to call Booster.save_model. The error message suggests that the user should export the model by calling Booster.save_model from the older version first and then load it back in the current version.",
        "Challenge_last_edit_time":1583958473892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60643094",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":14.6,
        "Challenge_reading_time":27.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":112.0486916667,
        "Challenge_title":"Unable to open pickled Sagemaker XGBoost model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4586.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Poster_created_time":1431970105067,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":4631.0,
        "Poster_view_count":333.0,
        "Solution_body":"<p>Which version of XGBoost are you using in the notebook? The model format has changed in XGBoost 1.0. See <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a>. Short version: if you're using 1.0 in the notebook, you can't load a pickled model. <\/p>\n\n<p>Here's a working example using XGBoost in script mode (which is much more flexible than the built in algo):<\/p>\n\n<ul>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":21.3,
        "Solution_reading_time":12.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1342709052703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"D\u00fcsseldorf, Germany",
        "Answerer_reputation_count":1889.0,
        "Answerer_view_count":654.0,
        "Challenge_adjusted_solved_time":16.1329663889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was playing with the AWS instances and trying to deploy some locally trained Keras models, but I find no documentation on that. Has anyone already been able to do it? <\/p>\n\n<p>I tried to use a similar approach to <a href=\"https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/pt\/blogs\/machine-learning\/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker\/<\/a>, but I had no success. I also found some examples for training keras models in the cloud, but I was not able to get the entry_point + artifacts right. <\/p>\n\n<p>Thanks for your time!<\/p>",
        "Challenge_closed_time":1538644957792,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538586879113,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy locally trained Keras models to AWS Sagemaker but is unable to find any documentation on it. They have tried a similar approach to deploying MXNet or TensorFlow models but have had no success. They are seeking advice on how to get the entry_point and artifacts right.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52632388",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":16.1329663889,
        "Challenge_title":"Is it possible to deploy a already trained Keras to Sagemaker?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1291.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Poster_created_time":1448029868996,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Portugal",
        "Poster_reputation_count":260.0,
        "Poster_view_count":52.0,
        "Solution_body":"<p>Yes, it is possible, and yes, the official documentation is not much of help.\nHowever, I wrote an <a href=\"https:\/\/gnomezgrave.com\/2018\/07\/05\/using-a-custom-model-for-ml-inference-with-amazon-sagemaker\" rel=\"nofollow noreferrer\">article on that<\/a>, and I hope it will help you.<\/p>\n\n<p>Let me know if you need more details. Cheers!<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.39,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":27.0735711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. <\/p>\n\n<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.<\/p>\n\n<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:<\/p>\n\n<p><a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker\" rel=\"noreferrer\">https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker<\/a>\n<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb<\/a><\/p>\n\n<p>My code is this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pipeline import PipelineModel\nfrom sagemaker.model import Model\n\ns3_model_bucket = 'bucket'\ns3_model_key_prefix = 'prefix'\ndata = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\nmodels = ss_model.create_model() # ss_model is my sagemaker.estimator\n\nmodel = PipelineModel(name=data, role=role, models= [models])\nss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>",
        "Challenge_closed_time":1558621559712,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558522248223,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has trained a semantic segmentation model using sagemaker and saved the output to an s3 bucket. They are now trying to load the model from s3 to predict some images in sagemaker. The user has looked at various sources but has not been successful in deploying the model. They have shared their code and are seeking assistance.",
        "Challenge_last_edit_time":1558524094856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56255154",
        "Challenge_link_count":8,
        "Challenge_participation_count":2,
        "Challenge_readability":22.6,
        "Challenge_reading_time":27.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":27.5865247222,
        "Challenge_title":"How to use a pretrained model from s3 to predict some data?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":7404.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558518328152,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"London, UK",
        "Poster_reputation_count":79.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>You can actually instantiate a Python SDK <code>model<\/code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:<\/p>\n\n<pre><code>trainedmodel = sagemaker.model.Model(\n    model_data='s3:\/\/...model path here..\/model.tar.gz',\n    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1\n    role=role)  # your role here; could be different name\n\ntrainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>\n\n<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:<\/p>\n\n<pre><code>predictor = sagemaker.predictor.RealTimePredictor(\n    endpoint='endpoint name here',\n    content_type='image\/jpeg',\n    accept='image\/png')\n<\/code><\/pre>\n\n<p>More on those abstractions:<\/p>\n\n<ul>\n<li><code>Model<\/code>: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html<\/a><\/li>\n<li><code>Predictor<\/code>:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":22.9,
        "Solution_reading_time":18.55,
        "Solution_score_count":13.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":112.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":20.8108333333,
        "Challenge_answer_count":0,
        "Challenge_body":"When using wandb, it shows step as X and not episode.\r\n\r\nHence, longer runs have more steps and it makes the comparaison between runs difficult.\r\n\r\n\r\n![photo_2020-11-17_13-47-41](https:\/\/user-images.githubusercontent.com\/13030198\/99403033-5052e400-28ea-11eb-92c0-a3efd14b654a.jpg)\r\n\r\n",
        "Challenge_closed_time":1605698611000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605623692000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an issue where loading configs from wandb results in incorrect HParams objects, which can be seen when attempting to load the model checkpoint or comparing the object with the info panel for the run on wandb. The user has provided a code snippet to reproduce the issue and has set acceptance criteria for the bug to be fixed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/MathisFederico\/LearnRL\/issues\/96",
        "Challenge_link_count":1,
        "Challenge_participation_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":137.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":20.8108333333,
        "Challenge_title":"Add episode to wandb",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":29,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":9.5907397222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m performing a series of experiments with AutoML and I need to see the featurized data. I mean, not just the new features names retrieved by method get_engineered_feature_names() or the featurization details retrieved by get_featurization_summary(), I refer to the whole transformed dataset, the one obtained after scaling\/normalization\/featurization that is then used to train the models.   <\/p>\n<p>Is it possible to access to this dataset or download it as a file?  <\/p>\n<p>Thanks.  <\/p>",
        "Challenge_closed_time":1618340955136,
        "Challenge_comment_count":0,
        "Challenge_created_time":1618306428473,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is conducting experiments with AutoML and needs to access the featurized dataset, including the transformed data obtained after scaling, normalization, and featurization, which is used to train the models. They are seeking information on whether it is possible to access or download this dataset as a file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/355323\/how-to-access-to-the-featurized-dataset-in-automat",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.3,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":9.5907397222,
        "Challenge_title":"How to access to the featurized dataset in Automated ML",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":82,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, currently, we don't store the dataset from scaling\/normalization\/featurization after the run is complete. This feature isn't supported at this time. Sorry for the inconvenience.<\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.1,
        "Solution_reading_time":2.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1352206833663,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":893.0,
        "Answerer_view_count":185.0,
        "Challenge_adjusted_solved_time":28.8670294445,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm trying to define a Sagemaker Training Job with an existing Python class. To my understanding, I could create my own container but would rather not deal with container management.<\/p>\n\n<p>When choosing \"Algorithm Source\" there is the option of \"Your own algorithm source\" but nothing is listed under resources. Where does this come from?<\/p>\n\n<p>I know I could do this through a notebook, but I really want this defined in a job that can be invoked through an endpoint.<\/p>",
        "Challenge_closed_time":1550361262523,
        "Challenge_comment_count":0,
        "Challenge_created_time":1550257341217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to create a Sagemaker training job with their own Tensorflow code without building a container. They are trying to define a training job with an existing Python class but do not want to deal with container management. They are looking for guidance on how to use the \"Your own algorithm source\" option under \"Algorithm Source\" as nothing is listed under resources. The user wants to define the job in a way that can be invoked through an endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/54715601",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":28.8670294445,
        "Challenge_title":"How do I create a Sagemaker training job with my own Tensorflow code without having to build a container?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":781.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Poster_created_time":1366768533200,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>As Bruno has said you will have to use a container somewhere, but you can use an existing container to run your own custom tensorflow code.<\/p>\n\n<p>There is a good example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">in the sagemaker github<\/a> for how to do this.<\/p>\n\n<p>The way this works is you modify your code to have an entry point which takes argparse command line arguments, and then you point a 'Sagemaker Tensorflow estimator' to the entry point. Then when you call fit on the sagemaker estimator it will download the tensorflow container and run your custom code in there.<\/p>\n\n<p>So you start off with your own custom code that looks something like this<\/p>\n\n<pre><code># my_custom_code.py\nimport tensorflow as tf\nimport numpy as np\n\ndef build_net():\n    # single fully connected\n    image_place = tf.placeholder(tf.float32, [None, 28*28])\n    label_place = tf.placeholder(tf.int32, [None,])\n    net = tf.layers.dense(image_place, units=1024, activation=tf.nn.relu)\n    net = tf.layers.dense(net, units=10, activation=None)\n    return image_place, label_place, net\n\n\ndef process_data():\n    # load\n    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n\n    # center\n    x_train = x_train \/ 255.0\n    m = x_train.mean()\n    x_train = x_train - m\n\n    # convert to right types\n    x_train = x_train.astype(np.float32)\n    y_train = y_train.astype(np.int32)\n\n    # reshape so flat\n    x_train = np.reshape(x_train, [-1, 28*28])\n    return x_train, y_train\n\n\ndef train_model(init_learn, epochs):\n    image_p, label_p, logit = build_net()\n    x_train, y_train = process_data()\n\n    loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=logit,\n        labels=label_p)\n    optimiser = tf.train.AdamOptimizer(init_learn)\n    train_step = optimiser.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for _ in range(epochs):\n            sess.run(train_step, feed_dict={image_p: x_train, label_p: y_train})\n\n\nif __name__ == '__main__':\n    train_model(0.001, 10)\n<\/code><\/pre>\n\n<p>To make it work with sagemaker we need to create a command line entry point, which will allow sagemaker to run it in the container it will download for us eventually.<\/p>\n\n<pre><code># entry.py\n\nimport argparse\nfrom my_custom_code import train_model\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        '--model_dir',\n        type=str)\n    parser.add_argument(\n        '--init_learn',\n        type=float)\n    parser.add_argument(\n        '--epochs',\n        type=int)\n    args = parser.parse_args()\n    train_model(args.init_learn, args.epochs)\n<\/code><\/pre>\n\n<p>Apart from specifying the arguments my function needs to take, we also need to provide a <code>model_dir<\/code> argument. This is always required, and is an S3 location which is where an model artifacts will be saved when the training job completes. Note that you don't need to specify what this value is (though you can) as Sagemaker will provide a default location in S3 for you.<\/p>\n\n<p>So we have modified our code, now we need to actually run it on Sagemaker. Go to the AWS console and fire up a small instance from Sagemaker. Download your custom code to the instance, and then create a jupyter notebook as follows:<\/p>\n\n<pre><code># sagemaker_run.ipyb\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nhyperparameters = {\n    'epochs': 10,\n    'init_learn': 0.001}\n\nrole = sagemaker.get_execution_role()\nsource_dir = '\/path\/to\/folder\/with\/my\/code\/on\/instance'\nestimator = TensorFlow(\n    entry_point='entry.py',\n    source_dir=source_dir,\n    train_instance_type='ml.t2.medium',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>Running the above will:<\/p>\n\n<ul>\n<li>Spin up an ml.t2.medium instance<\/li>\n<li>Download the tensorflow 1.12.0 container to the instance<\/li>\n<li>Download any data we specify in fit to the newly created instance in fit (in this case nothing)<\/li>\n<li>Run our code on the instance<\/li>\n<li>upload the model artifacts to model_dir<\/li>\n<\/ul>\n\n<p>And that is pretty much it. There is of course a lot not mentioned here but you can:<\/p>\n\n<ul>\n<li>Download training\/testing data from s3<\/li>\n<li>Save checkpoint files, and tensorboard files during training and upload them to s3<\/li>\n<\/ul>\n\n<p>The best resource I found was the example I shared but here are all the things I was looking at to get this working:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">example code again<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">documentation<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers\" rel=\"nofollow noreferrer\">explanation of environment variables<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":12.3,
        "Solution_reading_time":66.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":546.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1528629350990,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Pakistan",
        "Answerer_reputation_count":439.0,
        "Answerer_view_count":51.0,
        "Challenge_adjusted_solved_time":121.4413405556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a pre-trained <code>keras<\/code> model which I have hosted on <code>AWS<\/code> using <code>AWS SageMaker<\/code>. I've got an <code>endpoint<\/code> and I can make successful <code>predictions<\/code> using the <code>Amazon SageMaker Notebook instance<\/code>.<\/p>\n<p>What I do there is that I serve a <code>.PNG image<\/code> like the following and the model gives me correct prediction.<\/p>\n<pre><code>file= s3.Bucket(bucketname).download_file(filename_1, 'normal.png')\nfile_name_1='normal.png'\n\n\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\nendpoint = 'tensorflow-inference-0000-11-22-33-44-55-666' #endpoint\n\npredictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint, sagemaker_session)\ndata = np.array([resize(imread(file_name), (137, 310, 3))])\npredictor.predict(data)\n<\/code><\/pre>\n<p>Now I wanted to make predictions using a <code>mobile application<\/code>. For that I have to wrote a <code>Lambda function<\/code> in python and attached an <code>API gateway<\/code> to it. My <code>Lambda function<\/code> is the following.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nfrom scipy import signal\nfrom scipy.signal import butter, lfilter\nfrom scipy.io import wavfile\nimport scipy.signal as sps\nimport io\nfrom io import BytesIO\nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nfrom datetime import datetime\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom PIL import Image\n\nENDPOINT_NAME = 'tensorflow-inference-0000-11-22-33-44-55-666'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    image = Image.open(io.BytesIO(decoded_file_name))\n\n    data = np.array([resize(imread(image), (137, 310, 3))])\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='text\/csv', Body=data)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n<\/code><\/pre>\n<p>The third last line is giving me error <code>'PngImageFile' object has no attribute 'read'<\/code>.\nAny idea what I am missing here?<\/p>",
        "Challenge_closed_time":1618052704636,
        "Challenge_comment_count":0,
        "Challenge_created_time":1617615515810,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has a pre-trained Keras model hosted on AWS SageMaker and can make successful predictions using the Amazon SageMaker Notebook instance. However, when trying to make predictions using a mobile application, the user wrote a Lambda function in Python and attached an API gateway to it. The Lambda function is giving an error \"PngImageFile' object has no attribute 'read'\". The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1618052912968,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66950948",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.3,
        "Challenge_reading_time":32.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":121.4413405556,
        "Challenge_title":"How to make inference to a keras model hosted on AWS SageMaker via AWS Lambda function?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":114.0,
        "Challenge_word_count":251,
        "Platform":"Stack Overflow",
        "Poster_created_time":1528629350990,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Pakistan",
        "Poster_reputation_count":439.0,
        "Poster_view_count":51.0,
        "Solution_body":"<p>I was missing one thing which was causing this error. After receiving the image data I used python list and then <code>json.dump<\/code> that list (of lists). Below is the code for reference.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nimport io\nfrom io import BytesIO\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\n# grab environment variable of Lambda Function\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    data = np.array([resize(imread(io.BytesIO(decoded_file_name)), (137, 310, 3))])\n    \n    payload = json.dumps(data.tolist())\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n        \n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.0,
        "Solution_reading_time":15.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":106.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1535695625688,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":19.0,
        "Challenge_adjusted_solved_time":355.6374158334,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am learning Sagemaker and I have this entry point:<\/p>\n\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.estimator.model_fn import ModeKeys as Modes\n\nINPUT_TENSOR_NAME = 'inputs'\nSIGNATURE_NAME = 'predictions'\n\nLEARNING_RATE = 0.001\n\n\ndef model_fn(features, labels, mode, params):\n    # Input Layer\n    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n\n    # Convolutional Layer #1\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2 and Pooling Layer #2\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding='same',\n        activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Dense Layer\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n\n    # Logits Layer\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    # Define operations\n    if mode in (Modes.PREDICT, Modes.EVAL):\n        predicted_indices = tf.argmax(input=logits, axis=1)\n        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n\n    if mode in (Modes.TRAIN, Modes.EVAL):\n        global_step = tf.train.get_or_create_global_step()\n        label_indices = tf.cast(labels, tf.int32)\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n        tf.summary.scalar('OptimizeLoss', loss)\n\n    if mode == Modes.PREDICT:\n        predictions = {\n            'classes': predicted_indices,\n            'probabilities': probabilities\n        }\n        export_outputs = {\n            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, predictions=predictions, export_outputs=export_outputs)\n\n    if mode == Modes.TRAIN:\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    if mode == Modes.EVAL:\n        eval_metric_ops = {\n            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n        }\n        return tf.estimator.EstimatorSpec(\n            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\n\ndef serving_input_fn(params):\n    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\ndef read_and_decode(filename_queue):\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64),\n        })\n\n    image = tf.decode_raw(features['image_raw'], tf.uint8)\n    image.set_shape([784])\n    image = tf.cast(image, tf.float32) * (1. \/ 255)\n    label = tf.cast(features['label'], tf.int32)\n\n    return image, label\n\n\ndef train_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n\n\ndef eval_input_fn(training_dir, params):\n    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n\n\ndef _input_fn(training_dir, training_filename, batch_size=100):\n    test_file = os.path.join(training_dir, training_filename)\n    filename_queue = tf.train.string_input_producer([test_file])\n\n    image, label = read_and_decode(filename_queue)\n    images, labels = tf.train.batch(\n        [image, label], batch_size=batch_size,\n        capacity=1000 + 3 * batch_size)\n\n    return {INPUT_TENSOR_NAME: images}, labels\n\ndef neo_preprocess(payload, content_type):\n    import logging\n    import numpy as np\n    import io\n\n    logging.info('Invoking user-defined pre-processing function')\n\n    if content_type != 'application\/x-image' and content_type != 'application\/vnd+python.numpy+binary':\n        raise RuntimeError('Content type must be application\/x-image or application\/vnd+python.numpy+binary')\n\n    f = io.BytesIO(payload)\n    image = np.load(f)*255\n\n    return image\n\n### NOTE: this function cannot use MXNet\ndef neo_postprocess(result):\n    import logging\n    import numpy as np\n    import json\n\n    logging.info('Invoking user-defined post-processing function')\n\n    # Softmax (assumes batch size 1)\n    result = np.squeeze(result)\n    result_exp = np.exp(result - np.max(result))\n    result = result_exp \/ np.sum(result_exp)\n\n    response_body = json.dumps(result.tolist())\n    content_type = 'application\/json'\n\n    return response_body, content_type\n<\/code><\/pre>\n\n<p>And I am training it <\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='cnn_fashion_mnist.py',\n                       role=role,\n                       input_mode='Pipe',\n                       training_steps=1, \n                       evaluation_steps=1,\n                       train_instance_count=1,\n                       output_path=output_path,\n                       train_instance_type='ml.c5.2xlarge',\n                       base_job_name='mnist')\n<\/code><\/pre>\n\n<p>so far it is trying correctly and it tells me that everything when well, but when I check the output there is nothing there or if I try to deploy it I get the error saying it couldn't find the model because there is nothing in the bucker, any ideas or extra configurations? Thank you<\/p>",
        "Challenge_closed_time":1576626745670,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575346450973,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is learning Sagemaker and has created an entry point for a Tensorflow model. They are training the model using Sagemaker, but when they check the output, there is nothing there, and they cannot deploy the model due to the error of not finding the model in the bucket. The user is seeking help to understand if there are any extra configurations required.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59150100",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":17.6,
        "Challenge_reading_time":66.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":355.6374158334,
        "Challenge_title":"Sagemaker and Tensorflow model not saved",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":658.0,
        "Challenge_word_count":407,
        "Platform":"Stack Overflow",
        "Poster_created_time":1462770189772,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":45.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Looks like you are using one of the older Tensorflow versions.\nWe would recommend switching to a newer more straight-forward way of running Tensorflow in SageMaker (script mode) by switching to a more recent Tensorflow version.<\/p>\n\n<p>You can read more about it in our documentation:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html<\/a><\/p>\n\n<p>Here is an example that might help:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/tensorflow_script_mode_training_and_serving.ipynb<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":25.0,
        "Solution_reading_time":12.16,
        "Solution_score_count":2.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":61.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":176.56633,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully built a Sagemaker endpoint using a Tensorflow model. The pre and post processing is done inside &quot;inference.py&quot; which calls a handler function based on this tutorial: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html#how-to-implement-the-pre-and-or-post-processing-handler-s<\/a><\/p>\n<p>My questions are:<\/p>\n<ul>\n<li>Which method is good for validating user input data within inference.py?<\/li>\n<li>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/li>\n<li>How is this compatible with the API gateway placed above the endpoint?<\/li>\n<\/ul>\n<p>Here is the structure of the inference.py with the desired validation check as a comment:<\/p>\n<pre><code>import json\nimport requests\n\n\ndef handler(data, context):\n    &quot;&quot;&quot;Handle request.\n    Args:\n        data (obj): the request data\n        context (Context): an object containing request and configuration details\n    Returns:\n        (bytes, string): data to return to client, (optional) response content type\n    &quot;&quot;&quot;\n    processed_input = _process_input(data, context)\n    response = requests.post(context.rest_uri, data=processed_input)\n    return _process_output(response, context)\n\n\ndef _process_input(data, context):\n    if context.request_content_type == 'application\/json':\n        # pass through json (assumes it's correctly formed)\n        d = data.read().decode('utf-8')\n        data_dict = json.loads(data)\n\n\n        # -----&gt;   if data_dict['input_1'] &gt; 25000:\n        # -----&gt;       return some error specific message with status code 123\n\n\n        return some_preprocessing_function(data_dict)\n\n    raise ValueError('{{&quot;error&quot;: &quot;unsupported content type {}&quot;}}'.format(\n        context.request_content_type or &quot;unknown&quot;))\n\n\ndef _process_output(data, context):\n    if data.status_code != 200:\n        raise ValueError(data.content.decode('utf-8'))\n\n    response_content_type = context.accept_header\n    prediction = data.content\n    return prediction, response_content_type\n<\/code><\/pre>",
        "Challenge_closed_time":1645567161520,
        "Challenge_comment_count":0,
        "Challenge_created_time":1644929807890,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user has successfully built a Sagemaker endpoint using a Tensorflow model and is now seeking guidance on how to validate user input data within inference.py, return appropriate error messages with status codes to the user if validation tests fail, and how to make it compatible with the API gateway placed above the endpoint. The user has provided the structure of the inference.py with the desired validation check as a comment.",
        "Challenge_last_edit_time":1644931522732,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71126832",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":30.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":177.042675,
        "Challenge_title":"Amazon Sagemaker: User Input data validation in Inference Endpoint",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":220,
        "Platform":"Stack Overflow",
        "Poster_created_time":1613661928947,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":160.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>I will answer your questions inline below:<\/p>\n<ol>\n<li><em>Which method is good for validating user input data within inference.py?<\/em><\/li>\n<\/ol>\n<p>Seeing that you have a <code>handler<\/code> function, <code>input_handler<\/code> and <code>output_handler<\/code> are ignored. Thus, inside your <code>handler<\/code> function (as you are correctly doing) you can have the validation logic.<\/p>\n<ol start=\"2\">\n<li><em>If such validation tests fail (e.g. wrong data types or data not in allowed range, etc.), how is it possible to return appropriate error messages with status codes to the user?<\/em><\/li>\n<\/ol>\n<p>I like to think of my SageMaker endpoint as a web server. Thus, you can return any valid HTTP response code with a response message. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_batch_transform\/tensorflow_cifar-10_with_inference_script\/code\/inference.py#L47\" rel=\"nofollow noreferrer\">inference.py<\/a> file that I found as a reference.<\/p>\n<pre><code>_return_error(\n            415, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or &quot;Unknown&quot;)\n        )\n\ndef _return_error(code, message):\n    raise ValueError(&quot;Error: {}, {}&quot;.format(str(code), message))\n<\/code><\/pre>\n<ol start=\"3\">\n<li><em>How is this compatible with the API gateway placed above the endpoint?<\/em><\/li>\n<\/ol>\n<p>Please see this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">link<\/a> for details on Creating a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.5,
        "Solution_reading_time":22.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":178.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1657058369727,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":116.0,
        "Answerer_view_count":9.0,
        "Challenge_adjusted_solved_time":52.4795297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>New to Sagemaker..<\/p>\n<p>Trained a &quot;linear-learner&quot; classification model using the Sagemaker API, and it saved a &quot;model.tar.gz&quot; file in my s3 path. From what I understand SM just used an image of a scikit logreg model.<\/p>\n<p>Finally, I'd like to gain access to the model object itself, so I unpacked the &quot;model.tar.gz&quot; file only to find another file called &quot;model_algo-1&quot; with no extension.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" alt=\"contents of unknown file\" \/><\/a><\/p>\n<p>Can anyone tell me how I can find the &quot;real&quot; modeling object without using the inference\/Endpoint delpoy API provided by Sagemaker? There are some things I want to look at manually.<\/p>\n<p>Thanks,\nCraig<\/p>",
        "Challenge_closed_time":1658447826900,
        "Challenge_comment_count":0,
        "Challenge_created_time":1658258900593,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a linear-learner classification model using the Sagemaker API, which saved a \"model.tar.gz\" file in their s3 path. However, upon unpacking the file, they found another file called \"model_algo-1\" with no extension and are unsure how to access the actual modeling object without using the inference\/Endpoint deploy API provided by Sagemaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73042521",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":11.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":52.4795297222,
        "Challenge_title":"How to use scikit learn model from inside sagemaker 'model.tar.gz' file?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":42.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Poster_created_time":1474555414616,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":129.0,
        "Poster_view_count":6.0,
        "Solution_body":"<p>Linear-Learner is a built in algorithm written using MX-net and the binary is also MXNET compatible. You can't use this model outside of SageMaker as there is no open source implementation for this.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.7,
        "Solution_reading_time":2.54,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":33.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1978.4166666667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi, I just got started using vertex ai with google cloud console. I am trying to deploy this mode to an endpoint. https:\/\/tfhub.dev\/tensorflow\/efficientnet\/lite0\/feature-vector\/2 I successfully imported it into a google storage bucket and uploaded it to the model registry. However, when I attempt to deploy the model to an endpoint, I receive the following error.\n\nHello Vertex AI Customer,\n\nDue to an error, Vertex AI was unable to create endpoint \"Feature Vectors\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name: \n**path to project**\nError Messages: Model server terminated: model server container terminated: \nexit_code:       255\nreason: \"Error\"\nstarted_at {\n   seconds: 1669817118\n}\nfinished_at {\n   seconds: 1669817421\n}\n. Model server logs can be found at \n**some link**\n\nI have attempted to change the TensorFlow version and the folder that i import (I attempted to import the containing folder instead of the model folder) however nothing seems to help. Any suggestions would be greatly appreciated. Thank you!",
        "Challenge_closed_time":1676994480000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1669872180000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is facing an error while trying to deploy a model to an endpoint using Vertex AI with Google Cloud Console. The error message states that the model server container terminated with an exit code of 255 and provides a link to the model server logs. The user has tried changing the TensorFlow version and the folder that they import, but the issue persists. They are seeking suggestions to resolve the problem.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Model-Deployment-Error\/m-p\/495020#M885",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":11.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1978.4166666667,
        "Challenge_title":"Vertex AI Model Deployment Error",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":283.0,
        "Challenge_word_count":155,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Yep, the solution was pretty simple.\u00a0\n\nThe trick was to import the model as a tensorflow GraphDef and then export the model with the serve tags included. You can then use this exported model instead of the untagged model. Hope this helps!\u00a0\n\n\n\n# import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.python.platform import gfile\n\nmodel_file = \".\/efficientnet_lite0_feature-vector_2\/saved_model.pb\"\n\nwith tf.Session() as sess:\n\n    graph = tf.Graph()\n    graph_def = tf.GraphDef()\n    with open(model_file, \"rb\") as f:\n        graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(graph_def)\n\n# Export the model to \/tmp\/my-model.meta.\nmeta_graph_def = tf.serve.export_meta_graph(filename='.\/efficientnet_lite0_feature-vector_2\/info.meta')\n\n\u00a0\n\nView solution in original post",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.4,
        "Solution_reading_time":10.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":84.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1221810788500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paderborn, North-Rhine-Westphalia, Germany",
        "Answerer_reputation_count":68522.0,
        "Answerer_view_count":7896.0,
        "Challenge_adjusted_solved_time":0.6979177778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can load a specific version of a model using the mlflow client:<\/p>\n<pre><code>import mlflow\n\nmodel_version = 1\n\nmodel = mlflow.pyfunc.load_model(\n    model_uri=f&quot;models:\/c3760a15e6ac48f88ad7e5af940047d4\/{model_version}&quot;\n)\n<\/code><\/pre>\n<p>But is there a way to load the latest model version?<\/p>",
        "Challenge_closed_time":1637853200392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1637843541287,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is looking for a way to load the latest version of a model from the MLflow model registry, as they are currently only able to load a specific version using the mlflow client.",
        "Challenge_last_edit_time":1637850687888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70111193",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2.6830847222,
        "Challenge_title":"How can I load the latest model version from MLflow model registry?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1873.0,
        "Challenge_word_count":43,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351080779276,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Leuven, Belgium",
        "Poster_reputation_count":3126.0,
        "Poster_view_count":262.0,
        "Solution_body":"<p>There is no such thing, like load <code>latest<\/code>, but:<\/p>\n<ul>\n<li>You can specify the stage (<code>staging<\/code>, <code>production<\/code>) - see <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/concepts.html#referencing-artifacts\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<li>You can find latest version using the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_latest_versions\" rel=\"nofollow noreferrer\">get_latest_versions<\/a> function - but it will also return latest per stage<\/li>\n<\/ul>\n<p>So you need to define what <code>latest<\/code> means for you.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.3,
        "Solution_reading_time":8.4,
        "Solution_score_count":2.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":15.3321294445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/143408-top-predictors.png?platform=QnA\" alt=\"143408-top-predictors.png\" \/>    <\/p>\n<p>The &quot;top predictors by influence&quot; in the training reports of AutoML regression models is very useful (see reference image), but I'm looking for a way to display all of the predictors, not just the top 10. Any way I can visualise this either in the training report or using the data tables themselves would be very useful.<\/p>",
        "Challenge_closed_time":1635215186563,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635159990897,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is looking for a way to view all predictors in AutoML training reports, not just the top 10 predictors by influence that are currently displayed. They are seeking a visualization method for displaying all predictors.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/602784\/how-do-you-see-all-predictors-by-influence-not-jus",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":7.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":15.3321294445,
        "Challenge_title":"How do you see ALL predictors by influence not just the top predictors of AutoML training reports?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":77,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi, PowerBi is not currently supported here on Q&amp;A. Please post your question on the <a href=\"https:\/\/community.powerbi.com\/\">PowerBI community forum<\/a> for faster response. Thanks.<\/p>\n<hr \/>\n<p>--- *Kindly <em><strong>Accept Answer<\/strong><\/em> if the information helps. Thanks.*<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":3.83,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1114.6180555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\ndoing\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show-mlflow-logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![Screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![Screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![Screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### To reproduce\r\n\r\nsee above\r\n\r\n### Expected behavior\r\n\r\nconvert the experiment by ID\r\n\r\n### Environment\r\n\r\n- Aim Version 3.6\r\n- Python 3.8.1\r\n- pip3\r\n- Ubuntu 20.04.3 LTS\r\n",
        "Challenge_closed_time":1649939186000,
        "Challenge_comment_count":6,
        "Challenge_created_time":1645926561000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error while running mlflow.projects.run consistently with etag error. The error is related to Etag conflict on the environment definition.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aimhubio\/aim\/issues\/1415",
        "Challenge_link_count":4,
        "Challenge_participation_count":6,
        "Challenge_readability":13.3,
        "Challenge_reading_time":14.53,
        "Challenge_repo_contributor_count":47.0,
        "Challenge_repo_fork_count":181.0,
        "Challenge_repo_issue_count":2399.0,
        "Challenge_repo_star_count":2909.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1114.6180555556,
        "Challenge_title":"aim convert mlflow --experiment fails for experiment id, works for experiment name",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":81,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hey @luisoala, thanks for reporting the issue!\r\n@devfox-se could you please take a look at this? Thanks for reporting this @luisoala, will take a look soon! Hey @luisoala! We've released `v3.6.2` containing the fix for mlflow converter. Please check it out and let me know if there are any issues. thanks @alberttorosyan working through a few other deadlines atm, aiming for a test ~ next tuesday, will share result here @luisoala Hi, have you had a chance to test this?:) Closing due to inactivity, feel free to reopen in case this still persists.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":6.68,
        "Solution_score_count":null,
        "Solution_sentence_count":8.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1415722650716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Verona, VR, Italy",
        "Answerer_reputation_count":4811.0,
        "Answerer_view_count":713.0,
        "Challenge_adjusted_solved_time":8.4415813889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>On the limited Azure Machine Learning Studio, one can import data from an On-Premises SQL Server Database.\nWhat about the ability to do the exact same thing on a python jupyter notebook on a virtual machine from the Azure Machine Learning Services workspace ?<\/p>\n\n<p>It does not seem possible from what I've found in the documentation.\nData sources would be limited in Azure ML Services : \"Currently, the list of supported Azure storage services that can be registered as datastores are Azure Blob Container, Azure File Share, Azure Data Lake, Azure Data Lake Gen2, Azure SQL Database, Azure PostgreSQL, and Databricks File System\"<\/p>\n\n<p>Thank you in advance for your assistance<\/p>",
        "Challenge_closed_time":1558479194140,
        "Challenge_comment_count":0,
        "Challenge_created_time":1558448804447,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to import data from an On-Premises SQL Server Database to a python jupyter notebook on a virtual machine from the Azure Machine Learning Services workspace. However, it seems that this is not possible as the data sources are limited to specific Azure storage services.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56240481",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.7,
        "Challenge_reading_time":9.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":8.4415813889,
        "Challenge_title":"Can I import data from On-Premises SQL Server Database to Azure Machine Learning virtual machine?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1147.0,
        "Challenge_word_count":123,
        "Platform":"Stack Overflow",
        "Poster_created_time":1558447987352,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p>As of today, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-load-data#load-sql-data\" rel=\"nofollow noreferrer\">you can load SQL data, but only a MS SQL Server source (also on-premise) is supported<\/a>.<\/p>\n\n<p>Using <code>azureml.dataprep<\/code>, code would read along the lines of<\/p>\n\n<pre><code>import azureml.dataprep as dprep\n\nsecret = dprep.register_secret(value=\"[SECRET-PASSWORD]\", id=\"[SECRET-ID]\")\n\nds = dprep.MSSQLDataSource(server_name=\"[SERVER-NAME]\",\n                           database_name=\"[DATABASE-NAME]\",\n                           user_name=\"[DATABASE-USERNAME]\",\n                           password=secret)\n\ndflow = dprep.read_sql(ds, \"SELECT top 100 * FROM [YourDB].[ATable]\")\n# print first records\ndflow.head(5)\n<\/code><\/pre>\n\n<p>As far as I understand the APIs are under heavy development and <code>azureml.dataprep<\/code> may be soon superseded by functionality provided by the <a href=\"https:\/\/aka.ms\/azureml\/concepts\/datasets\" rel=\"nofollow noreferrer\">Dataset class<\/a>.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.0,
        "Solution_reading_time":12.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":82.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1450242937020,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Syria",
        "Answerer_reputation_count":235.0,
        "Answerer_view_count":36.0,
        "Challenge_adjusted_solved_time":0.2286694444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run a python program using Pycharm IDE but unable to do so without stumbling into \"Your system has run out of application memory\". After some research I came across a suggestion of using Microsoft Azure ML. Can anyone point me to some helpful links that can get me started or any other suggestions at all?<\/p>\n\n<p>Edit: I am working with a data that has 400,000 samples and ~5000 samples and I want to use chi2 feature selection but I am unable to run the program.<\/p>",
        "Challenge_closed_time":1454732632400,
        "Challenge_comment_count":1,
        "Challenge_created_time":1454731809190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to run a large Python program on Pycharm IDE due to the error \"Your system has run out of application memory\". They are seeking suggestions for alternatives, and specifically mention working with a dataset of 400,000 samples and ~5000 features, and wanting to use chi2 feature selection.",
        "Challenge_last_edit_time":1483522935080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35237226",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":6.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.2286694444,
        "Challenge_title":"Cannot run huge Python program",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":118.0,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Poster_created_time":1454200887396,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>You can use: PyPy to run your program with less memory usage and more speed. see this <a href=\"http:\/\/pypy.org\/\" rel=\"nofollow\">pypy site<\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":4.8,
        "Solution_reading_time":1.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":21.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1432655047272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":463.0,
        "Answerer_view_count":76.0,
        "Challenge_adjusted_solved_time":989.9238102778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I've been using Amazon Sagemaker Notebooks to build a pytorch model for an NLP task.\nI know you can use Sagemaker to train, deploy, hyper parameter tuning, and model monitoring.<\/p>\n<p>However, it looks like you have to create an inference endpoint in order to monitor the model's inference performance.<\/p>\n<p>I already have a EC2 instance setup to perform inference tasks on our model, which is currently on a development box and rather not use an endpoint to make<\/p>\n<p>Is it possible to use Sagemaker to train, run hyperparam tuning and model eval without creating an endpoint.<\/p>",
        "Challenge_closed_time":1604012509390,
        "Challenge_comment_count":0,
        "Challenge_created_time":1600448783673,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is using Amazon Sagemaker Notebooks to build a pytorch model for an NLP task. They want to use Sagemaker for training, hyperparameter tuning, and model evaluation without creating an inference endpoint. They already have an EC2 instance set up for inference tasks and do not want to use an endpoint.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63960011",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":12.6,
        "Challenge_reading_time":8.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":989.9238102778,
        "Challenge_title":"Using AWS Sagemaker for model performance without creating endpoint",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":494.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1285379271580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":625.0,
        "Poster_view_count":110.0,
        "Solution_body":"<p>If you don't want to keep an inference endpoint up, one option is to use SageMaker Processing to run a job that takes your trained model and test dataset as input, performs inference and computes evaluation metrics, and saves them to S3 in a JSON file.<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">This Jupyter notebook example<\/a> steps through (1) preprocessing training and test data, (2) training a model, then (3) evaluating the model<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.3,
        "Solution_reading_time":8.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.4231186111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I want to create a compute cluster in Azure ML that has one vGPU and can scale its vCPU's within the specified min and max. Is this possible? And can someone explain how this can be done or point me to a tutorial?<\/p>\n<p>Thanks in advance!<\/p>",
        "Challenge_closed_time":1680518356223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1680516832996,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user wants to create a compute cluster in Azure ML with one vGPU and the ability to scale vCPU's within a specified range. They are seeking guidance or a tutorial on how to achieve this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1195654\/custom-compute-cluster",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.8,
        "Challenge_reading_time":3.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4231186111,
        "Challenge_title":"Custom Compute Cluster",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":49,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Unfortunately, you cannot create a compute cluster with a single vGPU that can scale its vCPUs independently in Azure ML. The available VM sizes in Azure are predefined with a fixed ratio of vCPUs, memory, and GPU resources. However, you can create a compute cluster with different VM sizes that can scale within a specified range of nodes, depending on your requirements<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace#create-a-workspace\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace#create-a-workspace<\/a><\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":16.3,
        "Solution_reading_time":7.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":64.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":0.3284980556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using .NII file format which represents, the neuroimaging dataset. I need to use Auto ML to label the dataset of images that are nearly 2GB in size per patient. The main issue is with using Auto ML to label the dataset of images with.NII file extension and classify whether the patient is having dementia or not.<\/p>\n<p><strong>Requirement:<\/strong> Forget about the problem domain of implementation like dementia. I would like to know about the procedure of using Auto ML for Computer vision applications through ML studio to use.NII file format dataset images.<\/p>\n<p>Any help would be thankful.<\/p>",
        "Challenge_closed_time":1652091549923,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652090367330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in using AutoML to label and classify MRI images in .NII file format for dementia diagnosis. The main issue is with implementing AutoML for computer vision applications through ML studio to use the .NII file format dataset images. The user is seeking guidance on the procedure for using AutoML in this context.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72170169",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":8.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.3284980556,
        "Challenge_title":"Implementing Computer Vison on AutoML to classify dementia using MRI images in .NII file format",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":112,
        "Platform":"Stack Overflow",
        "Poster_created_time":1651093614703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Netherland",
        "Poster_reputation_count":19.0,
        "Poster_view_count":12.0,
        "Solution_body":"<p>The requirement of using .nii or other file formats in Azure auto ML is a challenging task. Unfortunately, Auto ML image input format will be using in only JSON format. Kindly check the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-automl-images-schema\" rel=\"nofollow noreferrer\">document<\/a><\/p>\n<p>Answering regarding requirement of .nii format of dataset, there are different file format convertors available like &quot;<em><strong><a href=\"http:\/\/medicalimageconverter.com\/?msclkid=403b597ecf8111ecac65b3422c7b95b5\" rel=\"nofollow noreferrer\">Medical Image Convertor<\/a><\/strong><\/em>&quot;. This software is commercial and can be used for 10days for free. Convert .nii file formats into JPG and proceed with the general documentation provided in the top of the answer.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.9,
        "Solution_reading_time":10.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":90.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1363541433296,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Twin Cities, MN, USA",
        "Answerer_reputation_count":348.0,
        "Answerer_view_count":24.0,
        "Challenge_adjusted_solved_time":7.8782044444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.<\/p>\n<p>On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.<\/p>\n<p>Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3:\/\/my_bucket\/models\/model.tar.gz. I can see the tar file in S3.<\/p>\n<p>But when I'm trying to deploy the model, it keeps giving the error message &quot;Failed to extract model data archive&quot;.<\/p>\n<p>The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.<\/p>\n<p>The code for uploading to S3<\/p>\n<pre><code>import boto3\ns3 = boto3.client(&quot;s3&quot;, \n              region_name='eu-central-1', \n              aws_access_key_id=AWS_KEY_ID, \n              aws_secret_access_key=AWS_SECRET)\ns3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models\/model.tar.gz')\n<\/code><\/pre>\n<p>The code for creating the estimator and deploying:<\/p>\n<pre><code>import boto3\nfrom sagemaker.sklearn.estimator import SKLearnModel\n\n...\n\nmodel_data = 's3:\/\/my_bucket\/models\/model.tar.gz'\nsklearn_model = SKLearnModel(model_data=model_data,\n                             role=role,\n                             entry_point=&quot;my-script.py&quot;,\n                             framework_version=&quot;0.23-1&quot;)\npredictor = sklearn_model.deploy(instance_type=&quot;ml.t2.medium&quot;, initial_instance_count=1)                             \n<\/code><\/pre>\n<p>The error message:<\/p>\n<blockquote>\n<p>error message: UnexpectedStatusException: Error hosting endpoint\nsagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed\nto extract model data archive for container &quot;container_1&quot; from URL\n&quot;s3:\/\/my_bucket\/models\/model.tar.gz&quot;. Please ensure that the object\nlocated at the URL is a valid tar.gz archive<\/p>\n<\/blockquote>\n<p>Is there a way to see why the archive is invalid?<\/p>",
        "Challenge_closed_time":1611852069143,
        "Challenge_comment_count":5,
        "Challenge_created_time":1611565411923,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to deploy an existing Scikit-Learn model on Amazon SageMaker that was not trained on SageMaker but locally on their machine. They saved the model as model.joblib and tarred it to model.tar.gz. The user uploaded the model to their S3 bucket but encountered an error message \"Failed to extract model data archive\" when trying to deploy the model. The user is seeking a way to see why the archive is invalid.",
        "Challenge_last_edit_time":1611823707607,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65881699",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":11.7,
        "Challenge_reading_time":25.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":79.6270055556,
        "Challenge_title":"SageMaker failed to extract model data archive tar.gz for container when deploying",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1859.0,
        "Challenge_word_count":208,
        "Platform":"Stack Overflow",
        "Poster_created_time":1472970520888,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Amersfoort, Nederland",
        "Poster_reputation_count":424.0,
        "Poster_view_count":21.0,
        "Solution_body":"<p>I had a similar issue as well, along with a similar fix to Bas (per comment above).<\/p>\n<p>I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:<\/p>\n<p><code>tar -czf &lt;filename&gt; .\/&lt;directory-with-files&gt;<\/code><\/p>\n<p>but rather with the uploading step.<\/p>\n<p>Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):<\/p>\n<pre><code>bucket = 'bucket-name'\nkey = 'directory-inside-bucket'\nfile = 'the file name of the .tar.gz'\n\ns3_client = boto3.client('s3')\ns3_client.upload_file(file, bucket, key)\n<\/code><\/pre>\n<p>Docs: <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":13.9,
        "Solution_reading_time":14.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":120.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1313736279736,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation_count":207794.0,
        "Answerer_view_count":16864.0,
        "Challenge_adjusted_solved_time":26.7536241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a data set with 16 columns and 100,000 rows which I'm trying to prepare for a matrix-factorization training. I'm using the following code to split it and turn it into a sparse matrix.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\ny=data[[1]]\nX=lil_matrix(100000,15).astype('float32')\ny=np.array(y).astype('float32')\nX\n<\/code><\/pre>\n\n<p>But when I run it, I get this error:<\/p>\n\n<blockquote>\n  <p>&lt;1x1 sparse matrix of type ''  with 1 stored\n  elements in LInked List format> .<\/p>\n<\/blockquote>\n\n<p>When I try to plug it into a training\/testing split it gives me further errors:<\/p>\n\n<blockquote>\n  <p>Found input variables with inconsistent numbers of samples: [1,\n  100000]<\/p>\n<\/blockquote>",
        "Challenge_closed_time":1562801273076,
        "Challenge_comment_count":6,
        "Challenge_created_time":1562690689400,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to prepare a data set with 16 columns and 100,000 rows for matrix-factorization training. They are attempting to split the data and turn it into a sparse matrix, but encounter an error when running the code. The error message indicates that there is an inconsistency in the number of samples, causing further errors when attempting to use the data for training\/testing split.",
        "Challenge_last_edit_time":1562705343183,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56957206",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":30.7176877778,
        "Challenge_title":"how to turn a matrix into a sparse matrix and protobuf it",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":498.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1545524391676,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kansas City, MO, USA",
        "Poster_reputation_count":91.0,
        "Poster_view_count":24.0,
        "Solution_body":"<p>Your linked <code>notebook<\/code> is creating a 'blank' sparse matrix, and setting selected elements from data it reads from a <code>csv<\/code>.<\/p>\n\n<p>A simple example of this:<\/p>\n\n<pre><code>In [565]: from scipy import sparse                                                                           \nIn [566]: M = sparse.lil_matrix((10,5), dtype=float)                                                         \nIn [567]: M                                                                                                  \nOut[567]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 0 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>Note that I use <code>(10,5)<\/code> to specify the matrix shape.  The () matter!  That's why I stressed reading the <code>docs<\/code>.  In the link the relevant line is:<\/p>\n\n<pre><code>X = lil_matrix((lines, columns)).astype('float32')\n<\/code><\/pre>\n\n<p>Now I can set a couple elements, just as I would an dense array:<\/p>\n\n<pre><code>In [568]: M[1,2] = 12.3                                                                                      \nIn [569]: M[3,1] = 1.1                                                                                       \nIn [570]: M                                                                                                  \nOut[570]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>I can use <code>toarray<\/code> to display the matrix as a dense array (don't try this with large dimensions).<\/p>\n\n<pre><code>In [571]: M.toarray()                                                                                        \nOut[571]: \narray([[ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. , 12.3,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  1.1,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ]])\n<\/code><\/pre>\n\n<hr>\n\n<p>If I omit the (), it makes a (1,1) matrix with just one element, the first number.<\/p>\n\n<pre><code>In [572]: sparse.lil_matrix(10,5)                                                                            \nOut[572]: \n&lt;1x1 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 1 stored elements in LInked List format&gt;\nIn [573]: _.A                                                                                                \nOut[573]: array([[10]], dtype=int64)\n<\/code><\/pre>\n\n<p>Look again at your code.  You set the <code>X<\/code> value twice, once it is a dataframe.  The second time is this bad <code>lil<\/code> initialization.  The second time does not make use of the first <code>X<\/code>.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\n...\nX=lil_matrix(100000,15).astype('float32')\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":1562801656230,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":25.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":283.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1595479476676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Massachusetts, USA",
        "Answerer_reputation_count":246.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":538.3183666667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use API workflow (python code) to find a model version that has the best metric (for instance, \u201caccuracy\u201d) among several model versions. I understand we can use web UI to do so, but I would love to write python code to achieve this. Could someone help me?<\/p>",
        "Challenge_closed_time":1595480605940,
        "Challenge_comment_count":0,
        "Challenge_created_time":1593542659820,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking help to find the best model version with a specific metric (such as accuracy) using Python code in MLflow API workflow. They are aware that this can be done through the web UI but prefer to achieve it through Python code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62664183",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.8,
        "Challenge_reading_time":4.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":538.3183666667,
        "Challenge_title":"MLflow: find model version with best metric using python code",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":445.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419619351727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":16.0,
        "Solution_body":"<pre><code>import mlflow \nclient = mlflow.tracking.MlflowClient()\nruns = client.search_runs(&quot;my_experiment_id&quot;, &quot;&quot;, order_by=[&quot;metrics.rmse DESC&quot;], max_results=1)\nbest_run = runs[0]\n<\/code><\/pre>\n<p><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.search_runs<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":29.4,
        "Solution_reading_time":6.76,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":17.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":18.8037608333,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have built an XGBoost model using Amazon Sagemaker, but I was unable to find anything which will help me interpret the model and validate if it has learned the right dependencies.<\/p>\n\n<p>Generally, we can see Feature Importance for XGBoost by get_fscore() function in the python API (<a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html<\/a>) I see nothing of that sort in the sagemaker api(<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>).<\/p>\n\n<p>I know I can build my own model and then deploy that using sagemaker but I am curious if anyone has faced this problem and how they overcame it.<\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":1555001695856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1554934002317,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has built an XGBoost model using Amazon Sagemaker but is unable to find a way to interpret the model and validate if it has learned the right dependencies. They are specifically looking for a way to see the feature importance for XGBoost in Sagemaker, but have not found any function in the Sagemaker API to do so. The user is seeking advice on how to overcome this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/55621967",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":14.7,
        "Challenge_reading_time":11.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":18.8037608333,
        "Challenge_title":"Feature Importance for XGBoost in Sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3637.0,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419327925267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Mountain View, CA, USA",
        "Poster_reputation_count":123.0,
        "Poster_view_count":17.0,
        "Solution_body":"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle as pkl\nimport xgboost\nbooster = pkl.load(open(model_file, 'rb'))\nbooster.get_score()\nbooster.get_fscore()\n<\/code><\/pre>\n\n<p>Refer <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">XGBoost doc<\/a> for methods to get feature importance from the Booster object such as <code>get_score()<\/code> or <code>get_fscore()<\/code>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.42,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":83.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1595479476676,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Massachusetts, USA",
        "Answerer_reputation_count":246.0,
        "Answerer_view_count":21.0,
        "Challenge_adjusted_solved_time":168.9973711111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to create an MLOps Pipeline using Azure DevOps and Azure Databricks. From Azure DevOps, I am submitting a Databricks job to a cluster, which trains a Machine Learning Model and saves it into MLFlow Model Registry with a custom flavour (using PyFunc Custom Model).<\/p>\n<p>Now after the job gets over, I want to export this MLFlow Object (with all dependencies - Conda dependencies, two model files - one <code>.pkl<\/code> and one <code>.h5<\/code>, the Python Class with <code>load_context()<\/code> and <code>predict()<\/code> functions defined so that after exporting I can import it and call predict as we do with MLFlow Models).<\/p>\n<p>How do I export this entire MLFlow Model and save it as an AzureDevOps Artifact to be used in the CD phase (where I will deploy it to an AKS cluster with a custom base image)?<\/p>",
        "Challenge_closed_time":1629787454223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1629179063687,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an MLOps Pipeline using Azure DevOps and Azure Databricks. They have trained a Machine Learning Model and saved it into MLFlow Model Registry with a custom flavour. The user wants to export this MLFlow Object with all dependencies and save it as an Azure DevOps Artifact to be used in the CD phase. They plan to deploy it to an AKS cluster with a custom base image.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68812238",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":11.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":168.9973711111,
        "Challenge_title":"How to export a MLFlow Model from Azure Databricks as an Azure DevOps Artifacts for CD phase?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Poster_created_time":1601729162436,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Solution_body":"<p>There is no official way to export a Databricks MLflow run from one workspace to another. However, there is an &quot;unofficial&quot; tool that does most of the job with the main limitation being that notebook revisions linked to a run cannot be exported due to lack of a REST API endpoint for this.<\/p>\n<p><a href=\"https:\/\/github.com\/amesar\/mlflow-export-import\" rel=\"nofollow noreferrer\">https:\/\/github.com\/amesar\/mlflow-export-import<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":10.0,
        "Solution_reading_time":5.74,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":57.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":85.3072222222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use mlflow package in databricks to save the model into Azure Storage.<\/p>\n<p>The Script:<\/p>\n<p><code>abfss_path='abfss:\/\/mlops@dlsgdpeasdev03.dfs.core.windows.net'<\/code><\/p>\n<p><code>project = 'test'<\/code><\/p>\n<p><code>model_version = 'v1.0.1'<\/code><\/p>\n<p><code>model = {model training step}<\/code>  <br \/>\n<code>prefix_model_path = os.path.join(abfss_path, project, model_version)<\/code><\/p>\n<p><code>model_path = prefix_model_path<\/code><\/p>\n<p><code>print(model_path) # <\/code>abfss:\/\/mlops@dlsgdpeasdev03.dfs.core.windows.net\/test\/v1.0.1<\/p>\n<p><code>mlflow.sklearn.save_model(model, model_path)<\/code><\/p>\n<p>The message is successfully save the model. <\/p>\n<p>When I check the container and file does not exist, but I am able to load model with the same path. That mean the model file saved in databricks somewhere.<\/p>\n<p>I want to know where is the model file in databricks, and how to save the model directly from databricks notebook to Azure Storage.<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":1673889987903,
        "Challenge_comment_count":1,
        "Challenge_created_time":1673582881903,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is trying to save an ml model into Azure Storage Container using the mlflow package in Databricks. Although the message shows that the model is saved successfully, the user cannot find the file in the container. The user wants to know where the model file is saved in Databricks and how to save the model directly from the Databricks notebook to Azure Storage.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1160457\/how-databrick-save-ml-model-into-azure-storage-con",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.1,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":85.3072222222,
        "Challenge_title":"How databrick save ml model into Azure Storage Container?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":109,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=ff486681-dc42-4832-92a7-87a687f2ec26\">@Benny Lau ,Shui Hong - Group Office  <\/a>, <\/p>\n<p>Thanks for the ask and welcome to Microsoft Q&amp;A . <\/p>\n<p>As I understand the ask here is to where the model is saved and how you can save to the blob . <\/p>\n<p>As per the document here : [https:\/\/learn.microsoft.com\/en-us\/azure\/databricks\/mlflow\/models#api-commands<\/p>\n<p>You have three option and I assume that your  model file is getting stored in the DBFS on the Azure databricks cluster .<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/4d409166-ee35-4252-9294-6e7a4140ffab?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>Databricks can save a machine learning model to an Azure Storage Container using the <strong><code>dbutils.fs<\/code><\/strong> module. This module provides a set of functions for interacting with the Databricks file system (DBFS) and Azure Blob Storage. Here is an example of how to save a model to an Azure Storage Container:<\/p>\n<ol>\n<li> First, you will need to mount the Azure Storage Container to DBFS, this can be done using the <strong><code>dbutils.fs.mount<\/code><\/strong> function.<\/li>\n<\/ol>\n<pre><code>dbutils.fs.mount(\n  source='wasbs:\/\/&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net',\n  mount_point='\/mnt\/&lt;your-mount-point&gt;',\n  extra_configs={\n    &quot;fs.azure.account.auth.type&quot;: &quot;OAuth&quot;,\n    &quot;fs.azure.account.oauth.provider.type&quot;: &quot;org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider&quot;,\n    &quot;fs.azure.account.oauth2.client.id&quot;: &quot;&lt;your-client-id&gt;&quot;,\n    &quot;fs.azure.account.oauth2.client.secret&quot;: &quot;&lt;your-client-secret&gt;&quot;,\n    &quot;fs.azure.account.oauth2.client.endpoint&quot;: &quot;https:\/\/login.microsoftonline.com\/&lt;your-tenant-id&gt;\/oauth2\/token&quot;\n  }\n)\n\n<\/code><\/pre>\n<ol>\n<li> Once the container is mounted, you can use the <strong><code>dbutils.fs.cp<\/code><\/strong> function to copy the model from the local file system to the mount point.<\/li>\n<\/ol>\n<p>dbutils.fs.cp(&quot;path\/to\/local\/model&quot;, &quot;\/mnt\/&lt;your-mount-point&gt;\/model&quot;)<\/p>\n<ol>\n<li> You can also use <strong><code>model.save()<\/code><\/strong> method to save the model in the mounted container path<\/li>\n<\/ol>\n<p>model.save(&quot;\/mnt\/&lt;your-mount-point&gt;\/model&quot;)<\/p>\n<p>Note: Be sure to replace the placeholders in the above code with the appropriate values for your use case.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.7,
        "Solution_reading_time":33.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":231.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":61.6052525,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Have a small doubt. i could run a pipeline successfully and also register the model. I can locate the model on the AzureML UI .  <br \/>\nModel.get_model_path() shows that it is located in azureml-models\/model-name\/..   <\/p>\n<p>But was wondering where exactly they are stored in storage account? Becasue i dont find and container azureml-model listed.   <\/p>\n<p>Any lead on this will be helpful<\/p>",
        "Challenge_closed_time":1637886171656,
        "Challenge_comment_count":4,
        "Challenge_created_time":1637664392747,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has successfully run a pipeline and registered a model on AzureML UI. However, they are unable to locate the container \"azureml-model\" where the model is stored in the storage account. They are seeking assistance in finding the location of the container.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/637656\/where-are-registered-models-saved-in-storage-conta",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":6.6,
        "Challenge_reading_time":5.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":61.6052525,
        "Challenge_title":"Where are registered models saved in storage containers?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p> <a href=\"\/users\/na\/?userid=2dc066be-691a-47bd-9f7a-67e426d994d9\">@Antara Das  <\/a>  Thanks for the details. there is not an azureml-models container, run.register_model() copies the model files to the azureml container. <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.9,
        "Solution_reading_time":2.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1463778034387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":2.7701311111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I\u2019m using the following script to execute an AutoML run, also passing the test dataset<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>automl_settings = {\n    &quot;n_cross_validations&quot;: 10,\n    &quot;primary_metric&quot;: 'spearman_correlation',\n    &quot;enable_early_stopping&quot;: True,\n    &quot;max_concurrent_iterations&quot;: 10, \n    &quot;max_cores_per_iteration&quot;: -1,   \n    &quot;experiment_timeout_hours&quot;: 1,\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO}\nautoml_config = AutoMLConfig(task = 'regression',\n                             debug_log = 'automl_errors.log',\n                             compute_target = compute_target,\n                             training_data = training_data,\n                             test_data = test_data,\n                             label_column_name = label_column_name,\n                             model_explainability = True,\n                             **automl_settings                            )\n<\/code><\/pre>",
        "Challenge_closed_time":1635964127852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635954155380,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to execute an AutoML run and pass the test dataset to get metrics out of it. They have provided a script with various settings for the AutoML run, including the primary metric, featurization, and verbosity.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69827748",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":26.6,
        "Challenge_reading_time":10.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2.7701311111,
        "Challenge_title":"get metrics out of AutoMLRun based on test_data",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":85.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1405457120427,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation_count":3359.0,
        "Poster_view_count":555.0,
        "Solution_body":"<p>Note that the TEST DATASET SUPPORT is a feature still in PRIVATE PREVIEW. It'll probably be released as PUBLIC PREVIEW later in NOVEMBER, but until then, you need to be enrolled in the PRIVATE PREVIEW in order to see the &quot;Test runs and metrics&quot; in the UI. You can send me an email to cesardl at microsoft dot com and send me your AZURE SUBSCRIPTION ID to be enabled so you see it in the UI.<\/p>\n<p>You can see further info on how to get started here:\n<a href=\"https:\/\/github.com\/Azure\/automl-testdataset-preview\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/automl-testdataset-preview<\/a><\/p>\n<p>About how to use it, you need to either provide the test_Data (specific Test AML Tabular Dataset that for instance you loaded from a file os split manually previously)\nor you can provide a test_size which is the % (i.e. 0.2 is 20%) to be split from the single\/original dataset.<\/p>\n<p>About the TEST metrics, since you can make multiple TEST runs against a single model, you need to go to the specific TEST run available under the link &quot;Test results&quot;<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3pPPS.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":11.0,
        "Solution_reading_time":14.9,
        "Solution_score_count":3.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":177.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1341441916656,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5985.0,
        "Answerer_view_count":161.0,
        "Challenge_adjusted_solved_time":4157.9736122222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have two separate normalized text files that I want to train my BlazingText model on.<\/p>\n\n<p>I am struggling to get this to work and the documentation is not helping.<\/p>\n\n<p>Basically I need to figure out how to supply multiple files or S3 prefixes as \"inputs\" parameter to the sagemaker.estimator.Estimator.fit() method.<\/p>\n\n<p>I first tried:<\/p>\n\n<pre><code>s3_train_data1 = 's3:\/\/{}\/{}'.format(bucket, prefix1)\ns3_train_data2 = 's3:\/\/{}\/{}'.format(bucket, prefix2)\n\ntrain_data1 = sagemaker.session.s3_input(s3_train_data1, distribution='FullyReplicated', content_type='text\/plain', s3_data_type='S3Prefix')\n\ntrain_data2 = sagemaker.session.s3_input(s3_train_data2, distribution='FullyReplicated', content_type='text\/plain', s3_data_type='S3Prefix')\n\nbt_model.fit(inputs={'train1': train_data1, 'train2': train_data2}, logs=True)\n<\/code><\/pre>\n\n<p>this doesn't work because SageMaker is looking for the key specifically to be \"train\" in the inputs parameter.<\/p>\n\n<p>So then i tried:<\/p>\n\n<pre><code>bt_model.fit(inputs={'train': train_data1, 'train': train_data2}, logs=True)\n<\/code><\/pre>\n\n<p>This trains the model only on the second dataset and ignores the first one completely.<\/p>\n\n<p>Now finally I tried using a Manifest file using the documentation here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html<\/a><\/p>\n\n<p>(see manifest file format under \"S3Uri\" section)<\/p>\n\n<p>the documentation says the manifest file format is a JSON that looks like this example:<\/p>\n\n<pre><code>[\n\n{\"prefix\": \"s3:\/\/customer_bucket\/some\/prefix\/\"},\n\n\"relative\/path\/to\/custdata-1\",\n\n\"relative\/path\/custdata-2\"\n\n]\n<\/code><\/pre>\n\n<p>Well, I don't think this is valid JSON in the first place but what do I know, I still give it a try.<\/p>\n\n<p>When I try this:<\/p>\n\n<pre><code>s3_train_data_manifest = 'https:\/\/s3.us-east-2.amazonaws.com\/bucketpath\/myfilename.manifest'\n\ntrain_data_merged = sagemaker.session.s3_input(s3_train_data_manifest, distribution='FullyReplicated', content_type='text\/plain', s3_data_type='ManifestFile')\n\ndata_channel_merged = {'train': train_data_merged}\n\nbt_model.fit(inputs=data_channel_merged, logs=True)\n<\/code><\/pre>\n\n<p>I get an error saying:<\/p>\n\n<pre><code>ValueError: Error training blazingtext-2018-10-17-XX-XX-XX-XXX: Failed Reason: ClientError: Data download failed:Unable to parse manifest at s3:\/\/mybucketpath\/myfilename.manifest - invalid format\n<\/code><\/pre>\n\n<p>I tried replacing square brackets in my manifest file with curly braces ...but still I feel the JSON file format seems to be missing something that documentation fails to describe correctly?<\/p>",
        "Challenge_closed_time":1540883711836,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539786419823,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is having trouble training a SageMaker BlazingText model using multiple channels. They have tried supplying multiple files or S3 prefixes as \"inputs\" parameter to the sagemaker.estimator.Estimator.fit() method, but it did not work. They also tried using a Manifest file, but it resulted in an error saying \"Unable to parse manifest - invalid format\". The user is unsure if the JSON file format is missing something that the documentation fails to describe correctly.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52857309",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":15.3,
        "Challenge_reading_time":36.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":304.8033369444,
        "Challenge_title":"How to train SageMaker BlazingText model using multiple channels",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1053.0,
        "Challenge_word_count":265,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432179256928,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":17.0,
        "Poster_view_count":10.0,
        "Solution_body":"<p>You can certainly match multiple files with the same prefix, so your first attempt could have worked as long as you organize your files in your S3 bucket to suit. For e.g. the prefix: <code>s3:\/\/mybucket\/foo\/<\/code> will match the files <code>s3:\/\/mybucket\/foo\/bar\/data1.txt<\/code> and <code>s3:\/\/mybucket\/foo\/baz\/data2.txt<\/code><\/p>\n\n<p>However, if there is a third file in your bucket called <code>s3:\/\/mybucket\/foo\/qux\/data3.txt<\/code> that you <em>don't<\/em> want matched (while still matching the first two) there is no way to do achieve that with a single prefix. In these cases a manifest would work. So, in the above example, the manifest would simply be:<\/p>\n\n<pre><code>[\n  {\"prefix\": \"s3:\/\/mybucket\/foo\/\"},\n  \"bar\/data1.txt\",\n  \"baz\/data2.txt\"\n]\n<\/code><\/pre>\n\n<p><em>(and yes, this is valid json - it is an array whose first element is an object with an attribute called <code>prefix<\/code> and all subsequent elements are strings).<\/em><\/p>\n\n<p>Please double check your manifest (you didn't actually post it so I can't do that for you) and make sure it conforms to the above syntax.<\/p>\n\n<p>If you're still stuck please open up a thread on the AWS sagemaker forums - <a href=\"https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/forum.jspa?forumID=285<\/a> and after you do that we can setup a PM to try and get to the bottom of this (never post your AWS account id in a public forum like StackOverflow or even in AWS forums).<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1554755124827,
        "Solution_link_count":2.0,
        "Solution_readability":8.9,
        "Solution_reading_time":18.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":207.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1656670919183,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":76.5733497222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new with ruby and I want to use GCP AIPlatform but I'm struggeling with the payload.<\/p>\n<p>So far, I have :<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = ::Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  'data:image\/png;base64,' + Base64.strict_encode64(img.read)\nend\n\ninstance = Instance.new(:content =&gt; img)\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>Here is my proto<\/p>\n<pre><code>message Instance {\n  required bytes content = 1;\n};\n<\/code><\/pre>\n<p>But I have the following error : <code>Invalid type Instance to assign to submessage field 'instances'<\/code><\/p>\n<p>I read the documentation but for ruby SDK it's a bit light.\nThe parameters are OK, the JS example here : <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js<\/a> is working with those parameters<\/p>\n<p>What am I doing wrong ?<\/p>",
        "Challenge_closed_time":1656947266916,
        "Challenge_comment_count":1,
        "Challenge_created_time":1656671602857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to Ruby and is trying to use GCP AIPlatform for image classification prediction. They are struggling with the payload and are encountering an error message \"Invalid type Instance to assign to submessage field 'instances'\". The user has provided their code and proto and is seeking help to identify the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72827960",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":15.3,
        "Challenge_reading_time":18.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":76.5733497222,
        "Challenge_title":"How to get image classification prediction from GCP AIPlatform in ruby?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Poster_created_time":1656670919183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I managed it<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  Base64.strict_encode64(img.read)\nend\n\ninstance = Google::Protobuf::Value.new(:struct_value =&gt; {:fields =&gt; {\n  :content =&gt; {:string_value =&gt; img}\n}})\nendpoint = &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;\n\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: endpoint,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>The use of the Google::Protobuf::Value looks ugly to me but it works<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":16.6,
        "Solution_reading_time":9.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":55.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1619177157943,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":835.0,
        "Answerer_view_count":516.0,
        "Challenge_adjusted_solved_time":396.3547583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Challenge_closed_time":1652802212843,
        "Challenge_comment_count":5,
        "Challenge_created_time":1651375335713,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is following a GCP Jupyter notebook example and has encountered an issue where the function 'classif_model_eval_metrics' takes 'metrics: Output[Metrics]' and 'metricsc: Output[ClassificationMetrics]' as mandatory arguments, but the function is called without those arguments. The user is confused about why it works and what 'metrics: Output[Metrics]' and 'metricsc: Output[ClassificationMetrics]' of type 'kfp.v2.dsl.Output' are.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Challenge_link_count":3,
        "Challenge_participation_count":6,
        "Challenge_readability":23.4,
        "Challenge_reading_time":91.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":50,
        "Challenge_solved_time":396.3547583334,
        "Challenge_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":358.0,
        "Challenge_word_count":467,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":19.5,
        "Solution_reading_time":27.41,
        "Solution_score_count":2.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":180.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1351154914716,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":2564.0,
        "Answerer_view_count":451.0,
        "Challenge_adjusted_solved_time":0.9837333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Challenge_closed_time":1659712728223,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659709186783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to retrieve a model in Vertex AI that was created by running a training job in a Jupyter notebook. They have attempted to retrieve the model using the model ID and pipeline ID, but are receiving an error message related to metadata parsing.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":15.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.9837333334,
        "Challenge_title":"How do I retrieve a model in Vertex AI?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":45.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Poster_created_time":1351154914716,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Solution_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":31.0,
        "Solution_reading_time":2.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":9.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1444147981107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":46.0,
        "Answerer_view_count":2.0,
        "Challenge_adjusted_solved_time":126.5392991667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have the following simple setup in Azure ML. <a href=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" alt=\"ML setup\"><\/a> \nBasically the Reader is a SQL query to a DB which returns a vector called Pdelta, which is then passed to the R script for further processing  and the results are then returned back to the web service. The DB query is simple (<code>SELECT Pdelta FROM ...<\/code>) and it works fine. I have set the DB query as a web service paramater as well. <\/p>\n\n<p>Everything seems to work fine, but at the end when i publish it as a web service and test it, it somehow asks for an additional input parameter. The additional parameter gets called <code>PDELTA<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am wondering why is this happening, what is it that I am overlooking? I would like to make this web service ask for only one parameter - the SQL query (Delta Query) which would then deliver the Pdeltas. <\/p>\n\n<p>Any ideas or suggestions would be grealty appreciated! <\/p>",
        "Challenge_closed_time":1444147981107,
        "Challenge_comment_count":0,
        "Challenge_created_time":1443692439630,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has set up a simple Azure ML configuration where the Reader is a SQL query to a database that returns a vector called Pdelta, which is then passed to the R script for further processing. However, when the user publishes it as a web service and tests it, an additional input parameter called PDELTA is requested, which the user wants to eliminate to make the web service ask for only one parameter - the SQL query (Delta Query) that would deliver the Pdeltas.",
        "Challenge_last_edit_time":1456850075240,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32884296",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":126.5392991667,
        "Challenge_title":"Web service input into SQL query into R in Azure ML",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":347.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432829415467,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":501.0,
        "Poster_view_count":76.0,
        "Solution_body":"<p>You can remove the web service input block and publish the web service without it. That way the Pdelta input will be passed in only from the Reader module.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.6,
        "Solution_reading_time":1.97,
        "Solution_score_count":3.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":29.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1562055808543,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":895.0,
        "Answerer_view_count":53.0,
        "Challenge_adjusted_solved_time":3027.8199811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to deploy a TF2.0 model to SageMaker. So far, I managed to train the model and save it into an S3 bucket but when I'm calling the <code>.deploy()<\/code> method, I get the following error from cloud Watch <\/p>\n\n<p><code>ValueError: no SavedModel bundles found!<\/code><\/p>\n\n<p>Here is my training script: <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tf_model\"), save_format=\"tf\")\n\ndef model_fn(model_dir):\n    classifier = tf.keras.models.load_model(os.path.join(model_dir, \"tf_model\"))\n    return classifier\n<\/code><\/pre>\n\n<p>And here is the code that I wrote into Colab <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='tensorflow_estimator.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='ml.m5.large',\n                          framework_version='2.0.0', \n                          sagemaker_session=sagemaker_session,\n                          output_path=s3_output_location,\n                          hyperparameters={'epochs': 1,\n                                           'batch_size': 30,\n                                           'learning_rate': 0.001},\n                          py_version='py3')\n\n\ntf_estimator.fit({\"train\":train_data})\n\nfrom sagemaker.tensorflow.serving import Model\n\nmodel = Model(model_data='s3:\/\/path\/to\/model.tar.gz', \n              role=role,\n              framework_version=\"2.0.0\",\n              sagemaker_session=sagemaker_session)\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n<\/code><\/pre>\n\n<p>I already tried to look at <a href=\"https:\/\/stackoverflow.com\/questions\/57172147\/no-savedmodel-bundles-found-on-tensorflow-hub-model-deployment-to-aws-sagemak\">this thread<\/a> but I actually don't have the problem of versions in my tar.gz file as the structure is the following : <\/p>\n\n<pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 saved_model.pb\n\u2514\u2500\u2500 variables\n    \u251c\u2500\u2500 variables.data-00000-of-00001\n    \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>I feel I might be wrong when defining <code>model_fn()<\/code> in my training script but definitely don't what to replace that with. Would you guys have an idea? <\/p>\n\n<p>Thanks a lot for your help!  <\/p>",
        "Challenge_closed_time":1579799847967,
        "Challenge_comment_count":1,
        "Challenge_created_time":1579796394240,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"ValueError: no SavedModel bundles found!\" error when trying to deploy a TensorFlow 2.0 model to SageMaker. They have successfully trained and saved the model to an S3 bucket, but are encountering issues when calling the .deploy() method. The user has provided their training script and deployment code, and has already checked for version issues in their tar.gz file. They suspect that the issue may be with their model_fn() function but are unsure of how to proceed.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59882941",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":16.2,
        "Challenge_reading_time":64.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":68,
        "Challenge_solved_time":0.9593686111,
        "Challenge_title":"ValueError: no SavedModel bundles found! when trying to deploy a TF2.0 model to SageMaker",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":943.0,
        "Challenge_word_count":395,
        "Platform":"Stack Overflow",
        "Poster_created_time":1562055808543,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":895.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>I actually tried to modify my training script to the following : <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n\n<\/code><\/pre>\n\n<p>It seems that it's important to have a numerical name for your folder:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Save the model\nmodel.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1590696546172,
        "Solution_link_count":0.0,
        "Solution_readability":18.3,
        "Solution_reading_time":44.06,
        "Solution_score_count":4.0,
        "Solution_sentence_count":46.0,
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1645475560783,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":466.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":9.3981805555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to return output(or predictions) from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as &quot;Content-Type&quot;. I have tried multiple ways, but the sagemaker always returns with 'text\/html; charset=utf-8' as the &quot;Content-Type&quot;, and I would like SageMaker to return 'text\/csv' or 'text\/csv; charset=utf-8'.<\/p>\n<p>Here's the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#process-output\" rel=\"nofollow noreferrer\"><code>output_fn<\/code><\/a> from my inference-code:<\/p>\n<pre><code>** my other code **\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return output_float\n<\/code><\/pre>\n<p>above function returns number with float data-type and I got error(in cloudwatch logs) that  this function should only be returning string, tuple, dict or Respoonse instance.<\/p>\n<p>So, here are all the different ways I have tried to have SageMaker return my number with 'text\/csv' but only receives 'text\/html; charset=utf-8'<\/p>\n<ol>\n<li><code>return json.dumps(output_float)<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li>by using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\"><code>sagemaker.serializers.CSVSerializer<\/code><\/a> like this:\n<pre><code>from sagemaker.serializers import CSVSerializer\ncsv_serialiser = CSVSerializer(content_type='text\/csv')\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return csv_serialiser.serialize(output_float)\n<\/code><\/pre>\nI got <code>'NoneType' object has no attribute 'startswith'<\/code> error with this.<\/li>\n<li>as a tuple: <code>return (output_float,)<\/code>. I haven't noted down what this did, but it sure didn't return the number with 'text\/csv' as &quot;Content-Type&quot;.<\/li>\n<li>made changes in my model object to return a float on calling <code>.predict_proba<\/code> on my model-object and deployed it from sagemaker studio without using any custom inference code and deployed this from SageMaker studio. but got this error after sending a request to the endpoint: <code>'NoneType' object has no attribute 'startswith'<\/code>, but at my side, when I pass proper inputs to the unpicked model and call .predict_proba, i get float as expected.<\/li>\n<li>by returning <a href=\"https:\/\/tedboy.github.io\/flask\/generated\/generated\/flask.Response.html#flask.Response\" rel=\"nofollow noreferrer\"><code>flask.Response<\/code><\/a> like this:\n<pre><code>from flask import Response\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return Response(response=output_float, status=200, headers={'Content-Type':'text\/csv; charset=utf-8'})\n<\/code><\/pre>\nbut, I got some IndexError with this(I didn't notedown the traceback.)<\/li>\n<\/ol>\n<p>Some other info:<\/p>\n<ol>\n<li>Model I am using is completely outside of SageMaker, not from a training job or anything like that.<\/li>\n<li>All of the aforementioned endpoints have been deployed entirely from aws-cli with relevant .json files, except the one in point-8 above.<\/li>\n<\/ol>\n<p>How do I return number with &quot;text\/csv&quot; as content-type from sagemaker? I need my output in &quot;text\/csv&quot; content-type specifically for Model-Quality-Monitor. How do I do this?<\/p>",
        "Challenge_closed_time":1646685903080,
        "Challenge_comment_count":2,
        "Challenge_created_time":1646135687137,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to return output from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as \"Content-Type\", but SageMaker always returns with 'text\/html; charset=utf-8' as the \"Content-Type\". The user has tried multiple ways, including using sagemaker.serializers.CSVSerializer, returning a float, and returning flask.Response, but none of them worked. The user needs the output in 'text\/csv' content-type specifically for Model-Quality-Monitor.",
        "Challenge_last_edit_time":1646652069630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71308112",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":12.5,
        "Challenge_reading_time":49.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":152.8377619444,
        "Challenge_title":"How to return a float with 'text\/csv' as \"Content-Type\" from SageMaker endpoint that uses custom inference code?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":461.0,
        "Challenge_word_count":398,
        "Platform":"Stack Overflow",
        "Poster_created_time":1559910246180,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":2046.0,
        "Poster_view_count":369.0,
        "Solution_body":"<p>From the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L88\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example, I suggest testing by setting the Response as follows:<\/p>\n<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text\/csv&quot;)\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":27.4,
        "Solution_reading_time":5.57,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1480786532470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Answerer_reputation_count":111.0,
        "Answerer_view_count":44.0,
        "Challenge_adjusted_solved_time":2013.1323233333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>This is the first time I am using amazon web services to deploy my machine learning pre-trained model. I want to deploy my pre-trained TensorFlow model to Aws-Sagemaker. I am somehow able to deploy the endpoints successfully But whenever I call the <code>predictor.predict(some_data)<\/code> method to make prediction to invoking the endpoints it's throwing an error.<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-2020-04-07-04-25-27-055 in account 453101909370 for more information.\n<\/code><\/pre>\n\n<p>After going through the cloud watch logs I found this error.<\/p>\n\n<pre><code>#011details = \"NodeDef mentions attr 'explicit_paddings' not in Op&lt;name=Conv2D; signature=input:T, filter:T -&gt; output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=dilations:list(int),default=[1, 1, 1, 1]&gt;; NodeDef: {{node conv1_conv\/convolution}} = Conv2D[T=DT_FLOAT, _output_shapes=[[?,112,112,64]], data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](conv1_pad\/Pad, conv1_conv\/kernel\/read). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n<\/code><\/pre>\n\n<p>I don't know where I am wrong and I have wasted 2 days already to solve this error and couldn't find out the information regarding this. The detailed logs I have shared <a href=\"https:\/\/docs.google.com\/document\/d\/1NXsLRd6cfbNE55xSVq5d63ETt-cBmwZsOaic8IyF1Qw\/edit?usp=sharing\" rel=\"nofollow noreferrer\">here<\/a>. <\/p>\n\n<p>Tensorflow version of my notebook instance is 1.15<\/p>",
        "Challenge_closed_time":1593503184160,
        "Challenge_comment_count":0,
        "Challenge_created_time":1586244384423,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a pre-trained TensorFlow model on AWS Sagemaker. The endpoints are deployed successfully, but when the user tries to make predictions by calling the predictor.predict() method, it throws a ModelError with a server error message. The user has checked the cloud watch logs and found an error related to the Conv2D operation. The user is unsure about the cause of the error and has been unable to find a solution after spending two days on it. The TensorFlow version used by the user's notebook instance is 1.15.",
        "Challenge_last_edit_time":1586255907796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61074798",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":15.9,
        "Challenge_reading_time":29.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2016.3332602778,
        "Challenge_title":"Deploy pre-trained tensorflow model on the aws sagemaker - ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":740.0,
        "Challenge_word_count":214,
        "Platform":"Stack Overflow",
        "Poster_created_time":1480786532470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Nagpur, Sitabuldi, Nagpur, Maharashtra, India",
        "Poster_reputation_count":111.0,
        "Poster_view_count":44.0,
        "Solution_body":"<p>After a lot of searching and try &amp; error, I was able to solve this problem. In many cases, the problem arises because of the TensorFlow and Python versions.<\/p>\n<p><strong>Cause of the problem:<\/strong>\nTo deploy the endpoints, I was using the <code>TensorflowModel<\/code> on TF 1.12 and python 3 and which exactly caused the problem.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = TensorFlowModel(model_data = model_data,\n                                  role = role,\n                                  framework_version = '1.12',\n                                  entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Apparently, <code>TensorFlowModel<\/code> only allows python 2 on TF version 1.11, 1.12. 2.1.0.<\/p>\n<p><strong>How I fixed it:<\/strong> There are two TensorFlow solutions that handle serving in the Python SDK. They have different class representations and documentation as shown here.<\/p>\n<ol>\n<li><strong>TensorFlowModel<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/model.py#L47<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/tree\/v1.12.0\/src\/sagemaker\/tensorflow#deploying-directly-from-model-artifacts<\/a><\/li>\n<li>Key difference: Uses a proxy GRPC client to send requests<\/li>\n<li>Container impl:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/blob\/master\/src\/tf_container\/serve.py<\/a><\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>Model<\/strong> - <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L96<\/a><\/li>\n<\/ol>\n<ul>\n<li>Doc: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst<\/a><\/li>\n<li>Key difference: Utilizes the TensorFlow serving rest API<\/li>\n<li>Container impl: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/master\/container\/sagemaker\/serve.py<\/a><\/li>\n<\/ul>\n<p>Python 3 isn't supported using the <code>TensorFlowModel<\/code> object, as the container uses the TensorFlow serving API library in conjunction with the GRPC client to handle making inferences, however, the TensorFlow serving API isn't supported in Python 3 officially, so there are only Python 2 versions of the containers when using the <code>TensorFlowModel<\/code> object.\nIf you need Python 3 then you will need to use the <code>Model<\/code> object defined in #2 above.<\/p>\n<p>Finally, I used the <code>Model<\/code> with the TensorFlow version 1.15.1.<\/p>\n<blockquote>\n<pre><code>sagemaker_model = Model(model_data = model_data,\n                        role = role,\n                        framework_version='1.15.2',\n                        entry_point = 'train.py')\n<\/code><\/pre>\n<\/blockquote>\n<p>Also, here are the successful results.\n<a href=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OMsEf.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":14.0,
        "Solution_readability":21.1,
        "Solution_reading_time":48.42,
        "Solution_score_count":4.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":271.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1583123749267,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Washington D.C., DC, USA",
        "Answerer_reputation_count":317.0,
        "Answerer_view_count":35.0,
        "Challenge_adjusted_solved_time":45.4293536111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When testing on a local machine in Python I would normally use the following to read a training set with sub-directories of all the classes and files\/class:<\/p>\n\n<pre><code>train_path = r\"C:\\temp\\coins\\PCGS - Gold\\train\"\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p><strong>Found 4100 images belonging to 22 classes.<\/strong><\/p>\n\n<p>but on AWS SageMaker's Jupyter notebook I am now pulling the files from an S3 bucket.  I tried the following: <\/p>\n\n<pre><code>bucket = \"coinpath\"\n\ntrain_path = 's3:\/\/{}\/{}\/train'.format(bucket, \"v1\")   #note that the directory structure is coinpath\/v1\/train where coinpath is the bucket\n\ntrain_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(100,100), classes=\n['0','1',2','3' etc...], batch_size=32)\n<\/code><\/pre>\n\n<p>but I get: ** Found 0 images belonging to 22 classes.**<\/p>\n\n<p>Looking for some guidance on the right way to pull training data from S3.<\/p>",
        "Challenge_closed_time":1589768612456,
        "Challenge_comment_count":1,
        "Challenge_created_time":1589605066783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing issues while transitioning a TensorFlow model from a local machine to AWS SageMaker. They are unable to read the S3 bucket and are receiving an error message stating that 0 images have been found belonging to 22 classes. The user is seeking guidance on the correct way to pull training data from S3.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61832086",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.9,
        "Challenge_reading_time":14.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":45.4293536111,
        "Challenge_title":"Having issues reading S3 bucket when transitioning a tensorflow model from local machine to AWS SageMaker",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":608.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1583123749267,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Washington D.C., DC, USA",
        "Poster_reputation_count":317.0,
        "Poster_view_count":35.0,
        "Solution_body":"<p>From <a href=\"https:\/\/stackoverflow.com\/questions\/54736505\/ideal-way-to-read-data-in-bucket-stored-batches-of-data-for-keras-ml-training-in\">Ideal way to read data in bucket stored batches of data for Keras ML training in Google Cloud Platform?<\/a> \"ImageDataGenerator.flow_from_directory() currently does not allow you to stream data directly from a GCS bucket. \"<\/p>\n\n<p>I had to download the image from S3 first.  This is best for latency reasons as well. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.0,
        "Solution_reading_time":6.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1458100127643,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1083.0,
        "Answerer_view_count":86.0,
        "Challenge_adjusted_solved_time":8.0327202778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Using Amazon Sagemaker, I created an Xgboost model. After unpacking the resulting tar.gz file, I end up with a file \"xgboost-model\". <\/p>\n\n<p>The next step will be to upload the model directly from my S3 bucket, without downloading it using <em>pickle<\/em>. Here is what I tried:<\/p>\n\n<pre><code>obj = client.get_object(Bucket='...',Key='xgboost-model')\n\nxgb_model = pkl.load(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>But it throws me the error:<\/p>\n\n<pre><code>TypeError: embedded NUL character\n<\/code><\/pre>\n\n<p>Also tried this:<\/p>\n\n<pre><code>xgb_model = pkl.loads(open((obj['Body'].read())),\"rb\")\n<\/code><\/pre>\n\n<p>the outcome was the same.<\/p>\n\n<p>Another approach:<\/p>\n\n<pre><code>bucket='...'\nkey='xgboost-model'\n\nwith s3io.open('s3:\/\/{0}\/{1}'.format(bucket, key),mode='w') as s3_file:\n  pkl.dump(mdl, s3_file)\n<\/code><\/pre>\n\n<p>This giving the error:<\/p>\n\n<pre><code>CertificateError: hostname bucket doesn't match either of '*.s3.amazonaws.com', 's3.amazonaws.com'\n<\/code><\/pre>\n\n<p>This although the bucket is the same.<\/p>\n\n<p>How Can I upload the model in a pickle object so I can then use it it for predictions?<\/p>",
        "Challenge_closed_time":1531429065220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531399214863,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user created an Xgboost model using Amazon Sagemaker and wants to upload it directly from their S3 bucket without using pickle. However, they encountered errors when trying to load the model using pkl.load and pkl.loads, and also encountered a CertificateError when trying to dump the model using pkl.dump. The user is seeking a solution to upload the model in a pickle object for predictions.",
        "Challenge_last_edit_time":1531400147427,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51305956",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":15.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":8.2917658333,
        "Challenge_title":"S3 read Sagemaker trained model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2742.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Poster_created_time":1432680790120,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Solution_body":"<p>My assumption is you have trained the model using Sagemaker XGBoost built-in algorithm. You would like to use that model and do the predictions in your own hosting environment (not Sagemaker hosting).<\/p>\n\n<p><code>pickle.load(file)<\/code> reads a pickled object from the open file object file and <code>pickle.loads(bytes_object)<\/code> reads a pickled object from a bytes object and returns the deserialized object. Since you have the S3 object already downloaded (into memory) as bytes, you can use <code>pickle.loads<\/code> without using <code>open<\/code><\/p>\n\n<pre><code>xgb_model = pkl.loads(obj['Body'].read())\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.8,
        "Solution_reading_time":8.11,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1561143508792,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Challenge_adjusted_solved_time":6.0792997222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Having recently started using ClearML to manage the MLOps, I am facing the following problem:\nWhen running a script that trains a CatBoost in a binary classification problem using different class weights from my computer, it works perfectly, logs the results and no issues at all.\nOnce I try to run that remotely using the ClearML agent, it results in the following error:<\/p>\n<pre><code>&lt;!-- language: lang-none --&gt;\nTraceback (most recent call last):\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 102, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):**\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 313, in &lt;module&gt;\n    rfs.run(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/task_repository\/RecSys.git\/src\/cli\/model_training_remote.py&quot;, line 232, in run\n    model.fit(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 36, in _inner_patch\n    raise ex\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/__init__.py&quot;, line 34, in _inner_patch\n    ret = patched_fn(original_fn, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/clearml\/binding\/frameworks\/catboost_bind.py&quot;, line 110, in _fit\n    return original_fn(obj, *args, **kwargs)\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 5007, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2262, in _fit\n    train_params = self._prepare_train_params(\n  File &quot;\/root\/.clearml\/venvs-builds\/3.9\/lib\/python3.9\/site-packages\/catboost\/core.py&quot;, line 2194, in _prepare_train_params\n    _check_train_params(params)\n  File &quot;_catboost.pyx&quot;, line 6032, in _catboost._check_train_params\n  File &quot;_catboost.pyx&quot;, line 6051, in _catboost._check_train_params\n**_catboost.CatBoostError: catboost\/private\/libs\/options\/catboost_options.cpp:607: if loss-function is Logloss, then class weights should be given for 0 and 1 classes**\n\n<\/code><\/pre>\n<p>I do have the dictionary being connected:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>registered in the ClearML task as<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>and used as parameters for the model in the following call:<\/p>\n<pre><code>model = CatBoostClassifier(**model_params)\n<\/code><\/pre>\n<p>When running it from the container in ClearML interactive mode, it also works fine.<\/p>",
        "Challenge_closed_time":1659991938292,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659970052813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while running a script that trains a CatBoost in a binary classification problem using different class weights from their computer remotely using the ClearML agent. The error message indicates that if the loss-function is Logloss, then class weights should be given for 0 and 1 classes. The user has already connected the dictionary and registered it in the ClearML task, but the error persists. The script works fine when run from the container in ClearML interactive mode.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73279794",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":54.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":6.0792997222,
        "Challenge_title":"[Catboost][ClearML] Error: if loss-function is Logloss, then class weights should be given for 0 and 1 classes",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":339,
        "Platform":"Stack Overflow",
        "Poster_created_time":1659969003172,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<p>I think I understand the problem, basically I think the issue is:<\/p>\n<pre><code>task.connect(model_params, 'model_params')\n<\/code><\/pre>\n<p>Since this is a nested dict:<\/p>\n<pre><code>    model_params = {\n        &quot;loss_function&quot;: &quot;Logloss&quot;,\n        &quot;eval_metric&quot;: &quot;AUC&quot;,\n        &quot;class_weights&quot;: {0: 1, 1: 60},\n        &quot;learning_rate&quot;: 0.1\n    }\n<\/code><\/pre>\n<p>The class_weights is stored as a <code>String<\/code> key, but <code>catboost<\/code> expects <code>int<\/code> key, hence failing.\nOne option would be to remove the <code>task.connect(model_params, 'model_params')<\/code><\/p>\n<p>Another solution (until we fix it) would be to do:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>task.connect(model_params, 'model_params')\nmodel_params[&quot;class_weights&quot;] = {\n0: model_params[&quot;class_weights&quot;].get(&quot;0&quot;, model_params[&quot;class_weights&quot;].get(0))\n1: model_params[&quot;class_weights&quot;].get(&quot;1&quot;, model_params[&quot;class_weights&quot;].get(1))\n}\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":15.3,
        "Solution_reading_time":14.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":88.0,
        "Tool":"ClearML"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":992.0369444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nCreate a notebook instance in one of the configured region.\r\nRan the below query and got that error\r\n\r\n```\r\nselect * from aws_sagemaker_notebook_instance;\r\nError: hydrate call listAwsSageMakerNotebookInstanceTags failed with panic interface conversion: interface {} is *sagemaker.NotebookInstanceSummary, not *sagemaker.DescribeNotebookInstanceOutput\r\n\r\n```\r\n\r\n\r\n\r\n**Steampipe version (`steampipe -v`)**\r\n: v0.4.1\r\n\r\n**Plugin version (`steampipe plugin list`)**\r\naws: v0.15.0\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1623682749000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1620111416000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"the user encountered a challenge where zenml was failing to create a s3 bucket due to an incorrect regex in its name.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/turbot\/steampipe-plugin-aws\/issues\/364",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.39,
        "Challenge_repo_contributor_count":49.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":1491.0,
        "Challenge_repo_star_count":115.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":992.0369444444,
        "Challenge_title":"Getting an error from `aws_sagemaker_notebook_instance` table. Please see the detail below.",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":60,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":199.5710802778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi.  <\/p>\n<p>I am working on an ML model in Designer.  <\/p>\n<p>I have a dataset of c. 55,000 rows.   <\/p>\n<p>When I add an &quot;ID&quot; column (unique per row - so 55,000 IDs) to my dataset for training \/ scoring, I receive the error message:  <\/p>\n<blockquote>\n<p>ModuleExceptionMessage:ColumnUniqueValuesExceeded: Number of unique values in column: &quot;ID&quot; is greater than allowed.  <\/p>\n<\/blockquote>\n<p><strong>Question:<\/strong> is this error based on a physical cap on number of rows - or capacity based on e.g. Compute power associated with the instance?  <\/p>\n<p>I can run 20k rows through the model <em>without<\/em> the ID column - so it seems the unique rows is the challenge.  <\/p>\n<p>But then - how do I keep an identifying column in the scored dataset, if there is a cap on unique values?   <\/p>\n<p>Because I need the ID column to join with other data that is not able to be used in modelling as features etc.   <\/p>\n<p>Any guidance welcome! <\/p>",
        "Challenge_closed_time":1605654456912,
        "Challenge_comment_count":6,
        "Challenge_created_time":1604936001023,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an error message in Azure ML Designer when adding an \"ID\" column to a dataset for training\/scoring. The error message states that the number of unique values in the column exceeds the allowed limit. The user is unsure if this error is due to a physical cap on the number of rows or capacity based on compute power. The user needs the ID column to join with other data that cannot be used in modeling as features.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/156534\/azure-ml-id-column-for-joining-data-returns-no-of",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":6.5,
        "Challenge_reading_time":12.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":199.5710802778,
        "Challenge_title":"Azure ML: ID column for joining data returns \"No. Of unique values ... is greater than allowed\"",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":172,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>User can use Edit Metadata module to mark the ID column as &quot;ClearFeature&quot;, and thus this will not be used in Train Model. This should prevent the error. Please have a try and let me know if there is any questions. <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/edit-metadata\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/edit-metadata<\/a>    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/40390-microsoftteams-image-7.png?platform=QnA\" alt=\"40390-microsoftteams-image-7.png\" \/>    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":16.8,
        "Solution_reading_time":8.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":52.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1542628542703,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":11.0,
        "Answerer_view_count":0.0,
        "Challenge_adjusted_solved_time":43.2314219444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a Tensorflow-Model in SageMaker Studio following this tutorial:\n<a href=\"https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a>\nThe Model needs a Multidimensional Array as input. Invoking it from the Notebook itself is working:<\/p>\n<pre><code>import numpy as np\nimport json\ndata = np.load(&quot;testValues.npy&quot;)\npred=predictor.predict(data)\n<\/code><\/pre>\n<p>But I wasnt able to invoke it from a boto 3 client using this code:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n \nclient = boto3.client('runtime.sagemaker')\ndatain = np.load(&quot;testValues.npy&quot;)\ndata=datain.tolist();\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>This throws the Error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: Unknown&quot;}&quot;.\n<\/code><\/pre>\n<p>I guess the reason is the json Media Type but i have no clue how to get it back in shape.\nI tried this:<a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644<\/a> but it doesnt seem to change anything<\/p>",
        "Challenge_closed_time":1605773068536,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605617435417,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has deployed a Tensorflow-Model in SageMaker Studio that requires a multidimensional array as input. While invoking it from the Notebook works, the user encountered an error when trying to invoke it from a boto3 client. The error message suggests an unsupported media type, and the user suspects it is due to the JSON media type but is unsure how to fix it. The user tried a solution from a GitHub issue but it did not work.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64875623",
        "Challenge_link_count":4,
        "Challenge_participation_count":1,
        "Challenge_readability":16.8,
        "Challenge_reading_time":21.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":43.2314219444,
        "Challenge_title":"Invoking an endpoint in AWS with a multidimensional array",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":550.0,
        "Challenge_word_count":147,
        "Platform":"Stack Overflow",
        "Poster_created_time":1542628542703,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Germany",
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Solution_body":"<p>This fixed it for me:\nThe Content Type was missing.<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n\nclient = boto3.client('runtime.sagemaker',aws_access_key_id=..., aws_secret_access_key=...,region_name=...)\nendpoint_name = '...'\n\ndata = np.load(&quot;testValues.npy&quot;)\n\n\npayload = json.dumps(data.tolist())\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                                  ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nres = result['predictions']\nprint(&quot;test&quot;)\n\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":18.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":38.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1574151667340,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":456.0,
        "Answerer_view_count":31.0,
        "Challenge_adjusted_solved_time":13.4095258333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I got this error when I was trying to have a model registered in the model registry. Could someone help me?<\/p>\n<pre><code>RestException: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store. \nSupported schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. \nSee https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to setup a compatible server.\n<\/code><\/pre>",
        "Challenge_closed_time":1596626369480,
        "Challenge_comment_count":0,
        "Challenge_created_time":1596578095187,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user encountered an error while trying to register a model in the MLflow model registry. The error message indicates that the URI '.\/mlruns' is not supported for the model registry store and suggests using compatible servers such as PostgreSQL, MySQL, SQLite, or MSSQL. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":1598815085667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63255631",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":10.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":13.4095258333,
        "Challenge_title":"MLflow: INVALID_PARAMETER_VALUE: Unsupported URI '.\/mlruns' for model registry store",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":12594.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1419619351727,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Mlflow required DB as datastore for Model Registry\nSo you have to run tracking server with DB as backend-store and log model to this tracking server.\nThe easiest way to use DB is to use SQLite.<\/p>\n<pre><code>mlflow server \\\n    --backend-store-uri sqlite:\/\/\/mlflow.db \\\n    --default-artifact-root .\/artifacts \\\n    --host 0.0.0.0\n<\/code><\/pre>\n<p>And set MLFLOW_TRACKING_URI environment variable to <em>http:\/\/localhost:5000<\/em> or<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\n<\/code><\/pre>\n<p>After got to http:\/\/localhost:5000 and you can register a logged model from UI or from the code.<\/p>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":8.0,
        "Solution_reading_time":7.98,
        "Solution_score_count":27.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":72.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1309439921808,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3050.0,
        "Answerer_view_count":106.0,
        "Challenge_adjusted_solved_time":3196.0599155556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have wrapped my keras-tf-model into a Sklearn Pipeline, which also does some pre- and postprocessing. I want to serialize this model and capture its dependencies via MLflow.<\/p>\n\n<p>I have tried <code>mlflow.keras.save_model()<\/code>, which seems not appropriate. (it's not a \"pure\" keras model and as no <code>save()<\/code> attribute)<\/p>\n\n<p>I also tried <code>mlflow.sklearn.save_model()<\/code> and <code>mlflow.pyfunc.save_model()<\/code>, which both lead my to the same error: <\/p>\n\n<p><code>NotImplementedError: numpy() is only available when eager execution is enabled.<\/code><\/p>\n\n<p>(This error seems to stem from a clash between python and tensorflow, maybe?)<\/p>\n\n<p>I wonder, should it already\/ generally be possible to serialize these kind of \"hybrid\" models with mlflow?<\/p>\n\n<h3>Please finde a minimal example below<\/h3>\n\n<pre><code># In[1]:\n\n\nfrom mlflow.sklearn import save_model\nimport mlflow.sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\n\nfrom tensorflow.keras.models import Sequential\n\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\n\n# ### Save Keras Model\n\n# In[2]:\n\n\niris_data = load_iris() \n\nx = iris_data.data\ny_ = iris_data.target.reshape(-1, 1)\n\n# One Hot encode the class labels\nencoder = OneHotEncoder(sparse=False)\ny = encoder.fit_transform(y_)\n\n# Split the data for training and testing\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)\n\n# Build the model\nmodel = Sequential()\n\nmodel.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\nmodel.add(Dense(10, activation='relu', name='fc2'))\nmodel.add(Dense(3, activation='softmax', name='output'))\n\noptimizer = Adam(lr=0.001)\nmodel.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_x, train_y, verbose=2, batch_size=5, epochs=20)\n\n\n# In[3]:\n\n\nimport mlflow.keras\n\nmlflow.keras.save_model(model, \"modelstorage\/model40\")\n\n\n# ### Save Minimal SKlearn-Pipeline (with Keras)\n\n# In[4]:\n\n\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n\n# In[5]:\n\n\ndef define_model():\n    \"\"\"\n    Create fully connected network with given parameters.\n    \"\"\"\n    keras_model = Sequential()\n\n    keras_model.add(Dense(10, input_shape=(4,), activation='relu', name='fc1'))\n    keras_model.add(Dense(10, activation='relu', name='fc2'))\n    keras_model.add(Dense(3, activation='softmax', name='output'))\n\n    optimizer = Adam(lr=0.001)\n    keras_model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n# In[6]:\n\n\n# target_encoder = TargetEncoder() \nscaler = StandardScaler()\nkeras_model = KerasClassifier(define_model, batch_size=5, epochs=20)\n\n\n# In[7]:\n\n\npipeline = Pipeline([\n#     ('encoding', target_encoder),\n    ('scaling', scaler),\n    ('modeling', keras_model)\n])\n\n\n# In[8]:\n\n\npipeline.fit(train_x, train_y)\n\n\n# In[9]:\n\n\nmlflow.keras.save_model(pipeline, \"modelstorage\/model42\")   #not working\n\n\n# In[10]:\n\n\nimport mlflow.sklearn\n\nmlflow.sklearn.save_model(pipeline, \"modelstorage\/model43\")\n\nOutput from modelstorage\/model43\/conda.yaml:\n\n======================\nchannels:\n- defaults\ndependencies:\n- python=3.6.7\n- scikit-learn=0.21.2\n- pip:\n  - mlflow\n  - cloudpickle==1.2.1\nname: mlflow-env\n======================\n\nDoesn't seem to capture Tensorflow.\n<\/code><\/pre>",
        "Challenge_closed_time":1572628704763,
        "Challenge_comment_count":0,
        "Challenge_created_time":1561111222150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has wrapped a Keras-tf-model into a Sklearn Pipeline, which also does some pre- and postprocessing. The user wants to serialize this model and capture its dependencies via MLflow. The user has tried mlflow.keras.save_model(), mlflow.sklearn.save_model(), and mlflow.pyfunc.save_model(), but all lead to the same error: \"NotImplementedError: numpy() is only available when eager execution is enabled.\" The user wonders if it is possible to serialize these kinds of \"hybrid\" models with MLflow.",
        "Challenge_last_edit_time":1561122889067,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/56701139",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.1,
        "Challenge_reading_time":47.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":3199.3007258334,
        "Challenge_title":"Model-logging for \"hybrid models\" (e.g. SKlearn Pipeline including KerasWrapper) possible?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1470.0,
        "Challenge_word_count":334,
        "Platform":"Stack Overflow",
        "Poster_created_time":1336936830510,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":948.0,
        "Poster_view_count":132.0,
        "Solution_body":"<p>You can add extra dependencies when you save your model, for example if you have a keras step in your pipeline you can add keras &amp; tensorflow:<\/p>\n\n<pre><code>  conda_env = mlflow.sklearn.get_default_conda_env()\n  conda_env[\"dependencies\"] = ['keras==2.2.4', 'tensorflow==1.14.0'] + conda_env[\"dependencies\"]\n  mlflow.sklearn.log_model(pipeline, \"modelstorage\/model43\", conda_env = conda_env)\n<\/code><\/pre>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":5.36,
        "Solution_score_count":3.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":39.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1533910280492,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":49.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":468.8841677778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am beginning to train a semantic segmentation model in AWS Sagermaker and they provide the following metrics for the output. I understand mIOU, loss, and pixel accuracy, but I do not know what throughput is or how to interpret it. Please see the image below and let me know if you need additional information.\n<a href=\"https:\/\/i.stack.imgur.com\/dpsDp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dpsDp.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1596819625616,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595129540180,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is training a semantic segmentation model in AWS Sagemaker and is unsure about how to interpret the \"throughput\" metric provided in the output. They are familiar with other metrics such as mIOU, loss, and pixel accuracy.",
        "Challenge_last_edit_time":1595131642612,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62975940",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.4,
        "Challenge_reading_time":6.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":469.4681766667,
        "Challenge_title":"How to interpret the Throughput metric for Semantic Segmentation?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":67.0,
        "Challenge_word_count":73,
        "Platform":"Stack Overflow",
        "Poster_created_time":1449513251820,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":693.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Throughput is reported in records per second (i.e. images per second). It shows how fast the algorithm can iterate over training or validation data. For example, with a throughput of 30 records\/sec it would take a minute to iterate over 1800 images.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":3.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":56.2932952778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an Azure Data factory that receives data from a service bus and then I want to classify my data with an ML model.  <\/p>\n<p>Is there any solution to save and load the ML model on the Azure Data Factory pipeline?  <\/p>\n<p>For your information, I want to use cloud base solution. I don't use the PICKLE library. <\/p>",
        "Challenge_closed_time":1623860467256,
        "Challenge_comment_count":0,
        "Challenge_created_time":1623657811393,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking a cloud-based solution to save and load an ML model on an Azure Data Factory pipeline without using the PICKLE library. The ML model is intended to classify data received from a service bus.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/434449\/how-to-save-and-load-ml-model-with-azure-data-fact",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":56.2932952778,
        "Challenge_title":"How to save and load ML model with Azure Data Factory",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":71,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello <a href=\"\/users\/na\/?userid=b5c4f434-7ffe-0003-0000-000000000000\">@Mohsen Akhavan  <\/a>,    <br \/>\nThanks for the ask and using the Microsoft Q&amp;A platform  .    <\/p>\n<p>I think you can use the machine learning activity . Read and watch the video <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/transform-data-machine-learning-service\">here<\/a> .    <\/p>\n<p>The challenge in your case is the data is in EH and at this time ADF cannot read EH data . I suggest you to use a Azure stream analytics jobs and read the data from EH and write it to SQL or blob . Once the data is in any of these two sources ADF can be used to read the data .    <\/p>\n<p>Please do let me know how it goes .    <br \/>\nThanks     <br \/>\nHimanshu    <br \/>\nPlease do consider clicking on <strong>&quot;Accept Answer&quot;<\/strong> and <strong>&quot;Up-vote&quot;<\/strong> on the post that helps you, as it can be beneficial to other community members    <\/p>\n",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.9,
        "Solution_reading_time":11.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":134.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1244808478036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Challenge_adjusted_solved_time":18.9520191667,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I wonder if it's possible to run training Amazon SageMaker object detection model on a local PC?<\/p>",
        "Challenge_closed_time":1654073043172,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654004815903,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the possibility of training an Amazon SageMaker object detection model on a local PC.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72448994",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":18.9520191667,
        "Challenge_title":"Train Amazon SageMaker object detection model on local PC",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":70.0,
        "Challenge_word_count":25,
        "Platform":"Stack Overflow",
        "Poster_created_time":1442786553536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Kyiv",
        "Poster_reputation_count":71.0,
        "Poster_view_count":30.0,
        "Solution_body":"<p>You're probably referring to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">this<\/a> object detection algorithm which is part of of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Amazon SageMaker built-in algorithms<\/a>. <strong>Built-in algorithms must be trained on the cloud<\/strong>.<br \/>\nIf you're bringing your own Tensorflow or PyTorch model, you could use SageMaker training jobs to train either on the cloud or locally as @kirit noted.<\/p>\n<p>I would also look at <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-jumpstart.html\" rel=\"nofollow noreferrer\">SageMaker JumpStart<\/a> for a wide variety of object detection algorithm which are TF\/PT based.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":16.0,
        "Solution_reading_time":10.43,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416648155470,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":14749.0,
        "Answerer_view_count":968.0,
        "Challenge_adjusted_solved_time":7321.6952480556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The documentations of how to use SageMaker estimators are scattered around, sometimes obsolete, incorrect. Is there a one stop location which gives the comprehensive views of how to use SageMaker SDK Estimator to train and save models?<\/p>",
        "Challenge_closed_time":1630555307168,
        "Challenge_comment_count":0,
        "Challenge_created_time":1630555307170,
        "Challenge_favorite_count":14.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in finding comprehensive and accurate documentation on how to use SageMaker SDK Estimator for model training and saving.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69024005",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":27.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use SageMaker Estimator for model training and saving",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":6655.0,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416648155470,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Solution_body":"<h1>Answer<\/h1>\n<p>There is no one such resource from AWS that provides the comprehensive view of how to use SageMaker SDK Estimator to train and save models.<\/p>\n<h2>Alternative Overview Diagram<\/h2>\n<p>I put a diagram and brief explanation to get the overview on how SageMaker Estimator runs a training.<\/p>\n<ol>\n<li><p>SageMaker sets up a docker container for a training job where:<\/p>\n<ul>\n<li>Environment variables are set as in <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container. Environment Variables<\/a>.<\/li>\n<li>Training data is setup under <code>\/opt\/ml\/input\/data<\/code>.<\/li>\n<li>Training script codes are setup under <code>\/opt\/ml\/code<\/code>.<\/li>\n<li><code>\/opt\/ml\/model<\/code> and <code>\/opt\/ml\/output<\/code> directories are setup to store training outputs.<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json  &lt;--- From Estimator hyperparameter arg\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;        &lt;--- From Estimator fit method inputs arg\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 code\n\u2502   \u2514\u2500\u2500 &lt;code files&gt;              &lt;--- From Estimator src_dir arg\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;             &lt;--- Location to save the trained model artifacts\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure                   &lt;--- Training job failure logs\n<\/code><\/pre>\n<ol start=\"2\">\n<li><p>SageMaker Estimator <code>fit(inputs)<\/code> method executes the training script. Estimator <code>hyperparameters<\/code> and <code>fit<\/code> method <code>inputs<\/code> are provided as its command line arguments.<\/p>\n<\/li>\n<li><p>The training script saves the model artifacts in the <code>\/opt\/ml\/model<\/code> once the training is completed.<\/p>\n<\/li>\n<li><p>SageMaker archives the artifacts under <code>\/opt\/ml\/model<\/code> into <code>model.tar.gz<\/code> and save it to the S3 location specified to <code>output_path<\/code> Estimator parameter.<\/p>\n<\/li>\n<li><p>You can set Estimator <code>metric_definitions<\/code> parameter to extract model metrics from the training logs. Then you can monitor the training progress in the SageMaker console metrics.<\/p>\n<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gi8bU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I believe AWS needs to stop mass-producing verbose, redundant, wordy, scattered, and obsolete documents. AWS needs to understand <strong>A picture is worth thousand words<\/strong>.<\/p>\n<p>Have diagrams and piece document parts together in a <strong>context<\/strong> with a clear objective to achieve.<\/p>\n<hr \/>\n<h1>Problem<\/h1>\n<p>AWS documentations need serious re-design and re-structuring. Just to understand <strong>how to train and save a model<\/strong> forces us going through dozens of scattered,  fragmented, verbose, redundant documentations, which are often obsolete, incomplete, and sometime incorrect.<\/p>\n<p>It is well-summarized in <a href=\"https:\/\/nandovillalba.medium.com\/why-i-think-gcp-is-better-than-aws-ea78f9975bda\" rel=\"noreferrer\">Why I think GCP is better than AWS<\/a>:<\/p>\n<blockquote>\n<p>It\u2019s not that AWS is harder to use than GCP, it\u2019s that <strong>it is needlessly hard<\/strong>; a disjointed, sprawl of infrastructure primitives with poor cohesion between them.  <br><br>\nA challenge is nice, a confusing mess is not, and <strong>the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want<\/strong>, rather than focusing on cool interesting challenges.<\/p>\n<\/blockquote>\n<p>Especially the SageMaker team keeps changing implementations without updating documents. Its roll-out was also inconsistent, e.g. SDK version 2 was rolled out in the SageMaker Studio making the AWS examples in Github incompatible without announcing it. Whereas SageMaker instance still had SDK 1, hence code worked in Instance but not in Studio.<\/p>\n<p>It is mind-boggling that we have to go through these many documents below to understand how to use the SageMaker SDK Estimator for training.<\/p>\n<h2>Documents for Model Training<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"noreferrer\">Train a Model with Amazon SageMaker<\/a><\/li>\n<\/ul>\n<p>This document gives 20,000 feet overview of how SageMaker training but does not give any clue what to do.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<p>This document gives an overview of how SageMaker training looks like. However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<blockquote>\n<p>WARNING: This package has been deprecated. Please use the SageMaker Training Toolkit for model training and the SageMaker Inference Toolkit for model serving.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"noreferrer\">Step 4: Train a Model<\/a><\/li>\n<\/ul>\n<p>This document layouts the steps for training.<\/p>\n<blockquote>\n<p>The Amazon SageMaker Python SDK provides framework estimators and generic estimators to train your model while orchestrating the machine learning (ML) lifecycle accessing the SageMaker features for training and the AWS infrastructures<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#train-a-model-with-the-sagemaker-python-sdk\" rel=\"noreferrer\">Train a Model with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To train a model by using the SageMaker Python SDK, you:<\/p>\n<ul>\n<li>Prepare a training script<\/li>\n<li>Create an estimator<\/li>\n<li>Call the fit method of the estimator<\/li>\n<\/ul>\n<\/blockquote>\n<p>Finally this document gives concrete steps and ideas. However still missing comprehensiv details about Environment Variables, Directory structure in the SageMaker docker container**, S3 for uploading code, placing data, S3 where the trained model is saved, etc.<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"noreferrer\">Use TensorFlow with the SageMaker Python SDK<\/a><\/li>\n<\/ul>\n<p>This documents is focused on TensorFlow Estimator implementation steps. Use <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/get_started_mnist_train.ipynb\" rel=\"noreferrer\">Training a Tensorflow Model on MNIST<\/a> Github example to accompany with to follow the actual implementation.<\/p>\n<h2>Documents for passing parameters and data locations<\/h2>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-inputdataconfig\" rel=\"noreferrer\">How Amazon SageMaker Provides Training Information<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>This section explains how SageMaker makes training information, such as training data, hyperparameters, and other configuration information, available to your Docker container.<\/p>\n<\/blockquote>\n<p>This document finally gives the idea of how parameters and data are passed around but again, not comprehensive.<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#important-environment-variables\" rel=\"noreferrer\">SageMaker Docker Container Environment Variables<\/a><\/li>\n<\/ul>\n<p>This documentation is marked as <strong>deprecated<\/strong> but the only document which explains the SageMaker Environment Variables.<\/p>\n<blockquote>\n<h3>IMPORTANT ENVIRONMENT VARIABLES<\/h3>\n<ul>\n<li>SM_MODEL_DIR<\/li>\n<li>SM_CHANNELS<\/li>\n<li>SM_CHANNEL_{channel_name}<\/li>\n<li>SM_HPS<\/li>\n<li>SM_HP_{hyperparameter_name}<\/li>\n<li>SM_CURRENT_HOST<\/li>\n<li>SM_HOSTS<\/li>\n<li>SM_NUM_GPUS<\/li>\n<\/ul>\n<h3>List of provided environment variables by SageMaker Containers<\/h3>\n<ul>\n<li>SM_NUM_CPUS<\/li>\n<li>SM_LOG_LEVEL<\/li>\n<li>SM_NETWORK_INTERFACE_NAME<\/li>\n<li>SM_USER_ARGS<\/li>\n<li>SM_INPUT_DIR<\/li>\n<li>SM_INPUT_CONFIG_DIR<\/li>\n<li>SM_OUTPUT_DATA_DIR<\/li>\n<li>SM_RESOURCE_CONFIG<\/li>\n<li>SM_INPUT_DATA_CONFIG<\/li>\n<li>SM_TRAINING_ENV<\/li>\n<\/ul>\n<\/blockquote>\n<h2>Documents for SageMaker Docker Container Directory Structure<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker-workshop.com\/custom\/containers.html\" rel=\"noreferrer\">Running a container for Amazon SageMaker training<\/a><\/li>\n<\/ul>\n<pre><code>\/opt\/ml\n\u251c\u2500\u2500 input\n\u2502   \u251c\u2500\u2500 config\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameters.json\n\u2502   \u2502   \u2514\u2500\u2500 resourceConfig.json\n\u2502   \u2514\u2500\u2500 data\n\u2502       \u2514\u2500\u2500 &lt;channel_name&gt;\n\u2502           \u2514\u2500\u2500 &lt;input data&gt;\n\u251c\u2500\u2500 model\n\u2502   \u2514\u2500\u2500 &lt;model files&gt;\n\u2514\u2500\u2500 output\n    \u2514\u2500\u2500 failure\n<\/code><\/pre>\n<p>This document explains the directory structure and purpose of each directory.<\/p>\n<blockquote>\n<h3>The input<\/h3>\n<ul>\n<li>\/opt\/ml\/input\/config contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. resourceConfig.json is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn\u2019t support distributed training, we\u2019ll ignore it here.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;\/ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it\u2019s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.<\/li>\n<li>\/opt\/ml\/input\/data\/&lt;channel_name&gt;_&lt;epoch_number&gt; (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.<\/li>\n<\/ul>\n<h3>The output<\/h3>\n<ul>\n<li>\/opt\/ml\/model\/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.<\/li>\n<li>\/opt\/ml\/output is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.<\/li>\n<\/ul>\n<\/blockquote>\n<p>However, this is not up-to-date as it is based on <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"noreferrer\">SageMaker Containers<\/a> which is obsolete.<\/p>\n<h2>Documents for Model Saving<\/h2>\n<p>The information on where the trained model is saved and in what format are fundamentally missing. The training script needs to save the model under <code>\/opt\/ml\/model<\/code> and the format and sub-directory structure depend on the frameworks e,g TensorFlow, Pytorch. This is because SageMaker deployment uses the Framework dependent model-serving, e,g. TensorFlow Serving for TensorFlow framework.<\/p>\n<p>This is not clearly documented and causing confusions. The developer needs to specify which format to use and under which sub-directory to save.<\/p>\n<p>To use TensorFlow Estimator training and deployment:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/aws_sagemaker_studio\/frameworks\/keras_pipe_mode_horovod\/keras_pipe_mode_horovod_cifar10.html#Deploy-the-trained-model\" rel=\"noreferrer\">Deploy the trained model<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>Because <strong>we\u2019re using TensorFlow Serving for deployment<\/strong>, our training script <strong>saves the model in TensorFlow\u2019s SavedModel format<\/strong>.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/frameworks\/tensorflow\/code\/train.py#L159-L166\" rel=\"noreferrer\">amazon-sagemaker-examples\/frameworks\/tensorflow\/code\/train.py <\/a><\/li>\n<\/ul>\n<pre><code>    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    ckpt_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    model.save(ckpt_dir)\n<\/code><\/pre>\n<p>The code is saving the model in <code>\/opt\/ml\/model\/00000000<\/code> because this is for TensorFlow serving.<\/p>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"noreferrer\">Using the SavedModel format<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>The save-path follows a convention used by TensorFlow Serving where the last path component (1\/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.<\/p>\n<\/blockquote>\n<ul>\n<li><a href=\"https:\/\/www.tensorflow.org\/tfx\/tutorials\/serving\/rest_simple#save_your_model\" rel=\"noreferrer\">Train and serve a TensorFlow model with TensorFlow Serving<\/a><\/li>\n<\/ul>\n<blockquote>\n<p>To load our trained model into TensorFlow Serving we first need to save it in SavedModel format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. TensorFlow Serving allows us to select which version of a model, or &quot;servable&quot; we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.<\/p>\n<\/blockquote>\n<h2>Documents for API<\/h2>\n<p>Basically the SageMaker SDK Estimator implements the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\" rel=\"noreferrer\">CreateTrainingJob<\/a> API for training part. Hence, better to understand how it is designed and what parameters need to be defined. Otherwise working on Estimators are like walking in the dark.<\/p>\n<hr \/>\n<h1>Example<\/h1>\n<h2>Jupyter Notebook<\/h2>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\nbucket = sagemaker_session.default_bucket()\n\nmetric_definitions = [\n    {&quot;Name&quot;: &quot;train:loss&quot;, &quot;Regex&quot;: &quot;.*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*&quot;},\n    {&quot;Name&quot;: &quot;train:accuracy&quot;, &quot;Regex&quot;: &quot;.*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*&quot;},\n    {\n        &quot;Name&quot;: &quot;validation:accuracy&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;validation:loss&quot;,\n        &quot;Regex&quot;: &quot;.*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*&quot;,\n    },\n    {\n        &quot;Name&quot;: &quot;sec\/sample&quot;,\n        &quot;Regex&quot;: &quot;.* - \\d+s (\\d+)[mu]s\/sample - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+&quot;,\n    },\n]\n\nimport uuid\n\ncheckpoint_s3_prefix = &quot;checkpoints\/{}&quot;.format(str(uuid.uuid4()))\ncheckpoint_s3_uri = &quot;s3:\/\/{}\/{}\/&quot;.format(bucket, checkpoint_s3_prefix)\n\nfrom sagemaker.tensorflow import TensorFlow\n\n# --------------------------------------------------------------------------------\n# 'trainingJobName' msut satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}\n# --------------------------------------------------------------------------------\nbase_job_name = &quot;fashion-mnist&quot;\nhyperparameters = {\n    &quot;epochs&quot;: 2, \n    &quot;batch-size&quot;: 64\n}\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False\n)\nestimator.fit()\n<\/code><\/pre>\n<h2>fashion_mnist.py<\/h2>\n<pre><code>import os\nimport argparse\nimport json\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras import backend as K\n\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\nprint(&quot;Eager execution is: {}&quot;.format(tf.executing_eagerly()))\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\n\n\nimage_width = 28\nimage_height = 28\n\n\ndef load_data():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n    number_of_classes = len(set(y_train))\n    print(&quot;number_of_classes&quot;, number_of_classes)\n\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n    x_full = np.concatenate((x_train, x_test), axis=0)\n    print(x_full.shape)\n\n    print(type(x_train))\n    print(x_train.shape)\n    print(x_train.dtype)\n    print(y_train.shape)\n    print(y_train.dtype)\n\n    # ## Train\n    # * C: Convolution layer\n    # * P: Pooling layer\n    # * B: Batch normalization layer\n    # * F: Fully connected layer\n    # * O: Output fully connected softmax layer\n\n    # Reshape data based on channels first \/ channels last strategy.\n    # This is dependent on whether you use TF, Theano or CNTK as backend.\n    # Source: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n    if K.image_data_format() == 'channels_first':\n        x = x_train.reshape(x_train.shape[0], 1, image_width, image_height)\n        x_test = x_test.reshape(x_test.shape[0], 1, image_width, image_height)\n        input_shape = (1, image_width, image_height)\n    else:\n        x_train = x_train.reshape(x_train.shape[0], image_width, image_height, 1)\n        x_test = x_test.reshape(x_test.shape[0], image_width, image_height, 1)\n        input_shape = (image_width, image_height, 1)\n\n    return x_train, y_train, x_test, y_test, input_shape, number_of_classes\n\n# tensorboard --logdir=\/full_path_to_your_logs\n\nvalidation_split = 0.2\nverbosity = 1\nuse_multiprocessing = True\nworkers = multiprocessing.cpu_count()\n\n\ndef train(model, x, y, args):\n    # SavedModel Output\n    tensorflow_saved_model_path = os.path.join(args.model_dir, &quot;tensorflow\/saved_model\/0&quot;)\n    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n\n    # Tensorboard Logs\n    tensorboard_logs_path = os.path.join(args.model_dir, &quot;tensorboard\/&quot;)\n    os.makedirs(tensorboard_logs_path, exist_ok=True)\n\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tensorboard_logs_path,\n        write_graph=True,\n        write_images=True,\n        histogram_freq=1,  # How often to log histogram visualizations\n        embeddings_freq=1,  # How often to log embedding visualizations\n        update_freq=&quot;epoch&quot;,\n    )  # How often to write logs (default: once per epoch)\n\n    model.compile(\n        optimizer='adam',\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=['accuracy']\n    )\n    history = model.fit(\n        x,\n        y,\n        shuffle=True,\n        batch_size=args.batch_size,\n        epochs=args.epochs,\n        validation_split=validation_split,\n        use_multiprocessing=use_multiprocessing,\n        workers=workers,\n        verbose=verbosity,\n        callbacks=[\n            tensorboard_callback\n        ]\n    )\n    return history\n\n\ndef create_model(input_shape, number_of_classes):\n    model = Sequential([\n        Conv2D(\n            name=&quot;conv01&quot;,\n            filters=32,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=&quot;same&quot;,\n            activation='relu',\n            input_shape=input_shape\n        ),\n        MaxPooling2D(\n            name=&quot;pool01&quot;,\n            pool_size=(2, 2)\n        ),\n        Flatten(),  # 3D shape to 1D.\n        BatchNormalization(\n            name=&quot;batch_before_full01&quot;\n        ),\n        Dense(\n            name=&quot;full01&quot;,\n            units=300,\n            activation=&quot;relu&quot;\n        ),  # Fully connected layer\n        Dense(\n            name=&quot;output_softmax&quot;,\n            units=number_of_classes,\n            activation=&quot;softmax&quot;\n        )\n    ])\n    return model\n\n\ndef save_model(model, args):\n    # Save the model\n    # A version number is needed for the serving container\n    # to load the model\n    version = &quot;00000000&quot;\n    model_save_dir = os.path.join(args.model_dir, version)\n    if not os.path.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    print(f&quot;saving model at {model_save_dir}&quot;)\n    model.save(model_save_dir)\n\n\ndef parse_args():\n    # --------------------------------------------------------------------------------\n    # https:\/\/docs.python.org\/dev\/library\/argparse.html#dest\n    # --------------------------------------------------------------------------------\n    parser = argparse.ArgumentParser()\n\n    # --------------------------------------------------------------------------------\n    # hyperparameters Estimator argument are passed as command-line arguments to the script.\n    # --------------------------------------------------------------------------------\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=64)\n\n    # \/opt\/ml\/model\n    # sagemaker.tensorflow.estimator.TensorFlow override 'model_dir'.\n    # See https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/\\\n    # sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow\n    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n\n    # \/opt\/ml\/output\n    parser.add_argument(&quot;--output_dir&quot;, type=str, default=os.environ[&quot;SM_OUTPUT_DIR&quot;])\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == &quot;__main__&quot;:\n    args = parse_args()\n    print(&quot;---------- key\/value args&quot;)\n    for key, value in vars(args).items():\n        print(f&quot;{key}:{value}&quot;)\n\n    x_train, y_train, x_test, y_test, input_shape, number_of_classes = load_data()\n    model = create_model(input_shape, number_of_classes)\n\n    history = train(model=model, x=x_train, y=y_train, args=args)\n    print(history)\n    \n    save_model(model, args)\n    results = model.evaluate(x_test, y_test, batch_size=100)\n    print(&quot;test loss, test accuracy:&quot;, results)\n<\/code><\/pre>\n<h2>SageMaker Console<\/h2>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ctcLy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Notebook output<\/h2>\n<pre><code>2021-09-03 03:02:04 Starting - Starting the training job...\n2021-09-03 03:02:16 Starting - Launching requested ML instancesProfilerReport-1630638122: InProgress\n......\n2021-09-03 03:03:17 Starting - Preparing the instances for training.........\n2021-09-03 03:04:59 Downloading - Downloading input data\n2021-09-03 03:04:59 Training - Downloading the training image...\n2021-09-03 03:05:23 Training - Training image download completed. Training in progress.2021-09-03 03:05:23.966037: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:23.969704: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n2021-09-03 03:05:24.118054: W tensorflow\/core\/profiler\/internal\/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-09-03 03:05:26,842 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2021-09-03 03:05:26,852 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:27,734 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n\/usr\/local\/bin\/python3.7 -m pip install -r requirements.txt\nWARNING: You are using pip version 21.0.1; however, version 21.2.4 is available.\nYou should consider upgrading via the '\/usr\/local\/bin\/python3.7 -m pip install --upgrade pip' command.\n\n2021-09-03 03:05:29,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,045 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,062 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n2021-09-03 03:05:29,072 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {},\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;batch-size&quot;: 64,\n        &quot;epochs&quot;: 2\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {},\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;fashion_mnist&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 4,\n    &quot;num_gpus&quot;: 0,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;fashion_mnist.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;batch-size&quot;:64,&quot;epochs&quot;:2}\nSM_USER_ENTRY_POINT=fashion_mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=fashion_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=4\nSM_NUM_GPUS=0\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;batch-size&quot;:64,&quot;epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;fashion-mnist-2021-09-03-03-02-02-305&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-us-east-1-316725000538\/fashion-mnist-2021-09-03-03-02-02-305\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;fashion_mnist&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:4,&quot;num_gpus&quot;:0,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;fashion_mnist.py&quot;}\nSM_USER_ARGS=[&quot;--batch-size&quot;,&quot;64&quot;,&quot;--epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_HP_BATCH-SIZE=64\nSM_HP_EPOCHS=2\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/local\/lib\/python37.zip:\/usr\/local\/lib\/python3.7:\/usr\/local\/lib\/python3.7\/lib-dynload:\/usr\/local\/lib\/python3.7\/site-packages\n\nInvoking script with the following command:\n\n\/usr\/local\/bin\/python3.7 fashion_mnist.py --batch-size 64 --epochs 2\n\n\nTensorFlow version: 2.3.1\nEager execution is: True\nKeras version: 2.4.0\n---------- key\/value args\nepochs:2\nbatch_size:64\nmodel_dir:\/opt\/ml\/model\noutput_dir:\/opt\/ml\/output\n<\/code><\/pre>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":1656913410063,
        "Solution_link_count":25.0,
        "Solution_readability":17.3,
        "Solution_reading_time":377.84,
        "Solution_score_count":65.0,
        "Solution_sentence_count":201.0,
        "Solution_word_count":2374.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1554186784008,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":2175.0,
        "Answerer_view_count":434.0,
        "Challenge_adjusted_solved_time":64.5577547223,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>For the best run and fitted model from a previously run experiment, Looking for the python code.<\/p>",
        "Challenge_closed_time":1590393765270,
        "Challenge_comment_count":1,
        "Challenge_created_time":1590161357353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is searching for the Python code for the best run and fitted model from a previous experiment.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61958473",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.2,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":64.5577547223,
        "Challenge_title":"Code for Best Run and Model from previous experiment",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":25,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Below is the code you can reuse\n<a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/diabetes_regression\/evaluate\/evaluate_model.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/diabetes_regression\/evaluate\/evaluate_model.py<\/a><\/p>\n\n<p>Assuming in each previous experiment run, a model was registered with a tag that contains a metric of interest (test_mae for example), below is the code to retrieve the version with lowest mae.<\/p>\n\n<pre><code>from azureml.core.model import Model\n\nmodel_name = \"YOUR_MODEL_NAME\"\nmodel_path = \"LOCAL_PATH\u201d\nmodel_version_list = [(model.version,float(model.tags[\"test_mae\"])) for model in Model.list(workspace = ws,name =model_name)]\nmodel_version_list.sort(key = lambda a: a[0])\nlowest_mae_version =model_version_list[0][0]\nprint(\"best version is {} with mae at {}\".format(lowest_mae_version,model_version_list[0][1]))\nmodel = Model(name = model_name,workspace = ws, version =lowest_mae_version)\nmodel.download(model_path, exist_ok=True)\n<\/code><\/pre>\n\n<p>when the model has not been registered,models in an automl run and  would like to get all the models and compare the results depending on featurization, method used, and metrics, also with other data sets. The models are all inside the workspace with the GUI you can see them and download them by hand. <\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.2,
        "Solution_reading_time":17.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":137.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":31.5461152778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to save images that I configure during training to the output bucket in sagemaker.  I've read that all the information that needs to be saved during training goes into the model.tar.gz file.  I've tried saving plots using the model_dir and the output_data_dir to no avail.  The model itself is saved properly, but the additional information is not being stored with it.  I want to reload this additional information (the saved images) during inference but have heard that storing all the information in the model.tar.gz can cause slow inference.  I would love some help.<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>from sagemaker.pytorch import PyTorch\nestimator = PyTorch(entry_point='XXXXXXXX\/AWS\/mnist.py',\n                    role=role,\n                    py_version='py3',\n                    framework_version='1.8.0',\n                    instance_count=1,\n                    instance_type='ml.c5.xlarge',\n                    output_path='s3:\/\/XXXXX-bucket\/',\n                    )<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code># mnist.py\nimport os\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport argparse\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nfrom torch import nn\nimport matplotlib.pyplot as plt\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X.to(device))\n        loss = loss_fn(pred, y.to(device))\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}\/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X.to(device))\n            test_loss += loss_fn(pred, y.to(device)).item()\n            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n\n    test_loss \/= num_batches\n    correct \/= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n# Initialize the loss function\nif __name__=='__main__':\n    # default to the value in environment variable `SM_MODEL_DIR`. Using args makes the script more portable.\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n\n    args, _ = parser.parse_known_args()\n\n    training_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=True,\n        download=True,\n        transform=ToTensor()\n    )\n\n    test_data = datasets.FashionMNIST(\n        root=\"data\",\n        train=False,\n        download=True,\n        transform=ToTensor()\n    )\n\n    labels_map = {\n        0: \"T-Shirt\",\n        1: \"Trouser\",\n        2: \"Pullover\",\n        3: \"Dress\",\n        4: \"Coat\",\n        5: \"Sandal\",\n        6: \"Shirt\",\n        7: \"Sneaker\",\n        8: \"Bag\",\n        9: \"Ankle Boot\",\n    }\n\n    figure = plt.figure(figsize=(8, 8))\n    cols, rows = 3, 3\n    for i in range(1, cols * rows + 1):\n        sample_idx = torch.randint(len(training_data), size=(1,)).item()\n        img, label = training_data[sample_idx]\n        figure.add_subplot(rows, cols, i)\n        plt.title(labels_map[label])\n        plt.axis(\"off\")\n        plt.imsave(args.output_data_dir+'plot'+str(i)+'.jpg', img.squeeze(), cmap=\"gray\")\n\n    train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n    test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\n    # Display image and label.\n    train_features, train_labels = next(iter(train_dataloader))\n    print(f\"Feature batch shape: {train_features.size()}\")\n    print(f\"Labels batch shape: {train_labels.size()}\")\n    img = train_features[0].squeeze()\n    label = train_labels[0]\n    plt.imsave(args.output_data_dir+'sample.jpg', img, cmap=\"gray\")\n    print(\"Saved img.\")\n    print(f\"Label: {label}\")\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using {device} device\")\n\n    model = NeuralNetwork().to(device)\n    print(model)\n\n    learning_rate = 1e-3\n    batch_size = 64\n    epochs = 5\n    # ... train `model`, then save it to `model_dir`\n    \n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    epochs = 1\n    for t in range(epochs):\n        print(f\"Epoch {t+1}\\n-------------------------------\")\n        train_loop(train_dataloader, model, loss_fn, optimizer)\n        test_loop(test_dataloader, model, loss_fn)\n    print(\"Done!\")\n\n    \n\n\n\n    with open(os.path.join(args.model_dir, 'model.pth'), 'wb') as f:\n        torch.save(model.state_dict(), f)\n        plt.plot([1,2,3,4])\n        plt.ylabel('some numbers')\n        plt.show()\n        plt.savefig('test.jpeg')<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Challenge_closed_time":1661165052743,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661051216633,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to save images during training in Sagemaker, but is unable to do so using the model_dir and output_data_dir. They want to reload this additional information during inference, but storing all the information in the model.tar.gz can cause slow inference. The user has provided a code snippet using PyTorch to train a neural network and save the model.",
        "Challenge_last_edit_time":1661051486728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73431378",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":68.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":63,
        "Challenge_solved_time":31.6211416667,
        "Challenge_title":"Save images from sagemaker training",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":43.0,
        "Challenge_word_count":478,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510598465376,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"San Francisco, CA, United States",
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>I suspect there is an issue with string concatenation in <code>plt.imsave<\/code> because the environment variable <code>SM_OUTPUT_DATA_DIR<\/code> by default points to <code>\/opt\/ml\/output\/data<\/code> (that's the actual value of <code>args.output_data_dir<\/code>, since you don't pass this parameter) so the outcome is something like <code>\/opt\/ml\/output\/dataplot1.jpg<\/code>. The same happen if you use the <code>model_dir<\/code> in the same way. I'd rather use something like <code>os.path.join<\/code> like you're already doing for the model. <a href=\"https:\/\/nono.ma\/sagemaker-model-dir-output-dir-and-output-data-dir-parameters\" rel=\"nofollow noreferrer\">here<\/a> a nice exaplaination about these folders and environment variables in sagemaker.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.3,
        "Solution_reading_time":9.92,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":1.2572055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am getting started with Kedro, so I created the new kedro project for default iris dataset.<\/p>\n<p>I am able to succesfully run it with <code>kedro run<\/code> command. My question now is how do I run it as a python command? From the documentation I read that the command <code>kedro run<\/code>  runs the <code>src\/project-name\/run.py<\/code>. However, if I run the <code>run.py<\/code> I get <code>ModuleNotFoundError: No module named 'iris_workflow'<\/code>. I get the same error if I run the <code>run<\/code> method from <code>src\/project-name\/cli.py<\/code>.<\/p>\n<p>Everything works fine If I run <code>kedro run<\/code> in terminal.<\/p>\n<p>How do I run <code>kedro run<\/code> from a python script without <code>subprocess.run()<\/code>. If I import the <code>run.py<\/code> or <code>cli.py<\/code> in a script and run it, I get the same error <code>ModuleNotFoundError: No module named 'iris_workflow'<\/code>.<\/p>\n<p>This is the default workflow I created with <code>kedro new --starter=pandas-iris<\/code><\/p>",
        "Challenge_closed_time":1615038763867,
        "Challenge_comment_count":0,
        "Challenge_created_time":1615034237927,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an issue while trying to run Kedro project as a python command instead of a command line. They are getting a \"ModuleNotFoundError\" when trying to run the \"run.py\" or \"cli.py\" files from a script, and are seeking a solution to run \"kedro run\" from a python script without using \"subprocess.run()\".",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/66505695",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":13.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1.2572055556,
        "Challenge_title":"kedro run as a python command instead of command line",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":805.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1519724643532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":453.0,
        "Poster_view_count":79.0,
        "Solution_body":"<p>The problem is that your <code>src\/<\/code> folder which is where your project python package lives isn't on your Python path, so if you modify your <code>PYTHONPATH<\/code> first, you'll be able to run <code>run.py<\/code>:<\/p>\n<pre><code>~\/code\/kedro\/test-project\ntest-project \u276f PYTHONPATH=$PYTHONPATH:$pwd\/src python3 src\/test_project\/run.py\n<\/code><\/pre>\n<p>To more concretely answer your question, if you wanted to run Kedro from a Python script, you'd do something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\n\nsys.path.append(&quot;&lt;path-to-your-project-src&quot;)\nwith KedroSession.create(package_path.name) as session:\n    session.run()\n<\/code><\/pre>",
        "Solution_comment_count":5.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.4,
        "Solution_reading_time":9.03,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":71.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1553088438367,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":24.0755444445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Similar to <a href=\"https:\/\/stackoverflow.com\/questions\/57199472\/is-it-possible-to-set-change-mlflow-run-name-after-run-initial-creation#:%7E:text=It%20is%20possible%20to%20edit,you%27d%20like%20to%20edit.&amp;text=There%27s%20currently%20no%20stable%20public,the%20tag%20with%20key%20mlflow.\">this question<\/a>, I'd like to edit\/set the description of a run via code, instead of editing it via UI.<\/p>\n<p>To clarify, I don't want to set the description of my entire experiment, only of a single run.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ogUgu.png\" alt=\"Image showing what I want to edit\" \/><\/a><\/p>",
        "Challenge_closed_time":1660231264752,
        "Challenge_comment_count":0,
        "Challenge_created_time":1660221143727,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know how to set or edit the description of a single run in MLflow programmatically instead of doing it through the UI.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73320708",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.0,
        "Challenge_reading_time":9.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":2.8113958333,
        "Challenge_title":"Set run description programmatically in mlflow",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":89.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527525798183,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Sarajevo, Bosnia and Herzegovina",
        "Poster_reputation_count":736.0,
        "Poster_view_count":57.0,
        "Solution_body":"<p>There are two ways to set the description.<\/p>\n<h3>1. <code>description<\/code> parameter<\/h3>\n<p>You can set a description using a markdown string for your run in <code>mlflow.start_run()<\/code> using <code>description<\/code> parameter. Here is an example.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    with mlflow.start_run(description=run_description) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>2. <code>mlflow.note.content<\/code> tag<\/h3>\n<p>You can set\/edit run names by setting the tag with the key <code>mlflow.note.content<\/code>, which is what the UI (currently) does under the hood.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if __name__ == &quot;__main__&quot;:\n    # load dataset and other stuff\n\n    run_description = &quot;&quot;&quot;\n### Header\nThis is a test **Bold**, *italic*, ~~strikethrough~~ text.\n[And this is an example hayperlink](http:\/\/example.com\/).\n    &quot;&quot;&quot;\n\n    tags = {\n        'mlflow.note.content': run_description\n    }\n\n    with mlflow.start_run(tags=tags) as run:\n        # train model and other stuff\n<\/code><\/pre>\n<h3>Result<\/h3>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4zZa9.png\" alt=\"output of the given example\" \/><\/a><\/p>\n<hr \/>\n<p>If you set <code>description<\/code> parameter and <code>mlflow.note.content<\/code> tag in <code>mlflow.start_run()<\/code>, you'll get this error.<\/p>\n<pre><code>Description is already set via the tag mlflow.note.content in tags.\nRemove the key mlflow.note.content from the tags or omit the description.\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1660307815687,
        "Solution_link_count":4.0,
        "Solution_readability":8.8,
        "Solution_reading_time":23.72,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":186.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":431.4666666667,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThe product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nDSVM, DB\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\neverything runs\r\n\r\n### Other Comments\r\nquestion to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?\r\n\r\n",
        "Challenge_closed_time":1555413510000,
        "Challenge_comment_count":12,
        "Challenge_created_time":1553860230000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user needs to remove the reference to Azure ML SDK preview private index from an operationalize notebook as it is now available through regular PyPi as a GA product and preview versions are unsupported.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/695",
        "Challenge_link_count":0,
        "Challenge_participation_count":12,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":431.4666666667,
        "Challenge_title":"[BUG] Remove contrib from azureml",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":106,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"azureml_hyperdrive_wide_and_deep notebook does**n't** use any azureml contrib modules. papermill PR in progress is using azureml.contrib.notebook. If it's not desired in the yaml file, I can install this package only inside the notebook. Will this work? mmm, yeah that would be a workaround.  Not in the Hyperdrive notebooks. since @jingyanwangms is the only one using it, can you take care of this issue in your PR? Do we want to require people to install things at the beginning of notebooks, though?  azureml.contrib.notebook is required for submitting notebook through aml. But if we don't want azureml.contrib to install as default in base yaml files, @heatherbshapiro what would you recommend doing here? @miguelgfierro Sure. I can do it in my PR. Hey guys\r\nI am getting an error while trying to run this line \"from azureml.contrib.notebook import NotebookRunConfig, AzureMLNotebookHandler\".\r\n**The error is ModuleNotFoundError: No module named 'azureml.contrib'** although I have installed azureml-contrib-notebook from pip. What should I do?\r\n HI @Raman1121 , which file in the code is this line in? I am trying to experiment with jupyter notebook on Azure through API calls by following the code snippet given here - \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-notebook\/azureml.contrib.notebook.azuremlnotebookhandler?view=azure-ml-py Well, that code is not related to the Recommenders GitHub repo. Most likely you have not configured your conda or python settings appropriately.",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":7.2,
        "Solution_reading_time":18.91,
        "Solution_score_count":null,
        "Solution_sentence_count":21.0,
        "Solution_word_count":210.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1333532422647,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation_count":10340.0,
        "Answerer_view_count":1243.0,
        "Challenge_adjusted_solved_time":7.9249297222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am looking to run a pre-trained object detection model onto a folder of ~400k images which is about 1.5GB. When I've tried running locally, it was estimated to take ~8 days to complete (with keras yolov3). Thus, I am looking to use AWS SageMaker and S3.<\/p>\n<p>When I have uploaded the zip folder of my images in the SageMaker jupyter notebook and tried to unzip by using bash command, an error pops ups saying that I have insufficient space. The volume assigned to my notebook is 5GB EBS, I do have other heavy datasets in my jupyter notebook space which could be causing this issue.<\/p>\n<p>To tackle that, I am looking for a way where I can upload my data to S3 and run SageMaker to read the images hosted and run an object detection model over. However, it does not look like there's a method to unzip folders on S3 without using an additional service (read that AWS Lambda may help) as these services are paid by my school.<\/p>\n<p>I could possibly re-run my code to extract my images from URL. In this case, how can I save these images to S3 directly in this case? Also, does anyone know if I am able to run yolov3 on SageMaker or if there is a better model I can look to use. Appreciate any advice that may help.<\/p>",
        "Challenge_closed_time":1604629425800,
        "Challenge_comment_count":5,
        "Challenge_created_time":1604600896053,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in running a pre-trained object detection model on a folder of ~400k images locally, which is estimated to take ~8 days to complete. To solve this issue, the user is trying to use AWS SageMaker and S3, but is encountering an error of insufficient space when trying to unzip the uploaded zip folder of images in the SageMaker jupyter notebook. The user is looking for a way to upload the data to S3 and run SageMaker to read the images hosted and run an object detection model over. However, there is no method to unzip folders on S3 without using an additional service, which is paid by the user's school. The user is considering re-running the code to extract images from URL and",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64703268",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":8.7,
        "Challenge_reading_time":15.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":7.9249297222,
        "Challenge_title":"How to use AWS SageMaker and S3 for Object Detection?",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":256.0,
        "Challenge_word_count":238,
        "Platform":"Stack Overflow",
        "Poster_created_time":1507880230152,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Singapore",
        "Poster_reputation_count":180.0,
        "Poster_view_count":29.0,
        "Solution_body":"<p>yes u are right u can upload thousands of images using aws cli using $aws s3 cp  ; or $aws s3 sync<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":968.2322455556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to make transfer learning method on MXNet on Sagemaker instance. Train and serve start locally without any problem and I'm using that python code to predict:<\/p>\n\n<pre><code>def predict_mx(net, fname):\n    with open(fname, 'rb') as f:\n      img = image.imdecode(f.read())\n      plt.imshow(img.asnumpy())\n      plt.show()\n    data = transform(img, -1, test_augs)\n    plt.imshow(data.transpose((1,2,0)).asnumpy()\/255)\n    plt.show()\n    data = data.expand_dims(axis=0)\n    return net.predict(data.asnumpy().tolist())\n<\/code><\/pre>\n\n<p>I checked <code>data.asnumpy().tolist()<\/code> that is ok and pyplot draw images (firts is the original image, the second is the resized image). But <code>net.predict<\/code> raise an error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\n&lt;ipython-input-171-ea0f1f5bdc72&gt; in &lt;module&gt;()\n----&gt; 1 predict_mx(predictor.predict, '.\/data2\/burgers-imgnet\/00103785.jpg')\n\n&lt;ipython-input-170-150a72b14997&gt; in predict_mx(net, fname)\n     30     plt.show()\n     31     data = data.expand_dims(axis=0)\n---&gt; 32     return net(data.asnumpy().tolist())\n     33 \n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data)\n     89         if self.deserializer is not None:\n     90             # It's the deserializer's responsibility to close the stream\n---&gt; 91             return self.deserializer(response_body, response['ContentType'])\n     92         data = response_body.read()\n     93         response_body.close()\n\n~\/Projects\/Lab\/ML\/AWS\/v\/lib64\/python3.6\/site-packages\/sagemaker\/predictor.py in __call__(self, stream, content_type)\n    290         \"\"\"\n    291         try:\n--&gt; 292             return json.load(codecs.getreader('utf-8')(stream))\n    293         finally:\n    294             stream.close()\n\n\/usr\/lib64\/python3.6\/json\/__init__.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    297         cls=cls, object_hook=object_hook,\n    298         parse_float=parse_float, parse_int=parse_int,\n--&gt; 299         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n    300 \n    301 \n\n\/usr\/lib64\/python3.6\/json\/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    352             parse_int is None and parse_float is None and\n    353             parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354         return _default_decoder.decode(s)\n    355     if cls is None:\n    356         cls = JSONDecoder\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in decode(self, s, _w)\n    337 \n    338         \"\"\"\n--&gt; 339         obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    340         end = _w(s, end).end()\n    341         if end != len(s):\n\n\/usr\/lib64\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n    355             obj, end = self.scan_once(s, idx)\n    356         except StopIteration as err:\n--&gt; 357             raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    358         return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n\n<p>I tried to json.dumps my data, and there is no problem with that.<\/p>\n\n<p>Note that I didn't deployed the service on AWS yet, I want to be able to test the model and prediction locally before to make a larger train and to serve it later.<\/p>\n\n<p>Thanks for your help<\/p>",
        "Challenge_closed_time":1531584172427,
        "Challenge_comment_count":4,
        "Challenge_created_time":1528098536343,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to predict using transfer learning method on MXNet on Sagemaker instance. The error is raised by net.predict and is related to JSONDecodeError. The user has checked data.asnumpy().tolist() and it is okay. The user has also tried to json.dumps the data, and there is no problem with that. The user has not deployed the service on AWS yet and wants to test the model and prediction locally before making a larger train and serving it later.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50675708",
        "Challenge_link_count":0,
        "Challenge_participation_count":6,
        "Challenge_readability":10.9,
        "Challenge_reading_time":41.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":968.2322455556,
        "Challenge_title":"Sagemaker Predict on local instance, JSON Error",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1358.0,
        "Challenge_word_count":332,
        "Platform":"Stack Overflow",
        "Poster_created_time":1340280616088,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Laval, France",
        "Poster_reputation_count":2835.0,
        "Poster_view_count":281.0,
        "Solution_body":"<p>The call to <strong>net.predict<\/strong> is working fine. <\/p>\n\n<p>It seems that you are using the SageMaker Python SDK <strong>predict_fn<\/strong> for hosting. After the <strong>predict_fn<\/strong> is invoked, the MXNet container will try to serialize your prediction to JSON before sending it back to the client. You can see code that does that here: <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-mxnet-container\/blob\/master\/src\/mxnet_container\/serve\/transformer.py#L132<\/a><\/p>\n\n<p>The container is failing to serialize because <strong>net.predict<\/strong> does not return a serializable object. You can solve this issue by returning a list instead:<\/p>\n\n<pre><code>return net.predict(data.asnumpy().tolist()).asnumpy().tolist()\n<\/code><\/pre>\n\n<p>Another alternative is to use a <strong>transform_fn<\/strong> instead of <strong>prediction_fn<\/strong> so you can handle the output serialization yourself. You can see an example of a <strong>transform_fn<\/strong> here <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/e93eff66626c0ab1f292048451c4c3ac7c39a121\/examples\/cli\/host\/script.py#L41<\/a><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":16.4,
        "Solution_reading_time":18.7,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":114.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":17.0636369444,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>If I have a trained model in Using pickle, or Joblib.\nLets say its Logistic regression or XGBoost.<\/p>\n<p>I would like to host that model in AWS Sagemaker as endpoint without running a training job.\nHow to achieve that.<\/p>\n<pre><code>#Lets Say myBucketName contains model.pkl\nmodel = joblib.load('filename.pkl')  \n# X_test = Numpy Array \nmodel.predict(X_test)  \n<\/code><\/pre>\n<p>I am not interested to <code>sklearn_estimator.fit('S3 Train, S3 Validate' )<\/code> , I have the trained model<\/p>",
        "Challenge_closed_time":1599722577176,
        "Challenge_comment_count":0,
        "Challenge_created_time":1599661148083,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user wants to know how to host a pre-trained machine learning model, specifically a Logistic Regression or XGBoost model, on AWS Sagemaker as an endpoint without running a training job. They have the model saved using pickle or Joblib and do not want to use the <code>sklearn_estimator.fit('S3 Train, S3 Validate')<\/code> method.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63813624",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":17.0636369444,
        "Challenge_title":"Load a Picked or Joblib Pre trained ML Model to Sagemaker and host as endpoint",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1622.0,
        "Challenge_word_count":80,
        "Platform":"Stack Overflow",
        "Poster_created_time":1370509408500,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1573.0,
        "Poster_view_count":194.0,
        "Solution_body":"<p>For Scikit Learn for example, you can get inspiration from this public demo <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>Step 1: Save your artifact (eg the joblib) compressed in S3 at <code>s3:\/\/&lt;your path&gt;\/model.tar.gz<\/code><\/p>\n<p>Step 2: Create an inference script with the deserialization function <code>model_fn<\/code>. (Note that you could also add custom inference functions <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code> but for scikit the defaults function work fine)<\/p>\n<pre><code>%%writefile inference_script.py. # Jupiter command to create file in case you're in Jupiter\n\nimport joblib\nimport os\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>Step 3: Create a model associating the artifact with the right container<\/p>\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/&lt;your path&gt;\/model.tar.gz',\n    role='&lt;your role&gt;',\n    entry_point='inference_script.py',\n    framework_version='0.23-1')\n<\/code><\/pre>\n<p>Step 4: Deploy!<\/p>\n<pre><code>model.deploy(\n    instance_type='ml.c5.large',  # choose the right instance type\n    initial_instance_count=1)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":17.5,
        "Solution_reading_time":20.38,
        "Solution_score_count":5.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":122.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":17.5492611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I attempting to deploy the universal-sentence-encoder model to a aws Sagemaker endpoint and am getting the error <code>raise ValueError('no SavedModel bundles found!')<\/code><\/p>\n\n<p>I have shown my code below, I have a feeling that one of my paths is incorrect<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import Model\n\ndef tfhub_to_savedmodel(model_name,uri):\n    tfhub_uri = uri\n    model_path = 'encoder_model\/' + model_name\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        module = hub.Module(tfhub_uri) \n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n\n        # define the model outputs\n        # we want the class ids and probabilities for the top 3 classes\n        logits = module(inputs['text'])\n        outputs = {\n            'vector': logits,\n        }\n\n        # export the model\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n\n\nsagemaker_role = get_execution_role()\n\n!tar -C \"$PWD\" -czf encoder.tar.gz encoder_model\/\nmodel_data = Session().upload_data(path='encoder.tar.gz',key_prefix='model')\n\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'universal-sentence-encoder-large'}\n\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Challenge_closed_time":1563978858283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1563915680943,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error \"no SavedModel bundles found!\" while attempting to deploy the universal-sentence-encoder model to an AWS SageMaker endpoint. The user suspects that one of the paths in the code might be incorrect. The code involves converting a TensorFlow Hub model to a SavedModel and deploying it to SageMaker.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57172147",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":17.5492611111,
        "Challenge_title":"'no SavedModel bundles found!' on tensorflow_hub model deployment to AWS SageMaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2621.0,
        "Challenge_word_count":152,
        "Platform":"Stack Overflow",
        "Poster_created_time":1531840489147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation_count":425.0,
        "Poster_view_count":92.0,
        "Solution_body":"<p>I suppose you started from this example? <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container<\/a><\/p>\n\n<p>It looks like you're not saving the TF Serving bundle properly: the model version number is missing, because of this line:<\/p>\n\n<pre><code>model_path = 'encoder_model\/' + model_name\n<\/code><\/pre>\n\n<p>Replacing it with this should fix your problem:<\/p>\n\n<pre><code>model_path = '{}\/{}\/00000001'.format('encoder_model\/', model_name)\n<\/code><\/pre>\n\n<p>Your model artefact should look like this (I used the model in the notebook above):<\/p>\n\n<pre><code>mobilenet\/\nmobilenet\/mobilenet_v2_140_224\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/saved_model.pb\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.data-00000-of-00001\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.index\n<\/code><\/pre>\n\n<p>Then, upload to S3 and deploy.<\/p>",
        "Solution_comment_count":4.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":23.4,
        "Solution_reading_time":15.64,
        "Solution_score_count":6.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":76.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1505653015243,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1128.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.1140905556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to export a machine learning model I created in Azure Machine Learning studio. One of the required input is \"Path to blob beginning with container\"<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/xvSDd.png\" alt=\"Here is the screenshoot from azure export\"><\/p>\n\n<p>How do I find this path? I have already created a blob storage but I have no idea how to find the path to the blob storage. <\/p>",
        "Challenge_closed_time":1538035308943,
        "Challenge_comment_count":0,
        "Challenge_created_time":1538034898217,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to export a machine learning model from Azure Machine Learning studio and needs to provide the \"Path to blob beginning with container\" as input. However, the user is unsure how to find this path even though they have already created a blob storage.",
        "Challenge_last_edit_time":1538038508827,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52532078",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":6.4,
        "Challenge_reading_time":5.24,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.1140905556,
        "Challenge_title":"How to find the path to blob?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":15096.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":4.7,
        "Solution_score_count":3.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":44.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1369156710532,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Japan",
        "Answerer_reputation_count":189.0,
        "Answerer_view_count":39.0,
        "Challenge_adjusted_solved_time":2.8227241667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a project in which I do several optuna studies each having around 50 trials.<\/p>\n<p>The optuna documentation suggests saving each model to a file for later use on <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-define-objective-functions-that-have-own-arguments\" rel=\"nofollow noreferrer\">this FAQ section<\/a><\/p>\n<p>What I want is to have all the best models of different studies in a python list. How is that possible?<\/p>\n<p>This is somewhat similar to my code:<\/p>\n<pre><code>def objective(trial, n_epochs, batch_size):\n     params = {\n              'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n              'optimizer': trial.suggest_categorical(&quot;optimizer&quot;, [&quot;Adam&quot;, &quot;RMSprop&quot;, &quot;SGD&quot;]),\n              'n_epochs': n_epochs,\n              'batch_size': batch_size\n              }\n     model = clp_network(trial)\n     accuracy, model = train_and_evaluate(params, model, trial) # THIS LINE\n     return accuracy\n<\/code><\/pre>\n<pre><code>     for i in range(50):\n           study = optuna.create_study(direction=&quot;maximize&quot;, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n           study.optimize(lambda trial: objective(trial, e, b), n_trials=50, show_progress_bar=True)\n<\/code><\/pre>\n<p>I would like to either save the <code>model<\/code> variable in the line marked <strong>THIS LINE<\/strong>, or somehow get the best model as a variable from the study.<\/p>",
        "Challenge_closed_time":1661860813547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661850651740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is conducting several optuna studies, each with around 50 trials, and wants to save all the best models of different studies in a Python list. The user is following the optuna documentation to save each model to a file for later use but wants to save the model variable in the code or get the best model as a variable from the study.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73539873",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":2.8227241667,
        "Challenge_title":"saving trained models in optuna to a variable",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":23.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Poster_created_time":1612517804323,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Babol, Mazandaran, Iran",
        "Poster_reputation_count":125.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>The easiest way is to define a global variable to store a model for each trial as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nfrom collections import defaultdict\n\n\nmodels = defaultdict(dict)\n\ndef objective(t):\n    model = t.suggest_int(&quot;x&quot;, 0, 100)\n    models[t.study.study_name][t.number] = model\n    \n    return model\n\nfor _ in range(10):\n    s = optuna.create_study()\n    s.optimize(objective, n_trials=10)\n\n<\/code><\/pre>\n<p>However I reckon this approach is not scalable in terms of memory space, so I'd suggest removing non-best models after each <code>optimize<\/code> call or saving models on an external file as mentioned in Optuna's FAQ.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.8,
        "Solution_reading_time":8.49,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":82.0,
        "Tool":"Optuna"
    },
    {
        "Answerer_created_time":1550401247147,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":71.0,
        "Answerer_view_count":3.0,
        "Challenge_adjusted_solved_time":385.9981336111,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am currently working on setting up a pipeline in Amazon Sagemaker. For that I set up an xgboost-estimator and trained it on my dataset. The training job runs as expected and the freshly trained model is saved to the specified output bucket. Later I want to reimport the model, which is done by getting the mode.tar.gz from the output bucket, extracting the model and serializing the binary via pickle.<\/p>\n<pre><code># download the model artifact from AWS S3\n!aws s3 cp s3:\/\/my-bucket\/output\/sagemaker-xgboost-2021-09-06-12-19-41-306\/output\/model.tar.gz .\n\n# opens the downloaded model artifcat and loads it as 'model' variable\nmodel_path = &quot;model.tar.gz&quot;\nwith tarfile.open(model_path) as tar:\n    tar.extractall(path=&quot;.&quot;)\n\nmodel = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n<\/code><\/pre>\n<p>Whenever I try to tun this I receive an unpickling stack underflow:<\/p>\n<pre><code>---------------------------------------------------------------------------\nUnpicklingError                           Traceback (most recent call last)\n&lt;ipython-input-9-b88a7424f790&gt; in &lt;module&gt;\n     10     tar.extractall(path=&quot;.&quot;)\n     11 \n---&gt; 12 model = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n     13 \n\nUnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>So far I retrained the model to see, if the error occurs with a different model file and it does. I also downloaded the model.tar.gz and validated it via gunzip. When extracting the binary file xgboost-model is extracted correctly, I just can't pickle it. Every occurence of the error I found on stackoverflow points at a damaged file, but this one is generated directly by SageMaker and I do note perform any transformation on it, but extracting it from the model.tar.gz. Reloading a model like this seems to be quite a common use case, referring to the documentation and different tutorials.\nLocally I receive the same error with the downloaded file. I tried to step directly into pickle for debugging it but couldn't make much sense of it. The complete error stack looks like this:<\/p>\n<pre><code>Exception has occurred: UnpicklingError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)\nunpickling stack underflow\n  File &quot;\/sagemaker_model.py&quot;, line 10, in &lt;module&gt;\n    model = pkl.load(open('xgboost-model', 'rb'))\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 197, in _run_module_as_main (Current frame)\n    return _run_code(code, main_globals, None,\n<\/code><\/pre>\n<p>What could cause this issue and at which step during the process could I apply changes to fix or workaround the problem.<\/p>",
        "Challenge_closed_time":1632479560168,
        "Challenge_comment_count":4,
        "Challenge_created_time":1631089966887,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is encountering an unpickling stack underflow error when trying to serialize a SageMaker model that was successfully generated and saved to an output bucket. The error occurs when the user tries to reimport the model by downloading the model.tar.gz file from the output bucket, extracting the model, and serializing the binary via pickle. The user has tried retraining the model and validating the model.tar.gz file but still encounters the same error. The user is seeking advice on what could be causing the issue and how to fix or work around it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69099627",
        "Challenge_link_count":0,
        "Challenge_participation_count":7,
        "Challenge_readability":11.9,
        "Challenge_reading_time":45.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":5.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":385.9981336111,
        "Challenge_title":"what causes an unpickling stack underflow when trying to serialize a succesfully generated SageMaker model",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1747.0,
        "Challenge_word_count":387,
        "Platform":"Stack Overflow",
        "Poster_created_time":1550401247147,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":71.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>The issue rooted in the model version used for the xgboost framework. from 1.3.0 on the default output changed from pickle to json and the sagemaker documentation does not seem to have been updated accordingly. So if you want to read the model via<\/p>\n<pre><code>    tar.extractall(path=&quot;.&quot;)\n\nmodel = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n<\/code><\/pre>\n<p>as described in the sagemaker docs, you need to import the XGBOOST framework with with a former version, e.g. 1.2.1.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":6.39,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":12.4379594444,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello Microsoft Q&amp;A,\nwhen running azure ml pipelines I got the following error:\n&quot; permission denied when access stream. Reason: Some(This request is not authorized to perform this operation using this permission.) &quot;\nWhen I checked the data assets for the pipeline, I got the follwoing error:<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/37d53c8a-2bec-4fa5-b769-3d57010a96f7?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>The Azure machine learning workspace is inside a vnet and I'm using a service principal for the data store authentification:\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/aa1dd74a-f306-4ddc-a8ef-97b8fc6ae98a?platform=QnA\" alt=\"User's image\" \/><\/p>\n<p>I allready granted the workspace managed identity AND the service principal the reader role for ALL private endpointes. Moreover I checked all other permissions, for example that the workspace managed identiy has the blob storage reader role for the adls gen2 storage.\nDoes this has something to do with these changes:\n &quot;Azure Machine Learning Network Isolation Changes with Compute Instance and Compute Cluster&quot;<\/p>\n<p>Could you please help me.<\/p>",
        "Challenge_closed_time":1681272887700,
        "Challenge_comment_count":0,
        "Challenge_created_time":1681228111046,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is encountering an issue with Azure ML Workspace where they are unable to get access token for ADLS Gen2. They are receiving a permission denied error when accessing the stream and have checked the data assets for the pipeline. The workspace is inside a VNet and the user is using a service principal for data store authentication. They have already granted the workspace managed identity and the service principal the reader role for all private endpoints, and checked all other permissions. The user is unsure if this has something to do with recent changes to Azure Machine Learning Network Isolation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1220935\/azure-ml-workspace-unable-to-get-access-token-for",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":13.2,
        "Challenge_reading_time":16.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":12.4379594444,
        "Challenge_title":"Azure ML Workspace - Unable to get access token for ADLS Gen2",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":154,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hello @Lukas\nThanks for reaching out to us. I haven't seen the same error, but based on some researches and my personal experience, the error message you're seeing indicates that the user or service principal running the Azure ML pipeline does not have sufficient permissions to access the data assets required by the pipeline. It's possible that the recent changes to Azure Machine Learning Network Isolation could be a factor in this issue.<\/p>\n<p>Here are some steps you can take to further troubleshoot the issue:<\/p>\n<p>Check the credentials being used to access the data assets: Verify that the credentials being used to access the data assets are correct and have sufficient permissions to read the data. You can check this by attempting to manually access the data assets using the same credentials and seeing if you encounter any issues.<\/p>\n<p>Verify RBAC permissions: Make sure that the user or service principal running the pipeline has the necessary RBAC permissions to access the data assets. You can check this by reviewing the access policies and roles associated with the data assets, and making sure that the user or service principal is included in the appropriate role(s) with sufficient access.<\/p>\n<p>Check firewall and network settings: If the data assets are hosted in a private network, make sure that the firewall and network settings allow the pipeline to access the data. You may need to configure virtual network peering or VPN connections to enable access.<\/p>\n<p>Review pipeline configuration: Double-check the pipeline configuration to ensure that the correct data asset paths and permissions are specified. You can also try re-creating the pipeline from scratch to see if that resolves the issue.<\/p>\n<p>Review the Network Isolation changes: Review the recent Azure Machine Learning Network Isolation changes and ensure that they are not impacting your pipeline. You may need to update your pipeline configuration to account for any changes in network isolation.<\/p>\n<p>If none of the above steps resolve the issue, you can contact Microsoft support for further assistance. Please raise a support ticket if you have a support plan, please let us know if you have tried all above items but nothing works, I am happy to enable you a free ticket for this issue. <\/p>\n<p>I hope this helps! Let me know if you have any further questions.<\/p>\n<p>Regards,\nYutong<\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.0,
        "Solution_reading_time":30.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":401.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isCse":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":58.8322222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm working on time-series modeling. I'm comparing battle-of-algorithms against the autopilot machine learning approach to identify the model that best fits my use case.\nI understand that Amazon SageMaker Autopilot doesn't work with time series.\nIs there an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series?",
        "Challenge_closed_time":1601883088000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601671292000,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is working on time-series modeling and is comparing the battle-of-algorithms against the autopilot machine learning approach. However, they have found that Amazon SageMaker Autopilot does not work with time series. The user is looking for an alternative library or algorithm in the AWS ecosystem that implements battle-of-algorithms for time series.",
        "Challenge_last_edit_time":1668529103319,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUxaXaoPpqRyGdkh6aKK9uew\/can-i-use-sagemaker-autopilot-for-my-time-series-modeling",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.3,
        "Challenge_reading_time":5.36,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":58.8322222222,
        "Challenge_title":"Can I use SageMaker Autopilot for my time-series modeling?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":319.0,
        "Challenge_word_count":60,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":0.0,
        "Poster_isCse":0.0,
        "Poster_isExpert":0.0,
        "Poster_isModerator":0.0,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Amazon SageMaker Autopilot currently supports regression, binary classification, and multi-class classification. SageMaker supports only tabular data formatted in files with comma-separated values. For more information, see [Automate model development with Amazon SageMaker Autopilot][1].\n\n[1]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/autopilot-automate-model-development.html\n\nFor your use case, you can use [Amazon Forecast][2]. Amazon Forecast includes the [AutoML][3] feature that trains different models with your target time series, related time series, and item metadata. Amazon Forecast then uses the model with the best accuracy metrics.\n\n[2]: https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/what-is-forecast.html\n[3]: https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/automl.html",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1667925587776,
        "Solution_link_count":3.0,
        "Solution_readability":18.4,
        "Solution_reading_time":10.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1455444590636,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Answerer_reputation_count":903.0,
        "Answerer_view_count":94.0,
        "Challenge_adjusted_solved_time":0.2935611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>AWS Sagemaker uses <code>SM_USER_ARGS<\/code> (as documented <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#sm-user-args\" rel=\"nofollow noreferrer\">here<\/a>) as an environment variable in which it contains a string (list) of arguments as they are passed by the user. So the environment variable value looks like this: <code>'[\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optmize\"]'<\/code>.<\/p>\n\n<p>With <code>json.loads()<\/code> I am capable of transforming that string into a python list. Although, I want to create an abstract module that returns an <strong>argparse Namespace<\/strong> in a way that rest of the code remains intact whether I run it locally or in the AWS Sagemaker service.<\/p>\n\n<p>So, basically, what I want is a code that receives the input <code>[\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optmize\"]<\/code> and output <code>Namespace(test_size=0.2, random_seed='42', not_optmize=True, &lt;other_arguments&gt;... ])<\/code>.<\/p>\n\n<p>Does python <strong>argparse<\/strong> package helps me with that? I am trying to figure out a way that I do not need to re implement the argparse parser.<\/p>\n\n<p>Here is an example, I have this config.ini file:<\/p>\n\n<pre><code>[Docker]\nhome_dir = \/opt\nSM_MODEL_DIR = %(home_dir)s\/ml\/model\nSM_CHANNELS = [\"training\"]\nSM_NUM_GPUS = 1\nSM_NUM_CPUS =\nSM_LOG_LEVEL = 20\nSM_USER_ARGS = [\"--test_size\",\"0.2\",\"--random_seed\",\"42\"]\nSM_INPUT_DIR = %(home_dir)s\/ml\/input\nSM_INPUT_CONFIG_DIR = %(home_dir)s\/ml\/input\/config\nSM_OUTPUT_DIR = %(home_dir)s\/ml\/output\nSM_OUTPUT_INTERMEDIATE_DIR = %(home_dir)s\/ml\/output\/intermediate\n<\/code><\/pre>\n\n<p>I have this Argparser class:<\/p>\n\n<pre><code>import argparse\nimport configparser\nimport datetime\nimport json\nimport multiprocessing\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nfrom .files import JsonFile, YAMLFile\n\n\nclass ArgParser(ABC):\n\n    @abstractmethod\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        pass\n\n\nclass AWSArgParser(ArgParser):\n\n    def __init__(self):\n        configuration_file_path = 'config.ini'\n\n        self.environment = \"Sagemaker\" \\\n            if os.environ.get(\"SM_MODEL_DIR\", False) \\\n            else os.environ.get(\"ENVIRON\", \"Default\")\n\n        config = configparser.ConfigParser()\n        config.read(configuration_file_path)\n        if self.environment == \"Local\":\n            config[self.environment][\"home_dir\"] = str(pathlib.Path(__file__).parent.absolute())\n        if self.environment != 'Sagemaker':\n            config[self.environment][\"SM_NUM_CPUS\"] = str(multiprocessing.cpu_count())\n\n        for key, value in config[self.environment].items():\n            os.environ[key.upper()] = value\n\n        self.parser = argparse.ArgumentParser()\n        # AWS Sagemaker default environmental arguments\n        self.parser.add_argument(\n            '--model_dir',\n            type=str,\n            default=os.environ['SM_MODEL_DIR'],\n        )\n        self.parser.add_argument(\n            '--channel_names',\n            default=json.loads(os.environ['SM_CHANNELS']),\n        )\n        self.parser.add_argument(\n            '--num_gpus',\n            type=int,\n            default=os.environ['SM_NUM_GPUS'],\n        )\n        self.parser.add_argument(\n            '--num_cpus',\n            type=int,\n            default=os.environ['SM_NUM_CPUS'],\n        )\n        self.parser.add_argument(\n            '--user_args',\n            default=json.loads(os.environ['SM_USER_ARGS']),\n        )\n        self.parser.add_argument(\n            '--input_dir',\n            type=str,\n            default=os.environ['SM_INPUT_DIR'],\n        )\n        self.parser.add_argument(\n            '--input_config_dir',\n            type=Path,\n            default=os.environ['SM_INPUT_CONFIG_DIR'],\n        )\n        self.parser.add_argument(\n            '--output_dir',\n            type=Path,\n            default=os.environ['SM_OUTPUT_DIR'],\n        )\n\n        # Extra arguments\n        self.run_tag = datetime.datetime \\\n            .fromtimestamp(time.time()) \\\n            .strftime('%Y-%m-%d-%H-%M-%S')\n        self.parser.add_argument(\n            '--run_tag',\n            default=self.run_tag,\n            type=str,\n            help=f\"Run tag (default: 'datetime.fromtimestamp')\",\n        )\n        self.parser.add_argument(\n            '--environment',\n            type=str,\n            default=self.environment,\n        )\n\n        self.args = self.parser.parse_args()\n\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        &lt;parse self.args.user_args&gt;\n\n        return self.args\n<\/code><\/pre>\n\n<p>then I have my <code>train<\/code> script:<\/p>\n\n<pre><code>from utils.arg_parser import AWSArgParser\n\nif __name__ == '__main__':\n    logger.info(f\"Begin train.py\")\n\n    if os.environ[\"ENVIRON\"] == \"Sagemaker\":\n        arg_parser = AWSArgParser()\n        args = arg_parser.get_arguments()\n    else:\n        args = &lt;normal local parse&gt;\n<\/code><\/pre>",
        "Challenge_closed_time":1583965847987,
        "Challenge_comment_count":4,
        "Challenge_created_time":1583962823900,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to create an abstract module that returns an argparse Namespace in a way that the rest of the code remains intact whether it is run locally or in the AWS Sagemaker service. They are trying to figure out a way to avoid re-implementing the argparse parser and are wondering if the argparse package can help with this. The user has provided an example of their Argparser class and train script.",
        "Challenge_last_edit_time":1583964791167,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60644771",
        "Challenge_link_count":1,
        "Challenge_participation_count":5,
        "Challenge_readability":17.0,
        "Challenge_reading_time":56.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":44,
        "Challenge_solved_time":0.8400241667,
        "Challenge_title":"How to parse AWS Sagemaker SM_USER_ARGS with argparse into an argparse Namespace?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":782.0,
        "Challenge_word_count":349,
        "Platform":"Stack Overflow",
        "Poster_created_time":1455444590636,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Poster_reputation_count":903.0,
        "Poster_view_count":94.0,
        "Solution_body":"<p>Following <a href=\"https:\/\/stackoverflow.com\/users\/1126841\/chepner\">@chepner<\/a>'s comment an example solution would be something like this:<\/p>\n\n<p>config.ini file:<\/p>\n\n<pre><code>[Docker]\nhome_dir = \/opt\nSM_MODEL_DIR = %(home_dir)s\/ml\/model\nSM_CHANNELS = [\"training\"]\nSM_NUM_GPUS = 1\nSM_NUM_CPUS =\nSM_LOG_LEVEL = 20\nSM_USER_ARGS = [\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optimize\"]\nSM_INPUT_DIR = %(home_dir)s\/ml\/input\nSM_INPUT_CONFIG_DIR = %(home_dir)s\/ml\/input\/config\nSM_OUTPUT_DIR = %(home_dir)s\/ml\/output\nSM_OUTPUT_INTERMEDIATE_DIR = %(home_dir)s\/ml\/output\/intermediate\n<\/code><\/pre>\n\n<p>A <code>TrainArgParser<\/code> class like this:<\/p>\n\n<pre><code>class ArgParser(ABC):\n\n    @abstractmethod\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        pass\n\n\nclass TrainArgParser(ArgParser):\n\n    def __init__(self):\n        configuration_file_path = 'config.ini'\n\n        self.environment = \"Sagemaker\" \\\n            if os.environ.get(\"SM_MODEL_DIR\", False) \\\n            else os.environ.get(\"ENVIRON\", \"Default\")\n\n        config = configparser.ConfigParser()\n        config.read(configuration_file_path)\n        if self.environment == \"Local\":\n            config[self.environment][\"home_dir\"] = str(pathlib.Path(__file__).parent.absolute())\n        if self.environment != 'Sagemaker':\n            config[self.environment][\"SM_NUM_CPUS\"] = str(multiprocessing.cpu_count())\n\n        for key, value in config[self.environment].items():\n            os.environ[key.upper()] = value\n\n        self.parser = argparse.ArgumentParser()\n        # AWS Sagemaker default environmental arguments\n        self.parser.add_argument(\n            '--model_dir',\n            type=str,\n            default=os.environ['SM_MODEL_DIR'],\n        )\n        self.parser.add_argument(\n            '--channel_names',\n            default=json.loads(os.environ['SM_CHANNELS']),\n        )\n        self.parser.add_argument(\n            '--num_gpus',\n            type=int,\n            default=os.environ['SM_NUM_GPUS'],\n        )\n        self.parser.add_argument(\n            '--num_cpus',\n            type=int,\n            default=os.environ['SM_NUM_CPUS'],\n        )\n        self.parser.add_argument(\n            '--user_args',\n            default=json.loads(os.environ['SM_USER_ARGS']),\n        )\n        self.parser.add_argument(\n            '--input_dir',\n            type=str,\n            default=os.environ['SM_INPUT_DIR'],\n        )\n        self.parser.add_argument(\n            '--input_config_dir',\n            type=Path,\n            default=os.environ['SM_INPUT_CONFIG_DIR'],\n        )\n        self.parser.add_argument(\n            '--output_dir',\n            type=Path,\n            default=os.environ['SM_OUTPUT_DIR'],\n        )\n\n        # Extra arguments\n        self.run_tag = datetime.datetime \\\n            .fromtimestamp(time.time()) \\\n            .strftime('%Y-%m-%d-%H-%M-%S')\n        self.parser.add_argument(\n            '--run_tag',\n            default=self.run_tag,\n            type=str,\n            help=f\"Run tag (default: 'datetime.fromtimestamp')\",\n        )\n        self.parser.add_argument(\n            '--environment',\n            type=str,\n            default=self.environment,\n        )\n\n        self.args = self.parser.parse_args()\n\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        # Not in AWS Sagemaker arguments\n        self.parser.add_argument(\n            '--test_size',\n            default=0.2,\n            type=float,\n            help=\"Test dataset size (default: '0.2')\",\n        )\n        self.parser.add_argument(\n            '--random_seed',\n            default=42,\n            type=int,\n            help=\"Random number for initialization (default: '42')\",\n        )\n        self.parser.add_argument(\n            '--secrets',\n            type=YAMLFile.parse_string,\n            default='',\n            help=\"An yaml formated string (default: '')\"\n        )\n        self.parser.add_argument(\n            '--bucket_name',\n            type=str,\n            default='',\n            help=\"Bucket name of a remote storage (default: '')\"\n        )\n        self.args = self.parser.parse_args(self.args.user_args)\n\n        return self.args\n<\/code><\/pre>\n\n<p>and a entry_script for <code>train<\/code> would start like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\n\nfrom utils.arg_parser import TrainArgParser\n\nif __name__ == '__main__':\n    logger.info(f\"Begin train.py\")\n\n    arg_parser = TrainArgParser()\n    args = arg_parser.get_arguments()\n    print(args)\n<\/code><\/pre>\n\n<p>This should output something like this:<\/p>\n\n<pre><code>Namespace(bucket_name='', channel_names=['training'], environment='Docker', input_config_dir=PosixPath('\/opt\/ml\/input\/config'), input_dir='\/opt\/ml\/input', model_dir='\/opt\/ml\/model', num_cpus=8, num_gpus=1, output_dir=PosixPath('\/opt\/ml\/output'), random_seed=42, run_tag='2020-03-11-22-18-21', secrets={}, test_size=0.2, user_args=['--test_size', '0.2', '--random_seed', '42'])\n<\/code><\/pre>\n\n<p>But that is useless if AWS Sagemaker treats <code>SM_USER_ARGS<\/code> and <code>SM_HPS<\/code> as the same thing. :(<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":21.2,
        "Solution_reading_time":54.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":40.0,
        "Solution_word_count":263.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1597.1266666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nTrying to log model into mlflow using `mlflow.pytorch.log_model` in train end. Getting the above error only in multi gpu scenario. \r\n\r\n#### Code\r\n\r\n\r\nmnist script file - \r\n\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom argparse import ArgumentParser\r\n#from mlflow.pytorch.pytorch_autolog import __MLflowPLCallback\r\nfrom pytorch_lightning.logging import MLFlowLogger\r\nfrom sklearn.metrics import accuracy_score\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision import datasets, transforms\r\n\r\n\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self):\r\n        \"\"\"\r\n        Initializes the network\r\n        \"\"\"\r\n        super(LightningMNISTClassifier, self).__init__()\r\n\r\n        # mnist images are (1, 28, 28) (channels, width, height)\r\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n\r\n        # transforms for images\r\n        self.transform = transforms.Compose(\r\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\r\n        )\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument(\r\n            \"--batch-size\",\r\n            type=int,\r\n            default=64,\r\n            metavar=\"N\",\r\n            help=\"input batch size for training (default: 64)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--num-workers\",\r\n            type=int,\r\n            default=0,\r\n            metavar=\"N\",\r\n            help=\"number of workers (default: 0)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--lr\",\r\n            type=float,\r\n            default=1e-3,\r\n            metavar=\"LR\",\r\n            help=\"learning rate (default: 1e-3)\",\r\n        )\r\n        return parser\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        Forward Function\r\n        \"\"\"\r\n        batch_size, channels, width, height = x.size()\r\n\r\n        # (b, 1, 28, 28) -> (b, 1*28*28)\r\n        x = x.view(batch_size, -1)\r\n\r\n        # layer 1 (b, 1*28*28) -> (b, 128)\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 2 (b, 128) -> (b, 256)\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 3 (b, 256) -> (b, 10)\r\n        x = self.layer_3(x)\r\n\r\n        # probability distribution over labels\r\n        x = torch.log_softmax(x, dim=1)\r\n\r\n        return x\r\n\r\n    def cross_entropy_loss(self, logits, labels):\r\n        \"\"\"\r\n        Loss Fn to compute loss\r\n        \"\"\"\r\n        return F.nll_loss(logits, labels)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        \"\"\"\r\n        training the data as batches and returns training loss on each batch\r\n        \"\"\"\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, val_batch, batch_idx):\r\n        \"\"\"\r\n        Performs validation of data in batches\r\n        \"\"\"\r\n        x, y = val_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"val_loss\": loss}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average validation accuracy\r\n        \"\"\"\r\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\r\n        tensorboard_logs = {\"val_loss\": avg_loss}\r\n        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs}\r\n\r\n    def test_step(self, test_batch, batch_idx):\r\n        \"\"\"\r\n        Performs test and computes test accuracy\r\n        \"\"\"\r\n        x, y = test_batch\r\n        output = self.forward(x)\r\n        a, y_hat = torch.max(output, dim=1)\r\n        test_acc = accuracy_score(y_hat.cpu(), y.cpu())\r\n        return {\"test_acc\": torch.tensor(test_acc)}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average test accuracy score\r\n        \"\"\"\r\n        avg_test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\r\n        return {\"avg_test_acc\": avg_test_acc}\r\n\r\n    def prepare_data(self):\r\n        \"\"\"\r\n        Preprocess the input data.\r\n        \"\"\"\r\n        return {}\r\n\r\n    def train_dataloader(self):\r\n        \"\"\"\r\n        Loading training data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_train,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        \"\"\"\r\n        Loading validation data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\r\n\r\n        return DataLoader(\r\n            mnist_val,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        \"\"\"\r\n        Loading test data as batches\r\n        \"\"\"\r\n        mnist_test = datasets.MNIST(\r\n            \"dataset\", download=True, train=False, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_test,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def configure_optimizers(self):\r\n        \"\"\"\r\n        Creates and returns Optimizer\r\n        \"\"\"\r\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        self.scheduler = {\r\n            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n                self.optimizer,\r\n                mode=\"min\",\r\n                factor=0.2,\r\n                patience=2,\r\n                min_lr=1e-6,\r\n                verbose=True,\r\n            )\r\n        }\r\n        return [self.optimizer], [self.scheduler]\r\n\r\n    def optimizer_step(\r\n        self,\r\n        epoch,\r\n        batch_idx,\r\n        optimizer,\r\n        optimizer_idx,\r\n        second_order_closure=None,\r\n        on_tpu=False,\r\n        using_lbfgs=False,\r\n        using_native_amp=False,\r\n    ):\r\n        self.optimizer.step()\r\n        self.optimizer.zero_grad()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    from pytorch_autolog import autolog\r\n    autolog()\r\n    model = LightningMNISTClassifier()\r\n    mlflow_logger = MLFlowLogger(\r\n        experiment_name=\"Default\", tracking_uri=\"http:\/\/localhost:5000\/\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        logger=mlflow_logger,\r\n        gpus=2,\r\n        distributed_backend=\"ddp\",\r\n        max_epochs=1\r\n    )\r\n    trainer.fit(model)\r\n    trainer.test()\r\n\r\n```\r\n\r\nSample code from autolog - Callback class. \r\n\r\n```\r\n    class __MLflowPLCallback(pl.Callback):\r\n\r\n        def __init__(self):\r\n            super().__init__()\r\n\r\n        def on_train_end(self, trainer, pl_module):\r\n            \"\"\"\r\n            Logs the model checkpoint into mlflow - models folder on the training end\r\n            \"\"\"\r\n\r\n            mlflow.set_tracking_uri(trainer.logger._tracking_uri )\r\n            mlflow.set_experiment(trainer.logger._experiment_name)\r\n            mlflow.start_run(trainer.logger.run_id)\r\n            mlflow.pytorch.log_model(trainer.model, \"models\")\r\n            mlflow.end_run()\r\n\r\n\r\n```\r\n\r\n\r\n\r\n\r\nStack Trace\r\n\r\n```\r\nTraceback (most recent call last):                                                                                                                                                                          \r\n  File \"mnist.py\", line 231, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 218, in fit\r\n    return _run_and_log_function(self, original, args, kwargs)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 209, in _run_and_log_function\r\n    result = original(self, *args, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 992, in fit\r\n    results = self.spawn_ddp_children(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 462, in spawn_ddp_children\r\n    results = self.ddp_train(local_rank, q=None, model=model, is_master=True)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 560, in ddp_train\r\n    results = self.run_pretrain_routine(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1213, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 392, in train\r\n    self.run_training_teardown()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 872, in run_training_teardown\r\n    self.on_train_end()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 72, in on_train_end\r\n    callback.on_train_end(self, self.get_model())\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 120, in on_train_end\r\n    mlflow.pytorch.log_model(trainer.model, \"models\")\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 179, in log_model\r\n    signature=signature, input_example=input_example, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/models\/model.py\", line 154, in log\r\n    **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 300, in save_model\r\n    torch.save(pytorch_model, model_path, pickle_module=pickle_module, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 370, in save\r\n    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 443, in _legacy_save\r\n    pickler.dump(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/cloudpickle\/cloudpickle.py\", line 491, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 659, in save_reduce\r\n    self._batch_setitems(dictitems)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 890, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 524, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle _thread.lock objects\r\n\r\n\r\n```\r\n\r\n\r\n\r\n#### What have you tried?\r\nTried out the possibilities mentioned in the similar thread - https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/2186\r\n\r\nTried  wrapping the code inside, `trainer.is_global_zero`  . And also tried `trainer.global_rank == 0`. Also tried decorating the method as `@rank_zero_only`. But no luck. Getting the same error. \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu\r\n - Packaging - torch, pytorch-lightning, torchvision, mlflow",
        "Challenge_closed_time":1605489870000,
        "Challenge_comment_count":15,
        "Challenge_created_time":1599740214000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is using mlflow instead of tensorboard as a logger and is facing an issue where the checkpoints are being saved in the wrong location. The checkpoints should be in the `\\mlflow` folder, but they are being saved in the `\\1\\\\{guid}\\checkpoints` folder. The user is unsure if this is an mlflow or pytorch-lightning issue and is using pytorch-lightning 0.8.5 on macos running in python 3.7.6.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3444",
        "Challenge_link_count":2,
        "Challenge_participation_count":15,
        "Challenge_readability":14.2,
        "Challenge_reading_time":152.56,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":140,
        "Challenge_solved_time":1597.1266666667,
        "Challenge_title":"TypeError: can't pickle _thread.lock objects - Error while logging model into mlflow in multi gpu scenario",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":935,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Hi! thanks for your contribution!, great first issue! What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer. > What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer.\r\n\r\nThanks for the reply. Sure i will try and update here. > What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer.\r\n\r\nI removed the scheduler part and re-ran the script. Still experiencing the same error.  @lucadiliello @williamFalcon Any suggestions here ? One more observation from my end. In multi gpu scenario when i save the model using `torch.save(trainer.model, PATH)` i get the above mentioned error . However, when i try to save the save dict `torch.save(trainer.model.state_dict(), PATH)` the state dict is successfully getting saved. \r\n\r\nTested with 2 gpus and 0.9 version of pytorch lightning.  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I have the same error. Training with a single gpu works fine but with multiple the error is raised > I have the same error. Training with a single gpu works fine but with multiple the error is raised\r\n\r\nTraining with single GPU is fine because there is no need to create multiple processes, so you model is not pickled. What happens if you simply load your model in a shell and try to pickle it?\r\nSomething like:\r\n```python\r\n>>> model = MyModel(...)\r\n>>>\r\n>>> import pickle\r\n>>> pickle.dump(model, \"tmp_file.pk\")\r\n```\r\n\r\nIn my little experience, most of the pickle errors are caused by lambda functions or non-global functions. See [here](https:\/\/docs.python.org\/3\/library\/pickle.html#what-can-be-pickled-and-unpickled) the list of what can be pickled. > One more observation from my end. In multi gpu scenario when i save the model using `torch.save(trainer.model, PATH)` i get the above mentioned error . However, when i try to save the save dict `torch.save(trainer.model.state_dict(), PATH)` the state dict is successfully getting saved.\r\n> \r\n> Tested with 2 gpus and 0.9 version of pytorch lightning.\r\n\r\nPlease update to the latest version and let us know whether the error persist. > > I have the same error. Training with a single gpu works fine but with multiple the error is raised\r\n> \r\n> Training with single GPU is fine because there is no need to create multiple processes, so you model is not pickled. What happens if you simply load your model in a shell and try to pickle it?\r\n> Something like:\r\n> \r\n> ```python\r\n> >>> model = MyModel(...)\r\n> >>>\r\n> >>> import pickle\r\n> >>> pickle.dump(model, \"tmp_file.pk\")\r\n> ```\r\n> \r\n> In my little experience, most of the pickle errors are caused by lambda functions or non-global functions. See [here](https:\/\/docs.python.org\/3\/library\/pickle.html#what-can-be-pickled-and-unpickled) the list of what can be pickled.\r\n\r\nI tried adding as suggested:\r\n```\r\n>>> import pickle\r\n>>> pickle.dump(model, \"tmp_file.pk\")\r\n```\r\nHowever, this raise the error: TypeError: file must have a 'write' attribute\r\nI then tried with:\r\n```\r\n>>> import pickle\r\n>>> with open(\"tmp_file.pk\",\"wb\") as f:\r\n>>>         pickle.dump(model, f)\r\n```\r\nThis then again raised TypeError: can't pickle _thread.lock objects\r\nI am on the latest version of pytorch lightning This suggests that the problem is in your model. Can you post it here?\n\nA complete log of the error would also be useful. Thanks I ran into this as well. I wrote a little function to iterate through the model.__dict__.items() and check which caused pickle errors. It looks like there's a model attribute pointing to the trainer, which has that _thread.lock on it. Maybe there's a step that's supposed to clean this up that's being missed somehow? My quick work-around was to `delattr(model, \"trainer\")` before pickling the model, but I haven't actually tried loading the model again, so I this could cause other problems. same problem same problem on 1.2.4, works fine with 1.1.0. any ideas what might be causing the difference between the versions?",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":51.46,
        "Solution_score_count":null,
        "Solution_sentence_count":63.0,
        "Solution_word_count":630.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1226984969400,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Adelaide, Australia",
        "Answerer_reputation_count":5789.0,
        "Answerer_view_count":464.0,
        "Challenge_adjusted_solved_time":21214.2534619445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to train an object detection model starting from an existing model using the new Managed Spot Training feature,  The paramters used when creating my Estimator are as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>od_model = sagemaker.estimator.Estimator(get_image_uri(sagemaker.Session().boto_region_name, 'object-detection', repo_version=\"latest\"),\n                                         Config['role'],\n                                         train_instance_count = 1,\n                                         train_instance_type = 'ml.p3.16xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = (48 * 60 * 60),\n                                         train_use_spot_instances = True,\n                                         train_max_wait = (72 * 60 * 60),\n                                         input_mode = 'File',\n                                         checkpoint_s3_uri = Config['train_checkpoint_uri'],\n                                         output_path = Config['s3_output_location'],\n                                         sagemaker_session = sagemaker.Session()\n                                         )\n<\/code><\/pre>\n\n<p>(The references to <code>Config<\/code> in the above are a config data structure I'm using to extract\/centralise some parameters)<\/p>\n\n<p>When I run the above, I get the following exception:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: MaxWaitTimeInSeconds above 3600 is not supported for the given algorithm.<\/p>\n<\/blockquote>\n\n<p>If I change <code>train_max_wait<\/code> to 3600 I get this exception instead:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds<\/p>\n<\/blockquote>\n\n<p>However changing <code>max_run_time<\/code> to 3600 or less isn't going to work for me as I expect this model to take several days to train (large data set), in fact a single epoch takes more than an hour.<\/p>\n\n<p>The <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">AWS blog post on Managed Spot Training<\/a> say that <code>MaxWaitTimeInSeconds<\/code> is limited to an 60 minutes for:<\/p>\n\n<blockquote>\n  <p>For built-in algorithms and AWS Marketplace algorithms that don\u2019t use checkpointing, we\u2019re enforcing a maximum training time of 60 minutes (MaxWaitTimeInSeconds parameter).<\/p>\n<\/blockquote>\n\n<p>Earlier, the same blog post says:<\/p>\n\n<blockquote>\n  <p>Built-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification).<\/p>\n<\/blockquote>\n\n<p>So I don't think it's that my algorithm doesn't support Checkpointing.  In fact that blog post uses object detection and max run times of 48 hours.  So I don't think it's an algorithm limitation.<\/p>\n\n<p>As you can see above, I've set up a S3 URL for the checkpoints.  The S3 bucket does exist, and the training container has access to it (it's the same bucket that the training data and model outputs are placed, and I had no problems with access to those before turning on spot training.<\/p>\n\n<p>My boto and sagemaker libraries are current versions:<\/p>\n\n<pre><code>boto3 (1.9.239)\nbotocore (1.12.239)\nsagemaker (1.42.3)\n<\/code><\/pre>\n\n<p>As best I can tell from reading various docs, I've got everything set up correctly.  My use case is almost exactly what's described in the blog post linked above, but I'm using the SageMaker Python SDK instead of the console.<\/p>\n\n<p>I'd really like to try Managed Spot Training to save some money, as I have a very long training run coming up.  But limiting timeouts to an hour isn't going to work for my use case.  Any suggestions?<\/p>\n\n<p><strong>Update:<\/strong>  If I comment out the <code>train_use_spot_instances<\/code> and <code>train_max_wait<\/code> options to train on regular on-demand instances my training job is created successfully.  If I then try to use the console to clone the job and turn on Spot instances on the clone I get the same ValidationException.<\/p>",
        "Challenge_closed_time":1570492274472,
        "Challenge_comment_count":0,
        "Challenge_created_time":1569898766770,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to train an object detection model using SageMaker Managed Spot Training feature, but is encountering an error related to the MaxWaitTimeInSeconds parameter. The user has tried changing the parameter to 3600, but it resulted in another error. The AWS blog post on Managed Spot Training mentions that MaxWaitTimeInSeconds is limited to 60 minutes for built-in algorithms and AWS Marketplace algorithms that don't use checkpointing. However, the user's algorithm supports checkpointing, and the S3 bucket for checkpoints exists and is accessible. The user is seeking suggestions to resolve the issue and use Managed Spot Training to save costs.",
        "Challenge_last_edit_time":1569900383487,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58177548",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":49.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":164.8632505556,
        "Challenge_title":"SageMaker Managed Spot Training with Object Detection algorithm",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1234.0,
        "Challenge_word_count":490,
        "Platform":"Stack Overflow",
        "Poster_created_time":1226984969400,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Adelaide, Australia",
        "Poster_reputation_count":5789.0,
        "Poster_view_count":464.0,
        "Solution_body":"<p>I ran my script again today and it worked fine, no <code>botocore.exceptions.ClientError<\/code> exceptions.  Given that this issue affected both the Python SDK for Sagemaker and the console, I suspect it might have been an issue with the backend API and not my client code.<\/p>\n<p>Either way, it's working now.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1646271695950,
        "Solution_link_count":0.0,
        "Solution_readability":8.5,
        "Solution_reading_time":3.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":49.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1520413126203,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":37123.0,
        "Answerer_view_count":4058.0,
        "Challenge_adjusted_solved_time":47.4339141667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I trained a model in AZURE ML. Now i want to use that model in my ios app to predict the output\u00a0.<\/p>\n\n<p>How to download the model from AZURE and use it my swift code.<\/p>",
        "Challenge_closed_time":1525849618928,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525678856837,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has trained a model in AZURE ML and wants to use it in their iOS app to predict output. They are seeking guidance on how to download the model from AZURE and use it in their Swift code.",
        "Challenge_last_edit_time":1558224843256,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50209284",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":3.0,
        "Challenge_reading_time":2.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":47.4339141667,
        "Challenge_title":"How to use the trained model developed in AZURE ML",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":516.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1510206999776,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":81.0,
        "Poster_view_count":41.0,
        "Solution_body":"<p>As far as I know, the model could run in <strong>Azure Machine Learning Studio<\/strong>.It seems that you are unable to download it, the model could do nothing outside of Azure ML. <\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/41236871\/how-to-download-the-trained-models-from-azure-machine-studio\">Here<\/a> is a similar post for you to refer, I have also tried @Ahmet's \nmethod, but result is like @mrjrdnthms says.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1525850503192,
        "Solution_link_count":1.0,
        "Solution_readability":8.6,
        "Solution_reading_time":5.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1440734188430,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":1491.0,
        "Answerer_view_count":112.0,
        "Challenge_adjusted_solved_time":71.2072611111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a custom algorithm for text prediction. I want to deploy that in sagemaker. I am following this tutorial.\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf-example1.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/tf-example1.html<\/a>\n<br>\nThe only change from the tutorial is.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\niris_estimator = TensorFlow(entry_point='\/home\/ec2-user\/SageMaker\/sagemaker.py',\n                        role=role,\n                        output_path=model_artifacts_location,\n                        code_location=custom_code_upload_location,\n                        train_instance_count=1,\n                        train_instance_type='ml.c4.xlarge',\n                        training_steps=1000,\n                        evaluation_steps=100, source_dir=\".\/\", requirements_file=\"requirements.txt\")\n<\/code><\/pre>\n\n<p>.<\/p>\n\n<pre><code>%%time\nimport boto3\n\ntrain_data_location = 's3:\/\/sagemaker-&lt;my bucket&gt;'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>INFO: the dataset is at the root of the bucket.<\/p>\n\n<p>error log<\/p>\n\n<pre><code>ValueError: Error training sagemaker-tensorflow-2018-06-19-07-11-13-634: Failed Reason: AlgorithmError: uncaught exception during training: Import by filename is not supported.\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\n    fw.train()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/tf_container\/train_entry_point.py\", line 143, in train\n    customer_script = env.import_user_module()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 101, in import_user_module\n    user_module = importlib.import_module(script)\n  File \"\/usr\/lib\/python2.7\/importlib\/__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: Import by filename is not supported.\n<\/code><\/pre>",
        "Challenge_closed_time":1529655506467,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529399160327,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to deploy a custom algorithm for text prediction in Sagemaker. The error message states that \"Import by filename is not supported\" and occurs during training. The user has followed a tutorial and made only one change to the code.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50924494",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":19.9,
        "Challenge_reading_time":24.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":71.2072611111,
        "Challenge_title":"Sagemaker ImportError: Import by filename is not supported",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":470.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1440734188430,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":1491.0,
        "Poster_view_count":112.0,
        "Solution_body":"<p>I solved this issue, The problem was using absolute path for <code>entry_point<\/code>. \n<br>\nwhen you use a <code>source_dir<\/code> parameter the path to the <code>entry_point<\/code> should be relative to the <code>source_dir<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":9.1,
        "Solution_reading_time":3.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":30.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1508924024027,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":118.0,
        "Answerer_view_count":17.0,
        "Challenge_adjusted_solved_time":504.0913175,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Challenge_closed_time":1645469817663,
        "Challenge_comment_count":0,
        "Challenge_created_time":1643655088920,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has registered a scikit learn model on their MLflow Tracking server and is trying to access the model signature to retrieve a list of required inputs\/features. However, they are unable to find any utility or method in the mlflow API or the MLFlowClient API that will let them access the signature or inputs\/outputs attribute. They can see a list of inputs and outputs under each version of the model in the UI, but they don't want to download the artifacts and load them manually in their script.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":13.3,
        "Challenge_reading_time":12.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":4.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":504.0913175,
        "Challenge_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":904.0,
        "Challenge_word_count":173,
        "Platform":"Stack Overflow",
        "Poster_created_time":1466188731112,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Michigan",
        "Poster_reputation_count":414.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Solution_comment_count":3.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.5,
        "Solution_reading_time":5.3,
        "Solution_score_count":3.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":41.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1522870754323,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Redmond, WA, USA",
        "Answerer_reputation_count":366.0,
        "Answerer_view_count":173.0,
        "Challenge_adjusted_solved_time":740.4577925,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am able to create image and run azure ml service in one env but when I am moving to another env its not able to create image and failing with this error -<\/p>\n<p>Message: Received bad response from Model Management Service:\nResponse Code: 500\n{&quot;code&quot;:&quot;InternalServerError&quot;,&quot;statusCode&quot;:500,&quot;message&quot;:&quot;An internal server error occurred. Please try again. If the problem persists, contact support.&quot;,&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;8667981d-ef71-4e7c-a735-c43ef07b51b8&quot;}}'<\/p>\n<p>these logs are not helpful to find issue<\/p>",
        "Challenge_closed_time":1628037949483,
        "Challenge_comment_count":1,
        "Challenge_created_time":1627079442010,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in creating an image in Azure ML workspace. They are receiving an error message stating that there is a bad response from the Model Management Service with an internal server error. The logs provided are not helpful in identifying the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68505595",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":8.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":266.2520758334,
        "Challenge_title":"failing to create image in azure ml workspace",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1567209656790,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":417.0,
        "Poster_view_count":233.0,
        "Solution_body":"<p>As the error message said, this issue is an internal issue, please raise a support ticket to assign a support engineer to investigate it.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1629745090063,
        "Solution_link_count":0.0,
        "Solution_readability":12.7,
        "Solution_reading_time":1.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1513074641863,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Antarctica",
        "Answerer_reputation_count":155.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":14.8578630556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My question is somehow related to <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a> - however, the provided solution does not seem to work.<\/p>\n<p>I am constructing a simple model with heart-disease dataset but I wrap it into Pipeline as I use some featurization steps (scaling, encoding etc.) The full script below:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport pickle\n\n# data input\ndf = pd.read_csv('heart.csv')\n\n# numerical variables\nnum_cols = ['age',\n            'trestbps',\n            'chol',\n            'thalach',\n            'oldpeak'\n]\n\n# categorical variables\ncat_cols = ['sex',\n            'cp',\n            'fbs',\n            'restecg',\n            'exang',\n            'slope',\n            'ca',\n            'thal']\n\n# changing format of the categorical variables\ndf[cat_cols] = df[cat_cols].apply(lambda x: x.astype('object'))\n\n# target variable\ny = df['target']\n\n# features\nX = df.drop(['target'], axis=1)\n\n# data split:\n\n# random seed\nnp.random.seed(42)\n\n# splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2,\n                                                    stratify=y)\n\n# double check\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n# pipeline for numerical data\nnum_preprocessing = Pipeline([('num_imputer', SimpleImputer(strategy='mean')), # imputing with mean\n                                                   ('minmaxscaler', MinMaxScaler())]) # scaling\n\n# pipeline for categorical data\ncat_preprocessing = Pipeline([('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # filling missing values\n                                                ('onehot', OneHotEncoder(drop='first', handle_unknown='error'))]) # One Hot Encoding\n\n# preprocessor - combining pipelines\npreprocessor = ColumnTransformer([\n                                  ('categorical', cat_preprocessing, cat_cols),\n                                  ('numerical', num_preprocessing, num_cols)\n                                                           ])\n\n# initial model parameters\nlog_ini_params = {'penalty': 'l2', \n                  'tol': 0.0073559740277086005, \n                  'C': 1.1592424247511928, \n                  'fit_intercept': True, \n                  'solver': 'liblinear'}\n\n# model - Pipeline\nlog_clf = Pipeline([('preprocessor', preprocessor),\n                  ('clf', LogisticRegression(**log_ini_params))])\n\nlog_clf.fit(X_train, y_train)\n\n# dumping the model\nf = 'model\/log.pkl'\nwith open(f, 'wb') as file:\n    pickle.dump(log_clf, file)\n\n# loading it\nloaded_model = joblib.load(f)\n\n# double check on a single datapoint\nnew_data = pd.DataFrame({'age': 71,\n                         'sex': 0,\n                         'cp': 0,\n                         'trestbps': 112,\n                         'chol': 203,\n                         'fbs': 0,\n                         'restecg': 1,\n                         'thalach': 185,\n                         'exang': 0,\n                         'oldpeak': 0.1,\n                         'slope': 2,\n                         'ca': 0,\n                          'thal': 2}, index=[0])\n\nloaded_model.predict(new_data)\n\n<\/code><\/pre>\n<p>...and it works just fine.  Then I deploy the model to the Azure Web Service using these steps:<\/p>\n<ol>\n<li>I create the score.py file<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport json\n\ndef init():\n    global model\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n\ndef run(data):\n    try:\n        data = json.loads(data)\n        result = model.predict(data['data'])\n        # any data type, as long as it is JSON serializable.\n        return {'data' : result.tolist() , 'message' : 'Successfully classified heart diseases'}\n    except Exception as e:\n        error = str(e)\n        return {'data' : error , 'message' : 'Failed to classify heart diseases'}\n<\/code><\/pre>\n<ol>\n<li>I deploy the model:<\/li>\n<\/ol>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nws = Workspace.from_config()\n\nmodel = Model.register(workspace = ws,\n              model_path ='model\/log.pkl',\n              model_name = 'log',\n              tags = {'version': '1'},\n              description = 'Heart disease classification',\n              )\n\n# to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0'], conda_packages = ['scikit-learn==0.23.2'])\nenv.python.conda_dependencies = cd\n\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n\nmyenv = Environment.get(workspace=ws, name='env')\n\nmyenv.save_to_directory('.\/environ', overwrite=True)\n\naciconfig = AciWebservice.deploy_configuration(\n            cpu_cores=1,\n            memory_gb=1,\n            tags={'data':'heart disease classifier'},\n            description='Classification of heart diseases',\n            )\n\ninference_config = InferenceConfig(entry_script='score.py', environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                name='hd-model-log',\n                models=[model],\n                inference_config=inference_config,\n                deployment_config=aciconfig, \n                overwrite = True)\n\nservice.wait_for_deployment(show_output=True)\nurl = service.scoring_uri\nprint(url)\n<\/code><\/pre>\n<p>The deployment is fine:<\/p>\n<blockquote>\n<p>Succeeded\nACI service creation operation finished, operation &quot;Succeeded&quot;<\/p>\n<\/blockquote>\n<p>But I can not make any predictions with the new data. I try to use:<\/p>\n<pre><code>import pandas as pd\n\nnew_data = pd.DataFrame([[71, 0, 0, 112, 203, 0, 1, 185, 0, 0.1, 2, 0, 2],\n                         [80, 0, 0, 115, 203, 0, 1, 185, 0, 0.1, 2, 0, 0]],\n                         columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])\n<\/code><\/pre>\n<p>Following the answer from this topic (<a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a>) I transform the data:<\/p>\n<pre><code>test_sample = json.dumps({'data': new_data.to_dict(orient='records')})\n<\/code><\/pre>\n<p>And try to make some predictions:<\/p>\n<pre><code>import json\nimport requests\ndata = test_sample\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, data=data, headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>However, I encounter an error:<\/p>\n<blockquote>\n<p>200\n{'data': &quot;Expected 2D array, got 1D array instead:\\narray=[{'age': 71, 'sex': 0, 'cp': 0, 'trestbps': 112, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': &gt; 2}\\n {'age': 80, 'sex': 0, 'cp': 0, 'trestbps': 115, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': 0}].\\nReshape your data either using array.reshape(-1, &gt; 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&quot;, 'message': 'Failed to classify heart diseases'}<\/p>\n<\/blockquote>\n<p>How is it possible to adjust the input data to this form of predictions and add other output like predict_proba so I could store them in a separate output dataset?<\/p>\n<p>I know this error is somehow related either with the &quot;run&quot; part of the score.py file or the last code cell that calls the webservice, but I'm unable to find it.<\/p>\n<p>Would really appreciate some help.<\/p>",
        "Challenge_closed_time":1653558488392,
        "Challenge_comment_count":0,
        "Challenge_created_time":1653476427160,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in making predictions with Azure Machine Learning using new data that contains headers. The user has constructed a simple model with heart-disease dataset and wrapped it into Pipeline as they use some featurization steps. The model is deployed to the Azure Web Service, but the user is unable to make any predictions with the new data. The user is encountering an error related to the input data format and is seeking help to adjust the input data to this form of predictions and add other output like predict_proba so they could store them in a separate output dataset.",
        "Challenge_last_edit_time":1653505948283,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72376401",
        "Challenge_link_count":4,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":99.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":79,
        "Challenge_solved_time":22.7947866667,
        "Challenge_title":"Making predictions with Azure Machine learning with new data that contains headers (like pd.Dataframe)",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":280.0,
        "Challenge_word_count":761,
        "Platform":"Stack Overflow",
        "Poster_created_time":1513074641863,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Antarctica",
        "Poster_reputation_count":155.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>I believe I managed to solve the problem - even though I encountered some serious issues. :)<\/p>\n<ol>\n<li>As described here <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a> - I edited the <code>score.py<\/code> script:<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport numpy as np\nimport json\nimport pandas as pd\nimport numpy as np\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\nfrom inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n    \ndata_sample = PandasParameterType(pd.DataFrame({'age': pd.Series([0], dtype='int64'),\n                                                'sex': pd.Series(['example_value'], dtype='object'),\n                                                'cp': pd.Series(['example_value'], dtype='object'),\n                                                'trestbps': pd.Series([0], dtype='int64'),\n                                                'chol': pd.Series([0], dtype='int64'),\n                                                'fbs': pd.Series(['example_value'], dtype='object'),\n                                                'restecg': pd.Series(['example_value'], dtype='object'),\n                                                'thalach': pd.Series([0], dtype='int64'),\n                                                'exang': pd.Series(['example_value'], dtype='object'),\n                                                'oldpeak': pd.Series([0.0], dtype='float64'),\n                                                'slope': pd.Series(['example_value'], dtype='object'),\n                                                'ca': pd.Series(['example_value'], dtype='object'),\n                                                'thal': pd.Series(['example_value'], dtype='object')}))\n\ninput_sample = StandardPythonParameterType({'data': data_sample})\nresult_sample = NumpyParameterType(np.array([0]))\noutput_sample = StandardPythonParameterType({'Results':result_sample})\n\ndef init():\n    global model\n    # Example when the model is a file\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n@input_schema('Inputs', input_sample)\n@output_schema(output_sample)\ndef run(Inputs):\n    try:\n        data = Inputs['data']\n        result = model.predict_proba(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<ol start=\"2\">\n<li>In the deployment step I adjusted the <code>CondaDependencies<\/code>:<\/li>\n<\/ol>\n<pre><code># to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0', 'inference-schema==1.3.0'], conda_packages = ['scikit-learn==0.22.2.post1'])\nenv.python.conda_dependencies = cd\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n<\/code><\/pre>\n<p>as<\/p>\n<p>a) It is necessary to include <code>inference-schema<\/code> in the <code>Dependencies<\/code> file\nb) I downgraded <code>scikit-learn<\/code> to <code>scikit-learn==0.22.2.post1<\/code> version because of <a href=\"https:\/\/github.com\/hyperopt\/hyperopt\/issues\/668\" rel=\"nofollow noreferrer\">this issue<\/a><\/p>\n<p>Now, when I feed the model with new data:<\/p>\n<pre><code>new_data = {\n  &quot;Inputs&quot;: {\n    &quot;data&quot;: [\n      {\n        &quot;age&quot;: 71,\n        &quot;sex&quot;: &quot;0&quot;,\n        &quot;cp&quot;: &quot;0&quot;,\n        &quot;trestbps&quot;: 112,\n        &quot;chol&quot;: 203,\n        &quot;fbs&quot;: &quot;0&quot;,\n        &quot;restecg&quot;: &quot;1&quot;,\n        &quot;thalach&quot;: 185,\n        &quot;exang&quot;: &quot;0&quot;,\n        &quot;oldpeak&quot;: 0.1,\n        &quot;slope&quot;: &quot;2&quot;,\n        &quot;ca&quot;: &quot;0&quot;,\n        &quot;thal&quot;: &quot;2&quot;\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>And use it for prediction:<\/p>\n<pre><code>import json\nimport requests\ndata = new_data\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, str.encode(json.dumps(data)), headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>I get:<\/p>\n<p><code>200 [[0.02325369841858338, 0.9767463015814166]]<\/code><\/p>\n<p>Uff! Maybe someone will benefit from my painful learning path! :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1653559436590,
        "Solution_link_count":2.0,
        "Solution_readability":17.1,
        "Solution_reading_time":51.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":36.0,
        "Solution_word_count":293.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1333391842272,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"California, United States",
        "Answerer_reputation_count":1405.0,
        "Answerer_view_count":151.0,
        "Challenge_adjusted_solved_time":10042.3480277778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to build some <strong>neural network<\/strong> models for NLP and recommendation applications. The framework I want to use is <strong>TensorFlow<\/strong>. I plan to train these models and make predictions on Amazon web services. The application will be most likely <strong>distributed computing<\/strong>.<\/p>\n\n<p>I am wondering what are the pros and cons of SageMaker and EMR for TensorFlow applications?<\/p>\n\n<p>They both have TensorFlow integrated. <\/p>",
        "Challenge_closed_time":1573664904092,
        "Challenge_comment_count":0,
        "Challenge_created_time":1537510189837,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user wants to build neural network models for NLP and recommendation applications using TensorFlow and plans to train these models and make predictions on Amazon web services. They are looking for the pros and cons of using Amazon SageMaker and Amazon EMR for TensorFlow applications, as both have TensorFlow integrated and the application will most likely involve distributed computing.",
        "Challenge_last_edit_time":1537512451192,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52437599",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":8.3,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":8.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":10042.9761819444,
        "Challenge_title":"Pros and Cons of Amazon SageMaker VS. Amazon EMR, for deploying TensorFlow-based deep learning models?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":10776.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Poster_created_time":1341967360208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":2832.0,
        "Poster_view_count":368.0,
        "Solution_body":"<p>In general terms, they serve different purposes.<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/emr\/\" rel=\"noreferrer\"><strong>EMR<\/strong><\/a> is when you need to process massive amounts of data and heavily rely on Spark, Hadoop, and MapReduce (EMR = Elastic MapReduce). Essentially, if your data is in large enough volume to make use of the efficiencies of Spark, Hadoop, Hive, HDFS, HBase and Pig stack then go with EMR.<\/p>\n\n<p><strong>EMR Pros:<\/strong><\/p>\n\n<ul>\n<li>Generally, low cost compared to EC2 instances<\/li>\n<li>As the name suggests Elastic meaning you can provision what you need when you need it<\/li>\n<li>Hive, Pig, and HBase out of the box<\/li>\n<\/ul>\n\n<p><strong>EMR Cons:<\/strong><\/p>\n\n<ul>\n<li>You need a very specific use case to truly benefit from all the offerings in EMR. Most don't take advantage of its entire offering<\/li>\n<\/ul>\n\n<p><a href=\"https:\/\/aws.amazon.com\/sagemaker\/\" rel=\"noreferrer\"><strong>SageMaker<\/strong><\/a> is an attempt to make Machine Learning easier and distributed. The marketplace provides out of the box algos and models for quick use. It's a great service if you conform to the workflows it enforces. Meaning creating training jobs, deploying inference endpoints <\/p>\n\n<p><strong>SageMaker Pros:<\/strong><\/p>\n\n<ul>\n<li>Easy to get up and running with Notebooks<\/li>\n<li>Rich marketplace to quickly try existing models<\/li>\n<li>Many different example notebooks for popular algorithms<\/li>\n<li>Predefined kernels that minimize configuration<\/li>\n<li>Easy to deploy models<\/li>\n<li>Allows you to distribute inference compute by deploying endpoints<\/li>\n<\/ul>\n\n<p><strong>SageMaker Cons:<\/strong><\/p>\n\n<ul>\n<li>Expensive!<\/li>\n<li>Enforces a certain workflow making it hard to be fully custom<\/li>\n<li>Expensive!<\/li>\n<\/ul>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":12.6,
        "Solution_reading_time":22.46,
        "Solution_score_count":10.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":229.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1441651557140,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Parker, CO, USA",
        "Answerer_reputation_count":851.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":0.9014630556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to follow AWS Sagemaker tutorial to train a machine learning model with a Jupyter notebook environment. <\/p>\n\n<p>According to the tutorial, I'm supposed to copy the following code and run it to import required libraries and set environment variables. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker\/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\nprint(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n<\/code><\/pre>\n\n<p>And the expected outcome is below.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, I am getting this error.<\/p>\n\n<blockquote>\n  <p>KeyError                                  Traceback (most recent call last)\n   in ()\n       18               'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\n       19 my_region = boto3.session.Session().region_name # set the region of the instance\n  ---> 20 print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")<\/p>\n  \n  <p>KeyError: 'ap-northeast-2'<\/p>\n<\/blockquote>\n\n<p>I assume that this is happening because my region is <strong>\"ap-northeast-2\"<\/strong>. \nI have a feeling that I need to change the containers for my region.  <\/p>\n\n<p><strong>If my guess is correct, how can I find containers for my region?<\/strong><br>\n<strong>Also, am I overlooking anything else?<\/strong> <\/p>",
        "Challenge_closed_time":1576464384063,
        "Challenge_comment_count":1,
        "Challenge_created_time":1576462664100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering an error while trying to follow an AWS Sagemaker tutorial to train a machine learning model with a Jupyter notebook environment. The error is due to the absence of XGBoost containers for the user's region, which is \"ap-northeast-2\". The user is seeking guidance on how to find containers for their region and if they are overlooking anything else.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59349805",
        "Challenge_link_count":2,
        "Challenge_participation_count":3,
        "Challenge_readability":11.8,
        "Challenge_reading_time":32.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":0.4777675,
        "Challenge_title":"How to find XGBoost containers for different regions in AWS Sagemaker",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2197.0,
        "Challenge_word_count":264,
        "Platform":"Stack Overflow",
        "Poster_created_time":1539556112483,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Seoul, South Korea",
        "Poster_reputation_count":2954.0,
        "Poster_view_count":1143.0,
        "Solution_body":"<p>I expect your rational is correct. There isn't an entry for your region in the code. I don't know if there's a list of these containers per region. That being said, you find them in ECR (Elastic Container Registry). <\/p>\n\n<p>Keep in mind, that you can probably fix this quickly by switching to one of the supported regions. Otherwise:<\/p>\n\n<p>If AWS doesn't have a publicly listed container in your region, you can register the container yourself in AWS with ECR. You'll need to login to ECR using the AWS CLI and docker login.<\/p>\n\n<p>You can use the command <code>aws ecr get-login --region ap-northeast-2<\/code> in order to get the token you'll need for docker login.<\/p>\n\n<p>Then, clone this repo: <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a><\/p>\n\n<p>You can build this image locally and push it up to ECR. After that, login to the AWS console (or use the AWS CLI) and find the ARN of the image. It should match the format of the others in your code. <\/p>\n\n<p>After that, just add another key\/value entry into the code for your <code>containers<\/code> variable and use <code>'ap-northeast-2': '&lt;ARN of the docker image&gt;'<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1576465909367,
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":15.41,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":187.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1593662684510,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":41.0,
        "Answerer_view_count":11.0,
        "Challenge_adjusted_solved_time":119.8228480555,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am fairly new to TensorFlow (and SageMaker) and am stuck in the process of deploying a SageMaker endpoint. I have just recently succeeded in creating a Saved Model type model, which is currently being used to service a sample endpoint (the model was created externally). However, when I checked the image I am using for the endpoint, it says '...\/tensorflow-inference', which is not the direction I want to go in because I want to use a SageMaker TensorFlow serving container (I followed tutorials from the official TensorFlow serving GitHub repo-using sample models, and they are deployed correcting using the TensorFlow serving framework).<\/p>\n<p>Am I encountering this issue because my Saved Model does not have the correct 'serving' tag? I have not checked my tag sets yet but wanted to know if this would be the core reason to the problem. Also, most importantly, <strong>what are the differences between the two container types<\/strong>-I think having a better understanding of these two concepts would show me why I am unable to produce the correct image.<\/p>\n<hr \/>\n<p>This is how I deployed the sample endpoint:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Model(model_data =...)\n\npredictor = model.deploy(initial_instance_count=...)\n<\/code><\/pre>\n<p>When I run the code, I get a model, an endpoint configuration, and an endpoint. I got the container type by clicking on model details within the AWS SageMaker console.<\/p>",
        "Challenge_closed_time":1595219623720,
        "Challenge_comment_count":3,
        "Challenge_created_time":1594709403670,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing challenges in deploying a SageMaker endpoint using TensorFlow. They have created a Saved Model type model, but the image being used for the endpoint is 'tensorflow-inference' instead of the desired SageMaker TensorFlow serving container. The user is unsure if the issue is due to incorrect tag sets and is seeking a better understanding of the differences between the two container types.",
        "Challenge_last_edit_time":1594788261467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62889537",
        "Challenge_link_count":0,
        "Challenge_participation_count":5,
        "Challenge_readability":11.9,
        "Challenge_reading_time":19.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":141.7277916667,
        "Challenge_title":"TensorFlow Serving vs. TensorFlow Inference (container type for SageMaker model)",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1223.0,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Poster_created_time":1593662684510,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":41.0,
        "Poster_view_count":11.0,
        "Solution_body":"<p>There are different versions for the framework containers. Since the framework version I'm using is 1.15, the image I got had to be in a tensorflow-inference container. If I used versions &lt;= 1.13, then I would get sagemaker-tensorflow-serving images. The two aren't the same, but there's no 'correct' container type.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.5,
        "Solution_reading_time":4.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":17.4133102778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm facing an Error Message <br> &quot;The account-level service limit 'Number of elastic inference accelerators across all notebook instances.' is 0 Accelerators, with current utilization of 0 Accelerators and a request delta of 1 Accelerators. Please contact AWS support to request an increase for this limit.&quot;\n<a href=\"https:\/\/i.stack.imgur.com\/KWvQb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KWvQb.png\" alt=\"enter image description here\" \/><\/a>\n<br>\nI never used this account, I have 500$ in my account. But I'm unable to create the Juypter NoteBook on Aws SageMaker Instances.<\/p>\n<p>I already visited: <a href=\"https:\/\/stackoverflow.com\/questions\/53595157\/aws-sagemaker-deploy-fails\">AWS Sagemaker Deploy fails<\/a><\/p>",
        "Challenge_closed_time":1649144823327,
        "Challenge_comment_count":1,
        "Challenge_created_time":1649082135410,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is unable to create an AWS SageMaker instance due to an error message stating that the account-level service limit for the number of elastic inference accelerators across all notebook instances is 0, with a request delta of 1 accelerator. The user has not used this account before and has $500 in their account. They have tried visiting a related Stack Overflow post for help.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/71738894",
        "Challenge_link_count":3,
        "Challenge_participation_count":2,
        "Challenge_readability":12.1,
        "Challenge_reading_time":11.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":17.4133102778,
        "Challenge_title":"Unable to Create AWS Segamaker, Error: The account-level service limit 'Number of elastic inference accelerators across all notebook instances.'",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":109.0,
        "Challenge_word_count":106,
        "Platform":"Stack Overflow",
        "Poster_created_time":1571405791940,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"UK",
        "Poster_reputation_count":57.0,
        "Poster_view_count":39.0,
        "Solution_body":"<p>By default AWS limits the number of instances you can use, here you have the default <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">limits<\/a>. As the error message says, you have to request for a limit increase, you can do it from <a href=\"https:\/\/us-east-1.console.aws.amazon.com\/support\/home?region=us-east-1&amp;skipRegion=true#\/case\/create\" rel=\"nofollow noreferrer\">here<\/a>, it will take couple of days from my experience.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":15.4,
        "Solution_reading_time":6.4,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":51.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1620154324507,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"India",
        "Answerer_reputation_count":1169.0,
        "Answerer_view_count":2077.0,
        "Challenge_adjusted_solved_time":22.3193813889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>While trying out batch predictions in GCP Vertex AI for an AutoML model, the batch prediction results span over several files(which is not convenient from a user perspective). If it would have been a single batch prediction result file i.e. covering all the records in a single file, it would make the procedure much more simple.<\/p>\n<p>For instance, I had 5585 records in my input dataset file. The batch prediction results comprise of 21 files wherein each file has records in the range of 200-300, thus, covering 5585 records altogether.<\/p>",
        "Challenge_closed_time":1635085080550,
        "Challenge_comment_count":0,
        "Challenge_created_time":1635004730777,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an issue with batch predictions in GCP Vertex AI for an AutoML model. The batch prediction results were split into multiple files, which was inconvenient for the user. The user suggests that having a single batch prediction result file covering all the records would simplify the procedure.",
        "Challenge_last_edit_time":1635191683160,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69689785",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":7.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":22.3193813889,
        "Challenge_title":"batch predictions in GCP Vertex AI",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1708.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Poster_created_time":1492176971556,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"India",
        "Poster_reputation_count":105.0,
        "Poster_view_count":15.0,
        "Solution_body":"<p>Batch predictions on an image, text,video,tabular AutoML model, runs the jobs using distributed processing which means the data is distributed among an arbitrary cluster of virtual machines and is processed in an unpredictable order because of which you will get the prediction results stored across various files in Cloud Storage. Since the batch prediction output files are not generated with the same order as an input file, a feature request has been raised and you can track the update on this request from this <a href=\"https:\/\/issuetracker.google.com\/issues\/202080076\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>We cannot provide an ETA at this moment but you can follow the progress in the issue tracker and you can \u2018STAR\u2019 the issue to receive automatic updates and give it traction by referring to this <a href=\"https:\/\/developers.google.com\/issue-tracker\/guides\/subscribe#starring_an_issue\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n<p>However, if you are doing batch prediction for a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions#batch_request_input\" rel=\"nofollow noreferrer\">tabular AutoML model<\/a>, there you have the option to choose the BigQuery as storage where all the prediction output will be stored in a single table and then you can export the table data to a single CSV file.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1635092457400,
        "Solution_link_count":3.0,
        "Solution_readability":14.5,
        "Solution_reading_time":17.03,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":179.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":1267440784443,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Somewhere",
        "Answerer_reputation_count":15705.0,
        "Answerer_view_count":2150.0,
        "Challenge_adjusted_solved_time":0.1769625,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I can train a XGBoost model using Sagemaker images like so:<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.inputs import TrainingInput\nimport os\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\ntrain_file_name = 'train.csv'\nval_file_name = 'val.csv'\nrole_arn = 'arn:aws:iam::482777693429:role\/bla_instance_role'\n\nregion_name = boto3.Session().region_name\n\ns3_input_train = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, train_file_name), content_type='csv')\ns3_input_val = TrainingInput(s3_data='s3:\/\/{}\/{}\/{}'.format(s3_bucket_name, s3_prefix, val_file_name), content_type='csv')\n\nprint(type(s3_input_train))\n\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;13&quot;,\n        &quot;eta&quot;:&quot;0.15&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;objective&quot;:&quot;reg:squarederror&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\noutput_path = 's3:\/\/{}\/{}\/output'.format(s3_bucket_name, s3_prefix)\n\n# 1.5-1\n# 1.3-1\nestimator = sagemaker.estimator.Estimator(image_uri=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region_name, &quot;1.2-2&quot;), \n                                          hyperparameters=hyperparameters,\n                                          role=role_arn,\n                                          instance_count=1, \n                                          instance_type='ml.m5.2xlarge',\n                                          #instance_type='local', \n                                          volume_size=1, # 1 GB \n                                          output_path=output_path)\n\nestimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n<\/code><\/pre>\n<p>This work for all versions 1.2-2, 1.3-1 and 1.5-1. Unfortunately the following code only works for version 1.2-2:<\/p>\n<pre><code>import boto3\nimport os\nimport pickle as pkl \nimport tarfile\nimport pandas as pd\nimport xgboost as xgb\n\nfolder = r&quot;C:\\Somewhere&quot;\nos.chdir(folder)\n\ns3_prefix = 'some_model'\ns3_bucket_name = 'the_bucket'\nmodel_path = 'output\/sagemaker-xgboost-2022-04-30-10-52-29-877\/output\/model.tar.gz'\nsession = boto3.Session(profile_name='default')\nsession.resource('s3').Bucket(s3_bucket_name).download_file('{}\/{}'.format(s3_prefix, model_path), 'model.tar.gz')\nt = tarfile.open('model.tar.gz', 'r:gz')\nt.extractall()\n\nmodel_file_name = 'xgboost-model'\nwith open(model_file_name, &quot;rb&quot;) as input_file:\ne = pkl.load(input_file) \n<\/code><\/pre>\n<p>Otherwise I get a:<\/p>\n<pre><code>_pickle.UnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>Am I missing something? Is my &quot;pickle loading code wrong&quot;?<\/p>\n<p>The version of xgboost is 1.6.0 where I run the pickle code.<\/p>",
        "Challenge_closed_time":1651318339852,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651317702787,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while loading pickle files for xgboost images of version > 1.2-2 in Sagemaker. The user is able to train a model using Sagemaker images for all versions 1.2-2, 1.3-1 and 1.5-1, but the code for loading pickle files only works for version 1.2-2, resulting in an unpickling error for other versions. The user is seeking help to understand if they are missing something or if their pickle loading code is wrong. The version of xgboost used is 1.6.0.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72068059",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":17.4,
        "Challenge_reading_time":34.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":0.1769625,
        "Challenge_title":"cannot load pickle files for xgboost images of version > 1.2-2 in sagemaker - UnpicklingError",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Poster_created_time":1267440784443,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Somewhere",
        "Poster_reputation_count":15705.0,
        "Poster_view_count":2150.0,
        "Solution_body":"<p>I found the solution <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/2952\" rel=\"nofollow noreferrer\">here<\/a>. I will leave it in case someone come accross the same issue.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":11.8,
        "Solution_reading_time":2.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1388815483776,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":100.0,
        "Answerer_view_count":7.0,
        "Challenge_adjusted_solved_time":474.1775913889,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm having an issue to serve a model with reference to model registry. According to help, the path should look like this: <\/p>\n\n<p>models:\/model_name\/stage<\/p>\n\n<p>When I type in terminal: <br>\n<code>mlflow models serve -m models:\/ml_test_model1\/Staging --no-conda -h 0.0.0.0 -p 5003<\/code><\/p>\n\n<p>I got the error: <br>\n<code>mlflow.exceptions.MlflowException: Not a proper models:\/ URI: models:\/ml_test_model1\/Staging\/MLmodel. Models URIs must be of the form 'models:\/&lt;model_name&gt;\/&lt;version or stage&gt;'.<\/code><\/p>\n\n<p>Model is registered and visible in db and server. <br> \nIf I put absolute path, it works (experiment_id\/run_id\/artifacts\/model_name).<\/p>\n\n<p>mlflow version: 1.4 <br>\nPython version: 3.7.3<\/p>\n\n<p>Is it matter of some environmental settings or something different?<\/p>",
        "Challenge_closed_time":1577232243980,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575544859307,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while serving a model with reference to the model registry using MLflow. The error message suggests that the URI format is incorrect and should be in the form of 'models:\/<model_name>\/<version or stage>'. The model is registered and visible in the database and server, and the user is able to serve the model using an absolute path. The user is using MLflow version 1.4 and Python version 3.7.3 and is unsure if this is an environmental setting issue or something else.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59194004",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.4,
        "Challenge_reading_time":10.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":468.7179647222,
        "Challenge_title":"MLflow - Serving model by reference to model registry",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1225.0,
        "Challenge_word_count":104,
        "Platform":"Stack Overflow",
        "Poster_created_time":1575543357812,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Solution_body":"<p>That style of referencing model artefacts is fixed from mlflow v1.5 (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/pull\/2067\" rel=\"nofollow noreferrer\">Bug Fix<\/a>).<\/p>\n\n<p>You'll need to run <code>mlflow db upgrade &lt;db uri&gt;<\/code> to refresh your schemas before restarting your mlflow server.<\/p>\n\n<p>You may find listing registered models helpful:<\/p>\n\n<p><code>&lt;server&gt;:&lt;port&gt;\/api\/2.0\/preview\/mlflow\/registered-models\/list<\/code><\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1577251898636,
        "Solution_link_count":1.0,
        "Solution_readability":10.1,
        "Solution_reading_time":6.02,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":42.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1416193017423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Gensokyo",
        "Answerer_reputation_count":880.0,
        "Answerer_view_count":111.0,
        "Challenge_adjusted_solved_time":24.3919611111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have several million images in my training folder and want to specify a subset of them for training - the way to do this seems to be with a manifest file as described here.<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html<\/a><\/p>\n\n<p>But this seems to be geared towards labelled data. How can I start a sagemaker training job using sagemaker's Tensorflow <code>estimator.fit<\/code> with a list of files instead of the entire directory as input?<\/p>",
        "Challenge_closed_time":1570649242910,
        "Challenge_comment_count":0,
        "Challenge_created_time":1570561431850,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to use a subset of several million images in their training folder for training on Sagemaker with Tensorflow. They have found a way to do this with a manifest file, but it seems to be geared towards labelled data. The user is looking for a way to start a Sagemaker training job using Tensorflow estimator.fit with a list of files instead of the entire directory as input.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/58292566",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":24.3919611111,
        "Challenge_title":"How can I use a list of files as the training set on Sagemaker with Tensorflow?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":809.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Poster_created_time":1416193017423,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Gensokyo",
        "Poster_reputation_count":880.0,
        "Poster_view_count":111.0,
        "Solution_body":"<p>You can use an input type pipe parameter like so: <\/p>\n\n<pre><code>hyperparameters = {'save_checkpoints_secs':None,\n                   'save_checkpoints_steps':1000}\n\ntf_estimator = TensorFlow(entry_point='.\/my-training-file', role=role,\n                          training_steps=5100, evaluation_steps=100,\n                          train_instance_count=1, train_instance_type='ml.p3.2xlarge',\n                          input_mode = 'Pipe',\n                          train_volume_size=300, output_path = 's3:\/\/sagemaker-pocs\/test-carlsoa\/kepler\/model',\n                          framework_version = '1.12.0', hyperparameters=hyperparameters, checkpoint_path = None)\n<\/code><\/pre>\n\n<p>And create the manifest file pipe as an input:<\/p>\n\n<pre><code>train_data = sagemaker.session.s3_input('s3:\/\/sagemaker-pocs\/test-carlsoa\/manifest.json',\n                                        distribution='FullyReplicated',\n                                        content_type='image\/jpeg',\n                                        s3_data_type='ManifestFile',\n                                        attribute_names=['source-ref']) \n                                        #attribute_names=['source-ref', 'annotations']) \ndata_channels = {'train': train_data}\n<\/code><\/pre>\n\n<p>Note that you can use ManifestFile or AugmentedManifestFile depending on whether you have extra data or labels to provide. Now you can use data_channels as the input to the tf estimator:<\/p>\n\n<p><code>tf_estimator.fit(inputs=data_channels, logs=True)<\/code><\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":23.0,
        "Solution_reading_time":15.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":86.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1551797759387,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Krak\u00f3w, Poland",
        "Answerer_reputation_count":77.0,
        "Answerer_view_count":13.0,
        "Challenge_adjusted_solved_time":147.0432102778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained the AutoMl classification model on Vertex AI, unfortunately model does not work with batch predictions, whenever I try to score training dataset (same which was used for the successful model training) with batch predictions on Vertex AI I get a following error:<\/p>\n<p>&quot;Due to one or more errors, this training job was canceled on Nov 11, 2021 at 09:42AM&quot;.<\/p>\n<p>There is an option to get a details from this error and those say the following thing:<\/p>\n<p>&quot;Batch prediction job customer_value_label_cv_automl_gui encountered the following errors: INTERNAL&quot;<\/p>\n<p>Does anyone know what might be the reason for getting this kind of error? I am very surprised that the model cannot score the dataset that it was trained on. My dataset consists of 570 columns and about 300k of records. <a href=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DRbjn.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0MHrg.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Challenge_closed_time":1637235864267,
        "Challenge_comment_count":2,
        "Challenge_created_time":1636622783290,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user trained an AutoML classification model on Vertex AI, but encountered an internal error when attempting to use batch predictions on the same training dataset. The error message states that the batch prediction job encountered an internal error, and the user is unsure of the reason for the error. The dataset used for training consists of 570 columns and about 300k records.",
        "Challenge_last_edit_time":1636706508710,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/69925931",
        "Challenge_link_count":4,
        "Challenge_participation_count":3,
        "Challenge_readability":11.3,
        "Challenge_reading_time":15.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":170.3002713889,
        "Challenge_title":"Vertex AI model batch prediction failed with internal error",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":712.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Poster_created_time":1551797759387,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Krak\u00f3w, Poland",
        "Poster_reputation_count":77.0,
        "Poster_view_count":13.0,
        "Solution_body":"<p>We have been able to finally figure this out. As we were using model.batch_predict method described in the <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html\" rel=\"nofollow noreferrer\">official documentation<\/a> we unnecessary set the machine_type parameter. Finally, we were able to figure out that it was causing the issue, the machine was probably too weak. Once we removed this declaration this method started to use automatic resources and that solved the case. I wish Vertex AI errors were a little bit more informative because it took us a lot of trials and error to figure out.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":9.4,
        "Solution_reading_time":7.73,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":89.0,
        "Tool":"Vertex AI"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":8.8905811111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi community,   <br \/>\nI'm interested in what Azure Form Recogniser or another tool can do for us in terms of screening the correctness of uploaded applications. Think of applications for funding grants.  I haven't built any models yet, just wondering how feasible the below is.  A solution doesn't have to involve AI at all, but must be able to 'read' the uploaded documents.  <\/p>\n<p>A client uploads a set of standard documents (usually scanned PDF's)  using a file upload in our .net application.    <br \/>\nCan we:  <\/p>\n<ol>\n<li> Use form recogniser to extract key value pairs, after training a custom model.  <\/li>\n<li> Run a loop over these pairs to find missing information e.g. they forgot to add their date of birth, or didn't enter their income.  <\/li>\n<li> Report back to the user the missing information so they can correct the document and reupload them?  <br \/>\nPreferably in real time?  So they hit submit on the webpage, it extracts, analyses and provides a result in a few seconds?  <\/li>\n<\/ol>",
        "Challenge_closed_time":1647513011952,
        "Challenge_comment_count":0,
        "Challenge_created_time":1647481005860,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information on whether Azure Form Recognizer or another tool can be used to screen the correctness of uploaded applications, such as funding grant applications. They want to know if it is possible to use form recognizer to extract key value pairs, find missing information, and report back to the user in real-time so they can correct the document and re-upload it.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/775440\/form-recognizer-to-report-on-missing-information-i",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.0,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":8.8905811111,
        "Challenge_title":"Form recognizer to report on missing information in (near) real-time",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":181,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d4ec46af-03e5-46ab-ace4-d0dd3b8d93ba\">@Andrew Robertson  <\/a> Yes, you can use Azure form recognizer to analyze a document that is passed to the API and use the result of the analyze operation to report any missing fields in the form back to the user. This is the most widely used use case by most of the customers.     <\/p>\n<p>Form recognizer comes with a set of prebuilt APIs where it can extract common information from invoices, business cards, receipts etc. If you have a form that does not conform to the prebuilt API standards you need to create a custom model to extract the text in the form of a tags and their key:value pairs. The custom models require some basic training with some test forms and if all the forms that need extraction follow the same layout or guidelines the extraction results will be good.     <\/p>\n<p>In the case of custom forms the results are provided in almost real time where the form is submitted or <a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/form-recognizer-api-v3-0-preview-2\/operations\/AnalyzeDocument\">POST<\/a> request is sent to the API and an operation id is returned to retrieve the results using <a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/form-recognizer-api-v3-0-preview-2\/operations\/GetAnalyzeDocumentResult\">GET<\/a>.  Depending on your pricing tier of your resource if you intend to perform these actions synchronously you might have to limit the rate of requests sent to the API to avoid any TPS errors. If you are using async operations with a slight delay to fetch the results then you can design an application that can take large number of documents and provide results to the users within a short span of time.     <\/p>\n<p>I hope the above information is helpful.    <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":14.4,
        "Solution_reading_time":27.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":292.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":27.2427933334,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I think it would be beneficial to select and delete several experiments at the same time.<br>\nNow I have to delete one by one and it is very time consuming.<\/p>\n<p>Thank you!<\/p>",
        "Challenge_closed_time":1652874850582,
        "Challenge_comment_count":0,
        "Challenge_created_time":1652776776526,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is requesting a feature to be able to select and delete multiple experiments at once, as deleting them one by one is time-consuming.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/remove-multiple-runs-at-the-same-time\/2435",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":4.8,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":27.2427933334,
        "Challenge_title":"Remove multiple runs at the same time",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":249.0,
        "Challenge_word_count":39,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hey <a class=\"mention\" href=\"\/u\/lucasventura\">@lucasventura<\/a>, you can do it like <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/runs-table#filter-and-delete-unwanted-runs\">this<\/a>.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":30.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":18.5603936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>It was written in the Microsoft AzureML documentation, &quot;A run represents a single trial of an experiment. Runs are used to monitor the asynchronous execution of a trial&quot; and A Run object is also created when you submit or start_logging with the Experiment class.&quot;<\/p>\n<p>Related to <code>start_logging<\/code>, as far as I know, when we have simply started the run by executing this <code>start logging<\/code> method. We have to stop, or complete by <code>complete<\/code> method when the run is completed. This is because  <code>start_logging<\/code> is a synchronized way of creating an experiment. However, Run object created from <code>start_logging<\/code> is to monitor the asynchronous execution of a trial.<\/p>\n<p>Can anyone clarify whether <code>start_logging<\/code> will start asynchronous execution or synchronous execution?<\/p>",
        "Challenge_closed_time":1660048147607,
        "Challenge_comment_count":0,
        "Challenge_created_time":1659981330190,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is seeking clarification on whether the \"start_logging\" method in AzureML starts asynchronous or synchronous execution, as the documentation states that the Run object created from it is used to monitor asynchronous execution, but the user believes it to be a synchronized way of creating an experiment.",
        "Challenge_last_edit_time":1660267366787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73282180",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.5603936111,
        "Challenge_title":"In AzureML, start_logging will start asynchronous execution or synchronous execution?",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Poster_created_time":1525287643000,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Ottawa, ON, Canada",
        "Poster_reputation_count":27.0,
        "Poster_view_count":8.0,
        "Solution_body":"<p><strong>start_logging<\/strong> will be considered as <strong>asynchronous<\/strong> execution as this generates the multiple interactive run sessions. In a specific experiment, there is a chance of multiple interactive sessions, that work parallelly and there will be no scenario to be followed in sequential.<\/p>\n<p>The individual operation can be performed and recognized based on the parameters like <strong>args<\/strong>  and <strong>kwargs<\/strong>.<\/p>\n<p>When the start_logging is called, then an interactive run like <strong>jupyter notebook<\/strong> was created. The complete metrics and components which are created when the start_logging was called will be utilized. When the output directory was mentioned for each interactive run, based on the args value, the output folder will be called seamlessly.<\/p>\n<p>The following code block will help to define the operation of start_logging<\/p>\n<pre><code>experiment = Experiment(your_workspace, &quot;your_experiment_name&quot;)\n   run = experiment.start_logging(outputs=None, snapshot_directory=&quot;.&quot;, display_name=&quot;test&quot;)\n   ...\n   run.log_metric(&quot;Accuracy_Value&quot;, accuracy)\n   run.complete()\n<\/code><\/pre>\n<p>the below code block will be defining the basic syntax of start_logging<\/p>\n<pre><code>start_logging(*args, **kwargs)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":14.4,
        "Solution_reading_time":17.16,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":147.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1348841548500,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":5182.0,
        "Answerer_view_count":315.0,
        "Challenge_adjusted_solved_time":1.2466388889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm very new to SageMaker, and I've run into a bit of confusion as to how to achieve the output I am looking for. I am currently attempting to use the built-in RCF algorithm to perform anomaly detection on a list of stock volumes, like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\n<\/code><\/pre>\n<p>I have created a training job, model, and endpoint, and I'm trying now to invoke the endpoint using boto3. My current code looks like this:<\/p>\n<pre><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot; &quot;.join(apple_stock_volumes)\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>What I wanted was to get an anomaly score for every datapoint, and then to alert if the anomaly score was a few standard deviations above the mean. However, what I'm actually receiving is just a single anomaly score. The following is my output:<\/p>\n<pre><code>{'scores': [{'score': 0.7164874384}]}\n<\/code><\/pre>\n<p>Can anyone explain to me what's going on here? Is this an average anomaly score? Why can't I seem to get SageMaker to output a list of anomaly scores corresponding to my data? Thanks in advance!<\/p>\n<p>Edit: I have already trained the model on a csv of historical volume data for the last year, and I have created an endpoint to hit.<\/p>\n<p>Edit 2: I've accepted @maafk's answer, although the actual answer to my question was provided in one of his comments. The piece I was missing was that each data point must be on a new line in your csv input to the endpoint. Once I substituted <code>body = &quot; &quot;.join(apple_stock_volumes)<\/code> for <code>body = &quot;\\n&quot;.join(apple_stock_volumes)<\/code>, everything worked as expected.<\/p>",
        "Challenge_closed_time":1601377832670,
        "Challenge_comment_count":0,
        "Challenge_created_time":1601377003873,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to use the built-in RCF algorithm in SageMaker for anomaly detection on a list of stock volumes. They have created a training job, model, and endpoint, and are trying to invoke the endpoint using boto3. However, they are only receiving a single anomaly score instead of a score for every datapoint. The user is seeking an explanation for this and wants to know how to get SageMaker to output a list of anomaly scores corresponding to their data.",
        "Challenge_last_edit_time":1601394242040,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/64118186",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":24.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":0.2302213889,
        "Challenge_title":"Getting an anomaly score for every datapoint in SageMaker?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":54.0,
        "Challenge_word_count":273,
        "Platform":"Stack Overflow",
        "Poster_created_time":1589340693532,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>In your case, you'll want to get the standard deviation from getting the scores from historical stock volumes, and figuring out what your anomaly score is by calculating <code>3 * standard deviation<\/code><\/p>\n<p>Update your code to do inference on <em>multiple<\/em> records at once<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>apple_stock_volumes = [123412, 465125, 237564, 238172]\ndef inference():\n    client = boto3.client('sagemaker-runtime')\n    \n    body = &quot;\\n&quot;.join(apple_stock_volumes). # New line for each record\n    response = client.invoke_endpoint(\n        EndpointName='apple-volume-endpoint',\n        Body=body,\n        ContentType='text\/csv'\n    )\n    inference = json.loads(response['Body'].read())\n    print(inference)\n\ninference()\n<\/code><\/pre>\n<p>This will return a list of scores<\/p>\n<p>Assuming <code>apple_stock_volumes_df<\/code> has your volumes and the scores (after running inference on each record):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>score_mean = apple_stock_volumes_df['score'].mean()\nscore_std = apple_stock_volumes_df['score'].std()\nscore_cutoff = score_mean + 3*score_std\n<\/code><\/pre>\n<p>There is a great example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a> showing this<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1601398729940,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":17.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":112.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":24.0739286111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Challenge_closed_time":1661603882123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1661517215980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is encountering a \"BAD_REQUEST\" error while trying to fetch runs from search_runs in mlflow while training a machine learning model in an Azure environment. The error message indicates an issue with the input string. The mlflow version being used is 1.28.0 and the user has successfully created and run jobs in Azure studio.",
        "Challenge_last_edit_time":1661625379892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":12.2,
        "Challenge_reading_time":8.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24.0739286111,
        "Challenge_title":"Getting Bad request while searching run in mlflow",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Poster_created_time":1582101477803,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":171.0,
        "Poster_view_count":53.0,
        "Solution_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":9.0,
        "Solution_readability":17.0,
        "Solution_reading_time":16.36,
        "Solution_score_count":1.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":94.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":14.3124580556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi all;    <\/p>\n<p>I am creating an app to handle volunteers for a NGO. One of the things volunteers will be allowed to do is create events &amp; other projects. And a concern we have is a troll enters something hateful that they then point reporters to.    <\/p>\n<p>So... and I know this is probably beyond ML at present (but it never hurts to ask), is there an AzureML service\/app that we could feed all the text for the proposed event and it would rate the likelihood of it being problematic?     <\/p>\n<p>And if there isn't one that can work with no data from us, is there a service\/app that we could train for this once we have say 100,000 projects? We have no data for this at present but after running for a year we likely will have 100,000 projects to use for training.    <\/p>\n<p>thanks - dave<\/p>",
        "Challenge_closed_time":1671617821856,
        "Challenge_comment_count":0,
        "Challenge_created_time":1671566297007,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is creating an app for an NGO where volunteers can create events and projects. They are concerned about trolls entering hateful content and want to know if there is an AzureML service that can rate the likelihood of problematic content. They also ask if there is a service that can be trained with data from their app after running for a year.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1136505\/recognizing-problematic-content",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":9.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":14.3124580556,
        "Challenge_title":"Recognizing problematic content",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":150,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=2e000d14-7fa3-4e41-9299-8306855b228f\">@David Thielen  <\/a> It is possible to use Azure language services' <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cognitive-services\/language-service\/sentiment-opinion-mining\/overview\">sentiment analysis and opinion mining<\/a> offering to identify such problematic comments from your projects. This feature of the service does not require any training as the models behind the service are trained to provide sentiment of the text passed to the service. It returns either a postive, neutral or negative sentiment along with a score to justify the same. I think this should help you solve the issue that you are currently facing to identify trolls.    <\/p>\n<p>The service currently does not have an option to train custom sentiment models, the current recommendation for custom text is to use the azure machine learning service to train your own models.     <\/p>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.3,
        "Solution_reading_time":16.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":149.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1646907459852,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation_count":1624.0,
        "Answerer_view_count":1376.0,
        "Challenge_adjusted_solved_time":14.2479583334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I developed a designer to implement regression models in azure machine learning studio. I have taken the data set pill and then split the data set into train and test in prescribed manner. When I am trying to implement the evaluation metrics and run the pipeline, it was showing a warning and error in the moment I called the dataset for the operation. I am bit confused, with the same implementation, when i tried to run with linear regression and it worked as shown in the image. If the same approach is used to implement logistic regression it was showing some warning and error in building the evaluation metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DbEeq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>the above success is in linear regression. When it comes to logistic regression it was showing the warning and error in pipeline.<\/p>\n<p>Any help is appreciated.<\/p>",
        "Challenge_closed_time":1664021303563,
        "Challenge_comment_count":0,
        "Challenge_created_time":1663970010913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user encountered an error in implementing evaluation metrics in a regression model using Azure ML Designer. The error occurred when calling the dataset for the operation, and the user is confused as to why the same approach worked for linear regression but not for logistic regression. The user is seeking help to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/73833320",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":14.2479583334,
        "Challenge_title":"parameters error in azure ML designer in evaluation metrics in regression model",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Poster_created_time":1652172570283,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":19.0,
        "Poster_view_count":16.0,
        "Solution_body":"<p>Creating a sample pipeline with designer with mathematical format.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kCx3A.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>We need to create a compute instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7bQA5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NQWZV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MpxPY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bAfEv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the compute instance and click on create<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wIS0d.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Now the import data warning will be removed. In the same manner, we will be getting similar error in other pills too.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/zFK74.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zFK74.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yk5gY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Create a mathematical format. If not needed for your case, try to remove that math operation and give the remaining.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uhKlv.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Assign the column set. Select any option according to the requirement.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mcNZe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Finally, we can find the pills which have no warning or error.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":20.0,
        "Solution_readability":14.1,
        "Solution_reading_time":30.29,
        "Solution_score_count":1.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":189.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1442180190107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Challenge_adjusted_solved_time":2.4873480555,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I estimated a factorization machine model in sagemaker and it saved a file <code>model.tar.gz<\/code> into an s3 folder.<\/p>\n\n<p>Is there a way I can load this file in Python and access the parameter of the model, i.e. the factors, directly?<\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":1575844058683,
        "Challenge_comment_count":0,
        "Challenge_created_time":1575827808930,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has created a factorization machine model in sagemaker and saved it as a file in an s3 folder. They are seeking guidance on how to load the file in Python and access the model's parameters.",
        "Challenge_last_edit_time":1575835104230,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59238265",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":3.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4.5138202778,
        "Challenge_title":"sagemaker - factorization machines - deserialize model",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":164.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Poster_created_time":1456487654208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":1464.0,
        "Poster_view_count":62.0,
        "Solution_body":"<p>As of April 2019: yes. An official AWS blog post was created to show how to open the SageMaker Factorization Machines artifact and extract its parameters: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/<\/a><\/p>\n\n<p>That being said, be aware that Amazon SageMaker built-in algorithm are primarily built for deployment on AWS, and only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">SageMaker XGBoost<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">SageMaker BlazingText<\/a> are designed to produce artifacts interoperable with their open-source equivalent.<\/p>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":26.7,
        "Solution_reading_time":12.79,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":70.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1250158552416,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Challenge_adjusted_solved_time":6.8599055556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to execute Python script from Azure machine learning studio. I had a script bundle(zip file) connect to the Python script as input. There are python files, txt files and other type of files in this zip file. My question is how do I get the file path from this zip file. For example, if I have language model in this  zip file, named lm.pcl, what's the file path of this language model? \nThanks!<\/p>",
        "Challenge_closed_time":1539582490283,
        "Challenge_comment_count":0,
        "Challenge_created_time":1539557794623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to execute a Python script from Azure machine learning studio using a script bundle (zip file) that contains various types of files including Python and txt files. The user is seeking guidance on how to access the file path of a specific language model (lm.pcl) within the zip file.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52807787",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":4.7,
        "Challenge_reading_time":5.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6.8599055556,
        "Challenge_title":"Azure machine learning studio get access to the file in upload zip file",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":679.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Poster_created_time":1337362023536,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":581.0,
        "Poster_view_count":49.0,
        "Solution_body":"<p>They're available under the <code>.\/Script Bundle<\/code> directory. For example, if you were to load a pickled model from the zip file, you'd write something along these lines:<\/p>\n\n<pre><code>import pandas as pd\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\n    model = pickle.load(open(\".\/Script Bundle\/model.pkl\", \"rb\"))\n    ...\n<\/code><\/pre>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.0,
        "Solution_reading_time":4.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4.7084425,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi MSFT Community,     <\/p>\n<p>I followed this guide to set up a GPU: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-ubuntu-intro\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-ubuntu-intro<\/a>    <\/p>\n<p>VM: Standard NC12_Promo, 12 vCPUs, 112 Gib RAM    <br \/>\nOperating System: Linux    <br \/>\nOffer: Ubuntu-1804    <\/p>\n<p>I am ready to start deep learning training but I am confused about what to do next. I am doing a medical image classification project. I have 1 millions images store in Azure blob now. Do I need to download them to my VM in order to train? Or is it a better way to access image efficiently?    <\/p>\n<p>What are some good tutorials to set up the experiments? I've read a lot of documentation but still confused.     <\/p>\n<p>Thank you very much!    <br \/>\nBest Regards,    <br \/>\nClaire<\/p>",
        "Challenge_closed_time":1605057040680,
        "Challenge_comment_count":0,
        "Challenge_created_time":1605040090287,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking guidance on how to proceed with deep learning training on Azure for a medical image classification project. They are unsure whether they need to download the 1 million images stored in Azure blob to their VM for training or if there is a more efficient way to access them. They are also looking for tutorials to help set up the experiments.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/158168\/deep-learning-training-on-azure-steps-and-tutorial",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":9.0,
        "Challenge_reading_time":11.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":4.7084425,
        "Challenge_title":"Deep learning training on Azure steps and tutorial",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":127,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=d324c18d-0c6a-4b3e-878e-30b46421559e\">@gecheng  <\/a>     <br \/>\ncheck on the below AI training modules.    <br \/>\n<a href=\"https:\/\/aischool.microsoft.com\/en-us\/services\/learning-paths\">https:\/\/aischool.microsoft.com\/en-us\/services\/learning-paths<\/a>    <\/p>\n<p>AI Lab    <br \/>\n<a href=\"https:\/\/www.microsoft.com\/en-us\/ai\/ai-lab-projects\">https:\/\/www.microsoft.com\/en-us\/ai\/ai-lab-projects<\/a>    <\/p>\n<p>AI module gallery    <br \/>\n<a href=\"https:\/\/gallery.azure.ai\/browse\">https:\/\/gallery.azure.ai\/browse<\/a>    <\/p>\n<p>----------    <\/p>\n<p>Please don\u2019t forget to &quot;Accept the answer&quot; and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":9.67,
        "Solution_score_count":2.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":54.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1416346350292,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Challenge_adjusted_solved_time":3.3754333334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run custom python\/sklearn sagemaker script on AWS, basically learning from these examples: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>All works fine, if define the arguments, train the model and output the file:<\/p>\n<pre><code>parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\nparser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\nparser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n# train the model...\n\njoblib.dump(model, os.path.join(args.model_dir, &quot;model.joblib&quot;))\n<\/code><\/pre>\n<p>And call the job with:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test'}, wait=False)\n<\/code><\/pre>\n<p>In this case model gets stored on different auto-generated bucket, which I do not want. I want to get the output (.joblib file) in the same s3 bucket I took data from. So I add the parameter <code>model-dir<\/code>:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test', `model-dir`: 's3:\/\/path\/to\/model'}, wait=False)\n<\/code><\/pre>\n<p>But it results in error:\n<code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/path\/to\/model\/model.joblib'<\/code><\/p>\n<p>Same happens if I hardcode the output path inside the training script.<\/p>\n<p>So the main question, how can I get the output file in the bucket of my choice?<\/p>",
        "Challenge_closed_time":1610545645387,
        "Challenge_comment_count":0,
        "Challenge_created_time":1610533493827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to run a custom python\/sklearn sagemaker script on AWS and wants to save the output (.joblib file) in the same S3 bucket from where the data was taken. However, when the user adds the parameter \"model-dir\" to specify the output location, it results in an error \"FileNotFoundError: [Errno 2] No such file or directory\". The user is seeking a solution to save the output file in the bucket of their choice.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/65699980",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":23.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":3.3754333334,
        "Challenge_title":"Change model file save location on AWS SageMaker Training Job",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1244.0,
        "Challenge_word_count":159,
        "Platform":"Stack Overflow",
        "Poster_created_time":1572957474856,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":123.0,
        "Poster_view_count":18.0,
        "Solution_body":"<p>You can use parameter <code>output_path<\/code> when you define the estimator. If you use the\n<code>model_dir<\/code> I guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on S3). You can take a look at my <a href=\"https:\/\/github.com\/roccopietrini\/TFSagemakerDetection\" rel=\"nofollow noreferrer\">repo<\/a> for this specific case.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":5.66,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":4770.0555555556,
        "Challenge_answer_count":0,
        "Challenge_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_closed_time":1632465008000,
        "Challenge_comment_count":3,
        "Challenge_created_time":1615292808000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The SageMaker Operator Types fail the KubeBuilder V2 custom CRD definition validation check due to unescaped regex patterns. The user expected KubeBuilder to generate a CRD specification that includes AWS SageMaker Operator Types. The issue can be resolved by escaping the regex pattern with quotes.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":10.3,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4770.0555555556,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":23,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"Thanks for using amazon-sagemaker-operator-for-k8s. Please help us with the steps to replicate the issue, especially the installation\r\n\r\nOfficial documentation for reference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-operators-for-kubernetes.html I ran into this issue while myself and was resolved by making sure the SageMaker operator was applied and running by verifying with kubectl -n sagemaker-k8s-operator-system get pods Closing since there has been no activity in 90+ days",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":6.49,
        "Solution_score_count":null,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":162.5082036111,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,  <br \/>\nI've trained a classification model using Azure AutoML. In the &quot;Output&quot; folder of the best model page I can see these files:  <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/74033-image.png?platform=QnA\" alt=\"![![74035-image.png\" \/>]<a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/74033-image.png?platform=QnA\">1<\/a>]<a href=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/74033-image.png?platform=QnA\">1<\/a><\/p>\n<p><strong>1<\/strong>- How should I use this model to predict new observations. I want to do this on Azure Machine Learning Studio, so no need to deploy it as a web service or take it to a local computer as a pickle file.<\/p>\n<p><strong>2<\/strong>- I saw a few examples where people downloading their model as a PKL file and loading and running it to get predictions. I tried it by using the following code:<\/p>\n<pre><code>import pickle  \nPkl_Filename = &quot;testPickle.pkl&quot;  \nwith open(Pkl_Filename, 'rb') as file:    \n    Pickled_LR_Model = pickle.load(file)  \nPickled_LR_Model  \n<\/code><\/pre>\n<p><strong>and got the below error:<\/strong><\/p>\n<p>ModuleNotFoundError Traceback (most recent call last)  <br \/>\n&lt;ipython-input-14-1e47995f2929&gt; in &lt;module&gt;  <br \/>\n2 Pkl_Filename = &quot;testPickle.pkl&quot;  <br \/>\n3 with open(Pkl_Filename, 'rb') as file:  <br \/>\n----&gt; 4 Pickled_LR_Model = pickle.load(file)  <br \/>\n5  <br \/>\n6 Pickled_LR_Model<\/p>\n<p>ModuleNotFoundError: No module named 'azureml.automl.runtime._ml_engine.featurizer_suggestion'<\/p>\n<p>Also used the <em>joblib<\/em> library to load the model and got the same error. Please help me with detailed step-by-step instructions (including scripts) if it is possible. I'm new to Azure Machine Learning Studio.<\/p>",
        "Challenge_closed_time":1615407389616,
        "Challenge_comment_count":1,
        "Challenge_created_time":1614822360083,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has trained a classification model using Azure AutoML and is looking for guidance on how to use the model to predict new observations on Azure Machine Learning Studio without deploying it as a web service or taking it to a local computer as a pickle file. They have tried to download the model as a PKL file and load it using both pickle and joblib libraries, but encountered a ModuleNotFoundError. The user is seeking detailed step-by-step instructions, including scripts, to resolve the issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/297882\/how-to-use-a-model-trained-by-azure-automl",
        "Challenge_link_count":3,
        "Challenge_participation_count":3,
        "Challenge_readability":12.0,
        "Challenge_reading_time":23.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":162.5082036111,
        "Challenge_title":"How to use a model trained by Azure AutoML",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":200,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>I fixed this problem by creating a new compute instance and using it to load the pickle file. Sounds strange but it seems this error happens due to a mismatch between the azureml sdk on the jupyter instance and on the compute instance.<\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":8.1,
        "Solution_reading_time":2.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":43.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1626973229036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":199.0,
        "Answerer_view_count":37.0,
        "Challenge_adjusted_solved_time":30.4097575,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an already trained a TensorFlow model outside of SageMaker.<\/p>\n<p>I am trying to focus on deployment\/inference but I am facing issues with inference.<\/p>\n<p>For deployment I did this:<\/p>\n<pre><code>from sagemaker.tensorflow.serving import TensorFlowModel\ninstance_type = 'ml.c5.xlarge' \n\nmodel = TensorFlowModel(\n    model_data=model_data,\n    name= 'tfmodel1',\n    framework_version=&quot;2.2&quot;,\n    role=role, \n    source_dir='code',\n)\n\npredictor = model.deploy(endpoint_name='test', \n                                       initial_instance_count=1, \n                                       tags=tags,\n                                       instance_type=instance_type)\n<\/code><\/pre>\n<p>When I tried to infer the model I did this:<\/p>\n<pre><code>import PIL\nfrom PIL import Image\nimport numpy as np\nimport json\nimport boto3\n\nimage = PIL.Image.open('img_test.jpg')\nclient = boto3.client('sagemaker-runtime')\nbatch_size = 1\nimage = np.asarray(image.resize((512, 512)))\nimage = np.concatenate([image[np.newaxis, :, :]] * batch_size)\nbody = json.dumps({&quot;instances&quot;: image.tolist()})\n\nioc_predictor_endpoint_name = &quot;test&quot;\ncontent_type = 'application\/x-image'   \nioc_response = client.invoke_endpoint(\n    EndpointName=ioc_predictor_endpoint_name,\n    Body=body,\n    ContentType=content_type\n )\n<\/code><\/pre>\n<p>But I have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/x-image&quot;}&quot;.\n<\/code><\/pre>\n<p>I also tried:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\n\npredictor = Predictor(ioc_predictor_endpoint_name)\ninference_response = predictor.predict(data=body)\nprint(inference_response)\n<\/code><\/pre>\n<p>And have this error:<\/p>\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: application\/octet-stream&quot;}&quot;.\n<\/code><\/pre>\n<p>What can I do ? I don't know if I missed something<\/p>",
        "Challenge_closed_time":1651191185627,
        "Challenge_comment_count":0,
        "Challenge_created_time":1651077733827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has encountered an error while trying to infer a pre-trained TensorFlow model deployed outside of SageMaker using an image. The user has deployed the model successfully but is facing issues with inference. The error message indicates an unsupported media type for the image format. The user has tried different methods for inference but still faces the same error. The user is seeking guidance on how to resolve the issue.",
        "Challenge_last_edit_time":1651081710500,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72032469",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":27.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":31.5143888889,
        "Challenge_title":"How to infer a tensorflow pre trained deployed model with an image?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":145.0,
        "Challenge_word_count":191,
        "Platform":"Stack Overflow",
        "Poster_created_time":1606642099552,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":371.0,
        "Poster_view_count":55.0,
        "Solution_body":"<p>Have you tested this model locally? How does inference work with your TF model locally? This should show you how the input needs to be formatted for inference with that model in specific. Application\/x-image data format should be fine. Do you have a custom inference script? Check out this link here for adding an inference script with will let you control pre\/post processing and you can log each line to capture the error: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":9.9,
        "Solution_reading_time":7.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":77.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":52.1266738889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there any restriction on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file?  <\/p>\n<p>Does it cause latency in realtime data processing and getting the prediction results from the pickle file if we have a  model that let's say it 5MB and the other one is 500MB (The bigger file has better performance in terms of accuracy)?  <br \/>\nThanks,  <\/p>\n<p>John<\/p>",
        "Challenge_closed_time":1599800075416,
        "Challenge_comment_count":2,
        "Challenge_created_time":1599612419390,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is inquiring about any restrictions on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file. They are also concerned about the potential latency in real-time data processing and getting prediction results from a larger pickle file, even if it has better performance in terms of accuracy.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/89630\/ml-pickle-file-size-azure-machine-learning-service",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":9.8,
        "Challenge_reading_time":5.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":52.1266738889,
        "Challenge_title":"ML Pickle file size Azure Machine Learning Service",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":79,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=ccc37df1-9b57-4e84-920a-f92d8316aa7c\">@JJA  <\/a> Thanks, For ACI we recommend not using a model over 1GB in size.    <br \/>\nFor AKS you are limited by the memory resources that you request for your service, minus about 500mb for the running python process in the pod.    <\/p>\n<p>There will be no difference in prediction speed once the model is successfully deployed.    <br \/>\nRegistering will take longer as we have to upload the model, and deploying will take longer as the service must download the model.    <\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":83.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1321893442023,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":1611.0,
        "Answerer_view_count":189.0,
        "Challenge_adjusted_solved_time":367.2256972222,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am running a Python script with Tensorflow in Amazon Sagemaker notebook instance.  I have no trouble writing to the storage in the notebook normally, but for some reason I am unsuccessful when trying to save Tensorflow model checkpoints.  This code previously worked before it was ported to Sagemaker.<\/p>\n\n<p>Below is a reduced version of my code:<\/p>\n\n<pre><code>bucket = 'sagemaker-complaints-data'    \nprefix = 'DeepTestV2' # place to upload training files within the bucket\ntimestamp = str(int(time()))\nout_dir = os.path.abspath(os.path.join(bucket, prefix, \"runs\", timestamp))\ncheckpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\npath = saver.save(sess, checkpoint_prefix, global_step=current_step)\nprint(\"Saved model checkpoint to {}\\n\".format(path))\n<\/code><\/pre>\n\n<p>No errors are being thrown and the print statement is outputting the correct path.  I have researched whether there are any known issues with using checkpoints in Sagemaker but have come across literally no posts describing this.<\/p>",
        "Challenge_closed_time":1518715595183,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517393582673,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is facing an issue with saving Tensorflow model checkpoints to an Amazon Sagemaker notebook instance. The code previously worked before it was ported to Sagemaker, and the user is not receiving any error messages. The print statement is outputting the correct path, and the user has researched whether there are any known issues with using checkpoints in Sagemaker but has found no relevant posts.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48539564",
        "Challenge_link_count":0,
        "Challenge_participation_count":2,
        "Challenge_readability":9.3,
        "Challenge_reading_time":14.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":367.2256972222,
        "Challenge_title":"Tensorflow - Checkpoints not saving to Sagemaker Notebook Instance",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":690.0,
        "Challenge_word_count":136,
        "Platform":"Stack Overflow",
        "Poster_created_time":1321893442023,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":1611.0,
        "Poster_view_count":189.0,
        "Solution_body":"<p>I have found out where this is - for some reason \"checkpoints\" seems to be a reserved word - changing the word to \"checks\" allowed me to write the folder.  Hope this helps someone!<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":2.25,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":32.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1369252294280,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":324.0,
        "Answerer_view_count":32.0,
        "Challenge_adjusted_solved_time":527.1234527778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I create a xgboost classifier:<\/p>\n<pre><code>   xg_reg = xgb.XGBClassifier(objective ='reg:squarederror',  learning_rate = 0.1,\n                max_depth = 20, alpha = 10, n_estimators = 50, use_label_encoder=False)\n<\/code><\/pre>\n<p>After training the model, I am logging it to the MLFLow registry:<\/p>\n<pre><code>   mlflow.xgboost.log_model(\n        xgb_model = xg_reg, \n        artifact_path = &quot;xgboost-models&quot;,\n        registered_model_name = &quot;xgb-regression-model&quot;\n    )\n<\/code><\/pre>\n<p>In the remote UI, I can see the logged model:<\/p>\n<pre><code>artifact_path: xgboost-models\nflavors:\n  python_function:\n    data: model.xgb\n    env: conda.yaml\n    loader_module: mlflow.xgboost\n    python_version: 3.7.9\n  xgboost:\n    code: null\n    data: model.xgb\n    model_class: xgboost.sklearn.XGBClassifier\n    xgb_version: 1.5.2\nmlflow_version: 1.25.1\nmodel_uuid: 5fd42554cf184d8d96afae34dbb96de2\nrun_id: acdccd9f610b4c278b624fca718f76b4\nutc_time_created: '2022-05-17 17:54:53.039242\n<\/code><\/pre>\n<p>Now, on the server side, to load the logged model:<\/p>\n<pre><code>   model = mlflow.xgboost.load_model(model_uri=model_path)\n<\/code><\/pre>\n<p>which loads OK, but the model type is<\/p>\n<blockquote>\n<p>&lt;xgboost.core.Booster object at 0x00000234DBE61D00&gt;<\/p>\n<\/blockquote>\n<p>and the predictions are numpy.float32 (eg 0.5) instead of int64 (eg 0, 1) for the original model.<\/p>\n<p>Any ideas what can be wrong? Many thanks!<\/p>",
        "Challenge_closed_time":1655934198727,
        "Challenge_comment_count":0,
        "Challenge_created_time":1654036554297,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is facing an issue while loading a xgboost model from the MLFlow registry. Although the model loads successfully, the model type is different, and the predictions are in numpy.float32 instead of int64. The user is seeking help to resolve this issue.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/72454747",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":527.1234527778,
        "Challenge_title":"Problem when loading a xgboost model from mlflow registry",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":163.0,
        "Challenge_word_count":142,
        "Platform":"Stack Overflow",
        "Poster_created_time":1369252294280,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":324.0,
        "Poster_view_count":32.0,
        "Solution_body":"<p>It turns out this was caused by using different versions of mlflow. The model was uploaded to registry with the newest version but was loaded with a previous one. When updated the server to load it, it now works! :)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":2.69,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":40.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1623779095963,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Germany, Hesse",
        "Answerer_reputation_count":1341.0,
        "Answerer_view_count":128.0,
        "Challenge_adjusted_solved_time":4.1247302778,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to understand what an RFormula is in MLflow or spark.<\/p>\n<p>I have found these:<\/p>\n<p><a href=\"https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula\" rel=\"nofollow noreferrer\">https:\/\/george-jen.gitbook.io\/data-science-and-apache-spark\/rformula<\/a>\n<a href=\"https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html\" rel=\"nofollow noreferrer\">https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.ml.feature.RFormula.html<\/a><\/p>\n<p>but still cannot understand how to interpret an RFormula fully. I am not sure how to interpret the below table\n<a href=\"https:\/\/i.stack.imgur.com\/guLyU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/guLyU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>based on the formula &quot;y ~ x+ s&quot;, y is related to x and s, but in the table when y=0 and x=0 and s =a (i.e. third row), then the features is [0,1] and label is 0, so how shall I interpret this.<\/p>\n<p>I have found <a href=\"https:\/\/stackoverflow.com\/questions\/61290042\/spark-rformula-interpretation\">this<\/a> but still cannot understand my way through this problem.<\/p>",
        "Challenge_closed_time":1627334164712,
        "Challenge_comment_count":2,
        "Challenge_created_time":1627319315683,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to understand what an RFormula is in MLflow or Spark and has found some resources but is still struggling to interpret it fully. They are specifically confused about how to interpret a table based on the formula \"y ~ x+ s\" and are seeking help to understand it.",
        "Challenge_last_edit_time":1627371880352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/68533916",
        "Challenge_link_count":7,
        "Challenge_participation_count":3,
        "Challenge_readability":14.0,
        "Challenge_reading_time":16.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":4.1247302778,
        "Challenge_title":"what is features and how to interpret in RFormula",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":78.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Poster_created_time":1375186444008,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Auckland, New Zealand",
        "Poster_reputation_count":912.0,
        "Poster_view_count":288.0,
        "Solution_body":"<p>So your label is y. You parse x and s in rformula.<\/p>\n<p>x stays the same:<\/p>\n<pre><code>+-----------+---+\n|      x    | x |\n+-----------+---+\n|     1.0   |1.0|\n|     2.0   |2.0|\n|     0.0   |0.0|\n+-----------+---+\n<\/code><\/pre>\n<p>s:<\/p>\n<pre><code>+-----------+---+\n|       s   | s |\n+-----------+---+\n|       a   |1.0|\n|       b   |0.0|\n|       a   |1.0|\n+-----------+---+\n<\/code><\/pre>\n<p>I hope I could answer you question.\nRformula just converts the strings, standarize them and parse them into a vector.<\/p>",
        "Solution_comment_count":6.0,
        "Solution_last_edit_time":1627334932660,
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":5.71,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":57.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":1528790837107,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation_count":610.0,
        "Answerer_view_count":203.0,
        "Challenge_adjusted_solved_time":221.7831972222,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In Azure ML Studio, we have the option of choosing a number of inbuilt ML models like Classification, Regression, etc. , which we can drag and drop to our workflow.<\/p>\n\n<p>My question is, can I upload a custom ML model that I have built locally on my system in Python, and add it to the workflow?<\/p>",
        "Challenge_closed_time":1566202471556,
        "Challenge_comment_count":1,
        "Challenge_created_time":1565761282107,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if they can upload a custom ML model that they have built locally on their system in Python and add it to the workflow in Azure ML Studio, instead of using the inbuilt ML models.",
        "Challenge_last_edit_time":1565767277470,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/57488706",
        "Challenge_link_count":0,
        "Challenge_participation_count":3,
        "Challenge_readability":6.9,
        "Challenge_reading_time":4.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":122.5526247222,
        "Challenge_title":"Deploying custom model on Azure ML Studio",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1187.0,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Poster_created_time":1565761178208,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":10.0,
        "Solution_body":"<ol>\n<li>Take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio. Click the \u201cNew\u201d icon in the bottom left:\n<a href=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" alt=\"\"><\/a><\/li>\n<li>In the pane that comes up, click on dataset, and then \u201cFrom Local File\u201d:\n<a href=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" alt=\"\"><\/a><\/li>\n<li>Select the zip file where you stored your serialized model and click the tick. You expirement should look like this:\n<a href=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" alt=\"\"><\/a><\/li>\n<li>Put the following code to run your classification experiment:<\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]])\n<\/code><\/pre>\n\n<p><strong>Update<\/strong> \nIf you want to declare this experiment as an API you need to add web input and output to the Python script module.\n<a href=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Solution_comment_count":7.0,
        "Solution_last_edit_time":1566565696980,
        "Solution_link_count":8.0,
        "Solution_readability":11.5,
        "Solution_reading_time":19.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":144.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.2877788889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to figure out about standard connectors between SAP ERP product and Azure ML especially for NLP scenarios. Can you please suggest on this.<\/p>",
        "Challenge_closed_time":1664542897547,
        "Challenge_comment_count":0,
        "Challenge_created_time":1664541861543,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking information about standard connectors between SAP ERP and Azure ML, specifically for NLP scenarios. They are requesting suggestions on this matter.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1030800\/azure-ml-for-sap-erp",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":2.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":0.2877788889,
        "Challenge_title":"Azure ML for SAP ERP",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":30,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p><a href=\"\/users\/na\/?userid=dfa9d536-725c-462d-87c8-47fbafb1a2bc\">@D-0887  <\/a> Thanks for the question. Here is the blog that could help and <a href=\"https:\/\/github.com\/microsoft\/nlp-recipes\">nlp recipes<\/a>.    <br \/>\n<a href=\"https:\/\/blogs.sap.com\/2022\/08\/03\/azure-machine-learning-triggering-calculations-ml-in-sap-data-warehouse-cloud\/\">https:\/\/blogs.sap.com\/2022\/08\/03\/azure-machine-learning-triggering-calculations-ml-in-sap-data-warehouse-cloud\/<\/a>    <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.2,
        "Solution_reading_time":6.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":22.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":147.1668483334,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is AWS Sage Maker Auto Pilot suitable for NLP?<\/p>\n\n<p>We currently have a tensorflow model that does classification on input of a sequence of URLS (\nWe transform the URLs to Word vec and Char vec to feed it to the model).<\/p>\n\n<p>Looking at Sage Maker Auto Pilot documentation it says that it works on input in tabular form.\nI was wondering if we could use it to for our use case.<\/p>",
        "Challenge_closed_time":1578756147747,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578226347093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is questioning whether AWS Sage Maker Auto Pilot is suitable for their Natural Language Processing (NLP) needs, as their current tensorflow model uses URL sequences transformed into Word and Char vectors for classification, while Sage Maker Auto Pilot works with tabular input.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59599721",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":5.2,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":147.1668483334,
        "Challenge_title":"Is AWS Sage Maker Auto Pilot suitable for NLP?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":261.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Poster_created_time":1458550179920,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":383.0,
        "Poster_view_count":19.0,
        "Solution_body":"<p>No. SageMaker AutoPilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. Technically, I guess you could pass embeddings in CSV format, and pray that XGBoost figures them out, but I seriously doubt that this would deliver meaningful results :)<\/p>\n\n<p>Amazon Comprehend does support fully managed custom classification models <a href=\"https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html<\/a>. It may be worth taking a look at it.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":19.4,
        "Solution_reading_time":8.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":68.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":1124.7847222222,
        "Challenge_answer_count":0,
        "Challenge_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Challenge_closed_time":1623230615000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1619181390000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is unable to create a model in MLflowCatalog due to an error message stating that the registered model with the given name does not exist.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1124.7847222222,
        "Challenge_title":"All MLflow experiments are visible for any user",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":80,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":173.1223830556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have run automated ML experiments with imbalanced data (10:1, 20:1, sometimes 30:1) and deployed the best models which all showed fantastic results.    <\/p>\n<p>When I looked up the link    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data<\/a>    <br \/>\n, it says Azure automated ML can properly handle imbalance of up to 20:1.     <br \/>\nI started to wonder where the ratio 20:1 came from.     <\/p>\n<p>As far as I understand, Azure automated ML doesn't use upsampling, downsampling or resampling, and is more focused on a column of weights to make a class more or less important, and a performance metric dealing better with imbalanced data.    <\/p>\n<ul>\n<li> Does this 20:1 come from some theory? or from tons of experiments already conducted?    <\/li>\n<\/ul>\n<p>Azure automated ML shows the result with warning when I use 30:1(or more) imbalanced data, but I still wonder why it is 20:1.<\/p>",
        "Challenge_closed_time":1594030959316,
        "Challenge_comment_count":1,
        "Challenge_created_time":1593407718737,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user has run Azure Automated ML experiments with imbalanced data and deployed the best models with fantastic results. However, the user found that Azure Automated ML can handle imbalance of up to 20:1, according to a Microsoft link. The user wonders where the ratio 20:1 came from and if it is based on theory or experiments. Azure Automated ML does not use upsampling, downsampling, or resampling, but instead uses a column of weights to make a class more or less important and a performance metric that deals better with imbalanced data. The user received a warning when using 30:1 or more imbalanced data and is curious why the limit is 20:1.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/40727\/azure-automated-ml(interface)-how-do-models-create",
        "Challenge_link_count":1,
        "Challenge_participation_count":3,
        "Challenge_readability":13.5,
        "Challenge_reading_time":15.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":173.1223830556,
        "Challenge_title":"Azure Automated ML(interface) how do models created from an Automated ML experiment handle Imbalanced Data?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":155,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>In AutoML we use 5% minority class as threshold to classify imbalance\/non-imbalance. This is a heuristic, and is one guideline produced in the Guardrails to the question \u201cAt x% threshold level is the dataset balanced?\u201d. Since it is not possible to absolutely classify imbalance in all cases (depending on the dataset and its size and distribution, 5% or 10% or even higher may mean imbalance, whereas for very large datasets the minority class may have sufficient training samples for model to learn and get a reasonable imbalance-appropriate metric such as weighted AUC or balanced accuracy),  current Guardrails serve the goal of surfacing \u201csubstantial\u201d imbalance to user so the user can take any of the following measures:  <\/p>\n<p>\u2022\tWhen the user knows (either from their knowledge of their own data or from guardrails) that there is imbalance, Automated ML provides an option in the Automated ML config to provide sample weights \u2013 a user-specified weight array where user can specify to weight each sample with a weight. That way they can weigh the minority class more when submitting the data into Automated ML config. We will soon provide weighting option for imbalance classes from within AutoML that will be activated automatically when imbalance is detected.  <\/p>\n",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":19.3,
        "Solution_reading_time":15.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":205.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":22.7411330556,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have a notebook based on <a href=\"http:\/\/wandb.me\/lit-colab\" rel=\"noopener nofollow ugc\"> Supercharge your Training with PyTorch Lightning + Weights &amp; Biases<\/a> and I\u2019m wondering what the easiest approach to load a model with the best checkpoint after training finishes.<\/p>\n<p>I\u2019m assuming that after training the \u201cmodel\u201d instance will just have the weights of the most recent epoch, which might not be the most accurate model (in case it started overfitting etc).<\/p>\n<p>Specifically I was looking for an easy way to get the directory where the checkpoints artifacts are stored, which in my case look like this: <code>.\/MnistKaggle\/1vzsgin6\/checkpoints<\/code>, where <code>1vzsgin6<\/code> is the run id auto-generated by wandb.<\/p>\n<p>One (clunky) way to do it would be:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=\"MnistKaggle\")\ncheckpoint_dir_path = None\n\ndef my_after_save_checkpoint(checkpoint):\n  checkpoint_dir_path = checkpoint.dirpath\n\nwandb_logger.after_save_checkpoint = my_after_save_checkpoint\n\n# Now find the checkpoint file in the checkpoint_dir_path directory and load the model from that.\n<\/code><\/pre>\n<p>Is there an easier way?  I was sorta expecting the <code>WandbLogger<\/code> object to have an easy method like <code>get_save_checkpoint_dirpath()<\/code>, but I\u2019m not seeing anything.<\/p>\n<p>Thanks in advance for any help!<\/p>",
        "Challenge_closed_time":1667430011968,
        "Challenge_comment_count":0,
        "Challenge_created_time":1667348143889,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is seeking the easiest way to load the best model checkpoint after training with PyTorch Lightning and Weights & Biases. They are looking for a way to get the directory where the checkpoints artifacts are stored and are currently using a clunky method involving WandbLogger. They are hoping for an easier method to retrieve the checkpoint directory path.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/community.wandb.ai\/t\/easiest-way-to-load-the-best-model-checkpoint-after-training-w-pytorch-lightning\/3365",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":10.9,
        "Challenge_reading_time":18.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":22.7411330556,
        "Challenge_title":"Easiest way to load the best model checkpoint after training w\/ pytorch lightning",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1455.0,
        "Challenge_word_count":182,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tleyden\">@tleyden<\/a> , happy to help. Please review the following <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/extensions\/generated\/pytorch_lightning.loggers.WandbLogger.html#:~:text=(model)-,Log%20model%20checkpoints,-Log%20model%20checkpoints\" rel=\"noopener nofollow ugc\">resource<\/a> on model checkpointing and retrieval.<\/p>\n<p>A common flow would be to log a model checkpoint as in the example then to also log a \u201cbest model\u201d artifact. Since artifacts are versioned you don\u2019t have to worry about renaming the new \u201cbest model\u201d artifact. Then at the end of your run you not only have an artifact history of your model at each of the checkpoints but also a versioned history of all the best models.<\/p>",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.9,
        "Solution_reading_time":9.74,
        "Solution_score_count":null,
        "Solution_sentence_count":6.0,
        "Solution_word_count":91.0,
        "Tool":"Weights & Biases"
    },
    {
        "Answerer_created_time":1588516515763,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"UK",
        "Answerer_reputation_count":29087.0,
        "Answerer_view_count":3080.0,
        "Challenge_adjusted_solved_time":0.2190208333,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am following <a href=\"https:\/\/github.com\/mtm12\/SageMakerDemo\" rel=\"noreferrer\">this example<\/a> on how to train a machine learning model in Amazon-sagemaker.<\/p>\n<pre><code>data_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_highlevel_example\/output'.format(bucket)\n\nprint('training data will be uploaded to: {}'.format(data_location))\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nkmeans = KMeans(role=role,\n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10,\n                epochs=100,\n                data_location=data_location)\n<\/code><\/pre>\n<p>So after calling the fit function the model should be saved in the S3 bucket?? How can you load this model next time?<\/p>",
        "Challenge_closed_time":1595162985808,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595162197333,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary_original":"The user is trying to train a machine learning model in Amazon Sagemaker by following a specific example. After calling the fit function, the model should be saved in the S3 bucket, but the user is unsure how to load the model for future use.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/62980380",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":10.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":6.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":0.2190208333,
        "Challenge_title":"How to load trained model in amazon sagemaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":4776.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Poster_created_time":1310821356880,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Slovenia",
        "Poster_reputation_count":14913.0,
        "Poster_view_count":1093.0,
        "Solution_body":"<p>This can be done by using the sagemaker library combined with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"noreferrer\">Inference Model<\/a>.<\/p>\n<pre><code>model = sagemaker.model.Model(\n    image=image\n    model_data='s3:\/\/bucket\/model.tar.gz',\n    role=role_arn)\n<\/code><\/pre>\n<p>The options you're passing in are:<\/p>\n<ul>\n<li><code>image<\/code> - This is the ECR image you're using for inference (which should be for the algorithm you're trying to use). Paths are available <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"noreferrer\">here<\/a>.<\/li>\n<li><code>model_data<\/code> - This is the path of where your model is stored (in a <code>tar.gz<\/code> compressed archive).<\/li>\n<li><code>role<\/code> - This is the arn of a role that is capable of both pulling the image from ECR and getting the s3 archive.<\/li>\n<\/ul>\n<p>Once you've successfully done this you will need to setup an endpoint, this can be done by performing the following in your notebook through the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.Model.deploy\" rel=\"noreferrer\">deploy function<\/a>.<\/p>\n<pre><code>model.deploy(\n   initial_instance_count=1,\n   instance_type='ml.p2.xlarge'\n)\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":3.0,
        "Solution_readability":13.4,
        "Solution_reading_time":17.2,
        "Solution_score_count":8.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":128.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1512770138847,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Challenge_adjusted_solved_time":22.3412477778,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to use a local training job in SageMaker.<\/p>\n<p>Following this AWS notebook (<a href=\"http:\/\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb<\/a>) I was able to train and predict locally.<\/p>\n<p>There is any way to train locally and save the trained model in the Amazon SageMaker Training Job section?\nOtherwise, how can I properly save trained models I trained using local mode?<\/p>",
        "Challenge_closed_time":1596039571220,
        "Challenge_comment_count":0,
        "Challenge_created_time":1595954217667,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is trying to use a local training job in Amazon SageMaker and has successfully trained and predicted locally using an AWS notebook. However, the user is facing challenges in saving the trained model in the Amazon SageMaker Training Job section and is seeking guidance on how to properly save trained models trained using local mode.",
        "Challenge_last_edit_time":1595959142728,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/63138835",
        "Challenge_link_count":2,
        "Challenge_participation_count":2,
        "Challenge_readability":12.9,
        "Challenge_reading_time":8.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":7.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":23.7093202778,
        "Challenge_title":"How to save models trained locally in Amazon SageMaker?",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3195.0,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Poster_created_time":1464391892936,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Poster_reputation_count":2243.0,
        "Poster_view_count":148.0,
        "Solution_body":"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration\/debugging before using SageMaker for training your model.<\/p>\n<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz<\/code> file, upload that file to S3, and then create the Model (with the SDK or in the console).<\/p>\n<p>Documentation:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html<\/a><\/li>\n<\/ul>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":4.0,
        "Solution_readability":18.5,
        "Solution_reading_time":12.35,
        "Solution_score_count":2.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":79.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":29.7938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Which example? Describe the issue\r\n\r\nexample:  az ml online-deployment create --name blue --endpoint-name amlarc-runner-simple-849b --resource-group lt-westus2-r6-amlarc-rg --workspace-name lt-westus2-r6-arc-ws --file azureml-examples\/cli\/endpoints\/online\/\/amlarc\/blue-deployment.yml --all-traffic\r\ndescription:\r\nFile \"\/var\/azureml-server\/entry.py\", line 1, in <module>\r\n    import create_app\r\n  File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\r\n    import aml_framework\r\n  File \"\/var\/azureml-server\/aml_framework.py\", line 9, in <module>\r\n    from synchronous.framework import *\r\n  File \"\/var\/azureml-server\/synchronous\/framework.py\", line 3, in <module>\r\n    from flask import Flask, request, g, Request, Response, Blueprint\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/__init__.py\", line 21, in <module>\r\n    from .app import Flask, Request, Response\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/app.py\", line 26, in <module>\r\n    from . import cli, json\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/json\/__init__.py\", line 21, in <module>\r\n    from itsdangerous import json as _json\r\nImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)\r\n\r\n## Additional context\r\n\r\nhttps:\/\/ml.azure.com\/endpoints\/realtime\/amlarc-runner-simple-849b\/logs?wsid=\/subscriptions\/589c7ae9-223e-45e3-a191-98433e0821a9\/resourcegroups\/lt-westus2-r6-amlarc-rg\/workspaces\/lt-westus2-r6-arc-ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\r\n\r\n-\r\n",
        "Challenge_closed_time":1645604942000,
        "Challenge_comment_count":1,
        "Challenge_created_time":1645497684000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is reporting a bug related to updating test documentation to connect AzureML with GitHub actions. The steps to replicate the issue involve creating a new AzureML workspace, creating two new clusters, adding subscription ID to GitHub action secrets, installing Azure CLI, creating a Service Principal, and adding the output from the Service Principal as an action secret. The user has not mentioned any specific platform where the issue is happening.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/977",
        "Challenge_link_count":1,
        "Challenge_participation_count":1,
        "Challenge_readability":18.3,
        "Challenge_reading_time":24.77,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":650.0,
        "Challenge_repo_issue_count":1974.0,
        "Challenge_repo_star_count":882.0,
        "Challenge_repo_watch_count":2756.0,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":29.7938888889,
        "Challenge_title":"ImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":115,
        "Platform":"Github",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"This issue should be mitigated once the PR is merged: https:\/\/github.com\/Azure\/azureml-examples\/pull\/981",
        "Solution_comment_count":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":1.38,
        "Solution_score_count":null,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":1441425578580,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":433.0,
        "Answerer_view_count":56.0,
        "Challenge_adjusted_solved_time":12869.3964275,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I trained my model in Amazon-SageMaker and downloaded it to my local computer. Unfortunately, I don't have any idea how to run the model locally.<\/p>\n\n<p>The Model is in a directory with files like:<\/p>\n\n<pre><code>image-classification-0001.params\nimage-classification-0002.params\nimage-classification-0003.params\nimage-classification-0004.params\nimage-classification-0005.params\nimage-classification-symbol.json\nmodel-shapes.json\n<\/code><\/pre>\n\n<p>Would anyone know how to run this locally with Python, or be able to point me to a resource that could help? I am trying to avoid calling the model using the Amazon API.<\/p>\n\n<p>Edit: The model I used was created with code very similar to this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n\n<p>Any help is appreciated, I will award the bounty to whoever is most helpful, even if they don't completely solve the question. <\/p>",
        "Challenge_closed_time":1520719061830,
        "Challenge_comment_count":3,
        "Challenge_created_time":1520225638010,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user has trained a model in Amazon-SageMaker and downloaded it to their local computer, but they are unsure how to run the model locally using Python. They are seeking guidance or resources to help them avoid calling the model using the Amazon API.",
        "Challenge_last_edit_time":1536012091088,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49103679",
        "Challenge_link_count":1,
        "Challenge_participation_count":6,
        "Challenge_readability":15.7,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":137.0621722223,
        "Challenge_title":"How to Deploy Amazon-SageMaker Locally in Python",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3742.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441425578580,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":433.0,
        "Poster_view_count":56.0,
        "Solution_body":"<p>Following SRC's advice, I was able to get it to work by following the instructions in this <a href=\"https:\/\/stackoverflow.com\/questions\/47190614\/how-to-load-a-trained-mxnet-model\">question<\/a> and this <a href=\"http:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">doc<\/a> which describe how to load a MXnet model.<\/p>\n\n<p>I loaded the model like so:<\/p>\n\n<pre><code>lenet_model = mx.mod.Module.load('model_directory\/image-classification',5)\nimage_l = 64\nimage_w = 64\nlenet_model.bind(for_training=False, data_shapes=[('data',(1,3,image_l,image_w))],label_shapes=lenet_model._label_shapes)\n<\/code><\/pre>\n\n<p>Then predicted using the slightly modified helper functions in the previously linked documentation:<\/p>\n\n<pre><code>import mxnet as mx\nimport matplotlib.pyplot as plot\nimport cv2\nimport numpy as np\nfrom mxnet.io import DataBatch\n\ndef get_image(url, show=False):\n    # download and show the image\n    fname = mx.test_utils.download(url)\n    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n    if img is None:\n         return None\n    if show:\n         plt.imshow(img)\n         plt.axis('off')\n    # convert into format (batch, RGB, width, height)\n    img = cv2.resize(img, (64, 64))\n    img = np.swapaxes(img, 0, 2)\n    img = np.swapaxes(img, 1, 2)\n    img = img[np.newaxis, :]\n    return img\n\ndef predict(url, labels):\n    img = get_image(url, show=True)\n    # compute the predict probabilities\n    lenet_model.forward(DataBatch([mx.nd.array(img)]))\n    prob = lenet_model.get_outputs()[0].asnumpy()\n\n    # print the top-5\n    prob = np.squeeze(prob)\n    a = np.argsort(prob)[::-1]\n\n    for i in a[0:5]:\n       print('probability=%f, class=%s' %(prob[i], labels[i]))\n<\/code><\/pre>\n\n<p>Finally I called the prediction with this code:<\/p>\n\n<pre><code>labels = ['a','b','c', 'd','e', 'f']\npredict('https:\/\/eximagesite\/img_tst_a.jpg', labels )\n<\/code><\/pre>",
        "Solution_comment_count":2.0,
        "Solution_last_edit_time":1582341918227,
        "Solution_link_count":3.0,
        "Solution_readability":12.5,
        "Solution_reading_time":23.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":169.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1231266594036,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation_count":2800.0,
        "Answerer_view_count":262.0,
        "Challenge_adjusted_solved_time":35.1478936111,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I just started using aws sagemaker for running and maintaining models, experiments. just wanted to know is there any persistent layer for the sagemaker from where i can get data of my experiments\/models instead of looking into the sagemaker studio. Does sagemaker saves the experiments or its data like s3 location in any table  something like modelsdb? <\/p>",
        "Challenge_closed_time":1584546176820,
        "Challenge_comment_count":0,
        "Challenge_created_time":1584418712033,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is new to using AWS SageMaker for running and maintaining models and experiments. They are looking for a persistent layer where they can access data related to their experiments and models instead of having to look into SageMaker Studio. They are wondering if SageMaker saves experiment data, such as S3 location, in a table like ModelsDB.",
        "Challenge_last_edit_time":1584419644403,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/60716202",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":35.4068852778,
        "Challenge_title":"SageMaker experiments store",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":229.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Poster_created_time":1427130497092,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":187.0,
        "Poster_view_count":151.0,
        "Solution_body":"<p>SageMaker Studio is using the SageMaker API to pull all of the data its displaying.  Essentially there's no secret API here getting invoked.<\/p>\n\n<p>Quite a bit of what's being displayed with respect to experiments is from the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_Search.html\" rel=\"nofollow noreferrer\">search results<\/a>, the rest coming from either List* or Describe* calls.  Studio is taking the results from the search request and displaying it in the table format that you're seeing.  Search results when searching over resource ExperimentTrialComponent that have a source (such as a training job) will be enhanced with the original sources data ([result]::SourceDetail::TrainingJob) if supported (work is ongoing to add additional source detail resource types).<\/p>\n\n<p>All of the metadata that is related to resources in SageMaker is available via the APIs; there is no other location (in the cloud) like s3 for that data.<\/p>\n\n<p>As of this time there is no effort to determine if it's possible to add support into <a href=\"https:\/\/github.com\/VertaAI\/modeldb\" rel=\"nofollow noreferrer\">modeldb<\/a> for SageMaker that I'm aware of.  Given that modeldb appears to make some assumptions about it's talking to a relational database it would appear unlikely to be something that would be doable. (I only read the overview very quickly so this might be inaccurate.)<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":11.6,
        "Solution_reading_time":17.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":201.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1426694564423,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Challenge_adjusted_solved_time":1.6198194445,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a Tensorflow model which is working perfectly fine on my laptop (Tf 1.8 on OS HighSierra). However, I wanted to scale my operations up and use Amazon's Virtual Machine to run predictions faster. What is the best way to use my saved model and classify images in jpeg format which are stored locally? Thank you! <\/p>",
        "Challenge_closed_time":1532521671907,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532515840557,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to deploy their Tensorflow model on an AWS virtual machine to increase prediction speed. They are seeking advice on the best way to use their saved model to classify locally stored jpeg images.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51517103",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":7.4,
        "Challenge_reading_time":4.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":0.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.6198194445,
        "Challenge_title":"Deploy my own tensorflow model on a virtual machine with AWS",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":196.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Poster_created_time":1530739110150,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":391.0,
        "Poster_view_count":57.0,
        "Solution_body":"<p>you have two options:<\/p>\n\n<p>1) Start a virtual machine on AWS (known as an Amazon EC2 instance). You can pick from many different instance types, including GPU instances. You'll have full administrative access on this machine, meaning that you can copy you TF model to it and predict just like you would on your own machine. <\/p>\n\n<p>More details on getting started with EC2 here: <a href=\"https:\/\/aws.amazon.com\/ec2\/getting-started\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/ec2\/getting-started\/<\/a> <\/p>\n\n<p>I would also recommend using the Deep Learning Amazon Machine Image, which bundles all the popular ML\/DL tools as well as the NVIDIA environment for GPU training\/prediction : <a href=\"https:\/\/aws.amazon.com\/machine-learning\/amis\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/machine-learning\/amis\/<\/a><\/p>\n\n<p>2) If you don't want to manage virtual machines, I'd recommend looking at Amazon SageMaker. You'll be able to import your TF model and to deploy it on fully-managed infrastructure for prediction. <\/p>\n\n<p>Here's a sample notebook showing you how to bring your own TF model to SageMaker: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_iris_byom\/tensorflow_BYOM_iris.ipynb<\/a><\/p>\n\n<p>Hope this helps.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":6.0,
        "Solution_readability":14.3,
        "Solution_reading_time":19.14,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":156.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1530092504712,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Challenge_adjusted_solved_time":14.6893280556,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>In the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\">IO section of the kedro API docs<\/a> I could not find functionality w.r.t. storing trained models (e.g. <code>.pkl<\/code>, <code>.joblib<\/code>, <code>ONNX<\/code>, <code>PMML<\/code>)? Have I missed something?<\/p>",
        "Challenge_closed_time":1589225919163,
        "Challenge_comment_count":0,
        "Challenge_created_time":1589224779627,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is inquiring about the availability of IO functionality to store trained models in kedro, as they could not find any related information in the kedro API documentation.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61737613",
        "Challenge_link_count":1,
        "Challenge_participation_count":2,
        "Challenge_readability":10.5,
        "Challenge_reading_time":4.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.3165377778,
        "Challenge_title":"Is there IO functionality to store trained models in kedro?",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":795.0,
        "Challenge_word_count":39,
        "Platform":"Stack Overflow",
        "Poster_created_time":1441627039648,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Augsburg, Germany",
        "Poster_reputation_count":3635.0,
        "Poster_view_count":383.0,
        "Solution_body":"<p>There is the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PickleLocalDataSet.html#kedro.io.PickleLocalDataSet\" rel=\"nofollow noreferrer\"><code>pickle<\/code><\/a> dataset in <code>kedro.io<\/code>, that you can use to save trained models and\/or anything you want to pickle and is serialisable (models being a common object). It accepts a <code>backend<\/code> that defaults to <code>pickle<\/code> but can be set to <code>joblib<\/code> if you want to use <code>joblib<\/code> instead.<\/p>\n\n<p>I'm just going to quickly note that Kedro is moving to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\"><code>kedro.extras.datasets<\/code><\/a> for its datasets and moving away from having non-core datasets in <code>kedro.io<\/code>. You might want to look at <code>kedro.extras.datasets<\/code> and in Kedro 0.16 onwards <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.extras.datasets.pickle.PickleDataSet.html#kedro.extras.datasets.pickle.PickleDataSet\" rel=\"nofollow noreferrer\"><code>pickle.PickleDataSet<\/code><\/a> with <code>joblib<\/code> support.<\/p>\n\n<p>The Kedro <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/02_tutorial_template.html\" rel=\"nofollow noreferrer\"><code>spaceflights<\/code><\/a> tutorial in the documentation actually saves the trained linear regression model using the <code>pickle<\/code> dataset if you want to see an example of it. The relevant section is <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html#working-with-multiple-pipelines\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":1589277661208,
        "Solution_link_count":5.0,
        "Solution_readability":16.1,
        "Solution_reading_time":21.62,
        "Solution_score_count":3.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":137.0,
        "Tool":"Kedro"
    },
    {
        "Answerer_created_time":1393524211332,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":745.0,
        "Answerer_view_count":71.0,
        "Challenge_adjusted_solved_time":609.6229966667,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Have exhausted myself on this one so any help would be appreciated.<\/p>\n\n<p>I am trying to set up hosting my tensorflow model with Amazon Sagemaker and following the example found <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>This example uses hard coded feature columns with known dimensionality. <\/p>\n\n<pre><code>feature_columns = [tf.feature_column.numeric_column(INPUT_TENSOR_NAME, shape=[4])]\n<\/code><\/pre>\n\n<p>I need to avoid this as my dataset changes often.<\/p>\n\n<h1>Local Machine Set Up<\/h1>\n\n<p>Now on my local machine, I define a list of columns <\/p>\n\n<pre><code>my_feature_columns = []\n<\/code><\/pre>\n\n<p>With the following strategy<\/p>\n\n<pre><code>#Define placeholder nodes based on datatype being inserted\n\nfor key in train_x.keys():\n<\/code><\/pre>\n\n<p>Where train_x is the dataset without labels.<\/p>\n\n<p>'OBJECTS' become hashed buckets as there are many possible categories<\/p>\n\n<pre><code>    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n<\/code><\/pre>\n\n<p>'INT64' become categorical columns as there are only two possible categories (I have recoded booleans to 0\/1)<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n<\/code><\/pre>\n\n<p>'FLOATS' become continuous columns<\/p>\n\n<pre><code>    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n<\/code><\/pre>\n\n<p>On the local machine this yields a nice list of all of my features that can be given as an argument when instantiating a tf.estimator.DNNClassifier. As more categories are added to each OBJECT column, this is handled by<\/p>\n\n<pre><code>hash_bucket_size = len(train_x[key].unique())\n<\/code><\/pre>\n\n<h1>Sagemaker<\/h1>\n\n<p>From the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst#preparing-the-tensorflow-training-script\" rel=\"nofollow noreferrer\">Docs<\/a><\/p>\n\n<p><em>Preparing the TensorFlow training script\nYour TensorFlow training script must be a Python 2.7 source file. The SageMaker TensorFlow docker image uses this script by calling specifically-named functions from this script.<\/em><\/p>\n\n<p><em>The training script must contain the following:<\/em><\/p>\n\n<p><em>Exactly one of the following:\nmodel_fn: defines the model that will be trained.\nkeras_model_fn: defines the tf.keras model that will be trained.\nestimator_fn: defines the tf.estimator.Estimator that will train the model.<\/em><\/p>\n\n<p><em>train_input_fn: preprocess and load training data.<\/em><\/p>\n\n<p><em>eval_input_fn: preprocess and load evaluation data.<\/em><\/p>\n\n<p>Again, from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/iris_dnn_classifier.py\" rel=\"nofollow noreferrer\">example<\/a><\/p>\n\n<pre><code>def train_input_fn(training_dir, params):\n\"\"\"Returns input function that would feed the model during training\"\"\"\nreturn _generate_input_fn(training_dir, 'iris_training.csv')\n<\/code><\/pre>\n\n<p>This function is called by the sagemaker docker image, which adds its own argument for <strong>training_dir<\/strong>, it is not a global parameter.<\/p>\n\n<p>When trying to access my training data from the estimator_fn to build a my_feature_columns list<\/p>\n\n<pre><code>NameError: global name 'training_dir' is not defined\n<\/code><\/pre>\n\n<h1>I would love to be able to do something like this.<\/h1>\n\n<pre><code>def estimator_fn(run_config, params):\n\nmy_feature_columns = []\n\ntrain_x , _ , _ , _ = datasplitter(os.path.join(training_dir, 'leads_test_frame.csv'))\n\nfor key in train_x.keys():\n    if train_x[key].dtypes == 'object':\n\n        categorical_column = tf.feature_column.categorical_column_with_hash_bucket(\n                key = key,\n                hash_bucket_size = len(train_x[key].unique()))\n\n        my_feature_columns.append(tf.feature_column.embedding_column(\n                categorical_column=categorical_column,\n                dimension=5))\n\n    elif train_x[key].dtypes == 'int64':\n\n        categorical_column = tf.feature_column.categorical_column_with_identity(\n                key=key,\n                num_buckets=2)\n\n        my_feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n\n    elif train_x[key].dtypes == 'float':\n        my_feature_columns.append(\n        tf.feature_column.numeric_column(\n        key=key))\n\nreturn tf.estimator.DNNClassifier(feature_columns=my_feature_columns,\n                                  hidden_units=[10, 20, 10],\n                                  n_classes=2,\n                                  config=run_config)\n<\/code><\/pre>\n\n<p>Thanks to anyone who can help in any way. Will happily give more info if needed but feel like 4 pages is probably enough :-S<\/p>\n\n<p>Cheers!\nClem<\/p>",
        "Challenge_closed_time":1537837258768,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535642615980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user is trying to set up hosting their TensorFlow model with Amazon Sagemaker, but is facing challenges with generating feature columns. They are using a strategy on their local machine to define a list of columns, but are unable to access their training data from the estimator_fn to build a my_feature_columns list. They would like to be able to define the my_feature_columns list in the estimator_fn function.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52100549",
        "Challenge_link_count":3,
        "Challenge_participation_count":1,
        "Challenge_readability":16.5,
        "Challenge_reading_time":67.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":609.6229966667,
        "Challenge_title":"Procedural Generation of Feature Columns for use with Sagemaker Tensorflow Instance",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":165.0,
        "Challenge_word_count":458,
        "Platform":"Stack Overflow",
        "Poster_created_time":1491420193248,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"Windsor, UK",
        "Poster_reputation_count":163.0,
        "Poster_view_count":71.0,
        "Solution_body":"<p><strong>training_dir<\/strong> points to your training channel, i.e. <em>\/opt\/ml\/input\/data\/training<\/em>. You can hardcode this location inside your <strong>estimation_fn<\/strong>.<\/p>\n\n<p>When training starts, SageMaker makes the data for the channel available in the <em>\/opt\/ml\/input\/data\/<strong>channel_name<\/em><\/strong> directory in the Docker container.<\/p>\n\n<p>You can find more information here <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":26.9,
        "Solution_reading_time":9.81,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1390559330112,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation_count":96.0,
        "Answerer_view_count":14.0,
        "Challenge_adjusted_solved_time":1183.9241880556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am building a multiclass classifier on aws Sagemaker, and would love to use the predefined linearlearner algorithm for classification. <\/p>",
        "Challenge_closed_time":1531423415907,
        "Challenge_comment_count":3,
        "Challenge_created_time":1527161288830,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user wants to know if Sagemaker's linear learner algorithm can be used for multiclass classification.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50508217",
        "Challenge_link_count":0,
        "Challenge_participation_count":4,
        "Challenge_readability":13.7,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1183.9241880556,
        "Challenge_title":"Can sagemaker's linear learner be used for multiclass classification?",
        "Challenge_topic":"Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":410.0,
        "Challenge_word_count":29,
        "Platform":"Stack Overflow",
        "Poster_created_time":1527161034076,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":35.0,
        "Poster_view_count":14.0,
        "Solution_body":"<p>Yes, it is possible now.<\/p>\n\n<p>You can set the predictor_type hyper-parameter to <code>multiclass_classifier<\/code>.<\/p>\n\n<p>See the documentation here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html<\/a>.<\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":2.0,
        "Solution_readability":32.5,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":21.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1253986272627,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Challenge_adjusted_solved_time":3520.4706275,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have pretrained model artifacts stored in S3 buckets. I want to create a service that loads this model and uses it for inference.<\/p>\n\n<p>I am working in AWS ecosystem and confused between using ECS vs Sagemaker for model deployment?\nWhat are some pros\/cons for choosing one over other?<\/p>",
        "Challenge_closed_time":1578553162123,
        "Challenge_comment_count":0,
        "Challenge_created_time":1578049726853,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary_original":"The user has pre-trained model artifacts stored in S3 buckets and wants to create a service that loads this model and uses it for inference. They are confused between using AWS ECS and Sagemaker for model deployment and are seeking advice on the pros and cons of each option.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/59577521",
        "Challenge_link_count":0,
        "Challenge_participation_count":1,
        "Challenge_readability":6.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":3.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":139.8431305556,
        "Challenge_title":"AWS Sagemaker vs ECS for model hosting",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":3288.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Poster_created_time":1390885171883,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":"College Station, TX, United States",
        "Poster_reputation_count":426.0,
        "Poster_view_count":27.0,
        "Solution_body":"<p>SageMaker has a higher price mark but it is taking a lot of the heavy lifting of deploying a machine learning model, such as wiring the pieces (load balancer, gunicorn, CloudWatch, Auto-Scaling...) and it is easier to automate the processes such as A\/B testing.<\/p>\n\n<p>If you have a strong team of DevOps that have nothing more important to do, you can build a flow that will be cheaper than the SageMaker option. ECS and EKS are doing at the same time a lot of work to make it very easy for you to automate the machine learning model deployments. However, they will always be more general purpose and SageMaker with its focus on machine learning will be easier for these use cases. <\/p>\n\n<p>The usual pattern of using the cloud is to use the managed services early on as you want to move fast and you don't really know where are your future problems. Once the system is growing and you start feeling some pains here and there, you can decide to spend the time and improve that part of the system. Therefore, if you don't know the pros\/cons, start with using the simpler options. <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":1590723421112,
        "Solution_link_count":0.0,
        "Solution_readability":10.5,
        "Solution_reading_time":13.09,
        "Solution_score_count":7.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":196.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":1499498135632,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":675.0,
        "Answerer_view_count":26.0,
        "Challenge_adjusted_solved_time":114.3863163889,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are trying to execute and check what kind of output is provided by Predictive Maintenance Using Machine Learning on AWS sample data. We are referring <a href=\"https:\/\/aws.amazon.com\/solutions\/predictive-maintenance-using-machine-learning\/\" rel=\"nofollow noreferrer\">Predictive Maintenance Using Machine Learning<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/solutions\/latest\/predictive-maintenance-using-machine-learning\/welcome.html\" rel=\"nofollow noreferrer\">AWS Guide<\/a> to launch the sample template provided by the AWS. The template is executed properly and we can see the resources in account. Whenever we run the sagemaker notebook for the given example we are getting the error in CloudWatch logs as follows<\/p>\n\n<pre><code>ImportError: cannot import name 'replace_file' on line from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file.\n<\/code><\/pre>\n\n<p>This is the stage where the invoke the training job. We have tried following options to resolve the issue.<\/p>\n\n<ul>\n<li>Upgrading the mxnet module<\/li>\n<li>Upgrading the tensorflow module<\/li>\n<\/ul>\n\n<p>But no success.<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Error Traceback is as follows<\/p>\n\n<pre><code>  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/sagemaker_predictive_maintenance_entry_point.py\", line 10, in &lt;module&gt;\n    import gluonnlp\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/__init__.py\", line 25, in &lt;module&gt;\n    from . import data\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/__init__.py\", line 23, in &lt;module&gt;\n    from . import (batchify, candidate_sampler, conll, corpora, dataloader,\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/question_answering.py\", line 31, in &lt;module&gt;\n    from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file\n    ImportError: cannot import name 'replace_file'\n<\/code><\/pre>",
        "Challenge_closed_time":1588063849632,
        "Challenge_comment_count":0,
        "Challenge_created_time":1587652058893,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary_original":"The user is encountering an import error while executing AWS Predictive Maintenance Using Machine Learning sample. The error occurs while invoking the training job and the error traceback indicates that the 'replace_file' module cannot be imported. The user has tried upgrading the mxnet and tensorflow modules but without success.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/61389632",
        "Challenge_link_count":2,
        "Challenge_participation_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":27.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":2.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":114.3863163889,
        "Challenge_title":"Import error while Executing AWS Predictive Maintenance Using Machine Learning Sample",
        "Challenge_topic":"TensorFlow Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":398.0,
        "Challenge_word_count":213,
        "Platform":"Stack Overflow",
        "Poster_created_time":1473770138816,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":3.0,
        "Solution_body":"<p>A fix for this issue is being deployed to the official solution. In the meantime, you can make the changes described <a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/pull\/7\/files\" rel=\"nofollow noreferrer\">here<\/a> in your SageMaker environment by following the instructions below:<\/p>\n\n<p>1) In the notebook, please change the <code>framework_version<\/code> to <code>1.6.0<\/code>.<\/p>\n\n<pre><code>MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',\n          source_dir='sagemaker_predictive_maintenance_entry_point',\n          py_version='py3',\n          role=role, \n          train_instance_count=1, \n          train_instance_type=train_instance_type,\n          output_path=output_location,\n          hyperparameters={'num-datasets' : len(train_df),\n                           'num-gpus': 1,\n                           'epochs': 500,\n                           'optimizer': 'adam',\n                           'batch-size':1,\n                           'log-interval': 100},\n         input_mode='File',\n         train_max_run=7200,\n         framework_version='1.6.0')  &lt;- Change this to 1.6.0.\n<\/code><\/pre>\n\n<p>2) This will likely fix things, but just to be sure you don't have any stale packages, change the <code>requirements.txt<\/code> file as well.<\/p>\n\n<p>You'll need to open up a terminal in SageMaker.\n<a href=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" alt=\"enter image description here\"><\/a>\nimage taken from <a href=\"https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439<\/a><\/p>\n\n<p>and run<\/p>\n\n<pre><code>cd SageMaker\/sagemaker_predictive_maintenance_entry_point\/\nsudo vim requirements.txt  # (or sudo nano requirements.txt)\n<\/code><\/pre>\n\n<p>Change the contents to:<\/p>\n\n<pre><code>gluonnlp==0.9.1\npandas==0.22\n<\/code><\/pre>\n\n<p>Save it, and then run the example again.<\/p>\n\n<p>Feel free to comment on the issue as well:\n<a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6<\/a> <\/p>",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":7.0,
        "Solution_readability":16.4,
        "Solution_reading_time":28.12,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":160.0,
        "Tool":"Amazon SageMaker"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":214.3601405556,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hello,    <\/p>\n<p>I am running an AutoML experiment for a regression task, and looking at the YAML file which is generated it seems that TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos'.    <\/p>\n<p>I tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it doesn't change anything.    <\/p>\n<pre><code>automl_settings = {  \n    &quot;primary_metric&quot;: 'normalized_mean_absolute_error',  \n    &quot;featurization&quot;: 'auto',  \n    &quot;verbosity&quot;: logging.INFO,  \n    &quot;n_cross_validations&quot;: 5,  \n    &quot;auto_blacklist&quot;: False,  \n    &quot;blacklist_models&quot;: None,  \n    &quot;blacklist_algos&quot;: None  \n}  \nrun = experiment.submit(automl_config, show_output=True)  \n<\/code><\/pre>\n<p>The generated YAML file (excerpt):    <\/p>\n<pre><code>&quot;whitelist_models&quot;:null,  \n&quot;blacklist_algos&quot;:[&quot;TensorFlowDNN&quot;,&quot;TensorFlowLinearRegressor&quot;],  \n&quot;supported_models&quot;:[&quot;ElasticNet&quot;,&quot;GradientBoosting&quot;,&quot;LightGBM&quot;,&quot;TensorFlowLinearRegressor&quot;,&quot;TensorFlowDNN&quot;,&quot;LassoLars&quot;,&quot;DecisionTree&quot;,&quot;RandomForest&quot;,&quot;FastLinearRegressor&quot;,&quot;OnlineGradientDescentRegressor&quot;,&quot;ExtremeRandomTrees&quot;,&quot;TabnetRegressor&quot;,&quot;XGBoostRegressor&quot;,&quot;KNN&quot;,&quot;SGD&quot;],  \n&quot;private_models&quot;:[],  \n&quot;auto_blacklist&quot;:false  \n<\/code><\/pre>\n<p>Maybe the problem comes from the fact that Deep learning is set to 'Disabled' in the configuration settings, as shown on the following picture:    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/205364-image.png?platform=QnA\" alt=\"205364-image.png\" \/>    <\/p>\n<p>Are deep learning models not supported anymore by AutoML?    <\/p>",
        "Challenge_closed_time":1654236226443,
        "Challenge_comment_count":3,
        "Challenge_created_time":1653464529937,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is running an AutoML experiment for a regression task and noticed that TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos' in the generated YAML file. The user tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it didn't work. The user wonders if deep learning models are not supported anymore by AutoML as Deep learning is set to 'Disabled' in the configuration settings.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/863297\/automl-tensorflowdnn-and-tensorflowlinearregressor",
        "Challenge_link_count":1,
        "Challenge_participation_count":4,
        "Challenge_readability":26.5,
        "Challenge_reading_time":26.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":214.3601405556,
        "Challenge_title":"AutoML : TensorFlowDNN and TensorFlowLinearRegressor are blacklisted by default",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":142,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"<p>@ThierryL-3166  Thanks for the question.     <\/p>\n<p>As mentioned in the below document The following support models in AutoML TensorFlowDNN, TensorFlowLinearRegressor are deprecated.     <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-automl-core\/azureml.automl.core.shared.constants.supportedmodels.regression?view=azure-ml-py\">https:\/\/learn.microsoft.com\/en-us\/python\/api\/azureml-automl-core\/azureml.automl.core.shared.constants.supportedmodels.regression?view=azure-ml-py<\/a><\/p>\n",
        "Solution_comment_count":1.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":1.0,
        "Solution_readability":32.5,
        "Solution_reading_time":6.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning"
    },
    {
        "Answerer_created_time":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isCse":null,
        "Answerer_isExpert":null,
        "Answerer_isModerator":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Challenge_adjusted_solved_time":0.0377777778,
        "Challenge_answer_count":0,
        "Challenge_body":"From slack\n\nI\u2019m new to Polyaxon and still learning how to use the UI. I would like to delete all archived jobs from All Runs but I can\u2019t see the option to filter them. We are running v1.17.0. Can you advise how to do it please? Thanks!",
        "Challenge_closed_time":1649328108000,
        "Challenge_comment_count":0,
        "Challenge_created_time":1649327972000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary_original":"The user is having difficulty deleting all archived jobs from All Runs in Polyaxon as they cannot find the option to filter them. They are seeking advice on how to do this.",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1471",
        "Challenge_link_count":0,
        "Challenge_participation_count":0,
        "Challenge_readability":1.4,
        "Challenge_reading_time":3.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score_count":1.0,
        "Challenge_self_closed":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.0377777778,
        "Challenge_title":"Batch deletion of archived runs",
        "Challenge_topic":"Checkpoint Management",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":null,
        "Challenge_word_count":51,
        "Platform":"Tool-specific",
        "Poster_created_time":null,
        "Poster_isAwsEmployee":null,
        "Poster_isCse":null,
        "Poster_isExpert":null,
        "Poster_isModerator":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Solution_body":"You should first filter for archived runs, and then select all and use the actions above to delete the selection, you can eventually select a large table size, for example 50.\n\n  \n\nif you are an org admin and you need to delete archived runs cross-projects, you can use the button All Runs under the organization and follow the steps above:",
        "Solution_comment_count":0.0,
        "Solution_last_edit_time":null,
        "Solution_link_count":0.0,
        "Solution_readability":12.6,
        "Solution_reading_time":4.05,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":60.0,
        "Tool":"Polyaxon"
    }
]